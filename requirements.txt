-e .
-e ../../../mitibm2023
-e ../alpaca_eval # some bug fixes on repetitive generation yielding openai errors
-e ../FastChat[model_worker,llm_judge]
-e ../rosemary
-e ../DPPy
--extra-index-url https://download.pytorch.org/whl/cu118
torch==2.1.2+cu118
scipy
packaging
sentencepiece
datasets
deepspeed>=0.10.0
accelerate==0.22.0 # >=0.21.0,<0.23.0  # 0.23.0 will cause an incorrect learning rate schedule when using deepspeed, which is likely caused by https://github.com/huggingface/accelerate/commit/727d624322c67db66a43c559d8c86414d5ffb537
peft>=0.4.0
# bitsandbytes>=0.41.1 # remove since not using it.
evaluate>=0.4.0
tokenizers>=0.13.3
protobuf
# transformers==4.35.2 ## issues with using flash attention 2 with mixed-precision.
git+https://github.com/huggingface/transformers.git@3f69f415adcbdaedec154ba8eac220ef3276975d
huggingface_hub
openai<=0.28.1
tiktoken
rouge_score
wandb
termcolor
jsonlines
einops
flash-attn
scipy
auto-gptq
fire
flask
vllm 
openpyxl
tensorboard
omegaconf
pynvml
notebook
pyarrow
matplotlib
tensorboard
faiss-gpu
hdbscan
openpyxl