{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da1794b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/data-pruning/wpq/github/mitibm2023/external/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'x86_64', 'cluster': 'ccc', 'queue': 'alt_7d'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info, move_lsf_job_summary_to_save_dir\n",
    "from note_pruning_analysis import open_instruct_dir\n",
    "import getpass\n",
    "\n",
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "info = get_host_info()\n",
    "info.update({'queue': queue})\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "os.environ['TORCHELASTIC_ERROR_FILE'] = os.path.join(os.getcwd(), 'torchelastic_error_file') \n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = os.path.normpath(os.path.join(os.getcwd(), '../../../../mitibm2023/cache')) \\\n",
    "    if arch == 'ppc64le' else '/dccstor/data-pruning/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True, mode=0o777)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##\n",
    "##\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c421d1",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c09b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/oi2/llama-7b_sharegptv2_ep=2 using 6 GPUs, 1 batch size per GPU, 1 gradient accumulation steps, 30 effective batch size.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp7pssto1b', 'job_id': 1354503}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2p8zx1bt', 'job_id': 1354504}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2_mpillk', 'job_id': 1354505}]\n"
     ]
    }
   ],
   "source": [
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm\n",
    "debug = False\n",
    "if debug:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'INFO'\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "else:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'WARNING'\n",
    "    os.environ['NCCL_DEBUG'] = ''\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  650 if arch == 'ppc64le' else 64\n",
    "\n",
    "preprocessing_num_workers = 32\n",
    "report_to = 'wandb'\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16'\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float32'\n",
    "gradient_checkpointing = True\n",
    "use_fast_tokenizer = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "resume_from_checkpoint = True # resume from latest checkpoint if exists, otherwise train from scratch\n",
    "num_train_epochs = 2\n",
    "checkpointing_steps = 300 # (50_000 / 32) * 2 / 6 ~= 500 (data size of 50k, bsz=32, ep=2, total save 6 times at most)\n",
    "max_train_steps = None\n",
    "subsample_inds_file_list = [None]\n",
    "dataloader_sampler = 'RandomSampler'\n",
    "overwrite_cache = True\n",
    "\n",
    "\n",
    "# #####\n",
    "# job_name = 'dpo1'\n",
    "\n",
    "# # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-410m-deduped'; max_seq_length = 2048; abbr_model_name = 'pythia-410m'\n",
    "# # model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "\n",
    "# train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "\n",
    "\n",
    "# M = 60_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 50_000; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "    (10_000, 10),\n",
    "#     (30_000, 3),\n",
    "]]\n",
    "\n",
    "gen_output_md = 'llama7br512p4096'\n",
    "# gen_output_md = 'llama7b+sharegptv2ep2+r512p4096'\n",
    "# gen_output_model_name = 'all-mpnet-base-v2'\n",
    "\n",
    "scoring_fn_list = []\n",
    "scoring_fn_list += ['random_s=0']\n",
    "# scoring_fn_list += ['random_s=1']\n",
    "scoring_fn_list += [ \n",
    "    f'dppmap_k=vmf_gamma=1_kmd={gen_output_md}_kemb=grad+rp+loraB',\n",
    "    f'dppmap_k=rbf_gamma=1e-3_kmd={gen_output_md}_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1_kmd=mpnet_kemb=text+embedding',\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'dpo2_{dataset}:{abbr_model_name}'\n",
    "    \n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12\n",
    "# nodes = 2; num_gpus = 1; gpu_type = 'v100'; job_duration = 6; cpu_mem = 100; num_cpus = 32; max_train_steps = 5; checkpointing_steps = 2; report_to = 'tensorboard'\n",
    "    \n",
    "#####\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs = 1 # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "use_deepspeed = True\n",
    "deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate.conf'\n",
    "\n",
    "per_device_train_batch_size = 1; total_batch_size = 32\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"{effective_batch_size} effective batch size.\")\n",
    "\n",
    "# reference: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n",
    "launcher = f\"\"\"accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines {nodes} \\\n",
    "    --num_processes {num_gpus*nodes} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''} \\\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''} \\\n",
    "    {'--main_process_ip $master_addr' if use_deepspeed else ''} \\\n",
    "    {'--main_process_port $master_port' if use_deepspeed else ''} \\\n",
    "    {'--machine_rank $SLURM_PROCID' if use_deepspeed else ''} \\\n",
    "    {'--rdzv_backend c10d' if use_deepspeed and nodes>1 else ''} \\\n",
    "    {'--deepspeed_multinode_launcher standard' if use_deepspeed and nodes>1 else ''} \\\n",
    "\"\"\"\n",
    "\n",
    "cmds = []\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_inds_file_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (subsample_inds_file,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{dataset}\"\n",
    "    if any(job_name == y for y in ['dpo1']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "\n",
    "    if subsample_inds_file:\n",
    "        assert(num_train_epochs==1)\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                s = f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "    if subsample_inds_file is not None:\n",
    "        assert(dataloader_sampler=='SequentialSampler')\n",
    "        assert(num_train_epochs==1)\n",
    "    else:\n",
    "        assert(dataloader_sampler=='RandomSampler')\n",
    "\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    wandb_run_name = output_dir.replace('results/', '')\n",
    "\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {f'cd .. && CUDA_VISIBLE_DEVICES={os.environ[\"CUDA_VISIBLE_DEVICES\"]} ' if test_run else ''}{launcher}\n",
    "        open_instruct/dpo_tune.py \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --tokenizer_name {model_name_or_path} \\\n",
    "        {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        --train_file {train_file} \\\n",
    "        --max_seq_length {max_seq_length} \\\n",
    "        {'--subsample_inds_file '+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        --dataloader_sampler {dataloader_sampler} \\\n",
    "        --preprocessing_num_workers {preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "        --learning_rate 5e-7 \\\n",
    "        --lr_scheduler_type linear \\\n",
    "        --warmup_ratio 0.1 \\\n",
    "        --weight_decay 0. \\\n",
    "        --num_train_epochs {num_train_epochs} \\\n",
    "        --with_tracking \\\n",
    "        {'--report_to \"'+str(report_to)+'\"' if report_to else ''} \\\n",
    "        --checkpointing_steps {checkpointing_steps} \\\n",
    "        {'--max_train_steps '+str(max_train_steps) if max_train_steps else ''} \\\n",
    "        {'--resume_from_checkpoint' if resume_from_checkpoint else ''} \\\n",
    "        {'--low_cpu_mem_usage' if not use_deepspeed else ''} \\\n",
    "        {'--overwrite_cache' if overwrite_cache else ''} \\\n",
    "        --logging_steps 1 \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    # if test_run:\n",
    "    #     print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    if test_run:\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prune: {1k@10, 10k@3}, datasets={dolly, stanford_alpaca}, scoring={random, dppmapx2}\n",
    "# need to gen curriculum for 50k sft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b79b9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_dpo.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce604bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=5\n",
      "+ cd ..\n",
      "+ CUDA_VISIBLE_DEVICES=2,5\n",
      "+ accelerate launch --mixed_precision fp16 --num_machines 1 --num_processes 2 --use_deepspeed --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py --model_name_or_path results/baselines/huggyllama/llama-7b --tokenizer_name results/baselines/huggyllama/llama-7b --gradient_checkpointing --train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl --max_seq_length 2048 --preprocessing_num_workers 32 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --learning_rate 5e-7 --lr_scheduler_type linear --warmup_ratio 0.1 --weight_decay 0. --num_train_epochs 2 --with_tracking --report_to tensorboard --checkpointing_steps 500 --logging_steps 1 --output_dir results/dpo1/jpt_llama-7b_ultrafeedback\n",
      "[2024-01-08 20:41:08,547] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-01-08 20:41:17,396] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-08 20:41:17,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:662:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,611] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 251.17it/s]\n",
      "01/08/2024 20:41:25 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-201ebfceee303348/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 252.78it/s]\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:26,493] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.11s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:43,556] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.48B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "01/08/2024 20:42:20 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.\n",
      "[2024-01-08 20:42:20,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.1+23a11a39, git-hash=23a11a39, git-branch=master\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "[2024-01-08 20:42:20,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-01-08 20:42:20,779] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-01-08 20:42:20,891] [INFO] [utils.py:786:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 14.16 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.9 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:118:__init__] Reduce bucket size 16777216\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:119:__init__] Prefetch bucket size 15099494\n",
      "[2024-01-08 20:42:20,995] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 13.64 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2024-01-08 20:42:21,139] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.76 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:21,240] [INFO] [utils.py:786:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.4 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:23,090] [INFO] [utils.py:786:see_memory_usage] After creating fp16 partitions: 4\n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.4 GB         CA 14.6 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.99 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,190] [INFO] [utils.py:786:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.39 GB         CA 14.6 GB         Max_CA 15 GB \n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.81 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,742] [INFO] [utils.py:786:see_memory_usage] After creating fp32 partitions\n",
      "[2024-01-08 20:42:23,743] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 26.61 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,744] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 116.15 GB, percent = 16.7%\n",
      "[2024-01-08 20:42:23,836] [INFO] [utils.py:786:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 25.94 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 117.28 GB, percent = 16.8%\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 1; 31.75 GiB total capacity; 25.93 GiB already allocated; 3.34 GiB free; 27.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 0; 31.75 GiB total capacity; 25.94 GiB already allocated; 3.21 GiB free; 27.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 409110) of binary: /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/python3.10\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/accelerate\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 964, in launch_command\r\n",
      "    deepspeed_launcher(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 687, in deepspeed_launcher\r\n",
      "    distrib_run.run(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "open_instruct/dpo_tune.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "[1]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 1 (local_rank: 1)\r\n",
      "  exitcode  : 1 (pid: 409111)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 409110)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_dpo.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "19aebd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/baselines/huggyllama/llama-7b using 8 GPUs, 1 batch size per GPU, 16 gradient accumulation steps, Effective batch size 128\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_sharegpt50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi5_sharegpt50k:llama-7b -mem 768g -cores 1x128+8 -require a100_80gb -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --master_port=0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/sharegpt/sharegpt50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/sharegpt50k/random_s=0/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 1388918}]\n"
     ]
    }
   ],
   "source": [
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "add_hardwarespec_to_dirname = False\n",
    "save_strategy = 'steps'\n",
    "save_steps = 200 if getpass.getuser() in ('PTFMqngp', 'wpq') else 1_000_000\n",
    "save_total_limit = 1\n",
    "preprocessing_num_workers = 32\n",
    "evaluation_strategy = 'no' # set do_eval=False\n",
    "eval_steps = save_steps\n",
    "report_to = 'tensorboard wandb'\n",
    "suffix = None\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.03\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "subsample_inds_file_list = [None]\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "scoring_fn_and_pacing_fn = None\n",
    "\n",
    "\n",
    "# ########### sft baselines\n",
    "\n",
    "\n",
    "# job_name = 'oi2'; num_train_epochs_list = [2] \n",
    "# # model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "\n",
    "# # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; \n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# # train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'\n",
    "# # train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2'; max_train_samples_list=[100_000]\n",
    "\n",
    "# ## 50k sft datasets\n",
    "# # train_file = 'data/processed/flan_v2/flan_v250k_data.jsonl'; abbr_train_file = 'flan_v250k';\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca50k_data.jsonl'; abbr_train_file = 'stanford_alpaca50k'; \n",
    "# # train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# # train_file = 'data/processed/wizardlm/wizardlm50k_data.jsonl'; abbr_train_file = 'wizardlm50k'\n",
    "# # train_file = 'data/processed/sharegpt/sharegpt50k_data.jsonl'; abbr_train_file = 'sharegpt50k'\n",
    "# # train_file = 'data/processed/ultrachat/ultrachat50k_train_data.jsonl'; abbr_train_file = 'ultrachat50k'\n",
    "\n",
    "\n",
    "# # train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2'; max_train_samples_list=[100_000]\n",
    "# ###########\n",
    "\n",
    "\n",
    "############ pruning runs\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048; gen_output_md = 'mistral7br512p4096'\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048; gen_output_md = 'llama7br512p4096'\n",
    "\n",
    "\n",
    "# # 50k sft datasets\n",
    "# dataset = 'flan_v250k'; train_file = 'data/processed/flan_v2/flan_v250k_data.jsonl'; abbr_train_file = 'flan_v250k';\n",
    "# dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# dataset = 'stanford_alpaca50k'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca50k_data.jsonl'; abbr_train_file = 'stanford_alpaca50k'; \n",
    "# dataset = 'oasst2'; train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# dataset = 'wizardlm50k'; train_file = 'data/processed/wizardlm/wizardlm50k_data.jsonl'; abbr_train_file = 'wizardlm50k'\n",
    "dataset = 'sharegpt50k'; train_file = 'data/processed/sharegpt/sharegpt50k_data.jsonl'; abbr_train_file = 'sharegpt50k'\n",
    "# dataset = 'ultrachat50k'; train_file = 'data/processed/ultrachat/ultrachat50k_train_data.jsonl'; abbr_train_file = 'ultrachat50k'\n",
    "\n",
    "\n",
    "# dataset = 'flan_v2'; train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# dataset = 'stanford_alpaca'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca';\n",
    "# dataset = 'oasst2'; train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# dataset = 'wizardlmv2'; train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2';\n",
    "# dataset = 'sharegptv2'; train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2';\n",
    "# dataset = 'ultrachat200kv2'; train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2';\n",
    "\n",
    "\n",
    "# dataset = 'oasst1'; train_file = 'data/processed/oasst/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# dataset = 'open_orca_slim'; train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; \n",
    "# dataset = 'tulu_v2'; train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2';\n",
    "        \n",
    "# M = 80_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 40_000\n",
    "# M = 40_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 20_000\n",
    "# M = 30_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "#         (10_000, 10),\n",
    "        (30_000, 3),\n",
    "#         (40_000, 2),\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "scoring_fn_list = [\n",
    "    'random_s=0',\n",
    "#     'random_s=1',\n",
    "#     'log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n',\n",
    "#     'ifd_neg', 'log_pmi_neg',\n",
    "#     'numtoks_input_neg', 'numtoks_output_neg', 'numtoks_total_neg',\n",
    "#     f'dppmap_k=vmf_gamma=1_kmd=mpnet', #_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding',\n",
    "#     f'dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB',\n",
    "]\n",
    "\n",
    "scoring_fn_list += [ # vary kernel embedding model \n",
    "#     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd={kmd}_kemb=grad+rp+loraB'\n",
    "#     for kmd in ['llama7br256p4096', 'llama7br512p4096', 'pythia1br512p4096']\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "############ \n",
    "\n",
    "    \n",
    "# add_hardwarespec_to_dirname = True\n",
    "# job_name += '_debug' # wpq debug\n",
    "# max_train_samples_list=[128*2]\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "if arch == 'x86_64':\n",
    "    nodes = 1; num_gpus = 8; gpu_type = 'a100_80gb'; job_duration = 6\n",
    "    num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus); preprocessing_num_workers = 128 # tok takes quite a bit.\n",
    "    per_device_train_batch_size = 1\n",
    "    gradient_checkpointing = False\n",
    "    mixed_precision = 'bf16'; torch_dtype = 'float32'; use_flash_attn = False; use_tf32 = True \n",
    "    save_model_torch_dtype = 'bfloat16' # typically save fp32 weights, but for disk space sake, convert to bf16.\n",
    "else:\n",
    "    nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_checkpointing = True\n",
    "    mixed_precision = 'fp16'; torch_dtype = 'float32'; use_flash_attn = False; use_tf32 = False\n",
    "    save_model_torch_dtype = None\n",
    "\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "total_batch_size = 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "optimizer = 'adamw_hf'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\" \n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 \n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "load_in_8bit = False\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nnodes 1 --nproc_per_node={num_gpus} --rdzv_backend=c10d --master_port=0\" # assigns random port. https://github.com/pytorch/pytorch/issues/73320\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=${'SLURM_JOB_ID' if arch == 'ppcle64' else 'LSB_JOBID'} --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file = os.path.join(open_instruct_dir, 'scripts', 'error_file')\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "            \n",
    "    if any(job_name == y for y in ['oi2']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "        \n",
    "    if subsample_inds_file:\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            else:\n",
    "                return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            ('_dtype='+torch_dtype if torch_dtype is not None else '')+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)+\\\n",
    "            '_ngpus='+str(num_gpus)+\\\n",
    "            ('_fa2' if use_flash_attn else '')\n",
    "    if suffix:\n",
    "        output_dirname += suffix\n",
    "    output_dir = os.path.join(open_instruct_dir, 'results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join(open_instruct_dir, 'results', job_name), exist_ok=True)\n",
    "    if arch == 'x86_64':\n",
    "        wandb_run_name = 'ccc'+output_dir[output_dir.find('results'):][7:] # e.g., ccc/oi2/run_name\n",
    "    else:\n",
    "        wandb_run_name = output_dir.replace('results/', '') # e.g., oi2/run_name\n",
    "    \n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {'cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''} \\\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers={preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type={lr_scheduler_type} \\\n",
    "        --warmup_ratio={warmup_ratio} \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy={evaluation_strategy} \\\n",
    "        {'--eval_steps='+str(eval_steps) if eval_steps else ''} \\\n",
    "        {'--report_to '+str(report_to) if report_to else ''} \\\n",
    "        --run_name {wandb_run_name} \\\n",
    "        --logging_strategy=steps \\\n",
    "        --logging_first_step \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        --ddp_timeout=7200 \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''} \\\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        {'--torch_dtype='+str(torch_dtype) if torch_dtype else ''} \\\n",
    "        {'--save_model_torch_dtype='+str(save_model_torch_dtype) if save_model_torch_dtype else ''} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {f'--tf32=True' if use_tf32 else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --use_flash_attn {'True' if use_flash_attn else 'False'} \\\n",
    "        --low_cpu_mem_usage \\\n",
    "        --overwrite_cache \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #  --overwrite_cache   # if delete a dataset and need to refresh cache\n",
    "\n",
    "    if test_run:\n",
    "        print()\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64':\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        queue=queue,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12fc2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_sft.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed0c2894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ cd ..\n",
      "+ TORCHELASTIC_ERROR_FILE=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/error_file\n",
      "+ TORCH_CPP_LOG_LEVEL=INFO\n",
      "+ NCCL_DEBUG=INFO\n",
      "+ LOGLEVEL=INFO\n",
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/oasst1/oasst1_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=32 --per_device_train_batch_size=1 --gradient_accumulation_steps=128 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=200 --report_to tensorboard wandb --run_name /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=200 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --overwrite_output_dir --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/oasst1/random_s=0/inds_prune_size=10000_ep=10.pkl --dataloader_sampler SequentialSampler --use_flash_attn True --low_cpu_mem_usage --overwrite_cache --output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10\n",
      "[2024-01-19 02:04:37,834] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Saving args dict to /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10.args.json\n",
      "01/19/2024 02:04:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "01/19/2024 02:04:39 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_sampler=SequentialSampler,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=7200,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=200.0,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10/runs/Jan19_02-04-39_cccxc552,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=200,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Using custom data configuration default-b03eccd42e843020\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Using custom data configuration default-b03eccd42e843020\n",
      "Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset info from data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "[INFO|configuration_utils.py:715] 2024-01-19 02:04:39,135 >> loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-01-19 02:04:39,136 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3118] 2024-01-19 02:04:39,229 >> loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1222] 2024-01-19 02:04:39,229 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|modeling_utils.py:1304] 2024-01-19 02:04:39,230 >> You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:791] 2024-01-19 02:04:39,230 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "[INFO|modeling_utils.py:3950] 2024-01-19 02:04:41,778 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3958] 2024-01-19 02:04:41,778 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-01-19 02:04:41,781 >> loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-01-19 02:04:41,781 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "01/19/2024 02:04:41 - INFO - __main__ - [wpq] model.dtype=torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1648] 2024-01-19 02:04:41,845 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "Process #16 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #16 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "Process #17 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #17 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "Process #18 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #18 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "Process #19 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #19 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "Process #20 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #20 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "Process #21 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #21 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "Process #22 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #22 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "Process #23 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #23 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "Process #24 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #24 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "Process #25 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #25 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "Process #26 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #26 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "Process #27 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #27 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "Process #28 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #28 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "Process #29 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #29 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "Process #30 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #30 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "Process #31 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #31 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spawning 32 processes\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Spawning 32 processes\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   0%| | 0/33717 [00:Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   0%| | 22/33717 [00Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   1%| | 361/33717 [0Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=32):   3%| | 1001/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   5%| | 1719/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   7%| | 2251/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=32):  12%| | 4021/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):  14%|▏| 4841/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):  18%|▏| 5957/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "01/19/2024 02:04:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32): 100%|█| 33717/33717 \n",
      "Concatenating 32 shards\n",
      "01/19/2024 02:04:55 - INFO - datasets.arrow_dataset - Concatenating 32 shards\n",
      "Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00000_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00000_of_00016.arrow\n",
      "Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00001_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00001_of_00016.arrow\n",
      "Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00002_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00002_of_00016.arrow\n",
      "Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00003_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00003_of_00016.arrow\n",
      "Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00004_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00004_of_00016.arrow\n",
      "Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00005_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00005_of_00016.arrow\n",
      "Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00006_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00006_of_00016.arrow\n",
      "Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00007_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00007_of_00016.arrow\n",
      "Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00008_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00008_of_00016.arrow\n",
      "Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00009_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00009_of_00016.arrow\n",
      "Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00010_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00010_of_00016.arrow\n",
      "Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00011_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00011_of_00016.arrow\n",
      "Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00012_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00012_of_00016.arrow\n",
      "Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00013_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00013_of_00016.arrow\n",
      "Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00014_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00014_of_00016.arrow\n",
      "Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00015_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_*_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "01/19/2024 02:04:56 - INFO - __main__ - Subsample dataset according to indices: /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/oasst1/random_s=0/inds_prune_size=10000_ep=10.pkl\n",
      "01/19/2024 02:04:56 - INFO - __main__ - subsample_inds_file has 10000 indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wpq] Example 0 of train_dataset: \r\n",
      "{'dataset': 'oasst1', 'id': 'oasst1_20480', 'messages': [{'role': 'user', 'content': 'Cómo manejar un carro manual'}, {'role': 'assistant', 'content': 'Lo primero que tienes que hacer, si nunca has conducido un coche manual, es familiarizarte con el embrague y palanca de cambios. Si conduces habitualmente un coche automático, estarás acostumbrado a no utilizar para nada el pie izquierdo ni la palanca del cambio. Encontrarás tres pedales, siendo el embrague el que está situado a la izquierda y el que tendrás que pisar cada vez que cambies de marcha. Por otro lado, la palanca del cambio se ubica siempre en la consola central.\\n\\nPara arrancar un coche manual, es necesario seguir una serie de pasos que, al principio, pueden parecer muchos, pero que, con el tiempo, acabarás haciéndolos sin darte cuenta:\\n\\n1) Comprueba que la palanca del cambio está en punto muerto\\n2) Coloca el pie derecho en el pedal del freno\\n3) Arranca el motor\\n4) Pisa el embrague con el pie izquierdo\\n5) Coloca la palanca del cambio en la primera marcha, sin levantar el pedal del freno\\n6) Suelta el freno de mano\\n7) Suelta el pedal del freno\\nYa estás listo para iniciar la marcha, soltando suavemente el embrague, a medida que aceleras.\\n\\nUna vez que ya estás en marcha, debes hacer un uso correcto del cambio manual para cambiar las marchas de forma correcta. Un uso incorrecto de la caja de cambios manual puede repercutir negativamente en tu seguridad y también afectar gravemente al embrague y a la transmisión, lo que se traduce en serias averías de coste muy elevado. Para evitarlo, te explicamos cómo debes proceder:\\n\\nUna vez que hayas arrancado, pisa el acelerador muy lentamente. Notarás que el régimen del motor aumenta. En ese momento, comienza a soltar suavemente el pedal del embrague. Verás que el motor vuelve a bajar de vueltas. En ese momento, puedes presionar un poco más el acelerador y el coche comenzará a avanzar.\\n\\nAhora llega el momento de meter la segunda marcha. Dependiendo del tipo de coche y combustible, podrás circular a un régimen de giro más bajo o alto. El régimen de giro en coche de gasolina, por lo general, oscila entre loas 2.500 y 3.000 vueltas. Si el motor está sobrealimentado por turbo, te permitirá circular por debajo de ese rango, ya que algunos coches turbos modernos entregan la totalidad de su par motor, incluso por debajo de las 2.000 vueltas.\\n\\nUn coche con motor turbodiésel te permite circular a un régimen muy bajo, por debajo de las 2.000 vueltas, ya que la entrega de par se produce antes que en un motor de gasolina.\\n\\nCuando el coche alcance un régimen de vueltas apropiado, suelta el pedal del acelerador y vuelve a pisar el embrague. Coge la palanca del cambio y baja para meter segunda. Suelta el embrague y presiona nuevamente el acelerador. A partir de aquí, cada vez que quieras cambiar de marcha, deberás repetir el mismo proceso: soltar el acelerador, pisar embrague, meter la marcha, soltar embrague y volver a acelerar.\\n\\n¡Buen viaje!'}], 'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29907, 29980,  4346,\r\n",
      "          767, 29872,  4758,   443,  1559,   307, 12219,    13, 29966, 29989,\r\n",
      "          465, 22137, 29989, 29958,    13,  3410,  1903,  1489,   712,   260,\r\n",
      "          819,   267,   712, 14557, 29892,  1354, 28456,   756, 13417, 13321,\r\n",
      "          443,  1302,  1173, 12219, 29892,   831,  9985,   466, 11908,   378,\r\n",
      "          560,  7232,  1431,   434,   343,  5112,   273,  1113,   316, 10625,\r\n",
      "         2363, 29889,  6101, 13417,   778,  4760, 14162,   443,  1302,  1173,\r\n",
      "         3345, 22054, 29892, 23673,  1569,  1274,   520,   398,  1182,   912,\r\n",
      "          263,   694, 11824,   279,  1702, 25801,   560,  5036,  5951, 16026,\r\n",
      "         1867,  6836,   425,  5112,   273,  1113,   628, 26007, 29889,  1174,\r\n",
      "         9996,   279,  1569,  9941,  8939,  2122, 29892, 18200,   560,  7232,\r\n",
      "         1431,   434,   560,   712,  7919,  2990,   912,   263,   425,  5951,\r\n",
      "        16026,  1388,   343,   560,   712, 10331, 11964,   712, 20066,   279,\r\n",
      "         9747,  7763,   712, 10625,   583,   316,  8575, 29874, 29889,  7102,\r\n",
      "        16994, 19931, 29892,   425,  5112,   273,  1113,   628, 26007,   409,\r\n",
      "        13069,   983, 26692,   427,   425,  1136,  2963,  6555, 29889,    13,\r\n",
      "           13,  2177, 29874,   564,   661,  4287,   443,  1302,  1173, 12219,\r\n",
      "        29892,   831, 16632,  2628,  7025,   381,  1185,  7080,   316,  2331,\r\n",
      "          359,   712, 29892,   394,  3420,   601, 29892, 19796,  9541,  2265,\r\n",
      "        24312, 29892,  7046,   712, 29892,   378,   560, 13924, 29892, 22998,\r\n",
      "          279,  1569,   447,   455, 21183,   324,   359,  4457,  5424,   371,\r\n",
      "        21052, 29901,    13,    13, 29896, 29897,   422,   558,   434,  2291,\r\n",
      "          712,   425,  5112,   273,  1113,   628, 26007,  7919,   427, 15978,\r\n",
      "          286, 15009,    13, 29906, 29897,  1530,  6400,   560,  5036, 14923,\r\n",
      "         1859,   427,   560,  8939,   284,   628,   285, 26155,    13, 29941,\r\n",
      "        29897,   826,   661,  1113,   560, 10992,    13, 29946, 29897,   349,\r\n",
      "         8069,   560,  7232,  1431,   434,   378,   560,  5036,  5951, 16026,\r\n",
      "         1867,    13, 29945, 29897,  1530,  6400,   425,  5112,   273,  1113,\r\n",
      "          628, 26007,   427,   425,  8633,  8575, 29874, 29892,  4457, 14453,\r\n",
      "          424,   279,   560,  8939,   284,   628,   285, 26155,    13, 29953,\r\n",
      "        29897,  2166,  2554,   560,   285, 26155,   316, 24318,    13, 29955,\r\n",
      "        29897,  2166,  2554,   560,  8939,   284,   628,   285, 26155,    13,\r\n",
      "        29979, 29874,   707,  1569,  1051, 29877,  1702, 21855,   279,   425,\r\n",
      "         8575, 29874, 29892,   899, 29873,  1743,   480,   485,  9936,   560,\r\n",
      "         7232,  1431,   434, 29892,   263,  1612,  1458,   712,  1274,  7367,\r\n",
      "          294, 29889,    13,    13, 29965,  1056,  7763,   712,  9343,   707,\r\n",
      "         1569,   427,  8575, 29874, 29892,  2553,   267, 14557,   443, 17448,\r\n",
      "         1959, 29877,   628, 26007, 12219,  1702, 10625,  4447,  1869,  8575,\r\n",
      "          294,   316,  5954,  1959, 29874, 29889,   853, 17448, 10240, 29877,\r\n",
      "          316,   425,   274,  9919,   316, 10625,  2363, 12219, 11493,   337,\r\n",
      "          546,  7582,   381,  3480,  1926,  2503,   427,  5291,  2377,   332,\r\n",
      "         2368,   343,  6196,  2511,   522,   279,  8310,  9936,   394,  7232,\r\n",
      "         1431,   434,   343,   263,   425, 18750, 11861, 29892,   658,   712,\r\n",
      "          409,  3534, 24551,   427,   724,  3173,  4759,  8577,   316,  3438,\r\n",
      "        29872, 12287, 11858,   912, 29889, 12994,  3415,  3673,   417, 29892,\r\n",
      "          734, 28117, 14054, 28810,  4346,  2553,   267,  6449,   261, 29901,\r\n",
      "           13,    13, 29965,  1056,  7763,   712, 14842,   294,   564,   661,\r\n",
      "        29883,   912, 29892,   282,  8069,   560,  1274,  7367,  3136, 12287,\r\n",
      "          301,   296,  2503, 29889,  2216,   279,  1569,   712,   560,  6367,\r\n",
      "        19933,   628, 10992, 19291, 29874, 29889,  1174, 15371, 14341, 29892,\r\n",
      "          419, 24880,   263,   899, 12637,   480,   485,  9936,   560,  8939,\r\n",
      "          284,   628,  7232,  1431,   434, 29889,  1798,  1569,   712,   560,\r\n",
      "        10992, 22126,   345,   263,   289,  1175,   279,   316, 18679,  2152,\r\n",
      "          294, 29889,  1174, 15371, 14341, 29892,  2653, 11696,  2225,   291,\r\n",
      "          279,   443, 14534,  3627,   560,  1274,  7367,  3136,   343,   560,\r\n",
      "         1302,  1173, 19487, 20484,   263,  1029,  4096,   279, 29889,    13,\r\n",
      "           13, 29909, 15255, 10953, 29874,   560, 14341,   316, 11134,   425,\r\n",
      "        17329,  8575, 29874, 29889, 10034,   355, 17008,   628, 13306,   316,\r\n",
      "         1302,  1173,   343,  4145,   504,  1821, 29892,  2532, 11964, 19308,\r\n",
      "          263,   443,  6367, 19933,   316,   330,  3350,  3627, 13085,   288,\r\n",
      "        20478, 29889,  1260,  6367, 19933,   316,   330,  3350,   427,  1302,\r\n",
      "         1173,   316, 10489,   324,  1099, 29892,  1277,   658,  2498, 29892,\r\n",
      "        15199,  4233,  2637,   658,   294, 29871, 29906, 29889, 29945, 29900,\r\n",
      "        29900,   343, 29871, 29941, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29889,  6101,   560, 10992,  7919,  4166,   284,  2073,   912,\r\n",
      "         1277,  7013,   833, 29892,   734, 14257,   381, 29976, 19308,  1277,\r\n",
      "         2553,  7069,   316, 15371,   364,  4524, 29892,  9343,   712, 20071,\r\n",
      "         1302,  6609,  7013, 27737,  5400,   359,   875,  1727,   273,   425,\r\n",
      "         3001,  2368,   316,   480,   610, 10992, 29892,  1343, 10648,  1277,\r\n",
      "         2553,  7069,   316,  1869, 29871, 29906, 29889, 29900, 29900, 29900,\r\n",
      "        18679,  2152,   294, 29889,    13,    13,  2525,  1302,  1173,   378,\r\n",
      "        10992,  7013, 29890, 12143,   743,   295,   734,  3635,   568, 19308,\r\n",
      "          263,   443,  6367, 19933, 12287, 13085, 29892,  1277,  2553,  7069,\r\n",
      "          316,  1869, 29871, 29906, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29892,  9343,   712,   425,   875,  1727, 29874,   316,   610,\r\n",
      "          409,  7738, 12971,   712,   427,   443, 10992,   316, 10489,   324,\r\n",
      "         1099, 29889,    13,    13, 29907, 29884,  1743,   560,  1302,  1173,\r\n",
      "        10747,   749,   443,  6367, 19933,   316, 18679,  2152,   294, 11712,\r\n",
      "         1631,   912, 29892,   480,  2554,   560,  8939,   284,   628,  1274,\r\n",
      "         7367,  3136,   343, 22126,   345,   263, 20066,   279,   560,  7232,\r\n",
      "         1431,   434, 29889,   315, 21317,   425,  5112,   273,  1113,   628,\r\n",
      "        26007,   343,   289,  9919,  1702, 11134, 17329, 29889,  2166,  2554,\r\n",
      "          560,  7232,  1431,   434,   343,  2225, 16017,  8005, 29894,  2503,\r\n",
      "          560,  1274,  7367,  3136, 29889,   319,  8019,   316, 10592, 29983,\r\n",
      "        29892,  9747,  7763,   712,   439,   631,   294, 10625,  4447,   316,\r\n",
      "         8575, 29874, 29892,   316,   495,  1569, 21159,   381,   560, 11329,\r\n",
      "        14177, 29877, 29901,   899, 12637,   560,  1274,  7367,  3136, 29892,\r\n",
      "        20066,   279,  7232,  1431,   434, 29892, 11134,   425,  8575, 29874,\r\n",
      "        29892,   899, 12637,  7232,  1431,   434,   343,  1700,   369,   263,\r\n",
      "         1274,  7367,   279, 29889,    13,    13, 30180,  3727,   264,  3025,\r\n",
      "         1324, 29991,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\r\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\r\n",
      "         -100,  -100,  -100,  -100,  -100,  3410,  1903,  1489,   712,   260,\r\n",
      "          819,   267,   712, 14557, 29892,  1354, 28456,   756, 13417, 13321,\r\n",
      "          443,  1302,  1173, 12219, 29892,   831,  9985,   466, 11908,   378,\r\n",
      "          560,  7232,  1431,   434,   343,  5112,   273,  1113,   316, 10625,\r\n",
      "         2363, 29889,  6101, 13417,   778,  4760, 14162,   443,  1302,  1173,\r\n",
      "         3345, 22054, 29892, 23673,  1569,  1274,   520,   398,  1182,   912,\r\n",
      "          263,   694, 11824,   279,  1702, 25801,   560,  5036,  5951, 16026,\r\n",
      "         1867,  6836,   425,  5112,   273,  1113,   628, 26007, 29889,  1174,\r\n",
      "         9996,   279,  1569,  9941,  8939,  2122, 29892, 18200,   560,  7232,\r\n",
      "         1431,   434,   560,   712,  7919,  2990,   912,   263,   425,  5951,\r\n",
      "        16026,  1388,   343,   560,   712, 10331, 11964,   712, 20066,   279,\r\n",
      "         9747,  7763,   712, 10625,   583,   316,  8575, 29874, 29889,  7102,\r\n",
      "        16994, 19931, 29892,   425,  5112,   273,  1113,   628, 26007,   409,\r\n",
      "        13069,   983, 26692,   427,   425,  1136,  2963,  6555, 29889,    13,\r\n",
      "           13,  2177, 29874,   564,   661,  4287,   443,  1302,  1173, 12219,\r\n",
      "        29892,   831, 16632,  2628,  7025,   381,  1185,  7080,   316,  2331,\r\n",
      "          359,   712, 29892,   394,  3420,   601, 29892, 19796,  9541,  2265,\r\n",
      "        24312, 29892,  7046,   712, 29892,   378,   560, 13924, 29892, 22998,\r\n",
      "          279,  1569,   447,   455, 21183,   324,   359,  4457,  5424,   371,\r\n",
      "        21052, 29901,    13,    13, 29896, 29897,   422,   558,   434,  2291,\r\n",
      "          712,   425,  5112,   273,  1113,   628, 26007,  7919,   427, 15978,\r\n",
      "          286, 15009,    13, 29906, 29897,  1530,  6400,   560,  5036, 14923,\r\n",
      "         1859,   427,   560,  8939,   284,   628,   285, 26155,    13, 29941,\r\n",
      "        29897,   826,   661,  1113,   560, 10992,    13, 29946, 29897,   349,\r\n",
      "         8069,   560,  7232,  1431,   434,   378,   560,  5036,  5951, 16026,\r\n",
      "         1867,    13, 29945, 29897,  1530,  6400,   425,  5112,   273,  1113,\r\n",
      "          628, 26007,   427,   425,  8633,  8575, 29874, 29892,  4457, 14453,\r\n",
      "          424,   279,   560,  8939,   284,   628,   285, 26155,    13, 29953,\r\n",
      "        29897,  2166,  2554,   560,   285, 26155,   316, 24318,    13, 29955,\r\n",
      "        29897,  2166,  2554,   560,  8939,   284,   628,   285, 26155,    13,\r\n",
      "        29979, 29874,   707,  1569,  1051, 29877,  1702, 21855,   279,   425,\r\n",
      "         8575, 29874, 29892,   899, 29873,  1743,   480,   485,  9936,   560,\r\n",
      "         7232,  1431,   434, 29892,   263,  1612,  1458,   712,  1274,  7367,\r\n",
      "          294, 29889,    13,    13, 29965,  1056,  7763,   712,  9343,   707,\r\n",
      "         1569,   427,  8575, 29874, 29892,  2553,   267, 14557,   443, 17448,\r\n",
      "         1959, 29877,   628, 26007, 12219,  1702, 10625,  4447,  1869,  8575,\r\n",
      "          294,   316,  5954,  1959, 29874, 29889,   853, 17448, 10240, 29877,\r\n",
      "          316,   425,   274,  9919,   316, 10625,  2363, 12219, 11493,   337,\r\n",
      "          546,  7582,   381,  3480,  1926,  2503,   427,  5291,  2377,   332,\r\n",
      "         2368,   343,  6196,  2511,   522,   279,  8310,  9936,   394,  7232,\r\n",
      "         1431,   434,   343,   263,   425, 18750, 11861, 29892,   658,   712,\r\n",
      "          409,  3534, 24551,   427,   724,  3173,  4759,  8577,   316,  3438,\r\n",
      "        29872, 12287, 11858,   912, 29889, 12994,  3415,  3673,   417, 29892,\r\n",
      "          734, 28117, 14054, 28810,  4346,  2553,   267,  6449,   261, 29901,\r\n",
      "           13,    13, 29965,  1056,  7763,   712, 14842,   294,   564,   661,\r\n",
      "        29883,   912, 29892,   282,  8069,   560,  1274,  7367,  3136, 12287,\r\n",
      "          301,   296,  2503, 29889,  2216,   279,  1569,   712,   560,  6367,\r\n",
      "        19933,   628, 10992, 19291, 29874, 29889,  1174, 15371, 14341, 29892,\r\n",
      "          419, 24880,   263,   899, 12637,   480,   485,  9936,   560,  8939,\r\n",
      "          284,   628,  7232,  1431,   434, 29889,  1798,  1569,   712,   560,\r\n",
      "        10992, 22126,   345,   263,   289,  1175,   279,   316, 18679,  2152,\r\n",
      "          294, 29889,  1174, 15371, 14341, 29892,  2653, 11696,  2225,   291,\r\n",
      "          279,   443, 14534,  3627,   560,  1274,  7367,  3136,   343,   560,\r\n",
      "         1302,  1173, 19487, 20484,   263,  1029,  4096,   279, 29889,    13,\r\n",
      "           13, 29909, 15255, 10953, 29874,   560, 14341,   316, 11134,   425,\r\n",
      "        17329,  8575, 29874, 29889, 10034,   355, 17008,   628, 13306,   316,\r\n",
      "         1302,  1173,   343,  4145,   504,  1821, 29892,  2532, 11964, 19308,\r\n",
      "          263,   443,  6367, 19933,   316,   330,  3350,  3627, 13085,   288,\r\n",
      "        20478, 29889,  1260,  6367, 19933,   316,   330,  3350,   427,  1302,\r\n",
      "         1173,   316, 10489,   324,  1099, 29892,  1277,   658,  2498, 29892,\r\n",
      "        15199,  4233,  2637,   658,   294, 29871, 29906, 29889, 29945, 29900,\r\n",
      "        29900,   343, 29871, 29941, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29889,  6101,   560, 10992,  7919,  4166,   284,  2073,   912,\r\n",
      "         1277,  7013,   833, 29892,   734, 14257,   381, 29976, 19308,  1277,\r\n",
      "         2553,  7069,   316, 15371,   364,  4524, 29892,  9343,   712, 20071,\r\n",
      "         1302,  6609,  7013, 27737,  5400,   359,   875,  1727,   273,   425,\r\n",
      "         3001,  2368,   316,   480,   610, 10992, 29892,  1343, 10648,  1277,\r\n",
      "         2553,  7069,   316,  1869, 29871, 29906, 29889, 29900, 29900, 29900,\r\n",
      "        18679,  2152,   294, 29889,    13,    13,  2525,  1302,  1173,   378,\r\n",
      "        10992,  7013, 29890, 12143,   743,   295,   734,  3635,   568, 19308,\r\n",
      "          263,   443,  6367, 19933, 12287, 13085, 29892,  1277,  2553,  7069,\r\n",
      "          316,  1869, 29871, 29906, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29892,  9343,   712,   425,   875,  1727, 29874,   316,   610,\r\n",
      "          409,  7738, 12971,   712,   427,   443, 10992,   316, 10489,   324,\r\n",
      "         1099, 29889,    13,    13, 29907, 29884,  1743,   560,  1302,  1173,\r\n",
      "        10747,   749,   443,  6367, 19933,   316, 18679,  2152,   294, 11712,\r\n",
      "         1631,   912, 29892,   480,  2554,   560,  8939,   284,   628,  1274,\r\n",
      "         7367,  3136,   343, 22126,   345,   263, 20066,   279,   560,  7232,\r\n",
      "         1431,   434, 29889,   315, 21317,   425,  5112,   273,  1113,   628,\r\n",
      "        26007,   343,   289,  9919,  1702, 11134, 17329, 29889,  2166,  2554,\r\n",
      "          560,  7232,  1431,   434,   343,  2225, 16017,  8005, 29894,  2503,\r\n",
      "          560,  1274,  7367,  3136, 29889,   319,  8019,   316, 10592, 29983,\r\n",
      "        29892,  9747,  7763,   712,   439,   631,   294, 10625,  4447,   316,\r\n",
      "         8575, 29874, 29892,   316,   495,  1569, 21159,   381,   560, 11329,\r\n",
      "        14177, 29877, 29901,   899, 12637,   560,  1274,  7367,  3136, 29892,\r\n",
      "        20066,   279,  7232,  1431,   434, 29892, 11134,   425,  8575, 29874,\r\n",
      "        29892,   899, 12637,  7232,  1431,   434,   343,  1700,   369,   263,\r\n",
      "         1274,  7367,   279, 29889,    13,    13, 30180,  3727,   264,  3025,\r\n",
      "         1324, 29991,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:593] 2024-01-19 02:04:57,333 >> Using auto half precision backend\n",
      "[INFO|trainer.py:738] 2024-01-19 02:04:57,494 >> The following columns in the training set don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: messages, id, dataset. If messages, id, dataset are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1723] 2024-01-19 02:04:57,514 >> ***** Running training *****\n",
      "[INFO|trainer.py:1724] 2024-01-19 02:04:57,514 >>   Num examples = 10,000\n",
      "[INFO|trainer.py:1725] 2024-01-19 02:04:57,514 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1726] 2024-01-19 02:04:57,514 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1729] 2024-01-19 02:04:57,514 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1730] 2024-01-19 02:04:57,514 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1731] 2024-01-19 02:04:57,514 >>   Total optimization steps = 78\n",
      "[INFO|trainer.py:1732] 2024-01-19 02:04:57,515 >>   Number of trainable parameters = 6,738,423,808\n",
      "[INFO|integration_utils.py:718] 2024-01-19 02:04:57,519 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
      "  0%|                                                    | 0/78 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-01-19 02:05:01,563 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,569 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,572 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,576 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,577 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.6425, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7168, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.03}        \n",
      "  3%|█▏                                          | 2/78 [00:42<26:54, 21.24s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/spawnbase.py:372\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash gen_cmds_sft.sh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:177\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Ensure the subprocess really is terminated\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m         \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# add isalive check, to ensure exitstatus is set:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m child\u001b[38;5;241m.\u001b[39misalive()\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:646\u001b[0m, in \u001b[0;36mspawn.terminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill(signal\u001b[38;5;241m.\u001b[39mSIGCONT)\n\u001b[0;32m--> 646\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayafterterminate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misalive():\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_sft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499d6f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mmlu_s=0', 'results/oi2/llama-7b_ultrachat50k_ep=2')\n",
      "('mmlu_s=5', 'results/oi2/llama-7b_ultrachat50k_ep=2')\n",
      "('mmlu_s=0_chatfmt', 'results/oi2/llama-7b_ultrachat50k_ep=2')\n",
      "('mmlu_s=5_chatfmt', 'results/oi2/llama-7b_ultrachat50k_ep=2')\n",
      "('gsm_s=8_chatfmt', 'results/oi2/llama-7b_ultrachat50k_ep=2')\n",
      "('mmlu_s=0', 'results/oi2/llama-7b_sharegpt50k_ep=2')\n",
      "('mmlu_s=5', 'results/oi2/llama-7b_sharegpt50k_ep=2')\n",
      "('mmlu_s=0_chatfmt', 'results/oi2/llama-7b_sharegpt50k_ep=2')\n",
      "('mmlu_s=5_chatfmt', 'results/oi2/llama-7b_sharegpt50k_ep=2')\n",
      "#cmds:  9 \n",
      "\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gen_cmds_utils import remove_all_symlinks, create_unique_symlinks, get_chat_formatting_function, get_resource_for_task, OPENAI_MODEL_LIST\n",
    "\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "use_slow_tokenizer = True\n",
    "definitely_run_mtbench_on_non_alt7b_queue = False\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    'mmlu_s=5', \n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "#     'alpacafarm_ann=gpt35:turbo:1106',\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "task_names_mtbench = ['mtbench_ann=gpt:4:1106:preview_chatfmt'] # ann=gpt:4, ann=gpt:3.5:turbo:1106, gpt:4:1106:preview (gpt4-turbo)\n",
    "# task_names_mtbench = ['mtbench_ann=gpt:3.5:turbo:1106_chatfmt'] # for debug sake, prefer gpt4\n",
    "# task_names_mtbench = ['mtbench_ann=gpt:4_chatfmt']\n",
    "task_names_alpacafarm = ['alpacafarm_ann=chatgpt_chatfmt']\n",
    "task_names_chateval = task_names_mtbench + task_names_alpacafarm\n",
    "\n",
    "\n",
    "# # ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "# #     'huggyllama/llama-7b', \n",
    "# #     'mistralai/Mistral-7B-v0.1',\n",
    "# #     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "# #     'NousResearch/Llama-2-7b-hf',\n",
    "# #     'NousResearch/Llama-2-7b-chat-hf',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "# #     'HuggingFaceH4/zephyr-7b-alpha',\n",
    "#     'HuggingFaceH4/zephyr-7b-beta',\n",
    "# #     'codellama/CodeLlama-7b-hf',\n",
    "# #     'codellama/CodeLlama-7b-Python-hf',\n",
    "# #     'codellama/CodeLlama-7b-Instruct-hf',\n",
    "# ]]\n",
    "# # task_names = task_names + task_names_chatfmt\n",
    "# task_names = task_names_mtbench; definitely_run_mtbench_on_non_alt7b_queue = True\n",
    "\n",
    "# # oi5\n",
    "exp_dir = 'results/oi2'\n",
    "# exp_dir = 'results/oi5_flan_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_dolly:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlm50k:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegpt50k:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b_debug'\n",
    "# exp_dir = 'results/oi5_stanford_alpaca:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlmv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegptv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat200kv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_tulu_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_open_orca_slim:llama-7b'\n",
    "# exp_dir = 'results/dpo1'\n",
    "# exp_dir = 'results/dpo2_ultrafeedback:llama-7b+sharegptv2ep2'\n",
    "# subdir_filter_fn = lambda x: 'sharegpt' in x\n",
    "task_names = task_names + task_names_chatfmt\n",
    "# task_names = task_names_alpacafarm; definitely_run_mtbench_on_non_alt7b_queue = False\n",
    "# task_names = task_names_mtbench; definitely_run_mtbench_on_non_alt7b_queue = False\n",
    "# task_names = task_names\n",
    "# task_names = ['alpacafarm_ann=gpt35:turbo:1106_chatfmt']\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "num_gpus = 1\n",
    "if arch == 'x86_64': # ccc\n",
    "\n",
    "    if definitely_run_mtbench_on_non_alt7b_queue:\n",
    "        gpu_type = 'v100'; num_cpus = int(32/8*num_gpus); cpu_mem = int(240/8*num_gpus)\n",
    "    else:\n",
    "        gpu_type = 'a100_80gb'; num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus)\n",
    "    use_vllm = True; torch_dtype = 'bfloat16'\n",
    "else:\n",
    "    gpu_type = 'v100'\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    use_vllm = False; torch_dtype = 'float16'\n",
    "    \n",
    "    \n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else \\\n",
    "        ('alt_7d' if task_name.startswith('mtbench') else 'alt_1h')\n",
    "\n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    chat_formatting_function = get_chat_formatting_function(model_name_or_path)\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        if 'model_args' in ft_args:\n",
    "            ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "        else:\n",
    "            ft_args_model_name_or_path = ft_args['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 512 \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 3 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length 512 \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_context' if no_context else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314', 'gpt35_turbo_1106']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --max_new_tokens 2048 \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('mtbench'):\n",
    "        assert('chatfmt' in task_name)\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        judge_model = match.group(1).replace(':', '-')\n",
    "        if not judge_model in OPENAI_MODEL_LIST:\n",
    "            raise ValueError('fastchat does not support the judge model.')\n",
    "        os.makedirs(save_dir, exist_ok=True) # since not using python file, make the directory now.\n",
    "        fastchat_mtbench_data_dir = os.path.normpath(os.path.join(open_instruct_dir, '../FastChat/fastchat/llm_judge/data'))\n",
    "        question_file = os.path.join(fastchat_mtbench_data_dir, 'mt_bench/question.jsonl')\n",
    "        rating_file = os.path.join(save_dir, f'{judge_model}_single.jsonl')\n",
    "        question_begin, question_end = (0, 1) if False else (None, None)\n",
    "        model_id = os.path.basename(model_name_or_path) if 'results/baselines' in save_dir else 'tulu'\n",
    "        queue = None if definitely_run_mtbench_on_non_alt7b_queue else queue\n",
    "        cmd = \"\"\n",
    "        cmd += f\"\"\"\n",
    "            python -m fastchat.llm_judge.gen_model_answer \\\n",
    "                --model-path {model_name_or_path} \\\n",
    "                --model-id {model_id} \\\n",
    "                --bench-name mt_bench \\\n",
    "                --question-file {question_file} \\\n",
    "                {'--question-begin '+str(question_begin) if question_begin else ''} \\\n",
    "                {'--question-end '+str(question_end) if question_end else ''} \\\n",
    "                --max-new-token 2048 \\\n",
    "                --answer-file {os.path.join(save_dir, 'model_answer.jsonl')} \\\n",
    "                --dtype {torch_dtype} \\\n",
    "            && \\\n",
    "        \"\"\"\n",
    "        cmd += f\"\"\"\n",
    "            python -m fastchat.llm_judge.gen_judgment \\\n",
    "                --bench-name mt_bench \\\n",
    "                --judge-file {os.path.join(fastchat_mtbench_data_dir, 'judge_prompts.jsonl')} \\\n",
    "                --judge-model {judge_model} \\\n",
    "                --mode single \\\n",
    "                --question-file {question_file} \\\n",
    "                --answer-dir {save_dir} \\\n",
    "                --ref-answer-dir {os.path.join(fastchat_mtbench_data_dir, 'mt_bench/reference_answer')} \\\n",
    "                --output-file {rating_file} \\\n",
    "            && \\\n",
    "            python -m fastchat.llm_judge.show_result \\\n",
    "                --bench-name mt_bench \\\n",
    "                --input-file {rating_file} \\\n",
    "                --mode single \\\n",
    "                --save-to-json\n",
    "        \"\"\"\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "    if test_run:\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd.strip())]))\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    # print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5303ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alt_1h'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7ac978",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_eval.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3939309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python -m fastchat.llm_judge.gen_model_answer --model-path results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10 --model-id tulu --bench-name mt_bench --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --max-new-token 2048 --answer-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/model_answer.jsonl --dtype bfloat16\n",
      "[2024-01-20 13:17:44,894] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Output to results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/model_answer.jsonl\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 11.67it/s]\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 96%|█████████████████████████████████████████▍ | 77/80 [04:22<00:14,  4.96s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████| 80/80 [05:14<00:00,  3.93s/it]\n",
      "+ python -m fastchat.llm_judge.gen_judgment --bench-name mt_bench --judge-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl --judge-model gpt-4-1106-preview --mode single --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --answer-dir results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt --ref-answer-dir /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer --output-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\n",
      "[2024-01-20 13:23:26,885] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Stats:\n",
      "{\n",
      "    \"bench_name\": \"mt_bench\",\n",
      "    \"mode\": \"single\",\n",
      "    \"judge\": \"gpt-4-1106-preview\",\n",
      "    \"baseline\": null,\n",
      "    \"model_list\": [\n",
      "        \"model_answer\"\n",
      "    ],\n",
      "    \"total_num_questions\": 80,\n",
      "    \"total_num_matches\": 160,\n",
      "    \"output_path\": \"results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\"\n",
      "}\n",
      "  0%|                                                   | 0/160 [00:00<?, ?it/s]\n",
      "Num API calls: 1 Prompt Tokens: 371, Completion Tokens: 251\n",
      "Cost [gpt-4-turbo]: 0.011 (per-example: 0.0112)\n",
      "Cost [gpt-4]: 0.026 (per-example: 0.0262)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.001 (per-example: 0.0009)\n",
      "question: 81, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  1%|▎                                          | 1/160 [00:15<39:54, 15.06s/it]\n",
      "Num API calls: 2 Prompt Tokens: 736, Completion Tokens: 431\n",
      "Cost [gpt-4-turbo]: 0.020 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 0.048 (per-example: 0.0240)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.002 (per-example: 0.0008)\n",
      "question: 82, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  1%|▌                                          | 2/160 [00:25<32:10, 12.22s/it]\n",
      "Num API calls: 3 Prompt Tokens: 989, Completion Tokens: 622\n",
      "Cost [gpt-4-turbo]: 0.029 (per-example: 0.0095)\n",
      "Cost [gpt-4]: 0.067 (per-example: 0.0223)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.002 (per-example: 0.0007)\n",
      "question: 83, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  2%|▊                                          | 3/160 [00:35<29:28, 11.27s/it]\n",
      "Num API calls: 4 Prompt Tokens: 1362, Completion Tokens: 839\n",
      "Cost [gpt-4-turbo]: 0.039 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.091 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.003 (per-example: 0.0008)\n",
      "question: 84, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  2%|█                                          | 4/160 [00:50<33:23, 12.85s/it]\n",
      "Num API calls: 5 Prompt Tokens: 1720, Completion Tokens: 947\n",
      "Cost [gpt-4-turbo]: 0.046 (per-example: 0.0091)\n",
      "Cost [gpt-4]: 0.108 (per-example: 0.0217)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.004 (per-example: 0.0007)\n",
      "question: 85, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  3%|█▎                                         | 5/160 [00:54<24:25,  9.45s/it]\n",
      "Num API calls: 6 Prompt Tokens: 1994, Completion Tokens: 1134\n",
      "Cost [gpt-4-turbo]: 0.054 (per-example: 0.0090)\n",
      "Cost [gpt-4]: 0.128 (per-example: 0.0213)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.004 (per-example: 0.0007)\n",
      "question: 86, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  4%|█▌                                         | 6/160 [01:04<25:09,  9.80s/it]\n",
      "Num API calls: 7 Prompt Tokens: 2509, Completion Tokens: 1429\n",
      "Cost [gpt-4-turbo]: 0.068 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.161 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.005 (per-example: 0.0008)\n",
      "question: 87, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  4%|█▉                                         | 7/160 [01:21<30:46, 12.07s/it]\n",
      "Num API calls: 8 Prompt Tokens: 2786, Completion Tokens: 1694\n",
      "Cost [gpt-4-turbo]: 0.079 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.185 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.006 (per-example: 0.0008)\n",
      "question: 88, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  5%|██▏                                        | 8/160 [01:39<35:52, 14.16s/it]\n",
      "Num API calls: 9 Prompt Tokens: 3077, Completion Tokens: 1912\n",
      "Cost [gpt-4-turbo]: 0.088 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.207 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.007 (per-example: 0.0008)\n",
      "question: 89, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  6%|██▍                                        | 9/160 [01:52<34:03, 13.53s/it]\n",
      "Num API calls: 10 Prompt Tokens: 3431, Completion Tokens: 2175\n",
      "Cost [gpt-4-turbo]: 0.100 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.233 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.008 (per-example: 0.0008)\n",
      "question: 90, turn: 1, model: model_answer, score: 8, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  6%|██▋                                       | 10/160 [02:08<36:04, 14.43s/it]\n",
      "Num API calls: 11 Prompt Tokens: 3676, Completion Tokens: 2416\n",
      "Cost [gpt-4-turbo]: 0.109 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.255 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.009 (per-example: 0.0008)\n",
      "question: 91, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  7%|██▉                                       | 11/160 [02:22<35:09, 14.16s/it]\n",
      "Num API calls: 12 Prompt Tokens: 3995, Completion Tokens: 2699\n",
      "Cost [gpt-4-turbo]: 0.121 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 0.282 (per-example: 0.0235)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.009 (per-example: 0.0008)\n",
      "question: 92, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  8%|███▏                                      | 12/160 [02:32<31:50, 12.91s/it]\n",
      "Num API calls: 13 Prompt Tokens: 4313, Completion Tokens: 2978\n",
      "Cost [gpt-4-turbo]: 0.132 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 0.308 (per-example: 0.0237)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.010 (per-example: 0.0008)\n",
      "question: 93, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                      | 13/160 [02:48<33:49, 13.81s/it]\n",
      "Num API calls: 14 Prompt Tokens: 4633, Completion Tokens: 3118\n",
      "Cost [gpt-4-turbo]: 0.140 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.326 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.011 (per-example: 0.0008)\n",
      "question: 94, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  9%|███▋                                      | 14/160 [02:52<26:56, 11.08s/it]\n",
      "Num API calls: 15 Prompt Tokens: 4956, Completion Tokens: 3294\n",
      "Cost [gpt-4-turbo]: 0.148 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.346 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.012 (per-example: 0.0008)\n",
      "question: 95, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  9%|███▉                                      | 15/160 [03:05<27:49, 11.51s/it]\n",
      "Num API calls: 16 Prompt Tokens: 5258, Completion Tokens: 3564\n",
      "Cost [gpt-4-turbo]: 0.160 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.372 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.012 (per-example: 0.0008)\n",
      "question: 96, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 10%|████▏                                     | 16/160 [03:22<31:36, 13.17s/it]\n",
      "Num API calls: 17 Prompt Tokens: 5553, Completion Tokens: 3807\n",
      "Cost [gpt-4-turbo]: 0.170 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.395 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.013 (per-example: 0.0008)\n",
      "question: 97, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 11%|████▍                                     | 17/160 [03:29<26:44, 11.22s/it]\n",
      "Num API calls: 18 Prompt Tokens: 5809, Completion Tokens: 3970\n",
      "Cost [gpt-4-turbo]: 0.177 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.412 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.014 (per-example: 0.0008)\n",
      "question: 98, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 11%|████▋                                     | 18/160 [03:34<22:46,  9.62s/it]\n",
      "Num API calls: 19 Prompt Tokens: 6074, Completion Tokens: 4172\n",
      "Cost [gpt-4-turbo]: 0.186 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.433 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.014 (per-example: 0.0008)\n",
      "question: 99, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 12%|████▉                                     | 19/160 [03:49<25:50, 11.00s/it]\n",
      "Num API calls: 20 Prompt Tokens: 6291, Completion Tokens: 4389\n",
      "Cost [gpt-4-turbo]: 0.195 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.452 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.015 (per-example: 0.0008)\n",
      "question: 100, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 12%|█████▎                                    | 20/160 [04:01<26:17, 11.27s/it]\n",
      "Num API calls: 21 Prompt Tokens: 6610, Completion Tokens: 4583\n",
      "Cost [gpt-4-turbo]: 0.204 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.473 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.016 (per-example: 0.0008)\n",
      "question: 131, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 13%|█████▌                                    | 21/160 [04:12<26:33, 11.47s/it]\n",
      "Num API calls: 22 Prompt Tokens: 7105, Completion Tokens: 4723\n",
      "Cost [gpt-4-turbo]: 0.213 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.497 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.017 (per-example: 0.0008)\n",
      "question: 132, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 14%|█████▊                                    | 22/160 [04:17<21:30,  9.35s/it]\n",
      "Num API calls: 23 Prompt Tokens: 7621, Completion Tokens: 4925\n",
      "Cost [gpt-4-turbo]: 0.224 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.524 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.017 (per-example: 0.0008)\n",
      "question: 133, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 14%|██████                                    | 23/160 [04:23<19:07,  8.37s/it]\n",
      "Num API calls: 24 Prompt Tokens: 7986, Completion Tokens: 5112\n",
      "Cost [gpt-4-turbo]: 0.233 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.546 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.018 (per-example: 0.0008)\n",
      "question: 134, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 15%|██████▎                                   | 24/160 [04:34<20:58,  9.25s/it]\n",
      "Num API calls: 25 Prompt Tokens: 8306, Completion Tokens: 5361\n",
      "Cost [gpt-4-turbo]: 0.244 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.571 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.019 (per-example: 0.0008)\n",
      "question: 135, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 16%|██████▌                                   | 25/160 [04:49<24:21, 10.83s/it]\n",
      "Num API calls: 26 Prompt Tokens: 8728, Completion Tokens: 5491\n",
      "Cost [gpt-4-turbo]: 0.252 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.591 (per-example: 0.0227)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.020 (per-example: 0.0008)\n",
      "question: 136, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 16%|██████▊                                   | 26/160 [04:53<20:01,  8.97s/it]\n",
      "Num API calls: 27 Prompt Tokens: 9108, Completion Tokens: 5700\n",
      "Cost [gpt-4-turbo]: 0.262 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.615 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.021 (per-example: 0.0008)\n",
      "question: 137, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 17%|███████                                   | 27/160 [05:07<23:15, 10.49s/it]\n",
      "Num API calls: 28 Prompt Tokens: 9608, Completion Tokens: 5935\n",
      "Cost [gpt-4-turbo]: 0.274 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.644 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.021 (per-example: 0.0008)\n",
      "question: 138, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 18%|███████▎                                  | 28/160 [05:16<21:47,  9.91s/it]\n",
      "Num API calls: 29 Prompt Tokens: 10008, Completion Tokens: 6012\n",
      "Cost [gpt-4-turbo]: 0.280 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.661 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.022 (per-example: 0.0008)\n",
      "question: 139, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 18%|███████▌                                  | 29/160 [05:21<18:16,  8.37s/it]\n",
      "Num API calls: 30 Prompt Tokens: 10414, Completion Tokens: 6291\n",
      "Cost [gpt-4-turbo]: 0.293 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.690 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.023 (per-example: 0.0008)\n",
      "question: 140, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 19%|███████▉                                  | 30/160 [05:53<33:37, 15.52s/it]\n",
      "Num API calls: 31 Prompt Tokens: 10664, Completion Tokens: 6578\n",
      "Cost [gpt-4-turbo]: 0.304 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.715 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.024 (per-example: 0.0008)\n",
      "question: 141, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 19%|████████▏                                 | 31/160 [06:00<28:03, 13.05s/it]\n",
      "Num API calls: 32 Prompt Tokens: 10904, Completion Tokens: 6822\n",
      "Cost [gpt-4-turbo]: 0.314 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.736 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.025 (per-example: 0.0008)\n",
      "question: 142, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 20%|████████▍                                 | 32/160 [06:08<24:14, 11.36s/it]\n",
      "Num API calls: 33 Prompt Tokens: 11191, Completion Tokens: 7081\n",
      "Cost [gpt-4-turbo]: 0.324 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.761 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.025 (per-example: 0.0008)\n",
      "question: 143, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 21%|████████▋                                 | 33/160 [06:23<26:46, 12.65s/it]\n",
      "Num API calls: 34 Prompt Tokens: 11413, Completion Tokens: 7303\n",
      "Cost [gpt-4-turbo]: 0.333 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.781 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.026 (per-example: 0.0008)\n",
      "question: 144, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 21%|████████▉                                 | 34/160 [06:30<22:40, 10.80s/it]\n",
      "Num API calls: 35 Prompt Tokens: 11690, Completion Tokens: 7512\n",
      "Cost [gpt-4-turbo]: 0.342 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.801 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.027 (per-example: 0.0008)\n",
      "question: 145, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 22%|█████████▏                                | 35/160 [06:44<24:36, 11.81s/it]\n",
      "Num API calls: 36 Prompt Tokens: 12007, Completion Tokens: 7889\n",
      "Cost [gpt-4-turbo]: 0.357 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.834 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.028 (per-example: 0.0008)\n",
      "question: 146, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 22%|█████████▍                                | 36/160 [07:06<30:55, 14.96s/it]\n",
      "Num API calls: 37 Prompt Tokens: 12290, Completion Tokens: 8129\n",
      "Cost [gpt-4-turbo]: 0.367 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.856 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.029 (per-example: 0.0008)\n",
      "question: 147, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 23%|█████████▋                                | 37/160 [07:21<30:38, 14.94s/it]\n",
      "Num API calls: 38 Prompt Tokens: 12531, Completion Tokens: 8390\n",
      "Cost [gpt-4-turbo]: 0.377 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.879 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.029 (per-example: 0.0008)\n",
      "question: 148, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 24%|█████████▉                                | 38/160 [07:29<26:02, 12.81s/it]\n",
      "Num API calls: 39 Prompt Tokens: 12787, Completion Tokens: 8575\n",
      "Cost [gpt-4-turbo]: 0.385 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.898 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.030 (per-example: 0.0008)\n",
      "question: 149, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 24%|██████████▏                               | 39/160 [07:38<23:44, 11.77s/it]\n",
      "Num API calls: 40 Prompt Tokens: 13040, Completion Tokens: 8746\n",
      "Cost [gpt-4-turbo]: 0.393 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.916 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.031 (per-example: 0.0008)\n",
      "question: 150, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 25%|██████████▌                               | 40/160 [07:45<20:11, 10.10s/it]\n",
      "Num API calls: 41 Prompt Tokens: 13382, Completion Tokens: 9097\n",
      "Cost [gpt-4-turbo]: 0.407 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.947 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.032 (per-example: 0.0008)\n",
      "question: 151, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 26%|██████████▊                               | 41/160 [08:04<25:39, 12.94s/it]\n",
      "Num API calls: 42 Prompt Tokens: 13610, Completion Tokens: 9316\n",
      "Cost [gpt-4-turbo]: 0.416 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.967 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.032 (per-example: 0.0008)\n",
      "question: 152, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 26%|███████████                               | 42/160 [08:16<24:36, 12.51s/it]\n",
      "Num API calls: 43 Prompt Tokens: 14046, Completion Tokens: 9558\n",
      "Cost [gpt-4-turbo]: 0.427 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.995 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.033 (per-example: 0.0008)\n",
      "question: 153, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 27%|███████████▎                              | 43/160 [08:23<21:23, 10.97s/it]\n",
      "Num API calls: 44 Prompt Tokens: 14453, Completion Tokens: 9808\n",
      "Cost [gpt-4-turbo]: 0.439 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.022 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.034 (per-example: 0.0008)\n",
      "question: 154, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 28%|███████████▌                              | 44/160 [08:36<22:31, 11.65s/it]\n",
      "Num API calls: 45 Prompt Tokens: 14901, Completion Tokens: 10094\n",
      "Cost [gpt-4-turbo]: 0.452 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.053 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.035 (per-example: 0.0008)\n",
      "question: 155, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 28%|███████████▊                              | 45/160 [08:53<24:59, 13.04s/it]\n",
      "Num API calls: 46 Prompt Tokens: 15235, Completion Tokens: 10285\n",
      "Cost [gpt-4-turbo]: 0.461 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.074 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.036 (per-example: 0.0008)\n",
      "question: 156, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 29%|████████████                              | 46/160 [09:06<25:11, 13.26s/it]\n",
      "Num API calls: 47 Prompt Tokens: 15439, Completion Tokens: 10627\n",
      "Cost [gpt-4-turbo]: 0.473 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.101 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.037 (per-example: 0.0008)\n",
      "question: 157, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 29%|████████████▎                             | 47/160 [09:42<37:35, 19.96s/it]\n",
      "Num API calls: 48 Prompt Tokens: 15703, Completion Tokens: 10895\n",
      "Cost [gpt-4-turbo]: 0.484 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.125 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.037 (per-example: 0.0008)\n",
      "question: 158, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 30%|████████████▌                             | 48/160 [10:00<36:01, 19.30s/it]\n",
      "Num API calls: 49 Prompt Tokens: 16060, Completion Tokens: 11164\n",
      "Cost [gpt-4-turbo]: 0.496 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.152 (per-example: 0.0235)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.038 (per-example: 0.0008)\n",
      "question: 159, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 31%|████████████▊                             | 49/160 [10:16<34:19, 18.55s/it]\n",
      "Num API calls: 50 Prompt Tokens: 16494, Completion Tokens: 11498\n",
      "Cost [gpt-4-turbo]: 0.510 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.185 (per-example: 0.0237)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.039 (per-example: 0.0008)\n",
      "question: 160, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 31%|█████████████▏                            | 50/160 [10:27<29:46, 16.24s/it]\n",
      "Num API calls: 51 Prompt Tokens: 16745, Completion Tokens: 11601\n",
      "Cost [gpt-4-turbo]: 0.515 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.198 (per-example: 0.0235)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.040 (per-example: 0.0008)\n",
      "question: 101, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 32%|█████████████▍                            | 51/160 [10:35<24:42, 13.60s/it]\n",
      "Num API calls: 52 Prompt Tokens: 17011, Completion Tokens: 11667\n",
      "Cost [gpt-4-turbo]: 0.520 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.210 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.040 (per-example: 0.0008)\n",
      "question: 102, turn: 1, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 32%|█████████████▋                            | 52/160 [10:39<19:21, 10.76s/it]\n",
      "Num API calls: 53 Prompt Tokens: 17466, Completion Tokens: 11814\n",
      "Cost [gpt-4-turbo]: 0.529 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.233 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.041 (per-example: 0.0008)\n",
      "question: 103, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 33%|█████████████▉                            | 53/160 [10:43<15:42,  8.80s/it]\n",
      "Num API calls: 54 Prompt Tokens: 17662, Completion Tokens: 11921\n",
      "Cost [gpt-4-turbo]: 0.534 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.245 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.042 (per-example: 0.0008)\n",
      "question: 104, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 34%|██████████████▏                           | 54/160 [10:52<15:29,  8.77s/it]\n",
      "Num API calls: 55 Prompt Tokens: 18237, Completion Tokens: 12069\n",
      "Cost [gpt-4-turbo]: 0.544 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.271 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.042 (per-example: 0.0008)\n",
      "question: 105, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-math-v1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34%|██████████████▍                           | 55/160 [10:58<13:48,  7.89s/it]\n",
      "Num API calls: 56 Prompt Tokens: 18497, Completion Tokens: 12141\n",
      "Cost [gpt-4-turbo]: 0.549 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.283 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.043 (per-example: 0.0008)\n",
      "question: 106, turn: 1, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 35%|██████████████▋                           | 56/160 [11:02<11:55,  6.88s/it]\n",
      "Num API calls: 57 Prompt Tokens: 18700, Completion Tokens: 12210\n",
      "Cost [gpt-4-turbo]: 0.553 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 1.294 (per-example: 0.0227)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.043 (per-example: 0.0008)\n",
      "question: 107, turn: 1, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 36%|██████████████▉                           | 57/160 [11:06<10:17,  6.00s/it]\n",
      "Num API calls: 58 Prompt Tokens: 18924, Completion Tokens: 12329\n",
      "Cost [gpt-4-turbo]: 0.559 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.307 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.044 (per-example: 0.0008)\n",
      "question: 108, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 36%|███████████████▏                          | 58/160 [11:10<09:04,  5.33s/it]\n",
      "Num API calls: 59 Prompt Tokens: 19305, Completion Tokens: 12492\n",
      "Cost [gpt-4-turbo]: 0.568 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.329 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.044 (per-example: 0.0008)\n",
      "question: 109, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 37%|███████████████▍                          | 59/160 [11:15<08:57,  5.32s/it]\n",
      "Num API calls: 60 Prompt Tokens: 19657, Completion Tokens: 12711\n",
      "Cost [gpt-4-turbo]: 0.578 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.352 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.045 (per-example: 0.0008)\n",
      "question: 110, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 38%|███████████████▊                          | 60/160 [11:30<13:41,  8.21s/it]\n",
      "Num API calls: 61 Prompt Tokens: 20128, Completion Tokens: 12876\n",
      "Cost [gpt-4-turbo]: 0.588 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.376 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.046 (per-example: 0.0008)\n",
      "question: 111, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 38%|████████████████                          | 61/160 [11:42<15:24,  9.34s/it]\n",
      "Num API calls: 62 Prompt Tokens: 20407, Completion Tokens: 13032\n",
      "Cost [gpt-4-turbo]: 0.595 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.394 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.046 (per-example: 0.0007)\n",
      "question: 112, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 39%|████████████████▎                         | 62/160 [11:48<13:19,  8.15s/it]\n",
      "Num API calls: 63 Prompt Tokens: 20873, Completion Tokens: 13319\n",
      "Cost [gpt-4-turbo]: 0.608 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 1.425 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.048 (per-example: 0.0008)\n",
      "question: 113, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 39%|████████████████▌                         | 63/160 [12:03<16:47, 10.38s/it]\n",
      "Num API calls: 64 Prompt Tokens: 21360, Completion Tokens: 13686\n",
      "Cost [gpt-4-turbo]: 0.624 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.462 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.049 (per-example: 0.0008)\n",
      "question: 114, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 40%|████████████████▊                         | 64/160 [12:16<17:41, 11.05s/it]\n",
      "Num API calls: 65 Prompt Tokens: 21849, Completion Tokens: 14044\n",
      "Cost [gpt-4-turbo]: 0.640 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.498 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.050 (per-example: 0.0008)\n",
      "question: 115, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 41%|█████████████████                         | 65/160 [12:39<23:11, 14.65s/it]\n",
      "Num API calls: 66 Prompt Tokens: 22315, Completion Tokens: 14201\n",
      "Cost [gpt-4-turbo]: 0.649 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.522 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.051 (per-example: 0.0008)\n",
      "question: 116, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 41%|█████████████████▎                        | 66/160 [12:48<20:24, 13.02s/it]\n",
      "Num API calls: 67 Prompt Tokens: 22771, Completion Tokens: 14488\n",
      "Cost [gpt-4-turbo]: 0.662 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.552 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.052 (per-example: 0.0008)\n",
      "question: 117, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 42%|█████████████████▌                        | 67/160 [13:04<21:27, 13.84s/it]\n",
      "Num API calls: 68 Prompt Tokens: 23151, Completion Tokens: 14701\n",
      "Cost [gpt-4-turbo]: 0.673 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.577 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.053 (per-example: 0.0008)\n",
      "question: 118, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 42%|█████████████████▊                        | 68/160 [13:17<20:45, 13.54s/it]\n",
      "Num API calls: 69 Prompt Tokens: 23507, Completion Tokens: 14842\n",
      "Cost [gpt-4-turbo]: 0.680 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.596 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.053 (per-example: 0.0008)\n",
      "question: 119, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 43%|██████████████████                        | 69/160 [13:25<18:08, 11.96s/it]\n",
      "Num API calls: 70 Prompt Tokens: 23923, Completion Tokens: 15146\n",
      "Cost [gpt-4-turbo]: 0.694 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.626 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.054 (per-example: 0.0008)\n",
      "question: 120, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 44%|██████████████████▍                       | 70/160 [13:36<17:43, 11.82s/it]\n",
      "Num API calls: 71 Prompt Tokens: 24420, Completion Tokens: 15330\n",
      "Cost [gpt-4-turbo]: 0.704 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.652 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.055 (per-example: 0.0008)\n",
      "question: 121, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 44%|██████████████████▋                       | 71/160 [13:43<15:23, 10.37s/it]\n",
      "Num API calls: 72 Prompt Tokens: 25772, Completion Tokens: 15520\n",
      "Cost [gpt-4-turbo]: 0.723 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.704 (per-example: 0.0237)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.057 (per-example: 0.0008)\n",
      "question: 122, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 45%|██████████████████▉                       | 72/160 [13:53<15:04, 10.28s/it]\n",
      "Num API calls: 73 Prompt Tokens: 26340, Completion Tokens: 15794\n",
      "Cost [gpt-4-turbo]: 0.737 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.738 (per-example: 0.0238)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.058 (per-example: 0.0008)\n",
      "question: 123, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 46%|███████████████████▏                      | 73/160 [14:13<19:00, 13.11s/it]\n",
      "Num API calls: 74 Prompt Tokens: 26881, Completion Tokens: 15987\n",
      "Cost [gpt-4-turbo]: 0.748 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.766 (per-example: 0.0239)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.059 (per-example: 0.0008)\n",
      "question: 124, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 46%|███████████████████▍                      | 74/160 [14:36<22:51, 15.95s/it]\n",
      "Num API calls: 75 Prompt Tokens: 27565, Completion Tokens: 16232\n",
      "Cost [gpt-4-turbo]: 0.763 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.801 (per-example: 0.0240)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.060 (per-example: 0.0008)\n",
      "question: 125, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 47%|███████████████████▋                      | 75/160 [14:43<18:47, 13.27s/it]\n",
      "Num API calls: 76 Prompt Tokens: 28228, Completion Tokens: 16443\n",
      "Cost [gpt-4-turbo]: 0.776 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.833 (per-example: 0.0241)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.061 (per-example: 0.0008)\n",
      "question: 126, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████▉                      | 76/160 [14:54<17:32, 12.53s/it]\n",
      "Num API calls: 77 Prompt Tokens: 28764, Completion Tokens: 16607\n",
      "Cost [gpt-4-turbo]: 0.786 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.859 (per-example: 0.0241)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.062 (per-example: 0.0008)\n",
      "question: 127, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 48%|████████████████████▏                     | 77/160 [15:04<16:33, 11.97s/it]\n",
      "Num API calls: 78 Prompt Tokens: 29316, Completion Tokens: 16804\n",
      "Cost [gpt-4-turbo]: 0.797 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.888 (per-example: 0.0242)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.063 (per-example: 0.0008)\n",
      "question: 128, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 49%|████████████████████▍                     | 78/160 [15:17<16:32, 12.10s/it]\n",
      "Num API calls: 79 Prompt Tokens: 29951, Completion Tokens: 17028\n",
      "Cost [gpt-4-turbo]: 0.810 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 1.920 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.064 (per-example: 0.0008)\n",
      "question: 129, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 49%|████████████████████▋                     | 79/160 [15:38<20:07, 14.91s/it]\n",
      "Num API calls: 80 Prompt Tokens: 30388, Completion Tokens: 17372\n",
      "Cost [gpt-4-turbo]: 0.825 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 1.954 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.065 (per-example: 0.0008)\n",
      "question: 130, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 50%|█████████████████████                     | 80/160 [15:55<20:42, 15.53s/it]\n",
      "Num API calls: 81 Prompt Tokens: 31001, Completion Tokens: 17494\n",
      "Cost [gpt-4-turbo]: 0.835 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 1.980 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.066 (per-example: 0.0008)\n",
      "question: 81, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 51%|█████████████████████▎                    | 81/160 [16:00<16:07, 12.25s/it]\n",
      "Num API calls: 82 Prompt Tokens: 31436, Completion Tokens: 17675\n",
      "Cost [gpt-4-turbo]: 0.845 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.004 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.067 (per-example: 0.0008)\n",
      "question: 82, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 51%|█████████████████████▌                    | 82/160 [16:17<18:02, 13.88s/it]\n",
      "Num API calls: 83 Prompt Tokens: 31936, Completion Tokens: 17790\n",
      "Cost [gpt-4-turbo]: 0.853 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.025 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.068 (per-example: 0.0008)\n",
      "question: 83, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 52%|█████████████████████▊                    | 83/160 [16:23<14:50, 11.56s/it]\n",
      "Num API calls: 84 Prompt Tokens: 32525, Completion Tokens: 17991\n",
      "Cost [gpt-4-turbo]: 0.865 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.055 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.069 (per-example: 0.0008)\n",
      "question: 84, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 52%|██████████████████████                    | 84/160 [16:35<14:44, 11.64s/it]\n",
      "Num API calls: 85 Prompt Tokens: 33099, Completion Tokens: 18088\n",
      "Cost [gpt-4-turbo]: 0.874 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.078 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.069 (per-example: 0.0008)\n",
      "question: 85, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 53%|██████████████████████▎                   | 85/160 [16:42<12:43, 10.17s/it]\n",
      "Num API calls: 86 Prompt Tokens: 33520, Completion Tokens: 18224\n",
      "Cost [gpt-4-turbo]: 0.882 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.099 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.070 (per-example: 0.0008)\n",
      "question: 86, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 54%|██████████████████████▌                   | 86/160 [16:46<10:19,  8.38s/it]\n",
      "Num API calls: 87 Prompt Tokens: 34413, Completion Tokens: 18315\n",
      "Cost [gpt-4-turbo]: 0.894 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.131 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.071 (per-example: 0.0008)\n",
      "question: 87, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 54%|██████████████████████▊                   | 87/160 [16:53<09:31,  7.83s/it]\n",
      "Num API calls: 88 Prompt Tokens: 34796, Completion Tokens: 18418\n",
      "Cost [gpt-4-turbo]: 0.900 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.149 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.072 (per-example: 0.0008)\n",
      "question: 88, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 55%|███████████████████████                   | 88/160 [16:56<07:43,  6.44s/it]\n",
      "Num API calls: 89 Prompt Tokens: 35195, Completion Tokens: 18588\n",
      "Cost [gpt-4-turbo]: 0.910 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.171 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.072 (per-example: 0.0008)\n",
      "question: 89, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 56%|███████████████████████▎                  | 89/160 [17:06<08:49,  7.46s/it]\n",
      "Num API calls: 90 Prompt Tokens: 35702, Completion Tokens: 18746\n",
      "Cost [gpt-4-turbo]: 0.919 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.196 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.073 (per-example: 0.0008)\n",
      "question: 90, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 56%|███████████████████████▋                  | 90/160 [17:16<09:33,  8.19s/it]\n",
      "Num API calls: 91 Prompt Tokens: 36025, Completion Tokens: 18930\n",
      "Cost [gpt-4-turbo]: 0.928 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.217 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.074 (per-example: 0.0008)\n",
      "question: 91, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 57%|███████████████████████▉                  | 91/160 [17:21<08:31,  7.41s/it]\n",
      "Num API calls: 92 Prompt Tokens: 36398, Completion Tokens: 19155\n",
      "Cost [gpt-4-turbo]: 0.939 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.241 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.075 (per-example: 0.0008)\n",
      "question: 92, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 57%|████████████████████████▏                 | 92/160 [17:34<10:06,  8.93s/it]\n",
      "Num API calls: 93 Prompt Tokens: 36789, Completion Tokens: 19357\n",
      "Cost [gpt-4-turbo]: 0.949 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.265 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.076 (per-example: 0.0008)\n",
      "question: 93, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 58%|████████████████████████▍                 | 93/160 [17:46<11:03,  9.90s/it]\n",
      "Num API calls: 94 Prompt Tokens: 37244, Completion Tokens: 19523\n",
      "Cost [gpt-4-turbo]: 0.958 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.289 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.076 (per-example: 0.0008)\n",
      "question: 94, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 59%|████████████████████████▋                 | 94/160 [17:56<10:50,  9.86s/it]\n",
      "Num API calls: 95 Prompt Tokens: 37673, Completion Tokens: 19681\n",
      "Cost [gpt-4-turbo]: 0.967 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.311 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.077 (per-example: 0.0008)\n",
      "question: 95, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 59%|████████████████████████▉                 | 95/160 [18:03<09:46,  9.02s/it]\n",
      "Num API calls: 96 Prompt Tokens: 38077, Completion Tokens: 19900\n",
      "Cost [gpt-4-turbo]: 0.978 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.336 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.078 (per-example: 0.0008)\n",
      "question: 96, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 60%|█████████████████████████▏                | 96/160 [18:11<09:26,  8.85s/it]\n",
      "Num API calls: 97 Prompt Tokens: 38564, Completion Tokens: 20228\n",
      "Cost [gpt-4-turbo]: 0.992 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.371 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.079 (per-example: 0.0008)\n",
      "question: 97, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████▍                | 97/160 [18:23<10:16,  9.79s/it]\n",
      "Num API calls: 98 Prompt Tokens: 38937, Completion Tokens: 20480\n",
      "Cost [gpt-4-turbo]: 1.004 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.397 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.080 (per-example: 0.0008)\n",
      "question: 98, turn: 2, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 61%|█████████████████████████▋                | 98/160 [18:33<10:09,  9.83s/it]\n",
      "Num API calls: 99 Prompt Tokens: 39548, Completion Tokens: 20731\n",
      "Cost [gpt-4-turbo]: 1.017 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.430 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.081 (per-example: 0.0008)\n",
      "question: 99, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 62%|█████████████████████████▉                | 99/160 [18:41<09:31,  9.37s/it]\n",
      "Num API calls: 100 Prompt Tokens: 39834, Completion Tokens: 20886\n",
      "Cost [gpt-4-turbo]: 1.025 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.448 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.082 (per-example: 0.0008)\n",
      "question: 100, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 62%|█████████████████████████▋               | 100/160 [18:47<08:13,  8.22s/it]\n",
      "Num API calls: 101 Prompt Tokens: 40224, Completion Tokens: 21141\n",
      "Cost [gpt-4-turbo]: 1.036 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.475 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.083 (per-example: 0.0008)\n",
      "question: 131, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 63%|█████████████████████████▉               | 101/160 [19:01<09:43,  9.89s/it]\n",
      "Num API calls: 102 Prompt Tokens: 40789, Completion Tokens: 21294\n",
      "Cost [gpt-4-turbo]: 1.047 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.501 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.083 (per-example: 0.0008)\n",
      "question: 132, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 64%|██████████████████████████▏              | 102/160 [19:09<09:02,  9.36s/it]\n",
      "Num API calls: 103 Prompt Tokens: 41384, Completion Tokens: 21543\n",
      "Cost [gpt-4-turbo]: 1.060 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.534 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.084 (per-example: 0.0008)\n",
      "question: 133, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 64%|██████████████████████████▍              | 103/160 [19:26<11:13, 11.81s/it]\n",
      "Num API calls: 104 Prompt Tokens: 41796, Completion Tokens: 21774\n",
      "Cost [gpt-4-turbo]: 1.071 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.560 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.085 (per-example: 0.0008)\n",
      "question: 134, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 65%|██████████████████████████▋              | 104/160 [19:35<10:07, 10.84s/it]\n",
      "Num API calls: 105 Prompt Tokens: 42276, Completion Tokens: 21904\n",
      "Cost [gpt-4-turbo]: 1.080 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.583 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.086 (per-example: 0.0008)\n",
      "question: 135, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 66%|██████████████████████████▉              | 105/160 [19:44<09:22, 10.23s/it]\n",
      "Num API calls: 106 Prompt Tokens: 42759, Completion Tokens: 22006\n",
      "Cost [gpt-4-turbo]: 1.088 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.603 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.087 (per-example: 0.0008)\n",
      "question: 136, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 66%|███████████████████████████▏             | 106/160 [19:50<08:04,  8.96s/it]\n",
      "Num API calls: 107 Prompt Tokens: 43228, Completion Tokens: 22189\n",
      "Cost [gpt-4-turbo]: 1.098 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.628 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.088 (per-example: 0.0008)\n",
      "question: 137, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 67%|███████████████████████████▍             | 107/160 [20:00<08:19,  9.42s/it]\n",
      "Num API calls: 108 Prompt Tokens: 43794, Completion Tokens: 22335\n",
      "Cost [gpt-4-turbo]: 1.108 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.654 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.088 (per-example: 0.0008)\n",
      "question: 138, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 68%|███████████████████████████▋             | 108/160 [20:09<07:59,  9.22s/it]\n",
      "Num API calls: 109 Prompt Tokens: 44354, Completion Tokens: 22439\n",
      "Cost [gpt-4-turbo]: 1.117 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.677 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.089 (per-example: 0.0008)\n",
      "question: 139, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 68%|███████████████████████████▉             | 109/160 [20:15<07:05,  8.34s/it]\n",
      "Num API calls: 110 Prompt Tokens: 44853, Completion Tokens: 22645\n",
      "Cost [gpt-4-turbo]: 1.128 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.704 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.090 (per-example: 0.0008)\n",
      "question: 140, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 69%|████████████████████████████▏            | 110/160 [20:27<07:53,  9.47s/it]\n",
      "Num API calls: 111 Prompt Tokens: 45163, Completion Tokens: 22812\n",
      "Cost [gpt-4-turbo]: 1.136 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.724 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.091 (per-example: 0.0008)\n",
      "question: 141, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 69%|████████████████████████████▍            | 111/160 [20:40<08:36, 10.55s/it]\n",
      "Num API calls: 112 Prompt Tokens: 45510, Completion Tokens: 23024\n",
      "Cost [gpt-4-turbo]: 1.146 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.747 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.092 (per-example: 0.0008)\n",
      "question: 142, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 70%|████████████████████████████▋            | 112/160 [20:55<09:26, 11.80s/it]\n",
      "Num API calls: 113 Prompt Tokens: 45994, Completion Tokens: 23351\n",
      "Cost [gpt-4-turbo]: 1.160 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.781 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.093 (per-example: 0.0008)\n",
      "question: 143, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 71%|████████████████████████████▉            | 113/160 [21:05<08:44, 11.15s/it]\n",
      "Num API calls: 114 Prompt Tokens: 46312, Completion Tokens: 23501\n",
      "Cost [gpt-4-turbo]: 1.168 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.799 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.093 (per-example: 0.0008)\n",
      "question: 144, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 71%|█████████████████████████████▏           | 114/160 [21:15<08:25, 10.98s/it]\n",
      "Num API calls: 115 Prompt Tokens: 46687, Completion Tokens: 23738\n",
      "Cost [gpt-4-turbo]: 1.179 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.825 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.094 (per-example: 0.0008)\n",
      "question: 145, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 72%|█████████████████████████████▍           | 115/160 [21:29<08:48, 11.75s/it]\n",
      "Num API calls: 116 Prompt Tokens: 47090, Completion Tokens: 23949\n",
      "Cost [gpt-4-turbo]: 1.189 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.850 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.095 (per-example: 0.0008)\n",
      "question: 146, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 72%|█████████████████████████████▋           | 116/160 [21:41<08:44, 11.91s/it]\n",
      "Num API calls: 117 Prompt Tokens: 47502, Completion Tokens: 24108\n",
      "Cost [gpt-4-turbo]: 1.198 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.872 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.096 (per-example: 0.0008)\n",
      "question: 147, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 73%|█████████████████████████████▉           | 117/160 [21:55<08:59, 12.54s/it]\n",
      "Num API calls: 118 Prompt Tokens: 47802, Completion Tokens: 24338\n",
      "Cost [gpt-4-turbo]: 1.208 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.894 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.096 (per-example: 0.0008)\n",
      "question: 148, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████▏          | 118/160 [22:09<08:59, 12.84s/it]\n",
      "Num API calls: 119 Prompt Tokens: 48116, Completion Tokens: 24542\n",
      "Cost [gpt-4-turbo]: 1.217 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.916 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.097 (per-example: 0.0008)\n",
      "question: 149, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 74%|██████████████████████████████▍          | 119/160 [22:19<08:10, 11.96s/it]\n",
      "Num API calls: 120 Prompt Tokens: 48436, Completion Tokens: 24740\n",
      "Cost [gpt-4-turbo]: 1.227 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.937 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.098 (per-example: 0.0008)\n",
      "question: 150, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 75%|██████████████████████████████▊          | 120/160 [22:30<07:53, 11.84s/it]\n",
      "Num API calls: 121 Prompt Tokens: 48919, Completion Tokens: 24985\n",
      "Cost [gpt-4-turbo]: 1.239 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.967 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.099 (per-example: 0.0008)\n",
      "question: 151, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 76%|███████████████████████████████          | 121/160 [22:46<08:29, 13.06s/it]\n",
      "Num API calls: 122 Prompt Tokens: 49251, Completion Tokens: 25243\n",
      "Cost [gpt-4-turbo]: 1.250 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.992 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.100 (per-example: 0.0008)\n",
      "question: 152, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 76%|███████████████████████████████▎         | 122/160 [22:54<07:13, 11.41s/it]\n",
      "Num API calls: 123 Prompt Tokens: 49839, Completion Tokens: 25507\n",
      "Cost [gpt-4-turbo]: 1.264 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.026 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.101 (per-example: 0.0008)\n",
      "question: 153, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 77%|███████████████████████████████▌         | 123/160 [23:02<06:27, 10.47s/it]\n",
      "Num API calls: 124 Prompt Tokens: 50386, Completion Tokens: 25753\n",
      "Cost [gpt-4-turbo]: 1.276 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.057 (per-example: 0.0247)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.102 (per-example: 0.0008)\n",
      "question: 154, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 78%|███████████████████████████████▊         | 124/160 [23:10<05:52,  9.79s/it]\n",
      "Num API calls: 125 Prompt Tokens: 50927, Completion Tokens: 25977\n",
      "Cost [gpt-4-turbo]: 1.289 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.086 (per-example: 0.0247)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.103 (per-example: 0.0008)\n",
      "question: 155, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 78%|████████████████████████████████         | 125/160 [23:17<05:05,  8.74s/it]\n",
      "Num API calls: 126 Prompt Tokens: 51347, Completion Tokens: 26182\n",
      "Cost [gpt-4-turbo]: 1.299 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.111 (per-example: 0.0247)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.104 (per-example: 0.0008)\n",
      "question: 156, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 79%|████████████████████████████████▎        | 126/160 [23:28<05:29,  9.68s/it]\n",
      "Num API calls: 127 Prompt Tokens: 51943, Completion Tokens: 26694\n",
      "Cost [gpt-4-turbo]: 1.320 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.160 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.105 (per-example: 0.0008)\n",
      "question: 157, turn: 2, model: model_answer, score: -1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 79%|████████████████████████████████▌        | 127/160 [23:57<08:25, 15.31s/it]\n",
      "Num API calls: 128 Prompt Tokens: 52271, Completion Tokens: 26801\n",
      "Cost [gpt-4-turbo]: 1.327 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.176 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.106 (per-example: 0.0008)\n",
      "question: 158, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 80%|████████████████████████████████▊        | 128/160 [24:02<06:27, 12.12s/it]\n",
      "Num API calls: 129 Prompt Tokens: 52847, Completion Tokens: 26983\n",
      "Cost [gpt-4-turbo]: 1.338 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.204 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.107 (per-example: 0.0008)\n",
      "question: 159, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 81%|█████████████████████████████████        | 129/160 [24:12<05:55, 11.48s/it]\n",
      "Num API calls: 130 Prompt Tokens: 53384, Completion Tokens: 27175\n",
      "Cost [gpt-4-turbo]: 1.349 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.232 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.108 (per-example: 0.0008)\n",
      "question: 160, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 81%|█████████████████████████████████▎       | 130/160 [24:22<05:31, 11.04s/it]\n",
      "Num API calls: 131 Prompt Tokens: 53837, Completion Tokens: 27329\n",
      "Cost [gpt-4-turbo]: 1.358 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.255 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.108 (per-example: 0.0008)\n",
      "question: 101, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 82%|█████████████████████████████████▌       | 131/160 [24:28<04:36,  9.55s/it]\n",
      "Num API calls: 132 Prompt Tokens: 54282, Completion Tokens: 27494\n",
      "Cost [gpt-4-turbo]: 1.368 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.278 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.109 (per-example: 0.0008)\n",
      "question: 102, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 82%|█████████████████████████████████▊       | 132/160 [24:33<03:53,  8.34s/it]\n",
      "Num API calls: 133 Prompt Tokens: 55125, Completion Tokens: 27710\n",
      "Cost [gpt-4-turbo]: 1.383 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.316 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.111 (per-example: 0.0008)\n",
      "question: 103, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 83%|██████████████████████████████████       | 133/160 [24:49<04:48, 10.67s/it]\n",
      "Num API calls: 134 Prompt Tokens: 55453, Completion Tokens: 27842\n",
      "Cost [gpt-4-turbo]: 1.390 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.334 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.111 (per-example: 0.0008)\n",
      "question: 104, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 84%|██████████████████████████████████▎      | 134/160 [24:56<04:06,  9.50s/it]\n",
      "Num API calls: 135 Prompt Tokens: 56323, Completion Tokens: 28084\n",
      "Cost [gpt-4-turbo]: 1.406 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.375 (per-example: 0.0250)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.112 (per-example: 0.0008)\n",
      "question: 105, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 84%|██████████████████████████████████▌      | 135/160 [25:07<04:10, 10.02s/it]\n",
      "Num API calls: 136 Prompt Tokens: 56828, Completion Tokens: 28271\n",
      "Cost [gpt-4-turbo]: 1.416 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.401 (per-example: 0.0250)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.113 (per-example: 0.0008)\n",
      "question: 106, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 85%|██████████████████████████████████▊      | 136/160 [25:18<04:08, 10.37s/it]\n",
      "Num API calls: 137 Prompt Tokens: 57586, Completion Tokens: 28480\n",
      "Cost [gpt-4-turbo]: 1.430 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.436 (per-example: 0.0251)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.115 (per-example: 0.0008)\n",
      "question: 107, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 86%|███████████████████████████████████      | 137/160 [25:24<03:26,  8.98s/it]\n",
      "Num API calls: 138 Prompt Tokens: 57941, Completion Tokens: 28700\n",
      "Cost [gpt-4-turbo]: 1.440 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.460 (per-example: 0.0251)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.115 (per-example: 0.0008)\n",
      "question: 108, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 86%|███████████████████████████████████▎     | 138/160 [25:29<02:52,  7.86s/it]\n",
      "Num API calls: 139 Prompt Tokens: 58567, Completion Tokens: 28922\n",
      "Cost [gpt-4-turbo]: 1.453 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.492 (per-example: 0.0251)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.116 (per-example: 0.0008)\n",
      "question: 109, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████▌     | 139/160 [25:39<02:57,  8.45s/it]\n",
      "Num API calls: 140 Prompt Tokens: 59517, Completion Tokens: 29145\n",
      "Cost [gpt-4-turbo]: 1.470 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.534 (per-example: 0.0252)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.118 (per-example: 0.0008)\n",
      "question: 110, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 88%|███████████████████████████████████▉     | 140/160 [25:50<03:02,  9.10s/it]\n",
      "Num API calls: 141 Prompt Tokens: 60189, Completion Tokens: 29416\n",
      "Cost [gpt-4-turbo]: 1.484 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.571 (per-example: 0.0253)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.119 (per-example: 0.0008)\n",
      "question: 111, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 88%|████████████████████████████████████▏    | 141/160 [26:00<02:57,  9.36s/it]\n",
      "Num API calls: 142 Prompt Tokens: 60660, Completion Tokens: 29583\n",
      "Cost [gpt-4-turbo]: 1.494 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.595 (per-example: 0.0253)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.120 (per-example: 0.0008)\n",
      "question: 112, turn: 2, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 89%|████████████████████████████████████▍    | 142/160 [26:08<02:43,  9.08s/it]\n",
      "Num API calls: 143 Prompt Tokens: 61421, Completion Tokens: 29820\n",
      "Cost [gpt-4-turbo]: 1.509 (per-example: 0.0106)\n",
      "Cost [gpt-4]: 3.632 (per-example: 0.0254)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.121 (per-example: 0.0008)\n",
      "question: 113, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 89%|████████████████████████████████████▋    | 143/160 [26:19<02:43,  9.62s/it]\n",
      "Num API calls: 144 Prompt Tokens: 62519, Completion Tokens: 30070\n",
      "Cost [gpt-4-turbo]: 1.527 (per-example: 0.0106)\n",
      "Cost [gpt-4]: 3.680 (per-example: 0.0256)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.123 (per-example: 0.0009)\n",
      "question: 114, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 90%|████████████████████████████████████▉    | 144/160 [26:32<02:50, 10.65s/it]\n",
      "Num API calls: 145 Prompt Tokens: 63300, Completion Tokens: 30359\n",
      "Cost [gpt-4-turbo]: 1.544 (per-example: 0.0106)\n",
      "Cost [gpt-4]: 3.721 (per-example: 0.0257)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.124 (per-example: 0.0009)\n",
      "question: 115, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 91%|█████████████████████████████████████▏   | 145/160 [26:46<02:52, 11.52s/it]\n",
      "Num API calls: 146 Prompt Tokens: 63977, Completion Tokens: 30550\n",
      "Cost [gpt-4-turbo]: 1.556 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.752 (per-example: 0.0257)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.125 (per-example: 0.0009)\n",
      "question: 116, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 91%|█████████████████████████████████████▍   | 146/160 [26:55<02:31, 10.81s/it]\n",
      "Num API calls: 147 Prompt Tokens: 64763, Completion Tokens: 30806\n",
      "Cost [gpt-4-turbo]: 1.572 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.791 (per-example: 0.0258)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.126 (per-example: 0.0009)\n",
      "question: 117, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 92%|█████████████████████████████████████▋   | 147/160 [27:08<02:28, 11.43s/it]\n",
      "Num API calls: 148 Prompt Tokens: 65340, Completion Tokens: 30967\n",
      "Cost [gpt-4-turbo]: 1.582 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.818 (per-example: 0.0258)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.127 (per-example: 0.0009)\n",
      "question: 118, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 92%|█████████████████████████████████████▉   | 148/160 [27:18<02:11, 10.94s/it]\n",
      "Num API calls: 149 Prompt Tokens: 66123, Completion Tokens: 31203\n",
      "Cost [gpt-4-turbo]: 1.597 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.856 (per-example: 0.0259)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.129 (per-example: 0.0009)\n",
      "question: 119, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 93%|██████████████████████████████████████▏  | 149/160 [27:30<02:04, 11.30s/it]\n",
      "Num API calls: 150 Prompt Tokens: 67190, Completion Tokens: 31451\n",
      "Cost [gpt-4-turbo]: 1.615 (per-example: 0.0108)\n",
      "Cost [gpt-4]: 3.903 (per-example: 0.0260)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.130 (per-example: 0.0009)\n",
      "question: 120, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 94%|██████████████████████████████████████▍  | 150/160 [27:37<01:40, 10.04s/it]\n",
      "Num API calls: 151 Prompt Tokens: 68169, Completion Tokens: 31667\n",
      "Cost [gpt-4-turbo]: 1.632 (per-example: 0.0108)\n",
      "Cost [gpt-4]: 3.945 (per-example: 0.0261)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.132 (per-example: 0.0009)\n",
      "question: 121, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 94%|██████████████████████████████████████▋  | 151/160 [27:48<01:32, 10.28s/it]\n",
      "Num API calls: 152 Prompt Tokens: 69967, Completion Tokens: 31990\n",
      "Cost [gpt-4-turbo]: 1.659 (per-example: 0.0109)\n",
      "Cost [gpt-4]: 4.018 (per-example: 0.0264)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.134 (per-example: 0.0009)\n",
      "question: 122, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 95%|██████████████████████████████████████▉  | 152/160 [28:04<01:37, 12.20s/it]\n",
      "Num API calls: 153 Prompt Tokens: 71078, Completion Tokens: 32191\n",
      "Cost [gpt-4-turbo]: 1.677 (per-example: 0.0110)\n",
      "Cost [gpt-4]: 4.064 (per-example: 0.0266)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.135 (per-example: 0.0009)\n",
      "question: 123, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 96%|███████████████████████████████████████▏ | 153/160 [28:15<01:22, 11.76s/it]\n",
      "Num API calls: 154 Prompt Tokens: 72487, Completion Tokens: 32406\n",
      "Cost [gpt-4-turbo]: 1.697 (per-example: 0.0110)\n",
      "Cost [gpt-4]: 4.119 (per-example: 0.0267)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.137 (per-example: 0.0009)\n",
      "question: 124, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 96%|███████████████████████████████████████▍ | 154/160 [28:22<01:01, 10.24s/it]\n",
      "Num API calls: 155 Prompt Tokens: 73783, Completion Tokens: 32651\n",
      "Cost [gpt-4-turbo]: 1.717 (per-example: 0.0111)\n",
      "Cost [gpt-4]: 4.173 (per-example: 0.0269)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.139 (per-example: 0.0009)\n",
      "question: 125, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 97%|███████████████████████████████████████▋ | 155/160 [28:37<00:59, 11.81s/it]\n",
      "Num API calls: 156 Prompt Tokens: 74777, Completion Tokens: 32860\n",
      "Cost [gpt-4-turbo]: 1.734 (per-example: 0.0111)\n",
      "Cost [gpt-4]: 4.215 (per-example: 0.0270)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.140 (per-example: 0.0009)\n",
      "question: 126, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 98%|███████████████████████████████████████▉ | 156/160 [28:44<00:41, 10.39s/it]\n",
      "Num API calls: 157 Prompt Tokens: 75872, Completion Tokens: 33062\n",
      "Cost [gpt-4-turbo]: 1.751 (per-example: 0.0112)\n",
      "Cost [gpt-4]: 4.260 (per-example: 0.0271)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.142 (per-example: 0.0009)\n",
      "question: 127, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 98%|████████████████████████████████████████▏| 157/160 [28:58<00:34, 11.43s/it]\n",
      "Num API calls: 158 Prompt Tokens: 77001, Completion Tokens: 33420\n",
      "Cost [gpt-4-turbo]: 1.773 (per-example: 0.0112)\n",
      "Cost [gpt-4]: 4.315 (per-example: 0.0273)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.144 (per-example: 0.0009)\n",
      "question: 128, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 99%|████████████████████████████████████████▍| 158/160 [29:16<00:26, 13.32s/it]\n",
      "Num API calls: 159 Prompt Tokens: 78221, Completion Tokens: 33632\n",
      "Cost [gpt-4-turbo]: 1.791 (per-example: 0.0113)\n",
      "Cost [gpt-4]: 4.365 (per-example: 0.0274)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.145 (per-example: 0.0009)\n",
      "question: 129, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 99%|████████████████████████████████████████▋| 159/160 [29:31<00:13, 13.93s/it]\n",
      "Num API calls: 160 Prompt Tokens: 79023, Completion Tokens: 33817\n",
      "Cost [gpt-4-turbo]: 1.805 (per-example: 0.0113)\n",
      "Cost [gpt-4]: 4.400 (per-example: 0.0275)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.147 (per-example: 0.0009)\n",
      "question: 130, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      "100%|█████████████████████████████████████████| 160/160 [29:37<00:00, 11.11s/it]\n",
      "+ python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl --mode single --save-to-json\n",
      "Mode: single\n",
      "Input file: results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\n",
      "\n",
      "########## First turn ##########\n",
      "                   score\n",
      "model        turn       \n",
      "model_answer 1     3.325\n",
      "\n",
      "########## Second turn ##########\n",
      "                      score\n",
      "model        turn          \n",
      "model_answer 2     1.772152\n",
      "\n",
      "########## Average ##########\n",
      "                 score\n",
      "model                 \n",
      "model_answer  2.553459\n",
      "Input file: results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\n",
      "Save metrics to \n",
      "\tresults/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/metrics.json\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_eval.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6445ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 20 13:17:36 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   24C    P0              58W / 400W |      4MiB / 81920MiB |      0%   E. Process |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859d6d6",
   "metadata": {},
   "source": [
    "# Visualize Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b033ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./1387915.out does not have `--save_dir` specified. Probably still running.\n",
      "./1387916.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1387899.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=chatgpt_chatfmt\n"
     ]
    }
   ],
   "source": [
    "# if job successfully ran, lsf system will generate a summary in log_dir,\n",
    "# call this function to move lsf summary to save_dir if job is successful.\n",
    "move_lsf_job_summary_to_save_dir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4392e9ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/3595434990.py:369: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/3595434990.py:375: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dcb1a td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_dcb1a_row0_col0, #T_dcb1a_row1_col0, #T_dcb1a_row2_col0, #T_dcb1a_row3_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_dcb1a_row0_col1, #T_dcb1a_row1_col1, #T_dcb1a_row2_col1, #T_dcb1a_row3_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_dcb1a_row0_col2, #T_dcb1a_row0_col3, #T_dcb1a_row0_col4, #T_dcb1a_row0_col5, #T_dcb1a_row0_col6, #T_dcb1a_row0_col7, #T_dcb1a_row0_col8, #T_dcb1a_row0_col9, #T_dcb1a_row0_col10, #T_dcb1a_row0_col12, #T_dcb1a_row0_col16, #T_dcb1a_row0_col17, #T_dcb1a_row1_col10, #T_dcb1a_row1_col11, #T_dcb1a_row2_col13, #T_dcb1a_row2_col14, #T_dcb1a_row2_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row0_col11, #T_dcb1a_row0_col14, #T_dcb1a_row0_col15, #T_dcb1a_row1_col4, #T_dcb1a_row1_col6, #T_dcb1a_row1_col8, #T_dcb1a_row1_col9, #T_dcb1a_row2_col10, #T_dcb1a_row3_col2, #T_dcb1a_row3_col3, #T_dcb1a_row3_col5, #T_dcb1a_row3_col7, #T_dcb1a_row3_col12, #T_dcb1a_row3_col13, #T_dcb1a_row3_col16, #T_dcb1a_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row0_col13, #T_dcb1a_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row1_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #516ddb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row1_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #6384eb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row1_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row1_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row2_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row3_col6, #T_dcb1a_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c1a9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dcb1a_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #d4dbe6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dcb1a_row3_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dcb1a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dcb1a_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_dcb1a_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_dcb1a_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_dcb1a_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_dcb1a_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_dcb1a_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_dcb1a_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_dcb1a_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_dcb1a_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_dcb1a_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_dcb1a_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_dcb1a_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm(chatgpt)/WR*</th>\n",
       "      <th id=\"T_dcb1a_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm(chatgpt)/Len</th>\n",
       "      <th id=\"T_dcb1a_level0_col13\" class=\"col_heading level0 col13\" >MTBench(gpt:4:1106:preview)/Turn-1</th>\n",
       "      <th id=\"T_dcb1a_level0_col14\" class=\"col_heading level0 col14\" >MTBench(gpt:4:1106:preview)/Turn-2</th>\n",
       "      <th id=\"T_dcb1a_level0_col15\" class=\"col_heading level0 col15\" >MTBench(gpt:4:1106:preview)/Rating</th>\n",
       "      <th id=\"T_dcb1a_level0_col16\" class=\"col_heading level0 col16\" >Average</th>\n",
       "      <th id=\"T_dcb1a_level0_col17\" class=\"col_heading level0 col17\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_dcb1a_row0_col0\" class=\"data row0 col0\" >llama-7b_dolly_ep=2</td>\n",
       "      <td id=\"T_dcb1a_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_dcb1a_row0_col2\" class=\"data row0 col2\" >38.5</td>\n",
       "      <td id=\"T_dcb1a_row0_col3\" class=\"data row0 col3\" >38.5</td>\n",
       "      <td id=\"T_dcb1a_row0_col4\" class=\"data row0 col4\" >5.4</td>\n",
       "      <td id=\"T_dcb1a_row0_col5\" class=\"data row0 col5\" >10.8</td>\n",
       "      <td id=\"T_dcb1a_row0_col6\" class=\"data row0 col6\" >34.0</td>\n",
       "      <td id=\"T_dcb1a_row0_col7\" class=\"data row0 col7\" >31.3</td>\n",
       "      <td id=\"T_dcb1a_row0_col8\" class=\"data row0 col8\" >9.0</td>\n",
       "      <td id=\"T_dcb1a_row0_col9\" class=\"data row0 col9\" >44.0</td>\n",
       "      <td id=\"T_dcb1a_row0_col10\" class=\"data row0 col10\" >10.4</td>\n",
       "      <td id=\"T_dcb1a_row0_col11\" class=\"data row0 col11\" >20.1</td>\n",
       "      <td id=\"T_dcb1a_row0_col12\" class=\"data row0 col12\" >319.7</td>\n",
       "      <td id=\"T_dcb1a_row0_col13\" class=\"data row0 col13\" >34.0</td>\n",
       "      <td id=\"T_dcb1a_row0_col14\" class=\"data row0 col14\" >16.8</td>\n",
       "      <td id=\"T_dcb1a_row0_col15\" class=\"data row0 col15\" >25.5</td>\n",
       "      <td id=\"T_dcb1a_row0_col16\" class=\"data row0 col16\" >45.6</td>\n",
       "      <td id=\"T_dcb1a_row0_col17\" class=\"data row0 col17\" >-6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_dcb1a_row1_col0\" class=\"data row1 col0\" >llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_dcb1a_row1_col1\" class=\"data row1 col1\" >10000</td>\n",
       "      <td id=\"T_dcb1a_row1_col2\" class=\"data row1 col2\" >34.7</td>\n",
       "      <td id=\"T_dcb1a_row1_col3\" class=\"data row1 col3\" >37.0</td>\n",
       "      <td id=\"T_dcb1a_row1_col4\" class=\"data row1 col4\" >3.4</td>\n",
       "      <td id=\"T_dcb1a_row1_col5\" class=\"data row1 col5\" >10.0</td>\n",
       "      <td id=\"T_dcb1a_row1_col6\" class=\"data row1 col6\" >30.9</td>\n",
       "      <td id=\"T_dcb1a_row1_col7\" class=\"data row1 col7\" >30.1</td>\n",
       "      <td id=\"T_dcb1a_row1_col8\" class=\"data row1 col8\" >6.4</td>\n",
       "      <td id=\"T_dcb1a_row1_col9\" class=\"data row1 col9\" >35.4</td>\n",
       "      <td id=\"T_dcb1a_row1_col10\" class=\"data row1 col10\" >10.4</td>\n",
       "      <td id=\"T_dcb1a_row1_col11\" class=\"data row1 col11\" >37.5</td>\n",
       "      <td id=\"T_dcb1a_row1_col12\" class=\"data row1 col12\" >298.0</td>\n",
       "      <td id=\"T_dcb1a_row1_col13\" class=\"data row1 col13\" >33.6</td>\n",
       "      <td id=\"T_dcb1a_row1_col14\" class=\"data row1 col14\" >18.1</td>\n",
       "      <td id=\"T_dcb1a_row1_col15\" class=\"data row1 col15\" >25.9</td>\n",
       "      <td id=\"T_dcb1a_row1_col16\" class=\"data row1 col16\" >43.7</td>\n",
       "      <td id=\"T_dcb1a_row1_col17\" class=\"data row1 col17\" >-8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_dcb1a_row2_col0\" class=\"data row2 col0\" >llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_dcb1a_row2_col1\" class=\"data row2 col1\" >10000</td>\n",
       "      <td id=\"T_dcb1a_row2_col2\" class=\"data row2 col2\" >33.3</td>\n",
       "      <td id=\"T_dcb1a_row2_col3\" class=\"data row2 col3\" >37.1</td>\n",
       "      <td id=\"T_dcb1a_row2_col4\" class=\"data row2 col4\" >4.6</td>\n",
       "      <td id=\"T_dcb1a_row2_col5\" class=\"data row2 col5\" >9.4</td>\n",
       "      <td id=\"T_dcb1a_row2_col6\" class=\"data row2 col6\" >31.4</td>\n",
       "      <td id=\"T_dcb1a_row2_col7\" class=\"data row2 col7\" >28.7</td>\n",
       "      <td id=\"T_dcb1a_row2_col8\" class=\"data row2 col8\" >7.3</td>\n",
       "      <td id=\"T_dcb1a_row2_col9\" class=\"data row2 col9\" >35.9</td>\n",
       "      <td id=\"T_dcb1a_row2_col10\" class=\"data row2 col10\" >7.5</td>\n",
       "      <td id=\"T_dcb1a_row2_col11\" class=\"data row2 col11\" >33.9</td>\n",
       "      <td id=\"T_dcb1a_row2_col12\" class=\"data row2 col12\" >172.6</td>\n",
       "      <td id=\"T_dcb1a_row2_col13\" class=\"data row2 col13\" >38.2</td>\n",
       "      <td id=\"T_dcb1a_row2_col14\" class=\"data row2 col14\" >20.0</td>\n",
       "      <td id=\"T_dcb1a_row2_col15\" class=\"data row2 col15\" >29.1</td>\n",
       "      <td id=\"T_dcb1a_row2_col16\" class=\"data row2 col16\" >34.9</td>\n",
       "      <td id=\"T_dcb1a_row2_col17\" class=\"data row2 col17\" >-8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb1a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_dcb1a_row3_col0\" class=\"data row3 col0\" >score=random:s=\\d_pace=prune:size=10000:ep=10_avg (N=1)</td>\n",
       "      <td id=\"T_dcb1a_row3_col1\" class=\"data row3 col1\" >10000</td>\n",
       "      <td id=\"T_dcb1a_row3_col2\" class=\"data row3 col2\" >30.9</td>\n",
       "      <td id=\"T_dcb1a_row3_col3\" class=\"data row3 col3\" >34.8</td>\n",
       "      <td id=\"T_dcb1a_row3_col4\" class=\"data row3 col4\" >5.0</td>\n",
       "      <td id=\"T_dcb1a_row3_col5\" class=\"data row3 col5\" >8.4</td>\n",
       "      <td id=\"T_dcb1a_row3_col6\" class=\"data row3 col6\" >32.9</td>\n",
       "      <td id=\"T_dcb1a_row3_col7\" class=\"data row3 col7\" >25.7</td>\n",
       "      <td id=\"T_dcb1a_row3_col8\" class=\"data row3 col8\" >8.0</td>\n",
       "      <td id=\"T_dcb1a_row3_col9\" class=\"data row3 col9\" >41.0</td>\n",
       "      <td id=\"T_dcb1a_row3_col10\" class=\"data row3 col10\" >7.9</td>\n",
       "      <td id=\"T_dcb1a_row3_col11\" class=\"data row3 col11\" >28.2</td>\n",
       "      <td id=\"T_dcb1a_row3_col12\" class=\"data row3 col12\" >101.5</td>\n",
       "      <td id=\"T_dcb1a_row3_col13\" class=\"data row3 col13\" >33.2</td>\n",
       "      <td id=\"T_dcb1a_row3_col14\" class=\"data row3 col14\" >17.7</td>\n",
       "      <td id=\"T_dcb1a_row3_col15\" class=\"data row3 col15\" >25.5</td>\n",
       "      <td id=\"T_dcb1a_row3_col16\" class=\"data row3 col16\" >28.6</td>\n",
       "      <td id=\"T_dcb1a_row3_col17\" class=\"data row3 col17\" >-10.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0c91ad6f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/3595434990.py:369: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/3595434990.py:375: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_868ca td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_868ca_row0_col0, #T_868ca_row1_col0, #T_868ca_row2_col0, #T_868ca_row3_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_868ca_row0_col1, #T_868ca_row1_col1, #T_868ca_row2_col1, #T_868ca_row3_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_868ca_row0_col2, #T_868ca_row0_col3, #T_868ca_row0_col17, #T_868ca_row1_col17, #T_868ca_row2_col17, #T_868ca_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row0_col4, #T_868ca_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row0_col5, #T_868ca_row0_col7, #T_868ca_row0_col9, #T_868ca_row0_col10, #T_868ca_row0_col13, #T_868ca_row0_col14, #T_868ca_row0_col15, #T_868ca_row0_col16, #T_868ca_row1_col2, #T_868ca_row1_col4, #T_868ca_row1_col6, #T_868ca_row1_col8, #T_868ca_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row0_col6, #T_868ca_row0_col11, #T_868ca_row0_col12, #T_868ca_row1_col10, #T_868ca_row1_col11, #T_868ca_row1_col12, #T_868ca_row1_col14, #T_868ca_row2_col2, #T_868ca_row2_col3, #T_868ca_row2_col7, #T_868ca_row2_col9, #T_868ca_row2_col11, #T_868ca_row2_col12, #T_868ca_row3_col4, #T_868ca_row3_col5, #T_868ca_row3_col8, #T_868ca_row3_col11, #T_868ca_row3_col12, #T_868ca_row3_col13, #T_868ca_row3_col15, #T_868ca_row3_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row1_col3, #T_868ca_row3_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row1_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row2_col5, #T_868ca_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row2_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row2_col13, #T_868ca_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row2_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row2_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_868ca_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_868ca_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_868ca\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_868ca_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_868ca_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_868ca_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_868ca_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_868ca_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_868ca_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_868ca_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_868ca_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_868ca_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_868ca_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_868ca_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_868ca_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm(chatgpt)/WR*</th>\n",
       "      <th id=\"T_868ca_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm(chatgpt)/Len</th>\n",
       "      <th id=\"T_868ca_level0_col13\" class=\"col_heading level0 col13\" >MTBench(gpt:4:1106:preview)/Turn-1</th>\n",
       "      <th id=\"T_868ca_level0_col14\" class=\"col_heading level0 col14\" >MTBench(gpt:4:1106:preview)/Turn-2</th>\n",
       "      <th id=\"T_868ca_level0_col15\" class=\"col_heading level0 col15\" >MTBench(gpt:4:1106:preview)/Rating</th>\n",
       "      <th id=\"T_868ca_level0_col16\" class=\"col_heading level0 col16\" >Average</th>\n",
       "      <th id=\"T_868ca_level0_col17\" class=\"col_heading level0 col17\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_868ca_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_868ca_row0_col0\" class=\"data row0 col0\" >llama-7b_sharegpt50k_ep=2</td>\n",
       "      <td id=\"T_868ca_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_868ca_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_868ca_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "      <td id=\"T_868ca_row0_col4\" class=\"data row0 col4\" >6.4</td>\n",
       "      <td id=\"T_868ca_row0_col5\" class=\"data row0 col5\" >16.0</td>\n",
       "      <td id=\"T_868ca_row0_col6\" class=\"data row0 col6\" >29.9</td>\n",
       "      <td id=\"T_868ca_row0_col7\" class=\"data row0 col7\" >34.4</td>\n",
       "      <td id=\"T_868ca_row0_col8\" class=\"data row0 col8\" >8.0</td>\n",
       "      <td id=\"T_868ca_row0_col9\" class=\"data row0 col9\" >34.9</td>\n",
       "      <td id=\"T_868ca_row0_col10\" class=\"data row0 col10\" >13.2</td>\n",
       "      <td id=\"T_868ca_row0_col11\" class=\"data row0 col11\" >45.8</td>\n",
       "      <td id=\"T_868ca_row0_col12\" class=\"data row0 col12\" >279.4</td>\n",
       "      <td id=\"T_868ca_row0_col13\" class=\"data row0 col13\" >54.4</td>\n",
       "      <td id=\"T_868ca_row0_col14\" class=\"data row0 col14\" >36.0</td>\n",
       "      <td id=\"T_868ca_row0_col15\" class=\"data row0 col15\" >45.3</td>\n",
       "      <td id=\"T_868ca_row0_col16\" class=\"data row0 col16\" >50.3</td>\n",
       "      <td id=\"T_868ca_row0_col17\" class=\"data row0 col17\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_868ca_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_868ca_row1_col0\" class=\"data row1 col0\" >llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_868ca_row1_col1\" class=\"data row1 col1\" >10000</td>\n",
       "      <td id=\"T_868ca_row1_col2\" class=\"data row1 col2\" >32.9</td>\n",
       "      <td id=\"T_868ca_row1_col3\" class=\"data row1 col3\" >34.7</td>\n",
       "      <td id=\"T_868ca_row1_col4\" class=\"data row1 col4\" >6.8</td>\n",
       "      <td id=\"T_868ca_row1_col5\" class=\"data row1 col5\" >12.0</td>\n",
       "      <td id=\"T_868ca_row1_col6\" class=\"data row1 col6\" >35.0</td>\n",
       "      <td id=\"T_868ca_row1_col7\" class=\"data row1 col7\" >31.4</td>\n",
       "      <td id=\"T_868ca_row1_col8\" class=\"data row1 col8\" >8.9</td>\n",
       "      <td id=\"T_868ca_row1_col9\" class=\"data row1 col9\" >32.1</td>\n",
       "      <td id=\"T_868ca_row1_col10\" class=\"data row1 col10\" >8.7</td>\n",
       "      <td id=\"T_868ca_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "      <td id=\"T_868ca_row1_col12\" class=\"data row1 col12\" >nan</td>\n",
       "      <td id=\"T_868ca_row1_col13\" class=\"data row1 col13\" >46.0</td>\n",
       "      <td id=\"T_868ca_row1_col14\" class=\"data row1 col14\" >31.8</td>\n",
       "      <td id=\"T_868ca_row1_col15\" class=\"data row1 col15\" >38.9</td>\n",
       "      <td id=\"T_868ca_row1_col16\" class=\"data row1 col16\" >26.6</td>\n",
       "      <td id=\"T_868ca_row1_col17\" class=\"data row1 col17\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_868ca_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_868ca_row2_col0\" class=\"data row2 col0\" >score=random:s=\\d_pace=prune:size=10000:ep=10_avg (N=1)</td>\n",
       "      <td id=\"T_868ca_row2_col1\" class=\"data row2 col1\" >10000</td>\n",
       "      <td id=\"T_868ca_row2_col2\" class=\"data row2 col2\" >30.9</td>\n",
       "      <td id=\"T_868ca_row2_col3\" class=\"data row2 col3\" >33.0</td>\n",
       "      <td id=\"T_868ca_row2_col4\" class=\"data row2 col4\" >6.4</td>\n",
       "      <td id=\"T_868ca_row2_col5\" class=\"data row2 col5\" >13.6</td>\n",
       "      <td id=\"T_868ca_row2_col6\" class=\"data row2 col6\" >33.5</td>\n",
       "      <td id=\"T_868ca_row2_col7\" class=\"data row2 col7\" >30.7</td>\n",
       "      <td id=\"T_868ca_row2_col8\" class=\"data row2 col8\" >7.3</td>\n",
       "      <td id=\"T_868ca_row2_col9\" class=\"data row2 col9\" >28.3</td>\n",
       "      <td id=\"T_868ca_row2_col10\" class=\"data row2 col10\" >12.2</td>\n",
       "      <td id=\"T_868ca_row2_col11\" class=\"data row2 col11\" >nan</td>\n",
       "      <td id=\"T_868ca_row2_col12\" class=\"data row2 col12\" >nan</td>\n",
       "      <td id=\"T_868ca_row2_col13\" class=\"data row2 col13\" >46.4</td>\n",
       "      <td id=\"T_868ca_row2_col14\" class=\"data row2 col14\" >34.0</td>\n",
       "      <td id=\"T_868ca_row2_col15\" class=\"data row2 col15\" >40.2</td>\n",
       "      <td id=\"T_868ca_row2_col16\" class=\"data row2 col16\" >26.4</td>\n",
       "      <td id=\"T_868ca_row2_col17\" class=\"data row2 col17\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_868ca_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_868ca_row3_col0\" class=\"data row3 col0\" >llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_868ca_row3_col1\" class=\"data row3 col1\" >10000</td>\n",
       "      <td id=\"T_868ca_row3_col2\" class=\"data row3 col2\" >31.7</td>\n",
       "      <td id=\"T_868ca_row3_col3\" class=\"data row3 col3\" >37.0</td>\n",
       "      <td id=\"T_868ca_row3_col4\" class=\"data row3 col4\" >6.0</td>\n",
       "      <td id=\"T_868ca_row3_col5\" class=\"data row3 col5\" >9.6</td>\n",
       "      <td id=\"T_868ca_row3_col6\" class=\"data row3 col6\" >31.2</td>\n",
       "      <td id=\"T_868ca_row3_col7\" class=\"data row3 col7\" >33.1</td>\n",
       "      <td id=\"T_868ca_row3_col8\" class=\"data row3 col8\" >7.0</td>\n",
       "      <td id=\"T_868ca_row3_col9\" class=\"data row3 col9\" >30.0</td>\n",
       "      <td id=\"T_868ca_row3_col10\" class=\"data row3 col10\" >11.8</td>\n",
       "      <td id=\"T_868ca_row3_col11\" class=\"data row3 col11\" >nan</td>\n",
       "      <td id=\"T_868ca_row3_col12\" class=\"data row3 col12\" >nan</td>\n",
       "      <td id=\"T_868ca_row3_col13\" class=\"data row3 col13\" >43.5</td>\n",
       "      <td id=\"T_868ca_row3_col14\" class=\"data row3 col14\" >31.9</td>\n",
       "      <td id=\"T_868ca_row3_col15\" class=\"data row3 col15\" >37.7</td>\n",
       "      <td id=\"T_868ca_row3_col16\" class=\"data row3 col16\" >25.9</td>\n",
       "      <td id=\"T_868ca_row3_col17\" class=\"data row3 col17\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0b5a24f70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/3595434990.py:369: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/3595434990.py:375: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_62e0f td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_62e0f_row0_col0, #T_62e0f_row1_col0, #T_62e0f_row2_col0, #T_62e0f_row3_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_62e0f_row0_col1, #T_62e0f_row1_col1, #T_62e0f_row2_col1, #T_62e0f_row3_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_62e0f_row0_col2, #T_62e0f_row0_col5, #T_62e0f_row0_col8, #T_62e0f_row0_col9, #T_62e0f_row0_col12, #T_62e0f_row0_col16, #T_62e0f_row0_col17, #T_62e0f_row1_col10, #T_62e0f_row1_col14, #T_62e0f_row2_col3, #T_62e0f_row2_col4, #T_62e0f_row2_col7, #T_62e0f_row2_col11, #T_62e0f_row3_col6, #T_62e0f_row3_col13, #T_62e0f_row3_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row0_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row0_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row0_col10, #T_62e0f_row0_col11, #T_62e0f_row0_col13, #T_62e0f_row0_col14, #T_62e0f_row0_col15, #T_62e0f_row1_col2, #T_62e0f_row1_col3, #T_62e0f_row1_col6, #T_62e0f_row1_col7, #T_62e0f_row2_col8, #T_62e0f_row2_col9, #T_62e0f_row2_col10, #T_62e0f_row3_col4, #T_62e0f_row3_col5, #T_62e0f_row3_col7, #T_62e0f_row3_col12, #T_62e0f_row3_col16, #T_62e0f_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row1_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col8, #T_62e0f_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row1_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row1_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row1_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #6282ea;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row2_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row3_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_62e0f_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_62e0f_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_62e0f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_62e0f_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_62e0f_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_62e0f_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_62e0f_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_62e0f_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_62e0f_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_62e0f_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_62e0f_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_62e0f_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_62e0f_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_62e0f_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_62e0f_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm(chatgpt)/WR*</th>\n",
       "      <th id=\"T_62e0f_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm(chatgpt)/Len</th>\n",
       "      <th id=\"T_62e0f_level0_col13\" class=\"col_heading level0 col13\" >MTBench(gpt:4:1106:preview)/Turn-1</th>\n",
       "      <th id=\"T_62e0f_level0_col14\" class=\"col_heading level0 col14\" >MTBench(gpt:4:1106:preview)/Turn-2</th>\n",
       "      <th id=\"T_62e0f_level0_col15\" class=\"col_heading level0 col15\" >MTBench(gpt:4:1106:preview)/Rating</th>\n",
       "      <th id=\"T_62e0f_level0_col16\" class=\"col_heading level0 col16\" >Average</th>\n",
       "      <th id=\"T_62e0f_level0_col17\" class=\"col_heading level0 col17\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_62e0f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_62e0f_row0_col0\" class=\"data row0 col0\" >llama-7b_dolly_ep=2</td>\n",
       "      <td id=\"T_62e0f_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_62e0f_row0_col2\" class=\"data row0 col2\" >38.5</td>\n",
       "      <td id=\"T_62e0f_row0_col3\" class=\"data row0 col3\" >38.5</td>\n",
       "      <td id=\"T_62e0f_row0_col4\" class=\"data row0 col4\" >5.4</td>\n",
       "      <td id=\"T_62e0f_row0_col5\" class=\"data row0 col5\" >10.8</td>\n",
       "      <td id=\"T_62e0f_row0_col6\" class=\"data row0 col6\" >34.0</td>\n",
       "      <td id=\"T_62e0f_row0_col7\" class=\"data row0 col7\" >31.3</td>\n",
       "      <td id=\"T_62e0f_row0_col8\" class=\"data row0 col8\" >9.0</td>\n",
       "      <td id=\"T_62e0f_row0_col9\" class=\"data row0 col9\" >44.0</td>\n",
       "      <td id=\"T_62e0f_row0_col10\" class=\"data row0 col10\" >10.4</td>\n",
       "      <td id=\"T_62e0f_row0_col11\" class=\"data row0 col11\" >20.1</td>\n",
       "      <td id=\"T_62e0f_row0_col12\" class=\"data row0 col12\" >319.7</td>\n",
       "      <td id=\"T_62e0f_row0_col13\" class=\"data row0 col13\" >34.0</td>\n",
       "      <td id=\"T_62e0f_row0_col14\" class=\"data row0 col14\" >16.8</td>\n",
       "      <td id=\"T_62e0f_row0_col15\" class=\"data row0 col15\" >25.5</td>\n",
       "      <td id=\"T_62e0f_row0_col16\" class=\"data row0 col16\" >45.6</td>\n",
       "      <td id=\"T_62e0f_row0_col17\" class=\"data row0 col17\" >-6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62e0f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_62e0f_row1_col0\" class=\"data row1 col0\" >llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_62e0f_row1_col1\" class=\"data row1 col1\" >30000</td>\n",
       "      <td id=\"T_62e0f_row1_col2\" class=\"data row1 col2\" >36.1</td>\n",
       "      <td id=\"T_62e0f_row1_col3\" class=\"data row1 col3\" >35.0</td>\n",
       "      <td id=\"T_62e0f_row1_col4\" class=\"data row1 col4\" >4.4</td>\n",
       "      <td id=\"T_62e0f_row1_col5\" class=\"data row1 col5\" >10.2</td>\n",
       "      <td id=\"T_62e0f_row1_col6\" class=\"data row1 col6\" >31.2</td>\n",
       "      <td id=\"T_62e0f_row1_col7\" class=\"data row1 col7\" >30.3</td>\n",
       "      <td id=\"T_62e0f_row1_col8\" class=\"data row1 col8\" >8.6</td>\n",
       "      <td id=\"T_62e0f_row1_col9\" class=\"data row1 col9\" >42.1</td>\n",
       "      <td id=\"T_62e0f_row1_col10\" class=\"data row1 col10\" >11.6</td>\n",
       "      <td id=\"T_62e0f_row1_col11\" class=\"data row1 col11\" >26.7</td>\n",
       "      <td id=\"T_62e0f_row1_col12\" class=\"data row1 col12\" >277.6</td>\n",
       "      <td id=\"T_62e0f_row1_col13\" class=\"data row1 col13\" >35.8</td>\n",
       "      <td id=\"T_62e0f_row1_col14\" class=\"data row1 col14\" >19.7</td>\n",
       "      <td id=\"T_62e0f_row1_col15\" class=\"data row1 col15\" >27.8</td>\n",
       "      <td id=\"T_62e0f_row1_col16\" class=\"data row1 col16\" >42.6</td>\n",
       "      <td id=\"T_62e0f_row1_col17\" class=\"data row1 col17\" >-7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62e0f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_62e0f_row2_col0\" class=\"data row2 col0\" >llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_62e0f_row2_col1\" class=\"data row2 col1\" >30000</td>\n",
       "      <td id=\"T_62e0f_row2_col2\" class=\"data row2 col2\" >36.6</td>\n",
       "      <td id=\"T_62e0f_row2_col3\" class=\"data row2 col3\" >39.2</td>\n",
       "      <td id=\"T_62e0f_row2_col4\" class=\"data row2 col4\" >5.6</td>\n",
       "      <td id=\"T_62e0f_row2_col5\" class=\"data row2 col5\" >9.6</td>\n",
       "      <td id=\"T_62e0f_row2_col6\" class=\"data row2 col6\" >33.5</td>\n",
       "      <td id=\"T_62e0f_row2_col7\" class=\"data row2 col7\" >33.1</td>\n",
       "      <td id=\"T_62e0f_row2_col8\" class=\"data row2 col8\" >7.5</td>\n",
       "      <td id=\"T_62e0f_row2_col9\" class=\"data row2 col9\" >37.7</td>\n",
       "      <td id=\"T_62e0f_row2_col10\" class=\"data row2 col10\" >10.4</td>\n",
       "      <td id=\"T_62e0f_row2_col11\" class=\"data row2 col11\" >28.1</td>\n",
       "      <td id=\"T_62e0f_row2_col12\" class=\"data row2 col12\" >263.1</td>\n",
       "      <td id=\"T_62e0f_row2_col13\" class=\"data row2 col13\" >38.5</td>\n",
       "      <td id=\"T_62e0f_row2_col14\" class=\"data row2 col14\" >19.5</td>\n",
       "      <td id=\"T_62e0f_row2_col15\" class=\"data row2 col15\" >29.1</td>\n",
       "      <td id=\"T_62e0f_row2_col16\" class=\"data row2 col16\" >42.3</td>\n",
       "      <td id=\"T_62e0f_row2_col17\" class=\"data row2 col17\" >-6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62e0f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_62e0f_row3_col0\" class=\"data row3 col0\" >llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_62e0f_row3_col1\" class=\"data row3 col1\" >30000</td>\n",
       "      <td id=\"T_62e0f_row3_col2\" class=\"data row3 col2\" >36.9</td>\n",
       "      <td id=\"T_62e0f_row3_col3\" class=\"data row3 col3\" >37.1</td>\n",
       "      <td id=\"T_62e0f_row3_col4\" class=\"data row3 col4\" >3.0</td>\n",
       "      <td id=\"T_62e0f_row3_col5\" class=\"data row3 col5\" >9.4</td>\n",
       "      <td id=\"T_62e0f_row3_col6\" class=\"data row3 col6\" >34.1</td>\n",
       "      <td id=\"T_62e0f_row3_col7\" class=\"data row3 col7\" >30.3</td>\n",
       "      <td id=\"T_62e0f_row3_col8\" class=\"data row3 col8\" >8.0</td>\n",
       "      <td id=\"T_62e0f_row3_col9\" class=\"data row3 col9\" >40.9</td>\n",
       "      <td id=\"T_62e0f_row3_col10\" class=\"data row3 col10\" >10.6</td>\n",
       "      <td id=\"T_62e0f_row3_col11\" class=\"data row3 col11\" >26.5</td>\n",
       "      <td id=\"T_62e0f_row3_col12\" class=\"data row3 col12\" >260.5</td>\n",
       "      <td id=\"T_62e0f_row3_col13\" class=\"data row3 col13\" >40.1</td>\n",
       "      <td id=\"T_62e0f_row3_col14\" class=\"data row3 col14\" >18.2</td>\n",
       "      <td id=\"T_62e0f_row3_col15\" class=\"data row3 col15\" >29.4</td>\n",
       "      <td id=\"T_62e0f_row3_col16\" class=\"data row3 col16\" >41.8</td>\n",
       "      <td id=\"T_62e0f_row3_col17\" class=\"data row3 col17\" >-7.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0c91ae050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/3595434990.py:369: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/3595434990.py:375: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a3775 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_a3775_row0_col0, #T_a3775_row1_col0, #T_a3775_row2_col0, #T_a3775_row3_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a3775_row0_col1, #T_a3775_row1_col1, #T_a3775_row2_col1, #T_a3775_row3_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a3775_row0_col2, #T_a3775_row0_col3, #T_a3775_row0_col17, #T_a3775_row1_col17, #T_a3775_row2_col17, #T_a3775_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row0_col4, #T_a3775_row0_col5, #T_a3775_row0_col9, #T_a3775_row0_col13, #T_a3775_row0_col16, #T_a3775_row1_col2, #T_a3775_row1_col3, #T_a3775_row1_col14, #T_a3775_row1_col15, #T_a3775_row2_col7, #T_a3775_row3_col6, #T_a3775_row3_col8, #T_a3775_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row0_col6, #T_a3775_row0_col11, #T_a3775_row0_col12, #T_a3775_row1_col7, #T_a3775_row1_col10, #T_a3775_row1_col11, #T_a3775_row1_col12, #T_a3775_row2_col8, #T_a3775_row2_col9, #T_a3775_row2_col11, #T_a3775_row2_col12, #T_a3775_row3_col2, #T_a3775_row3_col3, #T_a3775_row3_col4, #T_a3775_row3_col5, #T_a3775_row3_col11, #T_a3775_row3_col12, #T_a3775_row3_col13, #T_a3775_row3_col14, #T_a3775_row3_col15, #T_a3775_row3_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row0_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row0_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row0_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #6282ea;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row0_col15, #T_a3775_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row1_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row1_col8, #T_a3775_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row1_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row2_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row2_col10, #T_a3775_row2_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row2_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3775_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3775_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a3775\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a3775_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_a3775_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_a3775_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_a3775_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_a3775_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_a3775_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_a3775_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_a3775_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_a3775_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_a3775_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_a3775_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_a3775_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm(chatgpt)/WR*</th>\n",
       "      <th id=\"T_a3775_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm(chatgpt)/Len</th>\n",
       "      <th id=\"T_a3775_level0_col13\" class=\"col_heading level0 col13\" >MTBench(gpt:4:1106:preview)/Turn-1</th>\n",
       "      <th id=\"T_a3775_level0_col14\" class=\"col_heading level0 col14\" >MTBench(gpt:4:1106:preview)/Turn-2</th>\n",
       "      <th id=\"T_a3775_level0_col15\" class=\"col_heading level0 col15\" >MTBench(gpt:4:1106:preview)/Rating</th>\n",
       "      <th id=\"T_a3775_level0_col16\" class=\"col_heading level0 col16\" >Average</th>\n",
       "      <th id=\"T_a3775_level0_col17\" class=\"col_heading level0 col17\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a3775_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a3775_row0_col0\" class=\"data row0 col0\" >llama-7b_sharegpt50k_ep=2</td>\n",
       "      <td id=\"T_a3775_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_a3775_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_a3775_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "      <td id=\"T_a3775_row0_col4\" class=\"data row0 col4\" >6.4</td>\n",
       "      <td id=\"T_a3775_row0_col5\" class=\"data row0 col5\" >16.0</td>\n",
       "      <td id=\"T_a3775_row0_col6\" class=\"data row0 col6\" >29.9</td>\n",
       "      <td id=\"T_a3775_row0_col7\" class=\"data row0 col7\" >34.4</td>\n",
       "      <td id=\"T_a3775_row0_col8\" class=\"data row0 col8\" >8.0</td>\n",
       "      <td id=\"T_a3775_row0_col9\" class=\"data row0 col9\" >34.9</td>\n",
       "      <td id=\"T_a3775_row0_col10\" class=\"data row0 col10\" >13.2</td>\n",
       "      <td id=\"T_a3775_row0_col11\" class=\"data row0 col11\" >45.8</td>\n",
       "      <td id=\"T_a3775_row0_col12\" class=\"data row0 col12\" >279.4</td>\n",
       "      <td id=\"T_a3775_row0_col13\" class=\"data row0 col13\" >54.4</td>\n",
       "      <td id=\"T_a3775_row0_col14\" class=\"data row0 col14\" >36.0</td>\n",
       "      <td id=\"T_a3775_row0_col15\" class=\"data row0 col15\" >45.3</td>\n",
       "      <td id=\"T_a3775_row0_col16\" class=\"data row0 col16\" >50.3</td>\n",
       "      <td id=\"T_a3775_row0_col17\" class=\"data row0 col17\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3775_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a3775_row1_col0\" class=\"data row1 col0\" >llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_a3775_row1_col1\" class=\"data row1 col1\" >30000</td>\n",
       "      <td id=\"T_a3775_row1_col2\" class=\"data row1 col2\" >37.9</td>\n",
       "      <td id=\"T_a3775_row1_col3\" class=\"data row1 col3\" >37.7</td>\n",
       "      <td id=\"T_a3775_row1_col4\" class=\"data row1 col4\" >6.0</td>\n",
       "      <td id=\"T_a3775_row1_col5\" class=\"data row1 col5\" >13.0</td>\n",
       "      <td id=\"T_a3775_row1_col6\" class=\"data row1 col6\" >30.6</td>\n",
       "      <td id=\"T_a3775_row1_col7\" class=\"data row1 col7\" >33.9</td>\n",
       "      <td id=\"T_a3775_row1_col8\" class=\"data row1 col8\" >8.7</td>\n",
       "      <td id=\"T_a3775_row1_col9\" class=\"data row1 col9\" >33.6</td>\n",
       "      <td id=\"T_a3775_row1_col10\" class=\"data row1 col10\" >11.4</td>\n",
       "      <td id=\"T_a3775_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "      <td id=\"T_a3775_row1_col12\" class=\"data row1 col12\" >nan</td>\n",
       "      <td id=\"T_a3775_row1_col13\" class=\"data row1 col13\" >53.1</td>\n",
       "      <td id=\"T_a3775_row1_col14\" class=\"data row1 col14\" >40.5</td>\n",
       "      <td id=\"T_a3775_row1_col15\" class=\"data row1 col15\" >46.9</td>\n",
       "      <td id=\"T_a3775_row1_col16\" class=\"data row1 col16\" >29.4</td>\n",
       "      <td id=\"T_a3775_row1_col17\" class=\"data row1 col17\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3775_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a3775_row2_col0\" class=\"data row2 col0\" >llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_a3775_row2_col1\" class=\"data row2 col1\" >30000</td>\n",
       "      <td id=\"T_a3775_row2_col2\" class=\"data row2 col2\" >35.0</td>\n",
       "      <td id=\"T_a3775_row2_col3\" class=\"data row2 col3\" >35.7</td>\n",
       "      <td id=\"T_a3775_row2_col4\" class=\"data row2 col4\" >5.8</td>\n",
       "      <td id=\"T_a3775_row2_col5\" class=\"data row2 col5\" >14.4</td>\n",
       "      <td id=\"T_a3775_row2_col6\" class=\"data row2 col6\" >32.8</td>\n",
       "      <td id=\"T_a3775_row2_col7\" class=\"data row2 col7\" >34.5</td>\n",
       "      <td id=\"T_a3775_row2_col8\" class=\"data row2 col8\" >7.5</td>\n",
       "      <td id=\"T_a3775_row2_col9\" class=\"data row2 col9\" >31.3</td>\n",
       "      <td id=\"T_a3775_row2_col10\" class=\"data row2 col10\" >12.8</td>\n",
       "      <td id=\"T_a3775_row2_col11\" class=\"data row2 col11\" >nan</td>\n",
       "      <td id=\"T_a3775_row2_col12\" class=\"data row2 col12\" >nan</td>\n",
       "      <td id=\"T_a3775_row2_col13\" class=\"data row2 col13\" >48.6</td>\n",
       "      <td id=\"T_a3775_row2_col14\" class=\"data row2 col14\" >39.2</td>\n",
       "      <td id=\"T_a3775_row2_col15\" class=\"data row2 col15\" >44.0</td>\n",
       "      <td id=\"T_a3775_row2_col16\" class=\"data row2 col16\" >28.5</td>\n",
       "      <td id=\"T_a3775_row2_col17\" class=\"data row2 col17\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3775_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a3775_row3_col0\" class=\"data row3 col0\" >llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_a3775_row3_col1\" class=\"data row3 col1\" >30000</td>\n",
       "      <td id=\"T_a3775_row3_col2\" class=\"data row3 col2\" >33.9</td>\n",
       "      <td id=\"T_a3775_row3_col3\" class=\"data row3 col3\" >35.2</td>\n",
       "      <td id=\"T_a3775_row3_col4\" class=\"data row3 col4\" >5.2</td>\n",
       "      <td id=\"T_a3775_row3_col5\" class=\"data row3 col5\" >11.0</td>\n",
       "      <td id=\"T_a3775_row3_col6\" class=\"data row3 col6\" >34.1</td>\n",
       "      <td id=\"T_a3775_row3_col7\" class=\"data row3 col7\" >34.2</td>\n",
       "      <td id=\"T_a3775_row3_col8\" class=\"data row3 col8\" >8.9</td>\n",
       "      <td id=\"T_a3775_row3_col9\" class=\"data row3 col9\" >32.8</td>\n",
       "      <td id=\"T_a3775_row3_col10\" class=\"data row3 col10\" >14.0</td>\n",
       "      <td id=\"T_a3775_row3_col11\" class=\"data row3 col11\" >nan</td>\n",
       "      <td id=\"T_a3775_row3_col12\" class=\"data row3 col12\" >nan</td>\n",
       "      <td id=\"T_a3775_row3_col13\" class=\"data row3 col13\" >45.9</td>\n",
       "      <td id=\"T_a3775_row3_col14\" class=\"data row3 col14\" >35.4</td>\n",
       "      <td id=\"T_a3775_row3_col15\" class=\"data row3 col15\" >40.6</td>\n",
       "      <td id=\"T_a3775_row3_col16\" class=\"data row3 col16\" >27.6</td>\n",
       "      <td id=\"T_a3775_row3_col17\" class=\"data row3 col17\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0b5a27c40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/3595434990.py:369: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/3595434990.py:375: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e51c0 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_e51c0_row0_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e51c0_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e51c0_row0_col2, #T_e51c0_row0_col3, #T_e51c0_row0_col4, #T_e51c0_row0_col5, #T_e51c0_row0_col6, #T_e51c0_row0_col7, #T_e51c0_row0_col8, #T_e51c0_row0_col9, #T_e51c0_row0_col10, #T_e51c0_row0_col11, #T_e51c0_row0_col12, #T_e51c0_row0_col13, #T_e51c0_row0_col14, #T_e51c0_row0_col15, #T_e51c0_row0_col16, #T_e51c0_row0_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e51c0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e51c0_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_e51c0_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_e51c0_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_e51c0_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_e51c0_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_e51c0_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_e51c0_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_e51c0_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_e51c0_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_e51c0_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_e51c0_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_e51c0_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm(chatgpt)/WR*</th>\n",
       "      <th id=\"T_e51c0_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm(chatgpt)/Len</th>\n",
       "      <th id=\"T_e51c0_level0_col13\" class=\"col_heading level0 col13\" >MTBench(gpt:4:1106:preview)/Turn-1</th>\n",
       "      <th id=\"T_e51c0_level0_col14\" class=\"col_heading level0 col14\" >MTBench(gpt:4:1106:preview)/Turn-2</th>\n",
       "      <th id=\"T_e51c0_level0_col15\" class=\"col_heading level0 col15\" >MTBench(gpt:4:1106:preview)/Rating</th>\n",
       "      <th id=\"T_e51c0_level0_col16\" class=\"col_heading level0 col16\" >Average</th>\n",
       "      <th id=\"T_e51c0_level0_col17\" class=\"col_heading level0 col17\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e51c0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e51c0_row0_col0\" class=\"data row0 col0\" >llama-7b_dolly_ep=2</td>\n",
       "      <td id=\"T_e51c0_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_e51c0_row0_col2\" class=\"data row0 col2\" >38.5</td>\n",
       "      <td id=\"T_e51c0_row0_col3\" class=\"data row0 col3\" >38.5</td>\n",
       "      <td id=\"T_e51c0_row0_col4\" class=\"data row0 col4\" >5.4</td>\n",
       "      <td id=\"T_e51c0_row0_col5\" class=\"data row0 col5\" >10.8</td>\n",
       "      <td id=\"T_e51c0_row0_col6\" class=\"data row0 col6\" >34.0</td>\n",
       "      <td id=\"T_e51c0_row0_col7\" class=\"data row0 col7\" >31.3</td>\n",
       "      <td id=\"T_e51c0_row0_col8\" class=\"data row0 col8\" >9.0</td>\n",
       "      <td id=\"T_e51c0_row0_col9\" class=\"data row0 col9\" >44.0</td>\n",
       "      <td id=\"T_e51c0_row0_col10\" class=\"data row0 col10\" >10.4</td>\n",
       "      <td id=\"T_e51c0_row0_col11\" class=\"data row0 col11\" >20.1</td>\n",
       "      <td id=\"T_e51c0_row0_col12\" class=\"data row0 col12\" >319.7</td>\n",
       "      <td id=\"T_e51c0_row0_col13\" class=\"data row0 col13\" >34.0</td>\n",
       "      <td id=\"T_e51c0_row0_col14\" class=\"data row0 col14\" >16.8</td>\n",
       "      <td id=\"T_e51c0_row0_col15\" class=\"data row0 col15\" >25.5</td>\n",
       "      <td id=\"T_e51c0_row0_col16\" class=\"data row0 col16\" >45.6</td>\n",
       "      <td id=\"T_e51c0_row0_col17\" class=\"data row0 col17\" >-6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0c91ae050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/3595434990.py:369: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/3595434990.py:375: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_09f62 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_09f62_row0_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_09f62_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_09f62_row0_col2, #T_09f62_row0_col3, #T_09f62_row0_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_09f62_row0_col4, #T_09f62_row0_col5, #T_09f62_row0_col6, #T_09f62_row0_col7, #T_09f62_row0_col8, #T_09f62_row0_col9, #T_09f62_row0_col10, #T_09f62_row0_col11, #T_09f62_row0_col12, #T_09f62_row0_col13, #T_09f62_row0_col14, #T_09f62_row0_col15, #T_09f62_row0_col16 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_09f62\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_09f62_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_09f62_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_09f62_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_09f62_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_09f62_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_09f62_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_09f62_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_09f62_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_09f62_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_09f62_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_09f62_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_09f62_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm(chatgpt)/WR*</th>\n",
       "      <th id=\"T_09f62_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm(chatgpt)/Len</th>\n",
       "      <th id=\"T_09f62_level0_col13\" class=\"col_heading level0 col13\" >MTBench(gpt:4:1106:preview)/Turn-1</th>\n",
       "      <th id=\"T_09f62_level0_col14\" class=\"col_heading level0 col14\" >MTBench(gpt:4:1106:preview)/Turn-2</th>\n",
       "      <th id=\"T_09f62_level0_col15\" class=\"col_heading level0 col15\" >MTBench(gpt:4:1106:preview)/Rating</th>\n",
       "      <th id=\"T_09f62_level0_col16\" class=\"col_heading level0 col16\" >Average</th>\n",
       "      <th id=\"T_09f62_level0_col17\" class=\"col_heading level0 col17\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_09f62_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_09f62_row0_col0\" class=\"data row0 col0\" >llama-7b_sharegpt50k_ep=2</td>\n",
       "      <td id=\"T_09f62_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_09f62_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_09f62_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "      <td id=\"T_09f62_row0_col4\" class=\"data row0 col4\" >6.4</td>\n",
       "      <td id=\"T_09f62_row0_col5\" class=\"data row0 col5\" >16.0</td>\n",
       "      <td id=\"T_09f62_row0_col6\" class=\"data row0 col6\" >29.9</td>\n",
       "      <td id=\"T_09f62_row0_col7\" class=\"data row0 col7\" >34.4</td>\n",
       "      <td id=\"T_09f62_row0_col8\" class=\"data row0 col8\" >8.0</td>\n",
       "      <td id=\"T_09f62_row0_col9\" class=\"data row0 col9\" >34.9</td>\n",
       "      <td id=\"T_09f62_row0_col10\" class=\"data row0 col10\" >13.2</td>\n",
       "      <td id=\"T_09f62_row0_col11\" class=\"data row0 col11\" >45.8</td>\n",
       "      <td id=\"T_09f62_row0_col12\" class=\"data row0 col12\" >279.4</td>\n",
       "      <td id=\"T_09f62_row0_col13\" class=\"data row0 col13\" >54.4</td>\n",
       "      <td id=\"T_09f62_row0_col14\" class=\"data row0 col14\" >36.0</td>\n",
       "      <td id=\"T_09f62_row0_col15\" class=\"data row0 col15\" >45.3</td>\n",
       "      <td id=\"T_09f62_row0_col16\" class=\"data row0 col16\" >50.3</td>\n",
       "      <td id=\"T_09f62_row0_col17\" class=\"data row0 col17\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0c91ad330>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# sort_rows = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b_12.13update_before', '../results/baselines/huggyllama/llama-7b_12.13update_before/'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "# #     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "# #     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "# #     # llama2\n",
    "#     ('llama2-7b_12.13update_before', '../results/baselines/NousResearch/Llama-2-7b-hf_12.13update_before/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b-chat', '../results/baselines/NousResearch/Llama-2-7b-chat-hf/'),\n",
    "# #     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "# #     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "# #     # mistral\n",
    "# #     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct-v0.1_12.13update_before', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1_12.13update_before'),\n",
    "#     ('mistral-7b-Instruct-v0.1', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     # zephyr\n",
    "#     ('zephyr-7b-beta_12.13update_before', '../results/baselines/HuggingFaceH4/zephyr-7b-beta_12.13update_before'),\n",
    "#     ('zephyr-7b-beta', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "\n",
    "# # baselines\n",
    "# save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b+lima_ep=2', '../results/ft1_ep=2/llama-7b_lima/'),\n",
    "# #     ('mistral-7b+lima_ep=2', '../results/ft1_ep=2/mistral-7b_lima/'), \n",
    "# ]\n",
    "# exp_dir = '../results/oi2/'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# # save_dirs = [\n",
    "# #     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "# #     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# # ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'tuluv1m' in x]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# # exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# # exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# ###### ultrachat\n",
    "# save_dirs = [\n",
    "#     # baselines \n",
    "#     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "#     ('mistral-7b_ultrachat200k_aftersplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k'),\n",
    "#     ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "    \n",
    "#     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "#     ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "#     ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-alpha'),\n",
    "#     ('mistral-7b-sft-beta+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "# # exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_ultrachat200k:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# exp_dir = '../results/oi5_ultrachat15:mistral-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "# dataset = 'stanford_alpaca'\n",
    "# dataset = 'open_orca_slim'\n",
    "# dataset = 'sharegptv2'\n",
    "# dataset = 'ultrachat200kv2'\n",
    "# dataset = 'wizardlm'\n",
    "# dataset = 'wizardlmv2'\n",
    "# dataset = 'tulu_v2'\n",
    "# dataset = 'flan_v2'\n",
    "# dataset = 'oasst1'\n",
    "# dataset = 'dolly'\n",
    "# dataset_list = [\n",
    "#     'dolly',\n",
    "#     'oasst1', \n",
    "#     'flan_v2', \n",
    "#     'stanford_alpaca', \n",
    "#     'wizardlmv2', \n",
    "#     'sharegptv2', \n",
    "#     'ultrachat200kv2',\n",
    "# ]; finetune_type = 'sft'\n",
    "dataset_list = [\n",
    "    'dolly',\n",
    "#     'oasst1', \n",
    "#     'flan_v250k', \n",
    "#     'stanford_alpaca50k', \n",
    "#     'wizardlm50k', \n",
    "    'sharegpt50k', \n",
    "#     'ultrachat50k',\n",
    "]; finetune_type = 'sft'\n",
    "# dataset_list = [\n",
    "#     'ultrafeedback',\n",
    "#     'ultrafeedbackfull',\n",
    "# ]; finetune_type = 'pref'\n",
    "\n",
    "## older\n",
    "# dataset = 'tulu_v1_mix'\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b'),\n",
    "#     ('llama-7b_lima_ep=5', '../results/oi2/llama-7b_lima_ep=5/'),\n",
    "#     ('llama-7b_lima_ep=10', '../results/oi2/llama-7b_lima_ep=10/'),\n",
    "]\n",
    "for dataset in dataset_list:\n",
    "    if dataset == 'tulu_v2':\n",
    "        save_dirs += [('llama-7b_tulu_v2:100k_ep=2', '../results/oi2/llama-7b_tulu_v2:100k_ep=2'),]\n",
    "    elif dataset == 'open_orca_slim':\n",
    "        save_dirs += [('llama-7b_openorcaslim:100k_ep=2', '../results/oi2/llama-7b_openorcaslim:100k_ep=2'),]\n",
    "    elif dataset == 'sharegptv2':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_sharegptv2_ep=2', '../results/oi2/llama-7b_sharegptv2_ep=2'),\n",
    "            ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),]\n",
    "    elif dataset == 'tulu_v1_mix':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "            # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "            # ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "        ]\n",
    "    elif dataset == 'ultrafeedback':\n",
    "        exp_dir = f'../results/dpo1/'\n",
    "        save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "    else:\n",
    "        save_dirs += [(f'llama-7b_{dataset}_ep=2', f'../results/oi2/llama-7b_{dataset}_ep=2'),]\n",
    "    \n",
    "    if finetune_type == 'sft':\n",
    "        exp_dir = f'../results/oi5_{dataset}:llama-7b'\n",
    "    elif finetune_type == 'pref':\n",
    "        exp_dir = f'../results/dpo2_{dataset}:llama-7b+sharegptv2ep2/'\n",
    "    save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'dppmapbd' not in x and 'semdedup' not in x]\n",
    "    \n",
    "    if finetune_type == 'pref':\n",
    "        sft_model_dataset = 'sharegptv2'\n",
    "        save_dirs += [('llama-7b_sharegptv2_ep=2', f'../results/oi2/llama-7b_{sft_model_dataset}_ep=2')]\n",
    "# ## just compare dppmap grad vs. text\n",
    "# save_dirs = [x for x in save_dirs if 'prune:size=10000:ep=10' in x[1] and (\n",
    "#         'random' in x[1] or \n",
    "#         'dppmap' in x[1]\n",
    "#     )\n",
    "# ]\n",
    "#####\n",
    "\n",
    "\n",
    "# ##### code instructions\n",
    "# save_dirs = [\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('codellama-7b', '../results/baselines/codellama/CodeLlama-7b-hf/'),\n",
    "#     ('codellama-7b-instruct', '../results/baselines/codellama/CodeLlama-7b-Python-hf/'),\n",
    "#     ('codellama-7b-python', '../results/baselines/codellama/CodeLlama-7b-Instruct-hf/'),\n",
    "# ]\n",
    "\n",
    "# exp_dir = '../results/oi2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'starcoder' in x]\n",
    "# exp_dir = '../results/oi6_starcoder_ep=5'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstr:codellama-7b'\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstrv2:codellama-7b'\n",
    "# exp_dir = '../results/oi5_starcoder_commentinstrv5:codellama-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "###### \n",
    "\n",
    "from llm.evaluate import detect_oom_evals\n",
    "oom_eval_paths = detect_oom_evals([x for l in [glob.glob(os.path.join(x[1], 'eval/*/*.out')) for x in save_dirs] for x in l])\n",
    "if oom_eval_paths: print(oom_eval_paths)\n",
    "    \n",
    "cols_avg_blacklist = ['AlpacaFarm/Len']\n",
    "\n",
    "# chat_fmt = False\n",
    "chat_fmt = True\n",
    "chat_fmt = 'both'\n",
    "# chat_fmt = 'auto' # base model no chatfmt, tuned model with chatfmt\n",
    "chat_fmt = 'mix'  # non-alpacaeval no chatfmt, alpacaeval chatfmt\n",
    "alpacafarm_judge = 'chatgpt'\n",
    "mtbench_judge = 'gpt:4:1106:preview'\n",
    "\n",
    "ft_args_fields = {\n",
    "    'run_name': ('run_name',),\n",
    "    'model_name_or_path': ('model_args.model_name_or_path', 'model_name_or_path'),\n",
    "    'subsample_mixture': ('data_args.subsample_mixture',),\n",
    "    'max_train_samples': ('data_args.max_train_samples', 'max_train_samples'),\n",
    "    'train_file': ('data_args.train_file', 'train_file'),\n",
    "}\n",
    "\n",
    "cols = []\n",
    "# cols += [f'AlpacaFarm({alpacafarm_judge})/WR',  f'AlpacaFarm({alpacafarm_judge})/ΔWR', f'AlpacaFarm({alpacafarm_judge})/Rep', f'AlpacaFarm({alpacafarm_judge})/Len']\n",
    "cols += ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] \n",
    "cols += [f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/Len']\n",
    "cols += [f'MTBench({mtbench_judge})/Turn-1',  f'MTBench({mtbench_judge})/Turn-2', f'MTBench({mtbench_judge})/Rating']\n",
    "# cols += [f'MTBench({mtbench_judge})/Turn-1']\n",
    "\n",
    "\n",
    "\n",
    "# cols = [f'BBH {x}' for x in ['reasoning', 'nlu', 'knowledge', 'multilingual']]; cols = [x+'/Direct' for x in cols] + [x+'/CoT' for x in cols]\n",
    "# cols = [f'MMLU {x}' for x in ['STEM', 'humanities', 'social sciences', 'other']]; cols = [x+'/0-shot' for x in cols] + [x+'/5-shot' for x in cols]\n",
    "\n",
    "if 'open_orca_slim' in exp_dir:\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'BBH/Direct', 'BBH/CoT']\n",
    "    cols = ['MMLU/0-shot', 'BBH/Direct']\n",
    "if 'starcoder' in exp_dir:\n",
    "    cols = ['Codex-Eval/Pass@1']\n",
    "    chat_fmt = 'both'\n",
    "print(f'chat_fmt={chat_fmt}')\n",
    "df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "\n",
    "cols = [x for x in cols if x in df.columns]\n",
    "df = df[list(ft_args_fields.keys()) + cols]\n",
    "if chat_fmt == 'both':\n",
    "    for col_lvl2 in ['', 'chatfmt']:\n",
    "        df[('Average', col_lvl2)] = df[list(set(df.columns) & set([(x, col_lvl2) for x in list(set(cols)-set(cols_avg_blacklist))]))].mean(axis=1)\n",
    "else:\n",
    "    df['Average'] = df[list(set(cols)-set(cols_avg_blacklist))].mean(axis=1)\n",
    "if sort_rows:\n",
    "    df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "    sort_value_col, sort_value_col_ascending = ('Average', 'chatfmt') if chat_fmt=='both' else 'Average', False\n",
    "#     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "    df = df.sort_values(by=sort_value_col, ascending=sort_value_col_ascending)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def compute_total_train_samples(x):\n",
    "    match = re.search(r'size=(\\d+)', x['run_name' if chat_fmt!='both' else ('run_name', '')])\n",
    "    total_train_samples = match.group(1) if match else None\n",
    "    return total_train_samples\n",
    "df.insert(1, 'total_train_samples' if chat_fmt!='both' else ('total_train_samples', ''), df.apply(compute_total_train_samples, axis=1))\n",
    "def extract_dataset_from_train_file(x):\n",
    "    if x is None: return None\n",
    "    x = x.split('/')[-1].split('.jsonl')[0]\n",
    "    if x.endswith('_data'): x = x[:-5]\n",
    "    if x.endswith('_train'): x = x[:-6]\n",
    "    return x\n",
    "df.insert(1, 'dataset' if chat_fmt!='both' else ('dataset', ''), df['train_file'].apply(extract_dataset_from_train_file))\n",
    "df = df.drop('train_file', axis=1)\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_name_or_path', 'subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral', 'zephyr']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x.lower())]\n",
    "        if not len(dfc): continue\n",
    "        from rosemary import pd_average_col_contains_substr\n",
    "        Ns = sorted(np.unique([int(x) for x in df['total_train_samples'].to_numpy() if x]).tolist())\n",
    "        datasets = sorted(np.unique([x for x in df['dataset'] if x is not None]).tolist())\n",
    "        for N in Ns+[None]:\n",
    "            for dataset in datasets:\n",
    "                dfc = df.copy()\n",
    "                dfc = dfc[dfc['total_train_samples'].apply(lambda x: int(x) == N if x else True)]\n",
    "                dfc = dfc[dfc['dataset'].apply(lambda x: x == dataset if x else True)]\n",
    "                if not len(dfc): continue\n",
    "                col_runname = 'run_name' if chat_fmt != 'both' else ('run_name', '')\n",
    "                substitute = True\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, '_random_', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=10000:ep=10', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=50000:ep=5', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=3', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=1', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=100000', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=200000', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=400000', substitute=substitute)\n",
    "                #     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "                col = ('Average', 'chatfmt') if chat_fmt == 'both' else 'Average'\n",
    "            #     col = 'AlpacaFarm/WR'\n",
    "            #     col = 'MMLU/0-shot'|\n",
    "            #     col = 'GSM/CoT'\n",
    "            #     col = 'BBH/Direct'\n",
    "            #     col = 'TydiQA/GP'\n",
    "                dfc = dfc.sort_values(by=[col], ascending=False)\n",
    "                dfc = dfc.drop(columns=['model_name_or_path', 'subsample_mixture', 'max_train_samples', 'dataset'], \n",
    "                               axis=1, level=0 if chat_fmt=='both' else None)\n",
    "                dfc = dfc.reset_index(drop=True)\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .applymap(lambda x: f'max-width: 60ch;', subset=['run_name'])\n",
    "                        .set_table_styles([{'selector': 'td', 'props': [('white-space', 'pre-wrap'), ('word-wrap', 'break-word')]}])\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .applymap(lambda x: 'text-decoration: underline;' \\\n",
    "                                  if x in dfc[list(set(dfc.columns) & set([(x, '') for x in cols]))+[col] if chat_fmt=='both' else cols+[col]].values.flatten() and chat_fmt=='both' else '')\n",
    "                        .format(precision=1))\n",
    "\n",
    "# llama-7b_tulu_v1_mix(paper)\n",
    "# MMLU/0-shot, MMLU/5-shot, GSM/Direct, GSM/CoT, BBH/Direct, BBH/CoT, TydiQA/GP, TydiQA/CB, CodexEval/Pass@1, AlpacaEval(vs.Davinci-003)\n",
    "# 44.8       , 47.1       , 7.0       , 25.0   , 38.5      , 38.5   , 43.5,    , 8.0      , 18.6,           , 48.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "deeba0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_name',\n",
       " 'nonchat',\n",
       " 'sort_by',\n",
       " 'total_train_samples',\n",
       " 'model_name_or_path',\n",
       " 'subsample_mixture',\n",
       " 'max_train_samples',\n",
       " 'MMLU/0-shot',\n",
       " 'MMLU/5-shot',\n",
       " 'GSM/Direct',\n",
       " 'GSM/CoT',\n",
       " 'BBH/Direct',\n",
       " 'BBH/CoT',\n",
       " 'TydiQA/CB',\n",
       " 'TydiQA/GP',\n",
       " 'Codex-Eval/Pass@1',\n",
       " 'AlpacaFarm(chatgpt)/WR*',\n",
       " 'AlpacaFarm(chatgpt)/Len',\n",
       " 'MTBench(gpt:4:1106:preview)/Turn-1',\n",
       " 'MTBench(gpt:4:1106:preview)/Turn-2',\n",
       " 'MTBench(gpt:4:1106:preview)/Rating',\n",
       " 'Average',\n",
       " 'ranking']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rosemary import parse_kv_from_string\n",
    "\n",
    "non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', ]\n",
    "# non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', ]\n",
    "\n",
    "def compute_nonchateval_average_performance(row):\n",
    "    L = [row[k] for k in non_chateval_task_names if k in row]\n",
    "    return np.average(L)\n",
    "\n",
    "def parse_prune_subset_size(row):\n",
    "    run_name = row['run_name']\n",
    "    match = re.search(r'(?<=pace=)([^_]+)', run_name)\n",
    "    if match:\n",
    "        pace = match.group(1).replace(':', '_')\n",
    "        kvs = parse_kv_from_string(pace)\n",
    "        return int(kvs['size'] / kvs['ep'])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_sort_by_from_run_name(row):\n",
    "    run_name = row['run_name']\n",
    "    match = re.search(r'score=([^_]+)', run_name)\n",
    "    if match:\n",
    "        sort_by = match.group(1).replace(':', '_')\n",
    "        kvs = parse_kv_from_string(sort_by)\n",
    "    else:\n",
    "        kvs = {}\n",
    "    return kvs\n",
    "\n",
    "\n",
    "def parse_sort_by_type(row):\n",
    "    d = row['sort_by']\n",
    "    if not d:\n",
    "        return None\n",
    "    if d[0] == 'dppmap':\n",
    "        if d['k']=='vmf' and d['kmd']=='mpnet': #and (d['gamma']==1 or d['gamma'] == 'auto1000'):\n",
    "            return 'vmf+text'\n",
    "        elif d['k']=='rbf' and d['kemb']=='text+embedding' and d['kmd'] == 'llama7br512p4096':\n",
    "            return f\"rbf+text_gamma={d['gamma']}\"\n",
    "        elif d['k']=='vmf' and d['kemb']=='grad+rp+loraB' and d['kmd'] == 'llama7br512p4096':\n",
    "            return f\"vmf+grad_gamma={d['gamma']}\"\n",
    "        else:\n",
    "            return None\n",
    "    elif d[0] == 'random':\n",
    "        return 'random'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "dfc.insert(1, 'sort_by' if chat_fmt!='both' else ('sort_by', ''), dfc.apply(parse_sort_by_from_run_name, axis=1))\n",
    "dfc.insert(1, 'sort_by_type' if chat_fmt!='both' else ('sort_by_type', ''), dfc.apply(parse_sort_by_type, axis=1))\n",
    "dfc.insert(1, 'subset_size' if chat_fmt!='both' else ('subset_size', ''), dfc.apply(parse_prune_subset_size, axis=1))\n",
    "dfc.insert(1, 'nonchat' if chat_fmt!='both' else ('nonchat', ''), dfc.apply(compute_nonchateval_average_performance, axis=1))\n",
    "\n",
    "\n",
    "dfc = dfc[dfc['sort_by_type'].notnull()]\n",
    "startswithstrs = ('rbf+text', 'random')\n",
    "startswithstrs = (\n",
    "    'random', \n",
    "    'rbf+text_gamma=0.001', \n",
    "    'vmf+grad_gamma=1',\n",
    "#     'rbf+text_gamma=auto1000', \n",
    "    'vmf+grad_gamma=auto1000',\n",
    ")\n",
    "dfc = dfc[dfc.apply(lambda x: x['sort_by_type'].startswith(startswithstrs)\n",
    "                   , axis=1)]\n",
    "dfc['subset_size'] = dfc['subset_size'].apply(lambda x: int(x) if x else x)\n",
    "dfc = dfc[dfc['subset_size']<=10_000]\n",
    "\n",
    "\n",
    "D = dfc.set_index(['sort_by_type', 'dataset', 'subset_size']).to_dict()\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class DKey:\n",
    "    sort_by_type: str\n",
    "    dataset: str\n",
    "    subset_size: int\n",
    "def convert_key_to_dataclass(d):\n",
    "    return {DKey(*k): v for k, v in d.items()}\n",
    "D = {k: convert_key_to_dataclass(v) for k, v in D.items()}\n",
    "\n",
    "list(D.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "662539cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqcAAAMWCAYAAADMBC9gAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3yNZxvA8d/J3kFkCBGxYsQWqrbaamtrU0qpvdvSGuW1Z1ujy2irlNJqjdbeNWLvldgRhEyZ537/eDikSUgkOSfk+r6ffN4++zrHk5zrPPd9X7dOKaUQQgghhBAiGzAzdQBCCCGEEEI8IcmpEEIIIYTINiQ5FUIIIYQQ2YYkp0IIIYQQItuQ5FQIIYQQQmQbkpwKIYQQQohsQ5JTIYQQQgiRbUhyKoQQQgghsg1JToUQQgghRLYhyalIs3HjxqHT6ViyZEmGzqPT6ShUqFCSdUFBQeh0OurUqZOhc4tX244dO9DpdHTv3t3UobzSlixZgk6nS/Wnffv2qR57+vRp3nnnHVxdXbG1taVMmTLMmTMHvV6fbF/59xJCZAULUwcghBAifQoVKsTVq1d50ezT5cqVo3z58snWV61aNcX99+/fz1tvvcWjR4+oUqUKhQoVYteuXQwZMoR9+/axcuVKdDpdZrwEIYRIlSSnQgjxmmrVqhXjxo1L077x8fF06tSJR48eMWvWLIYMGQJAZGQkDRs2ZNWqVTRt2lSekgohspw06wshhGDt2rUEBgZSrlw5Q2IK4ODgwFdffQXAzJkzTRWeECIHkeRUJLNu3TqqVauGnZ0dLi4utG3blgsXLqS6//Xr1/nwww/x9vbG2toaNzc32rRpw6FDhzIUx4wZM9DpdHz66aep7tOwYUN0Oh3bt2/P0LVE1jt16hSdO3emcOHC2NjY4OrqSvny5Rk8eDC3b99Otn9oaCh9+/YlX758WFtb4+fnxw8//JDiudevX0+PHj0oWbIkTk5O2NvbU65cOf73v/8RGxubbP8nfTLHjRvHhQsXaN++Pe7u7piZmfH7778b9jt79izdu3fHy8sLa2tr3N3dad++PadPn04xjvj4eKZOnYqvry82NjYULFiQoUOHEhkZSZ06ddDpdAQFBRn2f7avdXh4OIMGDcLLywsbGxtKlizJ7Nmzk/T1fNLH8+rVqwBJ+pH+tx93eq1fvx6Adu3aJdtWsWJFChcuzKlTp5LE/zwrVqzAysqKfPnyceLEiQzFJoTIWaRZXySxcOFC+vbti06no2bNmuTLl49///2XKlWq0Lx582T7nzx5knr16nHv3j18fX1p06YN165dY+3atfz5558sX76cd95556Vi6d69O2PGjGHx4sVMmDABC4ukt2tgYCBbtmyhWLFi1K1b96WuIYwjICCAGjVqEBMTQ9myZWnZsiXR0dFcuXKFuXPn0qpVK/Lly2fY/+HDh1SrVo3IyEhq1qzJvXv32LVrFz179kSv1/PBBx8kOX/Pnj159OgRfn5+lC1blrCwMA4ePMjo0aPZunUr//zzD+bm5sniOn/+PP7+/ri4uFC3bl0ePHiApaUlAL///jvt27cnNjaW8uXL88Ybb3D9+nV+/fVX/vzzTzZu3EitWrUM51JK8d5777F27Vrs7e1p2LAhlpaWLF68mD179iS7f58VGxtLvXr1uHz5MvXq1SMuLo6tW7cydOhQjh8/bhiE6OHhQbdu3Vi9ejVRUVF069bNcI68efOm+L6PGDGC8PBwPDw8qFevHrVr104xhuPHjwNaIpqSihUrcuXKFU6cOPHCRHjBggX079+fQoUK8c8//1CkSJHn7i+EEEkoIR4LCgpSNjY2ytLSUm3atMmwPi4uTnXq1EkBClCLFy9WSiml1+tVmTJlFKBGjhyp9Hq94ZjVq1crMzMz5eDgoG7dupXkOoDy9vZOsi4wMFABqnbt2knWd+zYUQFq7dq1yeIdPXq0AtTUqVMz9LpF1uvatasC1IwZM5JtO3v2rOEe2b59u+E+a9++vYqJiTHst3btWgWoggULJjvH77//rqKjo5OsCw8PV2+//bYC1NKlS5NsW7x4seE6/fv3VwkJCUm2BwYGKnt7e+Xg4KA2b96cZNvGjRuVpaWl8vLyUrGxsYb1P/74owKUj4+Pun79umH9vXv3VPny5Q3XCwwMTHKdJ+vLli2r7t69a9h26dIl5enpmeL97+3trZ735/vZ1/ffn9q1a6vg4OBkx+TOnVsB6vjx4ymec/DgwQpQ8+bNM6x78u/VrVs3w7ovvvhCAapMmTLJfveFECItJDkVBp9//rkCVNeuXZNtu3fvnrKzs0uSnG7bts2QLMTFxSU7pk2bNgpQEydOTLI+Pcnprl27FKCaNm2aZH1CQoLKnz+/srS0VHfu3En/ixVG1aRJEwWoY8eOPXe/J8mOk5OTunfvXrLtfn5+yRK857l48aICVJs2bZKsf5K8ubq6qqioqGTHDRo0SAHqyy+/TPG8AwcOVIBas2aNYV316tUVoH788cdk+2/evPmFyek///yT7LgFCxYoQL311ltJ1r8oOd20aZMaN26cOnr0qAoLC1PBwcFq3bp1qkSJEgpQlStXTpaQW1paKkBdvHgxxXM++TI4adIkw7pnk1O9Xm9IYN98800VGhqaanxCCPE80udUGOzevRsgxRqILi4uNGzYMMX93333XUNT6LO6dOmSZL+XUbNmTUqXLs2mTZu4fv26Yf2GDRu4efMmLVu2xM3N7aXPL4yjUqVKAPTr148dO3aQkJDwwv1dXFySrS9evDhAin1UL168yNy5cxkwYAA9evSge/fufPHFF4ZtKalfvz52dnbJ1v/zzz8AtGnTJsXjatasCcDBgwcBra/poUOH0Ol0KfbZrF+/Pnny5EnxXAB58uShQYMGydZ36NABgH379qVYZzQ1jRo1YuzYsZQvXx4nJyfc3d1p3rw5hw4donjx4hw+fJhff/01zed7kYSEBLp168acOXNo1KgRmzdvJnfu3Jl2fiFEziJ9ToXBrVu3APD29k5x+3/7mT3ZP7X+Z0/W37x5M0NxffjhhwwcOJAffviBsWPHAvDtt98C0KtXrwydWxjHiBEj2LNnDzt27KBu3bo4ODhQrVo1mjVrRvfu3XF2dk6yf4ECBVI8j6OjI0CSQU5KKYYPH87s2bNTrfsZERGR4vqCBQumuP7JoJ/8+fM/93Xdu3cPgPv37xMXF4erqys2NjapXis0NDTFban9zjk7O5MrVy4ePnzIgwcPUkzY08PBwYGBAwfSv39//v77b0Py+2TbgwcPiI6OTvHYqKgo4Om/wbNWrlxJQkIC5cqV488//0zxy6oQQqSVJKciy2RWse6uXbvy8ccf88MPP/DZZ58RHBzMhg0bKFSoUIpPm0T24+TkxLZt29i7dy9//vknO3bsYNu2bWzevJnJkyeze/duihUrZtjfzCztjTorV65k1qxZeHl5MXv2bKpVq4arqyuWlpbExcVhbW2datKaWiL55CnlswOOUpJaMfvs7Mn7/N+nzwULFuTBgwfcuHGDsmXLJjvuxo0bQMqJdI0aNbh06RLHjx/n66+/ZvDgwZkfuBAix5DkVBjky5eP8+fPc/XqVUqVKpVs+5PyNU94enqmuP6JtD59ehFnZ2fat2/PDz/8wN9//82RI0dITEzkgw8+kNlqXiE6nY4aNWpQo0YNAEJCQhg8eDC//PILo0ePfulm5rVr1wLaCPFmzZol2XblypWXOmeBAgW4fPkyM2fOTNPTShcXFywtLbl37x4xMTEpJr3Pdkv5r2vXrqW4Pjw8nIcPH2Jra0uuXLnSHP/zPHjwAAB7e/sk68uVK8fx48c5cuQITZs2TXbckSNHAFJMXL29vfn222+pU6cOQ4YMwdzcnAEDBmRKvEKInEf6nAqDJ/3oUkoSQkNDDf3w/rv/qlWrSExMTHbMTz/9lGS/jOjTpw8AixYt4vvvv8fc3Jz3338/w+cVpuPm5maYvejUqVMvfZ4nyVZKXQFeNuF98kT+SeL7IpaWllSpUgWlFGvWrEm2fdu2bdy/fz/V4+/fv8/WrVuTrV+xYgUA1apVS1IKy8rKCuCFfXdT8ttvvwHJS0Y9SexXr16d7JijR49y5coV/Pz8Uu3GU7RoUbZv346npycDBw5k/vz56Y5NCCFAklPxjPfffx9ra2t+/vlntmzZYlgfHx/PkCFDDH3OnqhTpw5lypQhKCiIzz//PEnT6dq1a1mzZg0ODg706NEjw7H5+/tTsWJF/vjjDwIDA2nWrJnhya3I/hYuXEhgYGCy9Rs2bADAy8vrpc/9ZJDUN998k+Qe3L17N9OnT3+pcw4bNgxbW1uGDx+eYrIZGxvL6tWrDU3d8PQL1Oeff56kn3VoaCgjRox44TWHDx+eJIENDAxkwoQJgDaQ7FlP7v3z58+neK7Jkycb+sM+ER8fz/jx41m1ahW2trbJvty1bt0aHx8fjh8/zuzZsw3ro6KiDNcfNmzYc19DsWLF2L59O/ny5aN///4sXLjwufsLIUSKTFkqQGQ/X331lQKUmZmZqlOnjmrfvr0qVKiQcnZ2NtQ6fVJKSimlTpw4oVxcXBSgSpYsqTp06GAoqWNhYaFWrlyZ7Bqko5TUs7755htD2Z2//vork16xMIZy5copQJUqVUq1bdtWvffee4Z1NjY2as+ePUqplOtmPqtbt24KUNu3bzesO3/+vLK3tzecv3379qpmzZpKp9Op4cOHp3i/PSklNXbs2FRj/v333w3l04oWLaqaN29uOPeT6x09etSwv16vV61bt1aAcnBwUC1btlRt2rRRuXPnVpUrV1ZvvPGGAtTNmzcNxzy579944w1VsWJFlStXLtWmTRvVvHlzw7U7d+6cLLaZM2cqQLm7u6v27durnj17qlGjRhm2A8ra2lpVr15dtW/fXjVt2tRQM9XGxkb99ttvKb7mvXv3KltbWwWoqlWrqnfffVfly5dPAapdu3ZJahkrlfq/19mzZ5W7u7vS6XTqm2++SfU9FkKIlEhyKpJZu3atqlq1qrK1tVW5c+dWLVu2VGfPnlVjx45NlpwqpdTVq1dVr169lJeXl7K0tFR58+ZVrVq1UgcOHEjx/C+bnF66dEkBqkCBAslqNIrsbd26dapHjx6qdOnSKleuXMrOzk4VL15cffDBB+rcuXOG/V4mOVVKS4aaN2+u3NzclJ2dnapQoYIhKXrZ5FQp7Z776KOPVLFixZSNjY1ydHRUvr6+qn379urXX39NUoRfKW3CismTJ6tixYopKysrlT9/fjVgwAAVHh6uihYtqnQ6XZLJAp697x8+fKg++ugj5enpqaysrJSvr6+aMWNGivd6fHy8GjNmjCpSpIihPumzr/Hzzz9XDRo0UAULFlS2trbKxsZGFS1aVH344YdJ3u+UnDp1SrVt21a5uLgoGxsbVbp0aTVr1iyVmJiYbN/n/XudPn1aubm5KZ1Op77//vvnXlMIIZ6lUyqVYaxCZDOTJ0/m008/ZezYsYa+ikK8Cm7cuIGPjw9Fixbl7NmzhvVBQUH4+PhQu3ZtduzYYboAhRAiG5E+p+KVEB4ezpdffomVlRW9e/c2dThCpOjEiRPEx8cnWXfnzh26d+9OQkICnTt3NlFkQgjx6pBSUiJbW7x4MTt37mTXrl3cvn2bwYMHy0AokW2NHDmSgwcPUr58edzd3bl9+zYBAQFERkbi7+//wgFFQgghJDkV2dzOnTtZunQprq6u9OvXjylTppg6JCFS1b17d5RSnDx5kn379mFubk7x4sVp164dQ4YMSbXovxBCiKekz6kQQgghhMg2pM+pEEIIIYTINiQ5FUIIIYQQ2Yb0OU0DvV7PrVu3cHR0lLnchUkopYiIiMDT0xMzM+N9p5R7X2QHprr/hRCmIclpGty6dStD0ysKkVmuX7+e4hzyWUXufZGdGPv+F0KYhiSnaeDo6AhofxidnJxMHI3IicLDw/Hy8jLci8Yi977IDkx1/wshTEOS0zR40pzp5OQkH9DCpIzdtC73vshOpGuJEDmDJKdCZAOJ+kSOhBzhbvRdXO1cqehWEXMzc1OHJYQQQhidJKdCmNiWq1uYcnAKd6LvGNa527nzcZWPqe9d34SRCSGEEMYnwx6FMKEtV7cwdMfQJIkpQEh0CEN3DGXL1S0mikwIIYQwDUlOhTCRRH0iUw5OQZF8krYn66YenEqiPtHYoQkhhBAmI836QpjIkZAjyZ6YPkuhCI4O5kjIEXztfI0YmRDGI/2thRD/JcmpECZyN/pumveT5FS8jqS/tRAiJdKsL4SJ6JU+Tfu52rlmcSRCGJ/0txZCpEaSUyFMYP+t/fzvwP+eu48OHR52HlR0q2ikqIQwDulvLYR4HklOhTAipRQ/n/2Zvlv6EhEfgbejN6Alos96sjyqyijpfydeO+npby2EyHkkORXCSOIT4xm/fzxTDk4hUSXSokgLfmv5G7PrzMbNzi3Jvu527syqM0v63YnXUnBUcJr2S2u/bCHE60UGRAlhBPcf3WfojqEcCTmCmc6MoZWG0rVUV3Q6HfW961PXq66MWBY5wr5b+/jq6Fdp2lf6WwuRM0lyKkQWOxd6joHbBnI76jaOlo5Mqz2NGvlrJNnH3Mwcfw9/E0UoRNYLDAtk5uGZ7LyxE9C6rqTU5/TJNnc7d+lvLUQOJcmpEFnon6B/GLN3DI8SHlHIqRDz6s3Dx9nH1GEJYTRhsWEsPL6QFedWkKASsNBZ0L5Ee0rmKcmYvWMAkiSp0t9aCCHJqRBZQK/0LDi+gIXHFwJQ3bM6U2tNxdna2cSRCWEc8fp4Vp1fxfzj8wmLDQOgdoHaDKs8zPAFzc7SLsU6p6OqjJL+1kLkYNkqOZ08eTJr1qzh3Llz2Nra8uabbzJ16lR8fZ8WIP/www/ZsmULt27dwsHBwbBPiRIlUj1v9+7dWbp0aZJ1jRo1YtOmTVn2WkTOFR0fzeg9o9lyTavT2LVUV4ZUGoKFWbb6dRMiy+y5uYfph6ZzJewKAEVzFWWE/wje9HwzyX7S31oIkZJs9Wm5c+dO+vXrh7+/PwkJCXz66ac0bNiQM2fOYG9vD0ClSpXo1KkTBQsWJDQ0lHHjxtGwYUMCAwMxN0/9D1rjxo1ZvHixYdna2jrLX4/IeW5G3mTgtoFceHABSzNLPq/2Oa2KtjJ1WEIYxZWHV5h+eDp7bu4BILd1bvpX6E+bYm1S/XIm/a2FEP+VrZLT/z7JXLJkCW5ubgQEBFCrVi0AevfubdheqFAhJk6cSLly5QgKCqJIkSKpntva2hoPD4+sCVwI4HDwYYbuGMqD2Ae42Lgwp+4cyruVN3VYQmS5hzEPWXB8ASvPryRRJWJhZkGnEp3oXa43TlZOpg5PCPGKyVbJ6X+FhWn9lPLkyZPi9qioKBYvXoyPjw9eXl7PPdeOHTtwc3Mjd+7c1KtXj4kTJ+Li4pLpMYucadWFVfzv3/+RoBIo5VKKuXXn4mEvX4bE6y1eH8/KcytZcHwB4XHhANT1qsuwysPwdvI2cXRCiFdVtk1O9Xo9gwcPpnr16vj5+SXZNn/+fEaOHElUVBS+vr5s3rwZKyurVM/VuHFj2rRpg4+PD5cvX+bTTz+lSZMm7N+/P8WuALGxscTGxhqWw8PDM++FiddKvD6eaQenseL8CgCaFGrC+OrjsbWwNXFkL0fufZEWSil239zN9EPTCQoPAqB47uKM9B9J1XxVTRucEOKVp1NKpVxozsT69u3Lxo0b2bNnDwUKFEiyLSwsjJCQEG7fvs2MGTO4efMme/fuxcbGJk3nvnLlCkWKFGHLli289dZbybaPGzeO8ePHJ1sfFhaGk5M0UQnNw5iHDN85nAPBBwAYWGEgH5T5AJ1O94Ij0y88PBxnZ+csvwfl3hcvcunBJaYfns6+W/sAyGOThwEVBtC6aOssG8hkrPtfCJE9ZMvktH///vzxxx/s2rULH5/n14SMi4sjd+7cfPfdd3To0CHN13B1dWXixIl8+OGHybal9PTIy8tL/jAKg0sPLjFg2wBuRN7AzsKOyTUnU69gvSy7nrE+nOXeF6l5EPOAr499zaoLq9ArPZZmlnQu1ZleZXrhaOWYpdeW5FSInCVbNesrpRgwYABr165lx44dL0xMnxyjlErygfoiN27c4P79++TLly/F7dbW1jKaX6Rqx/UdjNo1iuiEaPI75OfLel9SLHcxU4eVKeTeF/8VnxjP8nPLWXR8ERHxEQDUL1ifoZWG4uX0/L7+QgjxMrJVctqvXz+WL1/OH3/8gaOjI8HBwQA4Oztja2vLlStXWLlyJQ0bNsTV1ZUbN24wZcoUbG1tadq0qeE8JUqUYPLkybRu3ZrIyEjGjx9P27Zt8fDw4PLly4wcOZKiRYvSqFEjU71U8QpSSvH9qe+Zd2QeCkUVjyrMrD2TXDa5TB2aEJlOKcWO6zuYcXgG1yKuAVAiTwlG+o+U0k9CiCyVrZLTBQsWAFCnTp0k6xcvXkz37t2xsbFh9+7dzJkzhwcPHuDu7k6tWrXYt28fbm5uhv3Pnz9vGOlvbm7OiRMnWLp0KQ8fPsTT05OGDRvyxRdfyBMikWaPEh4xdu9YNgZtBKC9b3tGVhmJpZmliSMTIvOdDz3P9MPTOXBb60/tYuPCoIqDaFGkhRTIF0JkuWyVnL6o+6unpycbNmxI13lsbW35+++/MxybyLmCo4IZtH0QZ+6fwUJnwSdVP+Fd33dNHZYQme7+o/t8dewr1lxcg17psTKzomvprnxQ5gPsLe1NHZ4QIofIVsmpENnNsZBjDN4+mPsx98ltnZtZdWZR2aNy5l9InwhX90HkHXBwB+83QZ5QCSOJS4zj57M/882Jb4iMjwSgUaFGDKk0hPwO+U0cnRAip5HkVIhU/HHpD8bvH0+8Pp7iuYszr968rPmgPrMONo2C8FtP1zl5QuOpUKpF5l9PiMeUUmy7to0Zh2dwI/IGAKVcSjHSfySV3CuZODohRE4lyakQ/5GgT2B2wGyWnVkGwFsF3+J/Nf6HnaVd5l/szDr4tSvwny4t4be19e8ukwRVZIlzoeeYdmgah4IPAeBq68qgioNoXqQ5Zjoz4wUirQZCiP+Q5FSIZ4THhTNy50j23toLQN9yfelTrk/WfFjrE7Unpv9NTOHxOh1s+hhKNMv8a4sc696je3x59EvWXlyLQmFtbk230t3o6dcza76APY+0GgghUiDJqRCPBYYFMnDbQILCg7Axt2FijYk0KpSF5cau7kv6oZyMgvCb2n4u5bIuDpEjxCbG8uOZH/n2xLdEJ0QD2nS7gysNxtPB0/gBSauBECIVkpwKAey9uZcRO0cQER+Bh70H8+rOo6RLyay9aOSdtO/nkrWhiNeXUorNVzczK2AWNyNvAuDn4seoKqMo71beNEGlp9VAmviFyHEkORU5mlKKZWeWMStgFnqlp4JbBWbVmUVe27xZf/Gwm2nbz8E9a+MQr63T908z7eA0joQcAcDNzo3BFQfTrHAz4/Yr/a/0tBr41DRaWEKI7EGSU5FjxSbGMmH/BNZdXgdAm2JtGF11NFbmVll7YaXgwELYMtawqNMl302vINbOA1vvNyEyKmtjEq+Vu9F3mXd0Hn9c+gOFwsbchvf93qd76e7G71eakvS0GgghchxJTkWOdDf6LoN3DObE3ROY68wZ4T+CjiU6okspS8xMCXGwYRgc0SoBHNb5UVGdQikwe+bS+setnePjuzIJEz7hEq+UmIQYlp1Zxncnv+NRwiMAmhVuxuCKg/Gw9zBxdM9Ia2uAtBoIkSNJcipynNP3TjNw+0BCokNwsnJiRu0ZVPOslvUXjroHK7vAtX2gMyOo4se021uaRmaHGGu5DE9CDbsG48L4+C78HVueloGhlHaVaVJF6pRS/B30N7MCZnE76jYAZV3LMsp/FGVdy5o4uv+IjUR/cvVzv3LpFYToXHD1qob0OBUi55HkVOQoGwM38tnez4hNjKWwc2Hm1ZuHt5N31l/4zmn4pT08vAbWTtDuB45HlQKO8be+CptjK1PF7BxuPCSEXBzUl0D/+OM7JCJGklORqlP3TjH14FSO3T0GgIe9B0MqDqGJT5OsbwlIr2v/wto+mD0IBLQuLYqUWw3GxnWh+9UwqhWR0YBC5DSSnIocQa/0fHX0K749+S0AtQrUYkrNKThaOWb9xc9tgDW9IC4ScvtAx5Xg6ovb5XtP48OMf/WlUjzczdEm62MUr5w7UXeYd3Seoc+0rYUtPfx60K10N2wtbE0c3X8kxMKOybB3Lig90bYe9AzriRNRqbca6KvQNCLGhEELIUxFklPx2ouMi+ST3Z+w48YOAHr49WBghYGYZ3WJGqVgzyzY+gWgwKcWvLMU7PLwKC6Rn/69+tzDdYCHsw1VfPIQFRmRtbGKV8ajhEcsOb2ExacWG/qVtijSgoEVBuJunw37aAafgrUfwp1T2nK5jpwqMZL9S88APLfVQL6YCZEzSXIqXmvXw68zcPtALj28hJWZFeOrj+ftwm9n/YXjH8G6AXBylbbs3wsaTwZzS24+fETvZYc5fSscM53WjKkjacXHJ62cY5uXwtwsmzXNCpNQSrEhcAOzA2ZzJ1obxV7BrQIj/Ufil9fPxNGlQJ+oPSnd/j/Qx4OdCzSfCyWbU0mvyOd8heCwmBRbDZ79YiaEyHkkORWvrQO3DzBs5zDCYsNws3Vjbr25xvkQjwiGFR3hZgDozKHpNPD/AIBDQaH0+TGA+1FxuNhbsaBzJUKjYhn/5xluhz1twvRwtmFs81I09suX9fGKbO/43eNMOzSNE3dPAOBp78mQykNo5N0o+/UrBbh/GX7vC9cPaMu+zaD5HHBwA8DcTMfY5qXo+9MR+WImhEhGklPx2lFKsfL8SqYcnEKiSqRM3jLMqTsHNzu3rL/4zSOwohNE3ALb3NoUjD61APjl4DU+/+MU8YmKUvmc+KZrJQrk1mpONijlwcHAUEIiYnBz1J4YyQezCI4KZnbAbDYEbgC0fqW9yvSiS6ku2FhkwyZvpeDwD/DPGIiPBitHaDIVyndMVsy3sV8+FnSuKF/MhBDJSHIqXivxifFMPjiZVRe05vS3C7/N2GpjjfNBfuo3+P0jSIiBvL7QcQXkKUx8op4v/jrDsv1aH9NmZfIx/Z2y2Fk9/fUzN9PJqGRhEB0fzeLTi1lyagkxiTHo0NGyaEsGVhiIq52rqcNLWfgtrSvLpS3acqGa0Go+5CqY6iGN/fLJFzMhRDKSnIrXRmhMKEN3DCXgTgA6dAypNITupbtnfbOnXq+NRN41TVsu1hDafgc2zoRGxdHv5yPsv3IfgOENi9OvbtHs2RQrTE6v9Ky/sp45AXMIeRQCQEW3ioyqMopSLilXc8gWTq6G9cMg5iFY2MBbY6FqHzB78QQS8sVMCPFfkpyK18L50PMM3DaQW1G3cLB0YGqtqdQqUCvrLxwbqY1EPveXtvzmAKg/HszMORcczgdLD3PjwSPsrcyZ074CDUplw9HUIls4FnKMqQencuq+Nqo9v0N+hlUeRv2C9bPvl5noUC0pPb1GW85XHtp8A66+Jg1LCPFqk+RUvPK2XN3Cp3s+5VHCIwo6FuTLel9SOFfhrL/ww2vwS0e4cxLMrbSRyOU7ArDp1G2G/nqc6LhEvF3s+LZrZYq7G6Gmqnjl3Iq8xeyA2WwK2gSAvaU9vcr0onOpzlibW5s4uue4uBn+6A+RwdrAv9ojoeYwMJcJI4QQGSPJqXhl6ZWeRScWMf/YfACq5avG9NrTcbZ2zvqLX/sXVnaGqLtg7wrv/QwFq6LXK+Ztu8icLRcBqFE0L191rEAuO6usj0m8UqLjo/nu5HcsO7OM2MRYdOhoU6wN/Sv0J69tXlOHl7rYSPhnNAQs0ZbzFofWiyB/RZOGlVF6vZ64uDhThyHEa8vS0hJz87TVF89QclqvXj1Gjx7NW2+9leL27du388UXX7Bt27aMXEaIZKLjoxmzdwybr24GoHPJzgyrPAwLMyN83zr6M/w5SKvd6FEG2v8CubyIik1g2K/H2XQ6GIAe1X34tGkJLMxf3O9O5Bx6pWfd5XXMOzKPu4/uAuDv4c9I/5GUyFPCxNG9wNX98HsfeBCkLb/xEbz1OVhmsxmp0ikuLo7AwED0er2pQxHitZYrVy48PDxe2FUpQ5/kO3bs4IMPPkh1e0hICDt37szIJYRI5lbkLQZuG8j5B+exMLPg8zc+p3Wx1ll/YX0ibP4c9n+lLZdsAa0XgpU910Oj6bXsMOeCI7AyN2Niaz/ereyV9TGJV0rAnQCmHpzK2dCzAHg5ejGs8jDqedXLvv1KQZt+dPsk2DsPUODspY3E9zFCv+4sppTi9u3bmJub4+XlhVkaBnEJIdJHKUV0dDQhIdpAz3z5nl8qLksfMz18+BBr62zcZ0q8co7cOcKQHUMIjQklj00e5tSdQwW3Cll/4ZgwWN0TLmlPaqk9Cmp/DGZm7Lt8j34/H+FBdDx5HaxZ1KUSlbxzZ31M4pVxI+IGswNm88/VfwBwsHTgw7If0rFkR6zMs3mXj9snYG0fCDmtLZfvpM12ZmOE7jNGkJCQQHR0NJ6entjZ2Zk6HCFeW7a2WgtLSEgIbm5uz23iT3dyeuLECY4dO2ZY3r17NwkJCcn2Cw0NZf78+ZQqlY3Ln4hXym8XfmPigYkk6BMomackc+vOJZ+DEQp1378Mv7SHexfAwlZ7YuTXBqUUP+0PYtyfZ0jUK8oWcGZRl0rkc361mzhF5omMi+S7k9/x45kfidPHYaYzo22xtvQr3w8X22xePikxAfbNhe2TH08/mvfx9KNGmP7XiBITEwGwssrmXxKEeA08+QIYHx+fucnp2rVrGT9+PAA6nY5FixaxaNGiFPd1dHRk3rx56b2EEEkk6BOYfmg6y88tB6Chd0O+qP4FdpZGeMpxZSf82lWr3+joCR2Wg2cF4hL0jF13ml8OXgOgVXlPprQti41l2jp7i9dboj6RPy7/wbwj87gfo9W4rZqvKiP9R1I8d3ETR5cG9y9rT0tvHNSWS7wNb88Bh2w6AUAmyNbdKoR4TaT19yzdyWn37t2pU6cOSinq1avHp59+SoMGDZJd3MHBgVKlSmFjk/aZeSZPnsyaNWs4d+4ctra2vPnmm0ydOhVf36c18z788EO2bNnCrVu3cHBwMOxTokTqAwmUUowdO5Zvv/2Whw8fUr16dRYsWECxYsXS+/KFkYXFhjF853D+vf0vAP3L96d32d7G+SA5+C1sHAUqEfJXhvY/g6MH9yJj6ftTAIeCHqDTwceNS9C7VmH5cBMAHAo+xLRD0zgXeg4AbydvhlceTu0CtbP/PaIUHP4e/vlMm37U2kmbfrRch2TTjwohRFZJd3Lq7e2Nt7c3AIsXL6Z27doUKlQoU4LZuXMn/fr1w9/fn4SEBD799FMaNmzImTNnsLe3B6BSpUp06tSJggULEhoayrhx42jYsCGBgYGpPiKeNm0a8+bNY+nSpfj4+PDZZ5/RqFEjzpw5k67kWRjX5YeXGbBtANcjrmNrYcvkGpN5yzvlyhCZKjFeS0oPf68tl30Pms8DSxtO3Qyj97LD3AqLwdHagnkdKlC3hFvWxySyvevh15kZMJOt17YC4GjpSJ9yfehQogOWr0Ltz/BbWt3Sy1r8+NSClvMhlwzsy+m6d+/Ow4cP+f33300disghMjQgqlu3bpkVBwCbNm1KsrxkyRLc3NwICAigVi1tVGjv3r0N2wsVKsTEiRMpV64cQUFBFClSJNk5lVLMmTOHMWPG0LJlSwCWLVuGu7s7v//+O+3bt8/U1yAyx87rOxm1exRR8VHkd8jPvHrzjNMcGh2qNeMH7QZ0UH8sVB8MOh1/Hr/FiNXHiYnXUzivPd90rUxRN4esj0lkaxFxEXx74lt+OvsT8fp4zHRmvFP8HfqV70dum1dgYJxScOo3WD9UG/hnYaPNclald5qmHxWaRL3iYGAoIRExuDnaUMUnD+Zm8rRZiJeRKaP1Dx8+zIEDB3jw4EGyOnE6nY7PPvvspc4bFhYGQJ48eVLcHhUVxeLFi/Hx8cHLK+Vv94GBgQQHB1O/fn3DOmdnZ6pWrcr+/ftTTE5jY2OJjY01LIeHh79U/CL9lFL8cOoH5h6Zi0JR2b0ys+rMMs6HfMg5beDTg0CwcoC234FvE/R6xcx/zvH19ssA1C7uyrwOFXC2fQWehqWT3Ptpl6hPZM2lNXx19CtCY0IBeNPzTYZXHk6x3K9Il6HoUC0pPb1WW/asAK2/AddXoF9sNrLp1G3G/3mG22ExhnX5nG0Y27wUjf2MMGgTrVarDOoSr4sMfS1+9OgRTZo0oWrVqgwYMICxY8cybtw4xo0bx/jx4w3//TL0ej2DBw+mevXq+Pn5Jdk2f/58HBwccHBwYOPGjWzevDnVX8rgYK0gurt70jnN3d3dDdv+a/LkyTg7Oxt+Ukt8ReaKSYjh490fM+fIHBSKd4u/yzcNvzFOYnrhH/iuvpaY5ioIPTeDbxMiYuLp/eNhQ2L6Ya3C/NDd/7VMTEHu/bT69/a/vPvXu0zYP4HQmFAKORXi67e+ZmH9ha9OYnrhb5j/hpaYmllAnU+1+14S03TZdOo2fX86kiQxBQgOi6HvT0fYdOp2lly3Tp069O/fn8GDB5M3b14aNWrErFmzKFOmDPb29nh5efHRRx8RGRlpOGbJkiXkypWLv//+m5IlS+Lg4EDjxo25fftpjImJiQwdOpRcuXLh4uLCyJEjUUoluXZsbCwDBw7Ezc0NGxsbatSowaFDhwzbd+zYgU6n4++//6ZChQrY2tpSr149QkJC2LhxIyVLlsTJyYmOHTsSHR2dJe+PeLVlKDmdMGEC//zzD6NHj2b79u0opVi6dCkbN26kZs2a+Pv7c+bMmZc6d79+/Th16hQrVqxItq1Tp04cPXqUnTt3Urx4cd59911iYmJSOMvL+eSTTwgLCzP8XL9+PdPOLVJ2J+oO3Td1Z0PgBix0FoypOobPqn2GpVkWJ4FKaYXFl78LcRHgXR167QD3UgTdi6LN/H1sORuClYUZs98rxydNS77WTXVy7z/f1fCrDNg2gF7/9OLCgws4WTnxcZWPWdNyDbUK1Mr+A54AYiNg3UDtno+8A3l9taS0zih4FfrGZjGlFNFxCWn6iYiJZ+y606iUzvP4/8etO0NETHyazvffJPBFli5dipWVFXv37mXhwoWYmZkxb948Tp8+zdKlS9m2bRsjR45Mckx0dDQzZszgxx9/ZNeuXVy7do3hw4cbts+cOZMlS5bwww8/sGfPHkJDQ1m7dm2Sc4wcOZLffvuNpUuXcuTIEYoWLUqjRo0IDQ1Nst+4ceP46quv2LdvH9evX+fdd99lzpw5LF++nPXr1/PPP//w5Zdfpus1i5xBp9L72/CMYsWKUalSJVasWMH9+/dxdXVly5Yt1KtXj4SEBPz9/WncuDGTJ09O13n79+/PH3/8wa5du/Dx8XnuvnFxceTOnZvvvvuODh06JNt+5coVihQpwtGjRylfvrxhfe3atSlfvjxz5859YTzh4eE4OzsTFhaGk5NTul6LeLETd08wePtg7j66Sy7rXMysPZMq+apk/YUTYuHPwXBcK1FFxW7QdAZYWLH74l36Lz9K2KN43J2s+aZLZcp55cr6mFJhqntQ7n1NeFw4i44vYvm55SToEzDXmfOe73v0LdeXXDa5TB1e2l3dp5WIengV0D2efvSzbD/9aFbehzExMQQGBuLj44ONjQ3RcQmU+vzvTL1GWp2Z0Ag7q7T1tqtTpw7h4eEcOXIk1X1Wr15Nnz59uHfvHqA9OX3//fe5dOmSYYzG/PnzmTBhgqEl0dPTkyFDhjBixAhAm6TAx8eHSpUq8fvvvxMVFUXu3LlZsmQJHTt2BLSalYUKFWLw4MGMGDGCHTt2ULduXbZs2WKY3nzKlCl88sknXL58mcKFCwPQp08fgoKCko03Ea+v//6+pSZDT06vX79O7dq1AQwj5ePi4gCwsLCgQ4cOKT75TI1Siv79+7N27Vq2bdv2wsT0yTFKqST95J7l4+ODh4cHW7duNawLDw/nwIEDVKtWLc2xiazx5+U/eX/T+9x9dJeiuYqyvNly4ySmkSGw5G0tMdWZQ5Np0HwuytyS7/cE0u2Hg4Q9iqdCwVz82b+GSRNTYToJ+gRWnlvJ22veZtmZZSToE6iZvyZrWqzhk6qfvDqJaXyMVh5qcVMtMXX2gm5/QuP/ZfvEVKSuUqVKSZafJIP58+fH0dGRLl26cP/+/SRN53Z2dkkGD+fLl88wpWRYWBi3b9+matWqhu0WFhZUrlzZsHz58mXi4+OpXr26YZ2lpSVVqlTh7NmzSeIpW7as4b/d3d2xs7MzJKZP1j25thDPytCAKEdHR8PsUI6OjpiZmXHr1i3Ddmdn51T7daakX79+LF++nD/++ANHR0fDsc7Oztja2nLlyhVWrlxJw4YNcXV15caNG0yZMgVbW1uaNm1qOE+JEiWYPHkyrVu3RqfTMXjwYCZOnEixYsUMpaQ8PT1p1apVRl6+yIBEfSJzj8xl8enFANT1qsvkmpOxt7TP+ovfPg6/dITwG9oUjO8sgSL1iE1IZPTaU6wOuAFAu0oFmNjKTwrr51D7bu5j+uHpXHp4CYDCzoUZ4T+CGvlrmDiydLp9AtZ+CCGPu1iV7/x4+tGc+yT8eWwtzTkzoVGa9j0YGEr3xYdeuN+S9/2p4pPywN7/Xjs9npRYBAgKCuLtt9+mb9++TJo0iTx58rBnzx569uxJXFycYWYeS8ukXTd0Ol26uxOk1bPX0ul0KV77v4OohYAMJqdFihThwoULgPbktHTp0qxevZoePXqglGLNmjXpGlCxYMECQGuueNbixYvp3r07NjY27N69mzlz5vDgwQPc3d2pVasW+/btw83taa3J8+fPG0b6g9Y/Jioqit69e/Pw4UNq1KjBpk2bpMapiUTERTBy10j23NwDQO+yvelXvh9mOiOUrTnzh9asGR8NLkWhw0rIW5SQ8Bg+/CmAo9ceYqaD0c1K0aN6oVejD6HIVIFhgcw4PINdN3YB4GztTL/y/WhXvF3W94HOTIkJsHcO7JiiTT9q76rV6y3R9IWH5mQ6nS7NTes1i7mSz9mG4LCYFPud6gAPZxtqFnPN8r7qAQEB6PV6Zs6cidnjEmC//vprus7h7OxMvnz5OHDggKF8Y0JCAgEBAVSsWBHQPvef9HN9UvM8Pj6eQ4cOMXjw4Mx7QSJHy1ByWr9+fX744QfmzJmDubk5H374If3796dIkSLodDoCAwP53//+l+bzvejbm6enJxs2bEj3eXQ6HRMmTGDChAlpjkVkjavhV+m/tT9B4UHYmNvwRfUvaOzTOOsvrBTsnAY7Ht+PRepBu8Vgm4vj1x/y4Y8BBIfH4GxryVcdK1Cz2Os7TaNIWVhsGAuPL2TFuRUkqAQsdBa0L9GePuX64GztbOrw0uf+Ze1p6Y3HT/VKvA3N54J9XtPG9ZoxN9Mxtnkp+v50BB0kSVCfpKJjm5cyyiDKokWLEh8fz5dffknz5s0Ng6TSa9CgQUyZMoVixYpRokQJZs2axcOHDw3b7e3t6du3LyNGjCBPnjwULFiQadOmER0dTc+ePTPxFYmcLEPJ6ccff0yXLl0MyeBHH31ETEwMP/30E+bm5vTq1SvZSEGRc+27tY/hO4cTEReBu507c+vNpbRL6ay/cFw0/PHR01qOb3wEDb4AcwvWHr3BqN9OEpegp6ibA991rUyhvEboWiCyjXh9PKvOr2L+8fmExWotLrUL1GZY5WH4OL+433u2ohQc+k7rX5rwSJt+tOl0bZYzaQXIEo398rGgc8VkdU49jFzntFy5csyaNYupU6fyySefUKtWLSZPnkzXrl3TdZ5hw4Zx+/ZtunXrhpmZGT169KB169ZJWiOnTJmCXq+nS5cuREREULlyZf7++29y534FJp0Qr4QMjdbPKWTEcsYopfj57M9MPzwdvdJTzrUcc+rOIa+tEZ7ihN2EFR20fqZmlvD2LKjYlUS9YtqmcyzadQWA+iXdmP1eeRxtsmezrYzWzxp7bu5h+qHpXAnT7oOiuYoywn8Eb3q+aeLIXkLYTfijH1zZri371IaWX78W048ac7T+y5IZooR4sbT+vmXKDFFCpCYuMY6J/05k7SXtqWXLIi35vNrnWJkbYSaTG4dhRUetlqOdC7z3E3i/SdijeAatOMqO83cB6Fe3CMMa+GImHyQ5xuWHl5l+eDp7b+4FILd1bvpX6E+bYm2wMHvF/iwqBSdXwfrhEPt4+tEGE8C/l0w/akTmZjqqFXExdRhCvBYy/FdYKcWWLVu4ePEi9+/fT7G/58tOXypebfce3WPI9iEcu3sMM50ZwysPp3PJzsYZZHR8JawbAImx4FYaOvwCub25fDeSXksPc+VeFDaWZkxvV47m5TyzPh6RLTyMecj84/P59fyvJKpELMws6FSiE73L9cbJ6hV8Mhx1H9YP0Qb6AXhWhNaLZJYnIcQrLUPJ6cWLF2nVqhXnzp1LdTCTJKc505n7Zxi4bSB3ou/gaOXIjFozeDO/EZpK9XrYOl4bpQzg2xTafAPWjmw/H8LA5UeJiE3A09mGb7pWxi//KzbQRbyUeH08K8+tZP7x+UTERQBa+bJhlYfh7eRt4uhe0vlN2hewqBBt+tHao6DGUDB/xZ78CiHEf2Tor9iAAQO4fPkyU6dOpV69eri4SJOGgE1Bm/hsz2fEJMZQyKkQX9b7kkLOhbL+wrER8FsvuLBRW64xFOp9htLp+GbnZaZsOodS4F8oN/M7VcLV0TrrYxImpZRi141dzDg8g6DwIACK5y7OSP+RVM1X9fkHZ1exEfD3p3BkmbbsWkJ7WupZ3qRhCSFEZslQcrp7924GDx6cZF5ekXPplZ6vj33NNye+AaBG/hpMqzUNRyvHrL/4gyD4pYNWaNzcWhsIUvYdYuIT+fi34/x+TJscokMVL8a38MPKQvrive4uPrjI9EPT2X97PwB5bPIwoMIAWhdtjbnZKzqxQtBe+L0PPLwG6KBaP6j3GVhKzWYhxOsjQ8mptbV1mqYYFa+/qPgoPtn9CduvayOF3y/9PoMqDjJOEhC0B1Z2gUeh4OAB7ZdDgUoEh8XQ+8fDnLgRZqhH2OUNbyms/5oLjQll/rH5rLqwCr3SY2lmSedSnelVppdxvihlhfgY2PYF7P8aUOBcEFovgEKv2GxVQgiRBhlKThs1asTevXv58MMPMyse8Qq6EXGDAdsGcOnhJazMrBj35jiaF2lunIsfXgwbhoM+ATwraImpkycBVx/Q56cA7kbEktvOkvmdKslI2tdcfGI8y88tZ9HxRUTEa/1K6xesz9BKQ/FyeoXLKd06ps1qdvfxvOUVukCj/8n0o0KI11aGktNZs2ZRq1YtZs6cyYABA7CyMkJ5IJGtHAo+xNAdQ3kY+xBXW1fm1J1DWdeyWX/hxASt393BRdpy6TZaU76VHb8evs6YtaeIS9RTwsORb7tWxiuPXdbHJExCKcX269uZeXgm1yKuAVAiTwlG+o/E38PfxNFlQGIC7JkNO6doX77sXaHFl+DbxNSRCSFElkpXclq4cOFk6yIjIxk5ciQff/wxnp6emJsnbcbV6XRcvnw5Y1GKbOnX878y+cBkElQCpV1KM7fuXNzt3bP+wo8ewKr3nxYbrzsGag0nQa+Y9OdpFu8NAqBRaXdmvVsee2sZvfy6Oh96numHpnMg+AAALjYuDKo4iBZFWry6/UoB7l3UnpbePKwtl2wOb8+R6UeFEDlCuj61CxYsKP31BPH6eKYenMrK8ysBaOrTlPFvjsfGwgiDMu5dhOXvQehlsLSHNougZHMeRsfRf/lR9ly6B8Dg+sUYWK+YFNZ/Td1/dJ+vjn3Fmotr0Cs9VmZWdC3dlQ/KfIC95Ss8/axer00/uvnzx9OPOj+efvRdmX5UvJBOp2Pt2rW0atUq1X3OnTtH9+7dOXbsGCVKlODYsWNGi0+ItEpXcrpjx44sCkO8Kh7EPGDYzmEcCj6EDh2DKg6ih18P43xpubRVe2IaGwbOXlphfY8yXLgTQa9lh7l6Pxo7K3NmvVvOaPNZC+OKS4zj57M/s+jEIqLiowBoVKgRQyoNIb9DfhNHl0FhNx5PP7pDWy5cR+uq4lzAlFGJtNInwtV92ox0Du7g/SZkw6f3Y8eOxd7envPnz+Pg4JDu48eNG8fvv/+e6UltVp33dXft2jX69u3L9u3bcXBwoFu3bkyePBkLi9TTu9DQUAYMGMCff/6JmZkZbdu2Ze7cuUnuhxMnTtCvXz8OHTqEq6srAwYMYOTIkYbtp0+f5vPPPycgIICrV68ye/ZsBg8enGmvS9o7RZpdeHCBgdsGcjPyJvaW9kytOZXaXrWz/sJKwYGFWh9TpQevN7SpSB1c2XzmDoNXHCUqLpECuW35tmtlSuaTgSKvG6UU265tY8bhGdyIvAFAKZdSjPQfSSX3SiaOLoOUghO/woYRj6cftX08/egHMv3oq+LMOtg0CsJvPV3n5AmNp0KpFkYJIS4uLk37Xb58mWbNmuHtnfLkE0FBQfj4+KQ6sY7IPhITE2nWrBkeHh7s27eP27dv07VrVywtLfnf//6X6nGdOnXi9u3bbN68mfj4eN5//3169+7N8uXLAQgPD6dhw4bUr1+fhQsXcvLkSXr06EGuXLno3bs3ANHR0RQuXJh33nmHIUOGZP6LUxmwefNm9fHHH6e6/eOPP1bbtm3LyCWyhbCwMAWosLAwU4diMluublH+P/krvyV+qslvTdSlB5eMc+H4WKX+6K/UWCftZ+1HSsXHKL1er77cekEV+vgv5T3qL/Xeon3qfmSscWIyAVPdg9nh3j9z74zqvrG78lvip/yW+Km6K+uq3y/+rhL1iSaLKdNE3lVqReen9/c39ZS6e9HUUWU7WXkfPnr0SJ05c0Y9evTo5U5w+g+lxjo//Tc0/DhrP6f/yMRon6pdu7bq16+fGjRokHJxcVF16tRRgJo/f75q3LixsrGxUT4+PmrVqlWGY4AkP2PHjk123sDAQJVaarB48eJk51i8eLFSSqkHDx6onj17qrx58ypHR0dVt25ddezYMaWUUiEhIcrd3V1NmjTJcK69e/cqS0tLtWXLluee93nOnj2rqlevrqytrVXJkiXV5s2bFaDWrl1r2GfkyJGqWLFiytbWVvn4+KgxY8aouLg4w/axY8eqcuXKqe+//155eXkpe3t71bdvX5WQkKCmTp2q3N3dlaurq5o4cWKSawNq4cKFqlmzZsrW1laVKFFC7du3T128eFHVrl1b2dnZqWrVqqlLl55+Vl66dEm1aNFCubm5KXt7e1W5cmW1efPmF77O1GzYsEGZmZmp4OBgw7oFCxYoJycnFRub8ufhmTNnFKAOHTpkWLdx40al0+nUzZs3lVJKzZ8/X+XOnTvJOUaNGqV8fX1TPKe3t7eaPXt2mmJO6+9bhr6WT5s2jUuXLqW6PTAwkKlTp2bkEsLElFIsOr6IwdsH8yjhEVXzVeWXZr9QJFeRrL941D1Y1lKbCUdnBg0nQcuviNab0/+Xo8z45wJKQddq3vzYsyp57KVaxOvk3qN7jN03lvf+eo/Ddw5jbW5N77K9+av1X7Qs2hIz3Sv+VPHcBpj/Bpxdp00/Wm8M9Pgb8hY1dWQ5m1IQF5W2n5hw2DgSLZ9KdiLt/zaN0vZLy/nS+bRy6dKlWFlZsXfvXhYuXAjAZ599Rtu2bTl+/DidOnWiffv2nD2rlSG7ffs2pUuXZtiwYdy+fTvdE+i89957DBs2jNKlS3P79m1u377Ne++9B8A777xDSEgIGzduJCAggIoVK/LWW28RGhqKq6srP/zwA+PGjePw4cNERETQpUsX+vfvz1tvvfXc86YmMTGRVq1aYWdnx4EDB/jmm28YPXp0sv0cHR1ZsmQJZ86cYe7cuXz77bfMnj07yT6XL19m48aNbNq0iV9++YXvv/+eZs2acePGDXbu3MnUqVMZM2YMBw4cSHLcF198QdeuXQ39dzt27MiHH37IJ598wuHDh1FK0b9/f8P+kZGRNG3alK1bt3L06FEaN25M8+bNuXbtmmGfPn364ODg8NyfJ/bv30+ZMmVwd386ELlRo0aEh4dz+vTpFN+3/fv3kytXLipXrmxYV79+fczMzAyvb//+/dSqVStJBaZGjRpx/vx5Hjx48Nx/l8ySoWb948ePJ+mD8F9Vq1Zl2rRpGbmEMKFHCY/4bO9n/B30NwAdS3RkuP9wLM0ss/7id07D8vYQdg2snaDdD1CsATcfPqLX0sOcuR2OpbmOCS396FClYNbHI4wmNjGWH8/8yLcnviU6IRqAJoWaMLjSYDwdPE0cXSaICYe/P4GjP2nLriW1gX35ypk2LqGJj4b/ZdZ9prSm/ilprLP76S2wSvuAvmLFiiX7jH3nnXf44IMPAC152rx5M19++SXz58/Hw8MDCwsLHBwc8PDwSPN1nrC1tcXBwQELC4skx+/Zs4eDBw8SEhKCtbU2LfSMGTP4/fffWb16Nb1796Zp06b06tWLTp06UblyZezt7Zk8efJzz/s8mzdv5vLly+zYscNwzKRJk2jQoEGS/caMGWP470KFCjF8+HBWrFiRJHfR6/X88MMPODo6UqpUKerWrcv58+fZsGEDZmZm+Pr6MnXqVLZv307Vqk+nPX7//fd59913ARg1ahTVqlXjs88+o1GjRgAMGjSI999/37B/uXLlKFfu6e/5F198wdq1a1m3bp0hiZ0wYUKavzQEBwcnSUwBw3JwcHCqx7i5uSVZZ2FhQZ48eQzHBAcHJ5tg6dnz5s6dO03xZUSGktOwsDDs7VP/RbK1tTVali0yV3BUMAO3DeRs6FkszCwYXXU07Yq3M87Fz62HNb0hLhLyFIYOK8DVl4OBofT9KYD7UXG42FuxsEsl/AvlMU5MIssppdh8dTOzAmZxM/ImAH4ufoyqMorybuVNG1xmCdoDa/tqX7rQwZv9tVJoMv2oeAmVKiXvb12tWrVkyy8aZFS6dGmuXr0KYOhr+uwTupo1a7Jx48ZUjz9+/DiRkZG4uCSd6OTRo0dJSknOmDEDPz8/Vq1aRUBAgCGRfRnnz5/Hy8srSTJbpUqVZPutXLmSefPmcfnyZSIjI0lISMDJKem4hEKFCuHo+HT2OHd3d8zNzTF7ps+3u7s7ISEhSY4rW7Zsku0AZcqUSbIuJiaG8PBwnJyciIyMZNy4caxfv57bt2+TkJDAo0ePkjw5dXNzS5Y85kQZSk7z589PQEBAqtsDAgJe6tuZMK1jIccYtH0QoTGh5LHJw6w6s4wz6EQp2DMLtn4BKPCpDe8sAbs8LD9wjbHrThGfqCjt6cQ3XSuTP5dt1sckjOL0/dNMOziNIyFHAHCzc2NwxcE0K9zs1W++h+TTj+YqCK0WQqHqpo5M/JelnfYEMy2u7oOf0/ClvdNqbfR+Wq6dDs97OJQeGzZsID4+HoCbN29Sp06dJAmtre3z/9ZGRkaSL1++FCv65MqVy/Dfly9f5tatW+j1eoKCgpIkcllh//79dOrUifHjx9OoUSOcnZ1ZsWIFM2fOTLKfpWXS1kCdTpfiOr1en+pxTyrWpLTuyXHDhw9n8+bNzJgxg6JFi2Jra0u7du2SDGbr06cPP/3003NfV2RkJAAeHh4cPHgwybY7d+4YtqXEw8MjWZKdkJBAaGio4RgPDw/DedJ63syWoeS0WbNmLFy4kPfee4/69esn2bZ161aWLl1qaF4Qr4a1F9cy4d8JJOgT8M3ty7x684zTlBr/CNYNgJOrtGX/XtB4MvGYM+H3U/z4r/at/u2y+Zjerhy2VtmvRItIv5DoEOYdmce6y+tQKGzMbXjf7326l+6OXTo/qLOtW0cfTz96Tluu2FWbftTa8fnHCdPQ6dLetF6knjYqP/w2Kfc71Wnbi9QzWlmpf//9l65duyZZrlChwnOPeXbk/pMSREWLptz32crKisTExCTrKlasSHBwMBYWFhQqVCjF4+Li4ujcuTPvvfcevr6+fPDBB5w8edLwlDCl8z6Pr68v169f586dO4anlocOHUqyz759+/D29k7SF/XJE2JT2Lt3L927d6d169aAlmQGBQUl2Sc9zfrVqlVj0qRJhISEGN7HzZs34+TkRKlSpVI95uHDhwQEBBievG/btg29Xm/oslCtWjVGjx5NfHy8IdnevHkzvr6+RmnShwwmp6NHj+a3336jUaNGNGnShPLlywNw7NgxNm7ciIeHB5999llmxCmyWII+gZmHZ/LTWe0bWwPvBkysPtE4CUL4bVjREW4d0QaGNJkG/j0JjYrjo58P8O+VUHQ6GN7Ql4/qFJGJIF4DMQkxLDuzjO9OfsejhEcANCvcjMEVB+Nh/5q0tiQmaC0BO6c+nn7U7fH0o41NHZnILGbmWrmoX7sCOpImqI//TjWeYtR6p6tWraJy5crUqFGDn3/+mYMHD/L9999n2vkLFSpEYGAgx44do0CBAjg6OlK/fn2qVatGq1atmDZtGsWLF+fWrVusX7+e1q1bU7lyZUaPHk1YWBjz5s3DwcGBDRs20KNHD/76669Uz/u8Zv8GDRpQpEgRunXrxrRp04iIiDD0L33yGVGsWDGuXbvGihUr8Pf3Z/369axduzbT3ov0KlasGGvWrKF58+bodDo+++yzZE9j09Os37BhQ0qVKkWXLl2YNm0awcHBjBkzhn79+hneu4MHD9K1a1e2bt1K/vz5KVmyJI0bN6ZXr14sXLiQ+Ph4+vfvT/v27fH01B5EdezYkfHjx9OzZ09GjRrFqVOnmDt3bpKBZHFxcZw5c8bw3zdv3uTYsWM4ODik+sUmXdI09v85goKCVJMmTZSZmZnS6XRKp9MpMzMz1axZMxUYGJjR02cL2aGcTlZ6GPNQ9f6nt6FUz/yj841XpudGgFIzfLXSK1O8lbqyUyml1JlbYar6lK3Ke9RfqvTnm9Tm08HPP89r7nUpJaXX69WGKxtUg1UNDPdbx/Ud1fGQ45ly/mzj7gWlvqn7tKzQyi5KRd4zdVSvrGxdSkoprVzUzBJJS0nNLJllZaSU0kpJDRo0KMk6QH399deqQYMGytraWhUqVEitXLkyyT7lypVLsYTUE88rJaWUUjExMapt27YqV65cSUo+hYeHqwEDBihPT09laWmpvLy8VKdOndS1a9fU9u3blYWFhdq9e3eS6zg5Oan58+c/97zP86SUlJWVlSpRooT6888/FaA2bdpk2GfEiBHKxcVFOTg4qPfee0/Nnj1bOTs7G7Y/KSX1rG7duqmWLVsmWfff95v/lKx68r4dPXrUsG779u0KUA8ePDDsU7duXWVra6u8vLzUV199leK/Y3o8ycFsbW1V3rx51bBhw1R8fHyyGJ7Nx+7fv686dOigHBwclJOTk3r//fdVREREkvMeP35c1ahRQ1lbW6v8+fOrKVOmJNn+5PX+96d27drPjTetv286pTKn0u6DBw8MZaWKFi1qtEe/xhAeHo6zszNhYWHJOlK/6q6EXWHgtoFcDb+KrYUtk2pMooF3gxcfmBlO/Qa/fwQJMeBaQpvxKU9hNp26zdBfjxMdl4i3ix3fda1MMfec3QRqqnswM6978u5Jph2axrG7xwDwsPdgSMUhNPFp8vo8Ddfr4dC3j6cfjQEbZ2g6E8q0k+lHMyAr7/+YmBgCAwPx8fHBxiYDA9NekRmiXmd79+6lRo0aXLp0iSJFjFDuUKRbWn/fMm2GqNy5c+Pv759ZpxNGsPvGbkbuGklkfCT57PPxZb0v8c3jm/UX1uthx/9g13RtuVgjaPsdeitH5m6+wNytFwGoUTQvX3WsQC47qV/6KrsTdYe5R+by55U/AbC1sKWHXw+6le6GrcVrNKgt7Ib2ZStwp7ZcuO7j6Udf8WlVRdqYmYNPTVNHkaOsXbsWBwcHihUrxqVLlxg0aBDVq1eXxPQ1kCnJaXR0NEFBQdy/fz/FKc9q1aqVGZcRmUQpxdLTS5kVMAuFoqJbRWbVmYWLrcuLD86o2EhY+yGc0/oZ8eZAqD+OqHjF0J8D+Pu0NiKwR3UfPm1aAgvz12Ckdg71KOERS04vYfGpxYZ+pS2KtGBghYG427u/4OhXiFJwYuXj6UfDtelHG36hTT8qT0uFeCk///wzH374YYrbvL29OX36NBEREYwaNYpr166RN29e6tevn2wkvng1ZSg5jY6OZujQoSxevJiEhIRk25VS6HS6NI/Amzx5MmvWrOHcuXPY2try5ptvMnXqVHx9tad5oaGhjB07ln/++Ydr167h6upKq1at+OKLL3B2dk71vN27d2fp0qVJ1jVq1IhNmzal49W+HmITYxm/b7zhKVbbYm0ZXXU0luZGKKz/8Br80gHunAJzK2g+F8p35HpoNL2WHeZccARW5mZMau3HO5XTWLRamEyiPpEjIUe4G30XVztXKrpVxNzMHL3SsyFwA3MC5nAnWvuyUd61PKOqjMIvr5+Jo85kUffgz0FPv2wV8NdKRMksT0JkSIsWLZIUvH/WkxHkXbt2TVKZQLw+MpScDho0iO+//56mTZtSr169ZAV402vnzp3069cPf39/EhIS+PTTT2nYsCFnzpzB3t6eW7ducevWLWbMmEGpUqW4evUqffr04datW6xevfq5527cuDGLFy82LGek+O+rKiQ6hMHbB3Py3knMdeaMqjKK9r7tjdPf79q/sKITRN/TRi23/xm8qrDv8j36/XyEB9HxuDpas7BzJSp5vz79lV9XW65uYcrBKYbkE8Ddzp32Jdqz/dp2Ttw7AYCnvSdDKg+hkXej16df6RPn1muJadRdrcpEnU+g+mAwz7TeUkLkWI6OjkkK44ucJUN/RdeuXUuHDh34+eefMyWY/z7JXLJkCW5ubgQEBFCrVi38/Pz47bffDNuLFCnCpEmT6Ny5MwkJCYb6bCmxtrbO0RMCnLp3ikHbBhHyKARna2dm1p5J1XwpfyvNdEd/gj8Hgz4ePMpA+19QzgVYti+ICX+dIVGvKFfAmUVdKuPhLDPlZHdbrm5h6I6hqP/UdbwTrfUtBa1faa8yvehSqgs2Fq/Zv2lMOGz6BI49LpTtVgpaL5TpR4UQIpNkKDmNiYmhTp06mRRKcmFhYQDkyZP6FJVPRm8+LzEF2LFjB25ubuTOnZt69eoxceLEVJ/0xsbGEhsba1gODw9/ieizj7+u/MXYvWOJ08dRxLkIX9b7Ei8nIzSb6xO1Ucv7v9KWS7aA1guJM7Pl8zUnWXHoOgCtK+Rncpsy2FjKyFZTe9G9n6hPZMrBKckS02fZWtiyrtW616de6bMCd2uDngzTjw6AuqNl+lEhhMhEGUpOK1euzMWLFzMrliT0ej2DBw+mevXq+Pml3E/t3r17fPHFF/Tu3fu552rcuDFt2rTBx8eHy5cv8+mnn9KkSRP279+PuXnyhGjy5MmMHz8+U16HKSXqE5l3dB4/nPoBgDoF6jC55mQcrBxecGQmiAmD1T3h0mZtufbHUHsUd6Pi6fvTvxy++gAzHXzcpAS9ahZ+/Zp8X1EvuvePhBxJ0pSfkkcJj7gecf31Sk7jH2nT6v77tbacy1t7WpqWKSmFEEKkS4bqnP777780b96cjRs3Urly5cyMi759+7Jx40b27NlDgQIFkm0PDw+nQYMG5MmTh3Xr1iWbB/d5rly5QpEiRdiyZQtvvfVWsu0pPT3y8vJ6peqcRsZFMmr3KHbd2AVArzK96F+hv3HmKb9/GX5pD/cuaCOXWy+A0q05dTOM3ssOcyssBkcbC77sUIE6vmmbCSOnM1ad0xfd+xuubGDU7lEvPM/UmlNpWrhplsVpVDePaNOP3juvLVfqDg0nyvSjRvRK1DkVQryQUeqcfvPNNxQoUIA33niDatWqUbhw4WRPInU6XbqnTuvfvz9//fUXu3btSjExjYiIoHHjxjg6OrJ27dp0JaYAhQsXJm/evFy6dCnF5NTa2vqVHjB1LfwaA7YN4ErYFazNrZnw5gTjJQpXdsCv3SDmITjlh/bLwbM8fx6/xYjVx4mJ11PY1Z5vu1amiKsRnuCKdHnRve9q55qm86R1v2wtMR52z4Sd00AlaoXVW3wFxRuaOjIhhHitZSg5XbJkieG/9+7dy969e5Ptk57kVCnFgAEDWLt2LTt27MDHxyfZPuHh4TRq1Ahra2vWrVv3Ut90b9y4wf3798mXL1+6j83u/r39L8N2DCM8Lhw3Ozfm1Z1H6byljXPxg9/CxlHaB3n+ytD+Z/T27szYdI75Oy4DUMfXlbntK+Bsa4TSVSLTVXSriLudOyHRISn2O9Whw93OnYpuFU0QXSa6ewHW9oZbR7XlUq3g7dlgl3r/dyFeB8HBwXTp0oV9+/ZhaWnJw4cPTR3ScwUFBeHj48PRo0cpX768qcMRmSRDbbx6vf6FP2mtcQrQr18/fvrpJ5YvX46joyPBwcEEBwfz6JFWwDs8PJyGDRsSFRXF999/T3h4uGGfZ69TokQJ1q5dC0BkZCQjRozg33//JSgoiK1bt9KyZUuKFi1Ko0aNMvLysxWlFD+f/Zk+m/sQHhdO2bxlWdFshXES08R4+GsobBiuJaZl20P39URYutBr2WFDYvph7cJ8381fEtNXmLmZOR9X+RjQEtFnPVkeVWUU5q/qtI16Pfy7ABbV1BJTG2do+z28s0QSU/FcifpEDgUfYsOVDRwKPkSiPu2ffdnJ7NmzuX37NseOHePChQumDkdkstu3b9OxY0eKFy+OmZkZgwcPNnVIKcpWBfkWLFgAkKwCwOLFi+nevTtHjhzhwIEDABQtmrTIdWBgIIUKFQLg/PnzhpH+5ubmnDhxgqVLl/Lw4UM8PT1p2LAhX3zxxSvddP+s+MR4Jh2YxG8XtTJbLYq04PNqn2NtboTXFx0Kv3aFoN2ADuqPg+qDCLwfTa9lB7gUEom1hRlT25alVQWZxvF1UN+7PrPqzEqxzumoKqOo713fhNFlwMPr8Hvfx/cyUKSeNv2ok6dp4xLZXmp1fz+u8vEr9/tw+fJlKlWqRLFixVLdR6fTJfnMzai4uDisrGSaamOIjY3F1dWVMWPGMHv2bFOHkzqVCfR6vQoICFCrVq1Sq1atUgEBAUqv12fGqbOFsLAwBaiwsDBTh5LMveh7quuGrspviZ8qu7SsWnJqifHe+ztnlZpTVqmxTkpN8lTq3AallFI7z4eoMmM3Ke9Rf6mqk7ao49cfGCee15ip7sHnXTchMUEdvH1Qrb+8Xh28fVAlJCYYNbZMo9crdfRnpf5XQLuXJ3oodfBbbb3IFrLy/n/06JE6c+aMevTo0UsdvzlosyqzpIzyW+KX5KfMkjKqzJIyanPQ5kyOWLNo0SKVL18+lZiYmGR9ixYt1Pvvv6/Gjh2rypUrp77//nvl5eWl7O3tVd++fVVCQoKaOnWqcnd3V66urmrixImGY729vRVg+OnWrVuK1wZUYGBgqrF98803qkCBAsrW1la1atVKzZw5Uzk7Oxu2P4nt22+/VYUKFVI6nU4ppdTGjRtV9erVlbOzs8qTJ49q1qyZunTpUpJzHzhwQJUvX15ZW1urSpUqqTVr1ihAHT16NE3v2x9//KGKFi2qrK2tVZ06ddSSJUsUoB48eKCUUurevXuqffv2ytPTU9na2io/Pz+1fPnyJOeoXbu26t+/vxo0aJDKlSuXcnNzU998842KjIxU3bt3Vw4ODqpIkSJqw4YNhmO2b9+uALVp0yZVvnx5ZWNjo+rWravu3LmjNmzYoEqUKKEcHR1Vhw4dVFRUlOG4tLwnL6t27dpq0KBBmXKutErr71uGh25v2rSJIkWK4O/vz3vvvcd7772Hv78/RYsW5e+//87o6cVznAs9R4f1HTgScgRHS0e+futrupXuZpyyTBf+hu/qw4MgraxOz82o4o35bvcVui8+SHhMAhUK5mJd/+qULZAr6+MRRmduZo6/hz9NCzfF38P/1WzKj7wLKztrT0xjw6FAFeizB/w/AClvliMppYiOj07TT0RsBJMPTk6x/7V6/L8pB6cQERuRpvOpdBTPeeedd7h//z7bt283rAsNDWXTpk106tQJ0J6Cbty4kU2bNvHLL7/w/fff06xZM27cuMHOnTuZOnUqY8aMMbRIHjp0iMaNG/Puu+9y+/Zt5s6dm+73b+/evfTp04dBgwZx7NgxGjRowKRJk5Ltd+nSJX777TfWrFnDsWPHAIiKimLo0KEcPnyYrVu3YmZmRuvWrdHr9YDWTe/tt9+mVKlSBAQEMG7cOIYPH57m2AIDA2nXrh2tWrXi+PHjfPjhh4wePTrJPjExMVSqVIn169dz6tQpevfuTZcuXTh48GCS/ZYuXUrevHk5ePAgAwYMoG/fvrzzzju8+eabHDlyhIYNG9KlSxeio6OTHDdu3Di++uor9u3bx/Xr13n33XeZM2cOy5cvZ/369fzzzz98+eWXhv1f9J4AlC5dGgcHh1R/mjRpkub3KLvIULP+3r17adGiBfb29gwaNIjSpbX+jadPn2bJkiW0aNGC7du38+abUgsws/0T9A9j9o7hUcIjCjkVYl69efg4Jx9AlumUgn1fasX1UeBdA95dRoxVLkavOsFvR24A8E6lAkxs7Ye1xSuYsIic4exf2vSj0ffAzBLqfgJvDpLpR3O4RwmPqLo882bPuxN9hzdXpO0z8EDHA9hZ2qVp39y5c9OkSROWL19uqDqzevVq8ubNS926ddm9ezd6vZ4ffvgBR0dHSpUqRd26dTl//jwbNmzAzMwMX19fpk6dyvbt26latSqurq5YW1tja2v70jMqfvnllzRp0sSQNBYvXpx9+/bx119/JdkvLi6OZcuW4er6tLJH27Ztk+zzww8/4OrqypkzZ/Dz82P58uXo9Xq+//57bGxsKF26NDdu3KBv375pim3RokX4+voyffp0AHx9fTl16lSS5Dl//vxJEt4BAwbw999/8+uvv1KlShXD+nLlyjFmzBgAPvnkE6ZMmULevHnp1asXAJ9//jkLFizgxIkTvPHGG4bjJk6cSPXq1QHo2bMnn3zyCZcvX6Zw4cIAtGvXju3btzNq1Kg0vScAGzZsID4+PtXXbWtrm6b3JzvJ0F/hCRMm4OHhwYEDB5KNfB8xYgRVq1ZlwoQJyaYlFS9Pr/QsOL6AhccXAlDdszpTa03F2do56y+eEKtNQ3p8ubZcqTs0mU5ItJ7e3/zLsesPMTfTMbppSd6vXkgK64vsKSYMNn789D52KwWtF0G+sqaNS4h06tSpE7169WL+/PlYW1vz888/0759e8zMtEbRQoUKJZmf3t3dHXNzc8P2J+tCQkKee50mTZqwe/fuJOtKly5t+Bvv7e3N6dOnAW3MR+vWrZPsW6VKlWTJqbe3d5LEFODixYt8/vnnHDhwgHv37hmeDl67dg0/Pz/Onj1L2bJlk1TpqVat2nNjf9b58+fx9/dPFtuzEhMT+d///sevv/7KzZs3iYuLIzY2Fju7pF8aypZ9+vfC3NwcFxcXypQpY1jn7u4OkOy9ffY4d3d37OzsDInpk3XPPqV90XsC2nv5uslQcnrgwAGGDx+eYkmmfPny0atXL2bOnJmRS4hnRMdHM3rPaLZc2wJA11JdGVJpCBZmRnjSE3FHa/68cRB05tB4ClTpxfEbYfT+8TB3wmNxtrXk644VqVEsb9bHI8TLCNz1ePrR64AOqg/Uph+1eD0GR4qMs7Ww5UDHA2naN+BOAB9t/eiF+81/az6V3Cul6drp0bx5c5RSrF+/Hn9/f3bv3p1kkMt/a4DrdLoU1z3bRJyS7777zlA1B6BYsWJs2LCB/Pnzp3idtLC3t0/x9Xh7e/Ptt9/i6emJXq/Hz8+PuLi4dJ//ZU2fPp25c+cyZ84cypQpg729PYMHD04Ww4ve2yeJ+3/f2//u86J/j7S8J6VLl+bq1aupvqaaNWuycePGF730bCVDWU1cXFySb2X/5eTkZNSb6nV2M/ImA7cN5MKDC1iaWfJ5tc9pVbSVcS5++zj80hHCb2ildd5ZAkXqsfboDUb9dpK4BD3F3Bz4tmtlCuVN/gdHCJOLfwRbxsMBrSIIuQtBq4XgnfanLiJn0Ol0aW5af9PzzTTV/X3T880s6ZNtY2NDmzZt+Pnnn7l06RK+vr5UrJj5NYafJKHP8vb2TnG0vq+vL4cOHUqy7r/LKbl//z7nz5/n22+/pWbNmgDs2bMnyT4lS5bkxx9/JCYmxvD09N9//03ry8DX15cNGzY8N7a9e/fSsmVLOnfuDGjJ5YULFyhVqlSar5NZ0vKegDTrJ1OyZElWrFhBv379sLBIeqqEhARWrlxJyZIlMxSggMPBhxm6YygPYh/gYuPCnLpzKO9W3jgXP/OHNnVjfDS4FIMOK0jMU4SpG87yza4rANQv6cbs98rjaCP1S0U2dDPg8fSjj2s2Vnr/8fSjMkOZyJgndX+H7hiKDl2SBNVYdX87derE22+/zenTpw0JlSkNGDCAWrVqMWvWLJo3b862bdvYuHHjC7t55c6dGxcXF7755hvy5cvHtWvX+Pjjj5Ps07FjR0aPHk2vXr345JNPCAoKYsaMGWmO7cMPP2TWrFmMGjWKnj17cuzYMcNkQk/iK1asGKtXr2bfvn3kzp2bWbNmcefOHZMkp2l5TyD9zfpPBqBFRkZy9+5djh07hpWVlUleY2oyNFq/b9++HDhwgLfeeov169cTGBhIYGAgf/31F2+99RYHDhzgo49e3OQhUrf6wmp6/dOLB7EPKJmnJCveXmGcxFQp2DFVq2EaHw1F3oIPthBm702PJYcMiWn/ukX5pktlSUxF9pMYD9snw3cNtMTUwQM6rYbmcyQxFZnmSd1fNzu3JOvd7dyZVWdWltc5rVevHnny5OH8+fN07NgxS6+VFtWrV2fhwoXMmjWLcuXKsWnTJoYMGfLC2RzNzMxYsWIFAQEB+Pn5MWTIEMPApSccHBz4888/OXnyJBUqVGD06NFMnTo1zbH5+PiwevVq1qxZQ9myZVmwYIFhtP6TuudjxoyhYsWKNGrUiDp16uDh4UGrVq3S9yZkkrS8Jy+jQoUKVKhQgYCAAJYvX06FChVo2tRIU5ynkU6lp3ZFCkaNGpXqN5cRI0YwZcqUjJw+WwgPD8fZ2ZmwsDCcnJyMcs14fTzTD03nl3O/ANC4UGMmVJ+Q7j5JLyUuGv74CE5rs2zxxkfQ4Asu3Y+h97LDXLkXhY2lGTPeKcfbZaVAuTGY4h405XUz7O55WNMbbh/Tlku3gWYzZZanV1RW3ocxMTEEBgbi4+PzUtNhP5GoT+RIyBHuRt/F1c6Vim4VX83yalmgV69enDt3Ltmgquxg0qRJLFy4kOvXr5s6lBwhrb9vGR5JM3XqVHr27Mnvv/9OUFAQAIULF6ZFixYUL148o6fPkR7GPGT4zuEcCNY65Q+sMJAPynxgnNHvYTdhRQetn6mZJbw9Cyp2Zfu5EAb+cpSI2AQ8nW34pmtl/PIboUKAEOmh18OBhbBlHCTGgk0uLSkt087UkYnX3JO6vwJmzJhBgwYNsLe3Z+PGjSxdupT58+ebOiwA5s+fj7+/Py4uLuzdu5fp06fTv39/U4cl/iNThnkXL16ckSNHZsapcrxLDy4xYNsAbkTewM7Cjsk1J1OvYD3jXPzGYVjRESLvgJ0LvPcTqmA1Fu28zNRN51AK/AvlZkHnSuR1kNHNIpt5eE0biW+YfvQtaPmVTD8qhJEdPHiQadOmERERQeHChZk3bx4ffPBBll+3T58+/PTTTylu69y5MwsXLuTixYtMnDiR0NBQChYsyLBhw/jkk0+yPDaRPhlu1t+/fz9fffUVFy9e5P79+8lmuNDpdFy+fDlDQZqasZo2d1zfwahdo4hOiCa/Q36+rPclxXKnPr9xpjq+EtYN0J42uZWGDr8Q41CAUb+d4I9jtwDoUKUg41uUxsoiwxOLiXSSZv3nUAqOLYeNoyAuAizttAFPlXvILE+viVehWV+YXkhICOHh4Sluc3Jyws3NLcVtwniM0qy/bNky3n//fSwtLSlevDgFCxbMyOlyLKUU35/6nnlH5qFQVPGowszaM8llkyvrL65PhK0TYO8cbdm3GbT5htsx5vReuJ+TN8OwMNMxtkVpurzx+hX6Fa+4yLvaLE/n12vLXlWh1QJwKWLauIQQRufm5iYJ6GsiQ8nppEmT8PX1ZcuWLXh6StPZy3iU8Iixe8eyMUgrkNvetz0jq4zE0swIo99jI+C3XnDhcXHemsOg7hgCrj/kwx+PcC8yltx2lszvVIlqRVyyPh4h0uPsn4+nH72v9Y+uNxreHAgyCEUIIV5pGUpOr169yvTp0yUxfUnBUcEM2j6IM/fPYKGz4JOqn/Cu77vGuXhoIPzSAe6eBXNraPk1lH2HXw9dZ8zvp4hL1FPCw5Fvu1bGK0/aClILYRQxYVoT/nGtkgXuftB6IXiUef5xQjxHBnu4CSHS4EWzkT2RoeS0QIECxMbGZuQUOdaxkGMM3j6Y+zH3yW2dm1l1ZlHZo7JxLh60B1Z2gUehWu3H9stJyFeBSX+eZvHeIAAal/Zg5rvlsLc2wtSoQqTVlR3wez9ttjKdGVQfBHU+kelHxUuztLREp9Nx9+5dXF1djVMVRYgcRilFXFwcd+/exczMDCsrq+fun6HMo0+fPvz8888MGTIEc3NpSkurPy79wfj944nXx1M8d3Hm1ZtHfofk08NlicOLYcNw0CeAZwVov5wH5nnpv/ggey/dB2BI/eIMqFcUMzP5Iy2yibho2DpeKxMFkNtHe1pa8A3TxiVeeebm5hQoUIAbN24YyiEKIbKGnZ0dBQsWxMzs+QOrM5ScVqpUid9++40qVarQr18/fHx8UkxSa9WqlZHLvDYS9YnMCpjFsjPLAHir4Fv8r8b/0jyPc8YungB/fwoHF2nLfm2h5ddcCE3gg6V7uRYajZ2VObPeLU9jP4+sj0eItLoRAGs/hPsXteXKPaDBFzLLk8g0Dg4OFCtW7LnzkwshMsbc3BwLC4s0tU5kKDl96623DP/9wQfJi8QrpdDpdCQmJmbkMq+F8LhwRu4ayd6bewHoU64Pfcv1xUxnhLJMjx7Aqu5akyhAvTFQczj/nLnDkJXHiIpLxCuPLd92rUwJj2xaLkjkPInxsGs67JoBKlHrgtLyayiWtdNBipzJ3NxcWgCFyCYylJwuXrw4s+J4rQWGBTJw20CCwoOwMbdhYo2JNCrUyDgXv3cRlr8HoZfB0h7aLEKVeJuvtl1i5uYLAFQr7MLXnSqSx/75fUCEMJqQc7C2tzZTGYBfO2g6XaYfFUKIHCBDyWm3bt0yK47X1t6bexmxcwQR8RF42Hswr+48SrqUNM7FL22BVT0gNgycvaDDL0TnKcmI5UdZf/I2AN2qeTPm7VJYmkthfZEN6PVwYAFsGf90+tG3Z2ndUIQQQuQIMhQ7iyilWHZmGbMCZqFXeiq4VWBWnVnktc1rjItrA0f+/hSUHgpWg3d/5Ea8Pb0X7OfM7XAszXVMaOlHhyoycYLIJh5c1aYfvbpHWy7aAFp8CU75TBuXEEIIo5LkNAvEJsYyYf8E1l1eB0CbYm0YXXU0VuZGaDZPiIP1Q+Hoj9py+c7w9iwOXo+i7097uR8VR14HKxZ0roR/IWkiFdmAUnD0J9j0yePpR+2h0SSo1F2mHxVCiBxIktMMSNQnciTkCHej7+Jq50pFt4qExoQyeMdgTtw9gbnOnBH+I+hYoqNxaudF3dPql17bp9WAbDgR3viInw9eY+wfp0nQK0p7OvFN18rkz2Wb9fEI8SKRIbBu4NNZyrzegNYLIE9h08YlhBDCZCQ5fUlbrm5hysEp3Im+Y1iXxyYPCfoEwuPCcbJyYkbtGVTzrGacgIJPaTM+hV0Daydot5j4wvUY/8cpfvr3GgBvl83H9HblsLWSEakiGzjzB/w1RJt+1NwK6o6GNwfI9KNCCJHDSXL6ErZc3cLQHUNRJJ3uLjQmFAB3O3e+b/Q93k7exgno3Hr4rRfER2lPnDqs4L5tIT767gAHAkPR6WB4Q18+qlNEZj8RpvfooTb96IkV2rK7H7ReBB5+Jg1LCCFE9pCthmhPnjwZf39/HB0dcXNzo1WrVpw/f96wPTQ0lAEDBuDr64utrS0FCxZk4MCBhIWFPfe8Sik+//xz8uXLh62tLfXr1+fixYsvFWOiPpEpB6ckS0yTXA9FAYcCL3X+dFEKds+EFZ20xNSnNnywlTPx+Wjx1V4OBIbiYG3Bt10q069uUUlMheld3g4L3tQSU50Z1BgKvbZJYiqEEMIgWyWnO3fupF+/fvz7779s3ryZ+Ph4GjZsSFRUFAC3bt3i1q1bzJgxg1OnTrFkyRI2bdpEz549n3veadOmMW/ePBYuXMiBAwewt7enUaNGxMTEpDvGIyFHkjTlpyQkOoQjIUfSfe50iX8Ea3rB1gmAgiq9ofNvbLwcS9sF+7j58BGFXOxY+9Gb1C/lnrWxCPEicdGwYST82ArCb2rTj76/CeqPBQtrU0cnhBAiG8lWzfqbNm1KsrxkyRLc3NwICAigVq1a+Pn58dtvvxm2FylShEmTJtG5c2cSEhKwsEj+cpRSzJkzhzFjxtCyZUsAli1bhru7O7///jvt27dPV4x3o+9m6n4vJfw2rOgIt46AmQU0nY6+4vvM2XqReVu1J8I1i+Xlqw4VcbazzLo4hEiLG4cfTz96SVuu3BMafgFW9qaNSwghRLaUrZLT/3rSXJ8nT+olj8LCwnByckoxMQUIDAwkODiY+vWfTnno7OxM1apV2b9/f7qTU1c710zdL91uHtES04jbYJsb3v2RSM9qDPs5gL9Pa090e9bw4ZMmJbCQwvrClBLiYNc0reuJ0oNjPmj5FRSV6UeFEEKkLtsmp3q9nsGDB1O9enX8/FLuj3bv3j2++OILevfunep5goODAXB3T9q07e7ubtj2X7GxscTGxhqWw8PDDf9d0a0i7nbuhESHpNjvVIcOdzt3KrpVTP3FvayTq+GPfpAQA64loMMvXFMe9Jq/j/N3IrAyN+N/bcrQrpIR+ruK19Lz7v10CTkLa3pD8Altucw72vSjtrkzIUohhBCvs2z7aK1fv36cOnWKFStWpLg9PDycZs2aUapUKcaNG5ep1548eTLOzs6GHy8vL8M2czNzPq7yMaAlos96sjyqyijMM7Mcjl4P2ybCbz21xLR4Y+i5mX2hTrT4eg/n70Tg6mjNig/fkMRUZMjz7v1k9IkQuFv70hS4W1vWJ8K+L2FRbS0xtc0N7RZD2+8kMRVCCJEmOqVU6sPOTaR///788ccf7Nq1Cx8fn2TbIyIiaNSoEXZ2dvz111/Y2Nikeq4rV65QpEgRjh49Svny5Q3ra9euTfny5Zk7d26yY1J6euTl5WXoQgAp1zn1sPNgVJVR1PfOxGbL2Eitv965v7Tl6oNQ9T5n2YEbTPjrDIl6RbkCzizqUhkP59TfB/FqCw8Px9nZOck9mBXScu8DcGYdbBoF4beernNw1xLQu+e05WINtelHHT2yLF6RMxjr/hdCZA/ZqllfKcWAAQNYu3YtO3bsSDExDQ8Pp1GjRlhbW7Nu3brnJqYAPj4+eHh4sHXrVkNyGh4ezoEDB+jbt2+Kx1hbW2Nt/fwRxPW961PXq26yGaIy9Ynpw2taYf07p7Qi5c3nEef3Hp//fooVh64D0LpCfia3KYONpRQuFxmXlnufM+vg167w324tkXe0H3NraDoNKnaT6UeFEEKkW7ZKTvv168fy5cv5448/cHR0NPQJdXZ2xtbWlvDwcBo2bEh0dDQ//fQT4eHhhj5xrq6umJtrCVqJEiWYPHkyrVu3RqfTMXjwYCZOnEixYsXw8fHhs88+w9PTk1atWmUoXnMzc/w9/DN0jlRd+1erXxp9D+zdoP3P3M1Vjr7f/svhqw8w08HHTUrQq2ZhqV8qjEefqD0xfU6dX2xzQYUukpgKIYR4KdkqOV2wYAEAderUSbJ+8eLFdO/enSNHjnDgwAEAihYtmmSfwMBAChUqBMD58+eTFOYfOXIkUVFR9O7dm4cPH1KjRg02bdr0wqeuJnP0J/hzMOjjwaMsdPiFkxGO9P5qD7fDYnC0seDLDhWo4+tm6khFTnN1X9Km/JRE3tH286lpnJiEEEK8VrJVcvqi7q916tR54T4pnUen0zFhwgQmTJiQofiynD4RNn8O+7/Slku1hFYLWHc2jBGr9hGboKewqz3fda1MYVcH08YqcqbI509Ake79hBBCiP/IVslpjhYTBqt7wKUt2nKdT0isOYIZmy+yYMdlAOr6ujK3QwWcbKSwvjARhzTONpbW/YQQQoj/kOQ0O7h/GX5pD/cugIUttF5ARJG3GfTjEbadCwGgT+0ijGjki7mZ9OMTJuT9Jjh5arOUpdjvVKdt937T2JEJIYR4TUhyampXdsCv3SDmITjlh/bLCbQqRq/5+7gUEom1hRnT2pWlZfn8po5UCDAzh8ZTH4/W15E0QX38xanxFG0/IYQQ4iVk2yL8OcLBb+HHNlpiWsAfem1nV2R+Wn61h0shkXg42bCqTzVJTEX2UqoFvLsMnPIlXe/kqa0v1cI0cQkhhHgtyJNTU0iMh40j4fAP2nLZ9qjmc/j+39v8b8NZ9AoqFszFwi6VcHPMphUFRM5WqgWUaKaNyo+8o/Ux9X5TnpgKIYTIMElOjS06VGsSDdoN6KDBeGL8+zF67Wl+O3IDgHcrF+CLVn5YW8gHvcjGzMylXJQQQohMJ8mpMYWcg1/egwdBYOUAbb/nTr46fPjtAY5df4i5mY4xzUrS/c1CUlhfCCGEEDmSJKfGcuFvWN0T4iIglzd0XMmx2Hz0/nIPIRGxONtaMr9TRaoXzWvqSIUQQgghTEaS06ymFOybB5vHAgq8a8C7y1hz/hEfr9lPXIKe4u4OfNu1Mt4u9qaOVgghhBDCpCQ5zUrxMfDXYDj+i7Zc6X0SGk1h6uYrfLs7EIAGpdyZ/V55HKzln0IIIYQQQjKirBJxB1Z2hhsHQWcOTaYSVrobA346xq4LdwEYUK8oQ+oXx0wK6wshhBBCAJKcZo3bx+GXjhB+A2yc4Z2lXHL0p9eCfQTei8LW0pwZ75SjWdl8Lz6XEEIIIUQOIslpRugTk9d5PPcXrO0D8dHgUgw6rmT7XUcGfr2XiNgE8uey5ZuulSjt6Wzq6IUQQgghsh1JTl/WmXWwaRSE33q6ztoRYiO0/y7yFqrd9yw8EMq0vw+hFFQplIf5nSuS18HaNDELIYQQQmRzkpy+jDPrHs8trpKuf5KYFm/MozbLGLX2DOuOa8lrx6oFGde8NFYWMmOsEEIIIURqJDlNL32i9sT0v4npMxJvnaD9N/9y/FYkFmY6xrYoTZc3vI0XoxBCCCHEK0qS0/S6ui9pU34KzCNvYRt3kDz25ZnfqSJvFHYxUnBCCCGEEK82SU7TK/JOmnYrnyuG6R9UxyuPXRYHJIQQQgjx+pDkNL0c3NO02+DWNbGRxFQIIYQQIl1kdE46JXpV4w4u6FPpcqpXEIwLlj41jBuYEEIIIcRrQJLTdDp4NYzP47oAJEtQnyyPjevCwathRo5MCCGEEOLVJ8lpOoVExPC3vgp94wcTTJ4k24JxoW/8YP7WVyEkIsZEEQohhBBCvLqkz2k6uTnaAPC3vgqbYytTxewcbjwkhFwc1JdA/zjff7KfEEIIIYRIO0lO06mKTx7yOdsQHBaDHjP+1ZdKsl0HeDjbUMUnT8onEEIIIYQQqZJm/XQyN9MxtrmWkOr+s+3J8tjmpTA3++9WIYQQQgjxIpKcvoTGfvlY0LkiHs5Jm+49nG1Y0Lkijf3ymSgyIYQQQohXmzTrv6TGfvloUMqDg4GhhETE4OaoNeXLE1MhhBBCiJcnyWkaKKXViAoPD0+2rbSrJaVdLQGIiowwalwi53hy7z25F43lefe+EMZiqvtfCGEakpymQUSElnR6eXmZOBKR00VERODs7GzU64Hc+yJ7MPb9L4QwDZ2Sr6IvpNfruXXrFo6Ojuh0SZvtw8PD8fLy4vr16zg5OZkoQvE6eN69pJQiIiICT09PzMyM11X8efc+yP0vMseL7iNT3f9CCNOQJ6dpYGZmRoECBZ67j5OTk3w4i0yR2r1kiidGabn3Qe5/kTmedx/JE1Mhcg75CiqEEEIIIbINSU6FEEIIIUS2IclpBllbWzN27Fisra1NHYp4xb2K99KrGLPIfuQ+EkI8SwZECSGEEEKIbEOenAohhBBCiGxDklMhhBBCCJFtSHIqhBBCCCGyDUlOhRBCCCFEtiHJqRBCCCGEyDYkORVCCCGEENmGJKdCCCGEECLbkORUCCGEEEJkG5KcCiGEEEKIbEOSUyGEEEIIkW1IciqEEEIIIbINSU6FEEIIIUS2IcmpEEIIIYTINixMHcCrQK/Xc+vWLRwdHdHpdKYOR+RASikiIiLw9PTEzMx43ynl3hfZganufyGEaUhymga3bt3Cy8vL1GEIwfXr1ylQoIDRrif3vshOjH3/CyFMQ5LTNHB0dAS0P4xOTk4mjkbkROHh4Xh5eRnuRWORe19kB6a6/4UQpiHJaRo8ac50cnKSD2hhUsZuWpd7X2Q5fSJc3QeRd8DBHbzfBDPzFHeVriVC5AySnAohhDCNM+tg0ygIv/V0nZMnNJ4KpVqYLi4hhElJz3IhhBDGd2Yd/No1aWIKEH5bW39mnWniEkKYnCSnQgghjEufqD0xRaWw8fG6TR9r+wkhchxJToUQQhjX1X3Jn5gmoSD8prafECLHkeRUCCGEcYXdSNt+kXeyNg4hRLYkA6KEEEJkPX0iBO2Gk6vh1Jq0HePgnrUxCSGyJUlOhRBCZA2l4GaAlpCeXpP0SajODJQ+lQN12qh97zeNEqYQInuR5FQIIUTmCjn7+AnpangQ9HS9bW4o1RL82kH0fVjV/fGGZwdGPa5l2nhKqvVOhRCvN0lOhRBCZNyDq3DqNy0pDTn9dL2lPZRoCmXegcJ1wcLq6TbdslTqnE6ROqdC5GCSnAohhHg5kSFweq2WkN44+HS9mSUUawhl2kLxxmBln/LxpVpAiWZpniFKCJEzGD053bVr13O363Q6bG1tKViwIG5ubkaKSgghRJo8egjn/tIS0sCdT/uN6sygUE0o0w5KNtea8NPCzBx8amZZuEKIV4/Rk9M6deqkeX7kMmXKMGXKFBo3bpzFUQkhhEhV/CO4sElLSC/+A4lxT7flr6wlpKVbg6OH6WIUQrw2jJ6c/vDDD3z99ddcvHiRTp064evrC8C5c+dYvnw5vr6+dOnShfPnz/Pjjz/SvHlz/vnnH+rWrWvsUIUQIudKjIcrO+DkKji3HuIin25zLak12fu1hTyFTRaiEOL1ZPTkNCoqinv37nHhwoVkzfaff/45b7zxBubm5nz55Zd8+umnlC9fnsmTJ0tyKoQQWU2vh2v7tVH2p3+HR6FPt+UqqI2yL9MO3EubLEQhxOvP6Mnp3Llz6dWrV4r9ST08POjVqxdz5syhb9++5MuXjw8++ID58+cbO0whhMgZlILbx7WE9NQabdrQJ+zdtOb6Mu2ggD+ksUuWEEJkhNGT02vXrmFnZ5fqdnt7e65du2ZY9vHxISYmxhihCSFEznHvkpaQnlwF9y89XW/trA1oKtMWCtUCcynqIoQwLqP/1SlUqBDLly/no48+wsrKKsm2uLg4fvrpJ7y9vQ3rbty4gYuLi7HDFEKI10/YTa0W6anV2tPSJyxstJJPZd6BovXB0sZ0MQohcjyjJ6eDBg2iX79+VK1alb59+1K8eHEAzp8/z4IFCzh58iRfffWVYf81a9ZQpUoVY4cphBCvh6j7cOZ3LSm9ug/DbEw6cyhST0tISzQFa0dTRimEEAZGT0779u1LeHg448ePp0+fPoayUkoprK2tmTRpEn379gUgNjaW6dOnU7RoUWOHKYQQr67YCDi3QXtCenkb6BOebvOuro2yL9UK7KVVSgiR/eiUUurFu2W+Bw8esHnzZgIDAwGtub9BgwbkyZPHFOE8V3h4OM7OzoSFheHk5GTqcEQOZKp7UO79V0hCLFzcrPUhvfA3JDx6ui1fOW2kvV8bcC5guhhfktyHQuQsJuvpnjt3bt59911TXV4IIV59iQkQtAtO/gZn/4TYsKfbXIo+Lf2Ut5jpYhRCiHSSYZhCCPEqUQpuHNJmazq9FqJCnm5z9HxcHL+d9rRUSj8JIV5BJklOV6xYwZdffsnFixe5f/9+su06nY6EhIQUjhRCiBzqzmktIT31Gzy8+nS9bR4o3UpLSAtWAzMzk4UohBCZwejJ6fTp0/n4449xcXHhjTfekDJRQgiRmtBALRk9uRrunn263soBSjTTEtIidcHc0nQxCiFEJjN6cvr1119TtWpVtm7diq2trbEvL4QQ2VtEsNZcf3I13Dz8dL25FRRrqPUhLdYIrFKfzEQIIV5lRk9Og4ODGTlypCSmQgjxxKMH2oCmk6shaDcovbZeZwY+tbWEtMTbYJvLpGEKIYQxGD05LVq0KA8fPjT2ZYUQInuJi4YLG7WR9pc2Q2Lc020FqmgJaalW4OhushCFEMIUjJ6cDhs2jIkTJzJw4EAcHByMfXkhhDCdhDi4sl2rRXpuA8RHPd3mVvrxSPu2kLuQyUIUQghTM3pyam5ujpubGyVKlKBHjx74+Phgbm6ebL+uXbsaOzQhhMh8ej1c3avN1nTmD60J/4lc3tr0oWXagVtJ08UohBDZiNFniDJLQ5kTnU5HYmKiEaJJG5mdRJiazBD1ilEKbh3VRtqfWgMRt55uc3CH0m20hDR/JalFmgZyHwqRsxj9yen27duNfUkhRBZI1CsOBoYSEhGDm6MNVXzyYG6WwxOtuxe0J6QnV0Po5afrbZyhZAstIS1UE8yStxYJIYTQGD05rV27trEvKYTIZJtO3Wb8n2e4HRZjWJfP2YaxzUvR2C+fCSMzgYfXHz8hXQ3BJ5+ut7AF3yZaQlq0PlhYmy5GIYR4hbxWU4lMmTIFnU7H4MGDDetiYmLo168fLi4uODg40LZtW+7cuWO6IIV4xW06dZu+Px1JkpgCBIfF0PenI2w6ddtEkRlR1D04+C380Bjm+MGWsVpiamah1SBt8x2MuATvLNaK5UtiKoQQaZblT06XLVsGQJcuXdDpdIblF0nvgKhDhw6xaNEiypYtm2T9kCFDWL9+PatWrcLZ2Zn+/fvTpk0b9u7dm67zCyG0pvzxf54hpY7qCtAB4/88Q4NSHq9fE39MOJxbrz0hvbwd1JN+8TooVEMbZV+qJdjlMWmYQgjxqsvyAVFmZmbodDoePXqElZWVYfl5l03vgKjIyEgqVqzI/PnzmThxIuXLl2fOnDmEhYXh6urK8uXLadeuHQDnzp2jZMmS7N+/nzfeeCNN55fO+MLUssuAqP2X79Ph239feNwvvd6gWpHXYGri+Bi4+I+WkF74GxKeeVrsWUGbPtSvDTh5mi7GHED+BguRs2T5k9MnA6CsrKySLGemfv360axZM+rXr8/EiRMN6wMCAoiPj6d+/fqGdSVKlKBgwYLpSk6FEJqQiJgX75SO/bKlxAQI3KkNajr3F8SGP92Wt7iWkJZpBy5FTBejEEK8xrI8Of3vAKjMHhC1YsUKjhw5wqFDh5JtCw4OxsrKily5ciVZ7+7uTnBwcKrnjI2NJTY21rAcHh6e6r5CvE5edO+7Odqk6Txp3S/b0OvhxkEtIT3zO0TdfbrNqYD2dLTMO+BRRko/CSFEFjP6gKgePXpw4MCBVLcfPHiQHj16pOlc169fZ9CgQfz888/Y2GTeh+HkyZNxdnY2/Hh5eWXauYXIzl5071fxyUM+ZxtSS890aKP2q/i8Av0uldIGMW0eC3PLwQ+N4NC3WmJq5wL+H8D7m2DwSWj4BeQrK4mpEEIYgdGT0yVLlnD58uVUtwcGBrJ06dI0nSsgIICQkBAqVqyIhYUFFhYW7Ny5k3nz5mFhYYG7uztxcXE8fPgwyXF37tzBw8Mj1fN+8sknhIWFGX6uX7+epniEeNW96N43N9MxtnkpgGQJ6pPlsc1LZe/BUKFXYOd0+LoqLKwBe+dA2DWwcoRyHaDTbzDsPDSbCd7VIA0ThwghhMg8Rq9z+iJRUVFYWlqmad+33nqLkydPJln3/vvvU6JECUaNGoWXlxeWlpZs3bqVtm3bAnD+/HmuXbtGtWrVUj2vtbU11tZS+kXkPGm59xv75WNB54rJ6px6ZOc6p+G34fRabU77W0eerje3huINtX6kxRuBpa3pYhRCCAEYKTm9du0aQUFBhuVz586xa9euZPuFhoayYMECihYtmqbzOjo64ufnl2Sdvb09Li4uhvU9e/Zk6NCh5MmTBycnJwYMGEC1atVkMJQQGdDYLx/1Sriy/PgOroUHU9DJg47lamNlkY2+70aHwtl1Wj/SoD3wpACWzhwK19b6kJZops3eJIQQItswyifJ4sWLGT9+PDqdDp1Ox6RJk5g0aVKy/ZRSmJmZsXjx4ky79uzZszEzM6Nt27bExsbSqFEj5s+fn2nnFyIn2nJ1C1MOTuFO9NMJLX664s7HVT6mvnf95xyZxeKi4PxGLSG9tAX08U+3eVXVEtJSrcDB1WQhCiGEeL4sr3MKcPz4cY4dO4ZSih49etC7d+9kzeo6nQ4HBwf8/f2z3QAkqbEnTC271DkFLTEdumMo6j+l+HWPe53OqjPLuAlqQhxc3qo12Z/fCPHRT7e5l4EybbUC+bkKGi8mkankb7AQOYtRnpyWK1eOcuXKAXD16lXatm2brDleCJH9JeoTmXJwSrLEFECh0KFj6sGp1PWqi7mZedYFok+Eq3u1hPTMOoh5+HRbbh+tDqlfO3ArkXUxCCGEyBJG7yA2duxYY19SCJFJjoQcSdKU/18KRXB0MEdCjuDv4Z+5F1cKbh7RZms6tQYin6lV7ODxuBZpO/CsKCWfhBDiFWay0Qt37tzh8OHDPHjwAL1en2x7165dTRCVEOJ57kbfffFO6dgvTULOaQnpydXwIPDpeptc2lz2ZdqBd3XIyie14rWn1+uJi4szdRhCvLYsLS0xN0/b32mjJ6d6vZ5+/frx3XffpZiUPiHJqRDZj6td2gYSpXW/VD28Bqd+0xLSO6eerre0A9+mWkJa5C2wsMrYdYQA4uLiCAwMfO5nkhAi43LlyoWHhwe6F7RuGT05nTFjBosWLaJz5840bNiQrl27MnXqVBwdHZkzZw7Ozs5MnjzZ2GEJIdKgoltF3O3cCYkOSbHfqQ4d7nbuVHSrmP6TR97VapGeWg3Xn5lFzswSitbXElLfJmBln4FXIERSSilu376Nubk5Xl5emMmkC0JkOqUU0dHRhISEAJAv3/PrYRs9OV26dCmNGzdm2bJl3L9/H4BKlSpRr149unTpQtmyZQkICKDe/9m77/iazj+A4597s3eCLEQSRIjR2mLvVapKtbVq16y9ilLqZ9Zs0RZFUW2pltpBVFGUplaNROxE1MjeOb8/bnPrynATyb2JfN+v13nJPec5z/ne6yT55jnPaN7c0KEJIZ7DRG3CpDqTGBM4BhUqnQQ1fbT+xDoT9R8MlRAJf/+iSUivHwElVVsbXg01Uz9V6gjWhWA5VFEopaSkEBcXR8mSJbG2tjZ2OEK8tKysNIucRERE4OLiku0jfoMnp9evX+f9998H0P6FmpysmYvQxsaGvn37snr1asaPH2/o0IQQemjp2ZJFTRdlmOfU1dqViXUmPn8aqeR4uLpPk5Be3Q+pif8dK1VTM8q+cmewL4ArTYmXTmqq5g8ic3PpIiJEfkv/AzA5OblgJadWVlba5UltbW1RqVTaZl4ANzc3WcteiAKupWdLmnk042zEWR7EPcDZ2pkaLjWybjFNTda0jJ7/AS7vgqTo/46V8NW0kFZ5E4qXM8wbEOIZz+sDJ4R4cfp+nxk8OfX09CQkJATQjNwqX748e/fupVevXgAEBATg6upq6LCEEDlkAtSOT4DYOFAlZCyQlqbpO3r+B7j0E8Q9/O+YQ5n/pn5yrSJTPwlRgPXp04cnT57w008/GTsUUUQYPDlt3rw527dvZ+HChQD06tWLjz76iHv37qEoCkePHmXcuHGGDksIkROXdsDeiRB177999iWh7Vxw8tKMsr/wI0Td+e+4dQnN4/qqb0Hp2iADT4QQQmTC4MnpuHHjaN26NYmJiVhYWDB58mQiIiLYuHEjJiYmDBo0iBkzZhg6LCGEvi7tgO97w7Oj9aPu/bv/KRb2mgFNVbqAdxMwMdrUykLkq9Q0hVOhj4iITsDFzpI63sUwURvuiUBSUpL0mxUvDYM3Xbi7u9OmTRssLCwAMDExYdmyZTx69IgHDx6wcuVK7YguIUQBk5aqaTHNZBopHZVeh27fwLhr8MYKKN9CElPx0tp7IYyG8w7x7le/M3JLEO9+9TsN5x1i74WwfLtm06ZNGT58OKNGjaJEiRK0adOGRYsWUbVqVWxsbPDw8GDo0KHExMRoz1m3bh2Ojo7s27ePSpUqYWtrS9u2bQkL+y/O1NRUxowZg6OjI8WLF2fChAkoiu73e2JiIh988AEuLi5YWlrSsGFDTp8+rT0eGBiISqVi3759VK9eHSsrK5o3b05ERAR79uyhUqVK2Nvb0717d+Li4vLtMxKFV4F7rhYTE8OsWbOMHYYQIjM3j+s+ys9KnUHg9zqYWeZ/TEIY0d4LYQzZeJawSN1+1+GRCQzZeDZfE9T169djbm7OsWPHWLVqFWq1mmXLlnHx4kXWr1/PoUOHmDBhgs45cXFxLFy4kG+++YZff/2VW7du6XSl+/TTT1m3bh1r167lt99+49GjR2zfvl2njgkTJrBt2zbWr1/P2bNnKV++PG3atOHRo0c65WbMmMFnn33G8ePHuX37Nt26dWPJkiVs3ryZXbt2sX//fpYvX55vn48ovApMU0ZMTAzLli1j0aJFPH78mGnTphk7JCHEs2LuP79MTsoJUcAoikJ8curzC6J5lD99x8VMnyMogAqYseMSDcqX0OsRv5WZSY5mDfDx8WH+/Pna176+vtqvvby8+OSTTxg8eDArVqzQ7k9OTmbVqlWUK6eZGWP48OHMnDlTe3zJkiVMnjyZN998E4BVq1axb98+7fHY2FhWrlzJunXraNeuHQBfffUVBw4cYM2aNTrTQH7yySc0aNAAgP79+zN58mRCQkIoW7YsAF27duXw4cNMnDhR7/csigaDJadbtmxhzpw5XLt2jWLFitGrVy9mz56NWq3myy+/ZOrUqfzzzz94eXnxv//9z1BhCSFywlbPmTT0LSdEAROfnIrfR/ueX1APChAelUDVGfv1Kn9pZhuszfX/tVyzZk2d1wEBAcyZM4fLly8TFRVFSkoKCQkJxMXFaeeXtLa21iamoOlqlz6dY2RkJGFhYdStW1d73NTUlFq1amkf7YeEhJCcnKxNOkEz806dOnX4+++/deKpVq2a9mtXV1esra21iWn6vlOnTun9fkXRYZDkdOfOnXTv3h2AEiVKEB4ezvz581GpVDx+/JgvvviC8uXLM3/+fHr16pXtxKxCCCPyrK8ZlR8VRub9TlWa4571DR2ZEEWOjc1/S/neuHGDDh06MGTIEGbPnk2xYsX47bff6N+/P0lJSdrkNH2e8XQqlSpDn9K88vS1VCpVptdOS0vLl2uLws0gyenSpUtxcXFh//79VKtWjcePH/Pmm2+yZMkSkpOTmTNnDmPHjsXUtMD0MhBCZEZtAm3n/TsqX4Vugvrv48i2czXlhCiErMxMuDSzjV5lT4U+os/Xp59bbl3f2tTxfv4SvFZmuf++OXPmDGlpaXz66afa1Re///77HNXh4OCAu7s7J0+epHHjxoBmedczZ85Qo0YNAMqVK6ft5+rp6QlougqcPn2aUaNG5Tp+IZ5mkGzwzz//ZPjw4domficnJz755BMaNWrE6NGjpb+JEIWJ3+vQbUPW85z6vW682IR4QSqVSu9H6418nHF3sCQ8MiGr5wi4OVjSyMc536eVKl++PMnJySxfvpyOHTtqB0nl1MiRI5k7dy4+Pj5UrFiRRYsW8eTJE+1xGxsbhgwZwvjx4ylWrBhlypRh/vz5xMXF0b9//zx8R6IoM0hy+uTJE50+LqD5RgJo1qyZIUIQQuQlv9eh4mua0fsx9zV9TD3rS4upKFJM1Cqmd/RjyMazWT1HYHpHP4PMd/rKK6+waNEi5s2bx+TJk2ncuDFz5syhd+/ezz/5KWPHjiUsLIz33nsPtVpNv3796Ny5M5GRkdoyc+fOJS0tjV69ehEdHU2tWrXYt28fTk5Oef22RBGlUvKrs8lT1Go1Gzdu1PY7BXj48CHOzs4EBATQvHnz/A7hhURFReHg4EBkZCT29vbGDkcUQca6B+XeFwVBft6HCQkJhIaG4u3tjaVl7qY+23shjI93XtKZTsrdwZLpHf1oW8U9r0IVotDT9/vNYJ08b9y4wdmzZ7Wv0/8Ku3btGo6OjhnKp/dvEUIIIQqytlXcaeXnZtQVooR4mRgsOZ02bVqmc5cOHTo00/KpqfrNMyeEEEIYm4lahX+54sYOQ4iXgkGS0+nTpxviMkIIIYQQopCT5FQIIYQQQhQYakNcZMqUKZw5c8YQlxJCCCGEEIWYQZLTFStWUKdOHcqUKcOIESM4dOiQ9CkVQgghhBAZGCQ5ffDgAXv37qVDhw5s376dli1b4uLiQu/evfnpp5+Ij483RBhCCCGEEKKAM0hyampqSqtWrVixYgV37tzh+PHjDBgwgFOnTvHmm29SokQJ3njjDTZs2MCjR48MEZIQQgghhCiADJKcPqtevXrMmzePy5cvc/HiRT788EPu3r1Lnz59cHNzo3nz5ixfvpzw8HBjhCeEEEIIIYzEKMnp0ypVqsSUKVM4ffo0N2/eZOHChahUKsaMGcOXX35p7PCEEEKIQkGlUvHTTz9lW+by5cvUq1cPS0tLXn31VYPEJUROGT05fZqHhwcffPABBw8e5P79+7z77rvPPWflypVUq1YNe3t77O3t8ff3Z8+ePdrjTZs2RaVS6WyDBw/Oz7chhBCiqElLhdCjcH6r5t+0gjnod/r06djY2HDlyhUOHjyY4/NnzJiRL0ltftX7srt16xavvfYa1tbWuLi4MH78eFJSUrI959GjR/To0QN7e3scHR3p378/MTExOmXOnTtHo0aNsLS0xMPDg/nz5+scv3jxIl26dMHLywuVSsWSJUvy9H0ZbIWozMTFxfHw4UMURclwrEyZMhQrVuy5dZQuXZq5c+fi4+ODoiisX7+eTp068eeff1K5cmUABg4cyMyZM7XnWFtb592bEEIIUbRd2gF7J0LUvf/22ZeEtvPA73WDhJCUlKRXuZCQEF577TU8PT0zPX7jxg28vb0z/b0sCpbU1FRee+013NzcOH78OGFhYfTu3RszMzP+97//ZXlejx49CAsL48CBAyQnJ9O3b18GDRrE5s2bAYiKiqJ169a0bNmSVatWcf78efr164ejoyODBg0CNPlb2bJleeuttxg9enTevznFwFJTU5U5c+YoJUuWVNRqdZbbi3ByclJWr16tKIqiNGnSRBk5cuQL1RcZGakASmRk5AvVI0RuGeselHtfFAT5eR/Gx8crly5dUuLj43NXwcWfFWW6g6JMt39mc9BsF3/Ow2j/06RJE2XYsGHKyJEjleLFiytNmzZVAGXFihVK27ZtFUtLS8Xb21v54YcftOcAOtv06dMz1BsaGqpklRp8/fXXGer4+uuvFUVRlMePHyv9+/dXSpQoodjZ2SnNmjVTgoKCFEVRlIiICMXV1VWZPXu2tq5jx44pZmZmSkBAQLb1Zufvv/9WGjRooFhYWCiVKlVSDhw4oADK9u3btWUmTJig+Pj4KFZWVoq3t7cydepUJSkpSXt8+vTpyiuvvKKsWbNG8fDwUGxsbJQhQ4YoKSkpyrx58xRXV1fF2dlZ+eSTT3SuDSirVq1SXnvtNcXKykqpWLGicvz4ceXatWtKkyZNFGtra8Xf318JDg7WnhMcHKy8/vrriouLi2JjY6PUqlVLOXDgwHPfZ1Z2796tqNVqJTw8XLtv5cqVir29vZKYmJjpOZcuXVIA5fTp09p9e/bsUVQqlXL37l1FURRlxYoVipOTk04dEydOVHx9fTOt09PTU1m8eLFeMev7/WbwltNJkyaxcOFCKleuTJcuXShePO/WIk5NTeWHH34gNjYWf39/7f5NmzaxceNG3Nzc6NixI9OmTcu29TQxMZHExETt66ioqDyLUYiCTO59UeQpCiTH6Vc2LRX2TECTT2WoCFBpWlTLNgW1yfPrM7MGlUrvUNevX8+QIUM4duwYABUrVmTatGnMnTuXpUuX8s033/DOO+9w/vx5KlWqRFhYGC1btqRt27aMGzcOW1tbva8F8Pbbb3PhwgX27t1LQEAAAA4ODgC89dZbWFlZsWfPHhwcHPjiiy9o0aIFV69exdnZmbVr1/LGG2/QunVrfH196dWrF8OHD6dFixbEx8dnWW9WUlNTeeONNyhTpgwnT54kOjqasWPHZihnZ2fHunXrKFmyJOfPn2fgwIHY2dkxYcIEbZmQkBD27NnD3r17CQkJoWvXrly/fp0KFSpw5MgRjh8/Tr9+/WjZsiV169bVnjdr1iwWLVrEokWLmDhxIt27d6ds2bJMnjyZMmXK0K9fP4YPH67tahgTE0P79u2ZPXs2FhYWbNiwgY4dO3LlyhXKlCkDwODBg9m4cWO27z39EfyJEyeoWrUqrq6u2mNt2rRhyJAhXLx4kerVq2c498SJEzg6OlKrVi3tvpYtW6JWqzl58iSdO3fmxIkTNG7cGHNzc516582bx+PHj3Fycso2vrxg8OR048aNtG3blt27d+dZnefPn8ff35+EhARsbW3Zvn07fn5+AHTv3h1PT09KlizJuXPnmDhxIleuXOHHH3/Msr45c+bw8ccf51l8QhQWcu+LIi85Dv5XMo8qUzSP+ud66Ff8w3tgbqN37T4+Phn6Ar711lsMGDAA0CRPBw4cYPny5axYsQI3NzdMTU2xtbXFzc1N7+uks7KywtbWFlNTU53zf/vtN06dOkVERAQWFhYALFy4kJ9++omtW7cyaNAg2rdvz8CBA+nRowe1atXCxsaGOXPmZFtvdg4cOEBISAiBgYHac2bPnk2rVq10yk2dOlX7tZeXF+PGjWPLli06yWlaWhpr167Fzs4OPz8/mjVrxpUrV9i9ezdqtRpfX1/mzZvH4cOHdZLTvn370q1bNwAmTpyIv78/06ZNo02bNgCMHDmSvn37asu/8sorvPLKK9rXs2bNYvv27ezYsYPhw4cDMHPmTMaNG6fXZxAeHq6TmALa11nNdhQeHo6Li4vOPlNTU4oVK6Y9Jzw8HG9v7yzrfSmT08ePH9OpU6c8rdPX15egoCAiIyPZunUr7733HkeOHMHPz0/bPwKgatWquLu706JFC0JCQihXrlym9U2ePJkxY8ZoX0dFReHhoecPFyEKMbn3hSg8atasmWHf008N018HBQVlW0/lypW5efMmgLav6dOtqo0aNdIZaPysv/76i5iYmAxPQuPj4wkJCdG+XrhwIVWqVOGHH37gzJkz2kQ2N65cuYKHh4dOMlunTp0M5b777juWLVtGSEgIMTExpKSkYG9vr1PGy8sLOzs77WtXV1dMTExQq9U6+yIiInTOq1atms5x0OQZT+9LSEggKioKe3t7YmJimDFjBrt27SIsLIyUlBTi4+O5deuW9hwXF5cMyWNRZPDktGrVqoSFheVpnebm5pQvXx7QfLOePn2apUuX8sUXX2Qom/5XT3BwcJbJqYWFxQt90whRWMm9L4o8M2tNC6Y+bh6HTV2fX67HVvCsr9+1c8DGRv9W1uzs3r2b5ORkAO7evUvTpk11ElorK6tsz4+JicHd3Z3AwMAMxxwdHbVfh4SEcO/ePdLS0rhx44ZOIpcfTpw4QY8ePfj4449p06YNDg4ObNmyhU8//VSnnJmZmc5rlUqV6b60tLQsz1P92x0js33p540bN44DBw6wcOFCypcvj5WVFV27dtUZzJaTx/pubm6cOnVK59j9+/e1xzLj5uaWIclOSUnh0aNH2nPc3Ny09ehbb14zeHI6ffp0+vfvT//+/fOtRSYtLU2n39zT0r/h3N3d8+XaQgghCjGVSv9H6+Waa0blR4WReb9TleZ4ueb69TnNA7///ju9e/fWeZ1Z38OnPT1y39RUkxakN/g8y9zcnNRU3WmyatSoQXh4OKampnh5eWV6XlJSEj179uTtt9/G19eXAQMGcP78eW0rYWb1ZsfX15fbt29z//59bavl6dOndcocP34cT09PpkyZot2X3kJsDMeOHaNPnz507twZ0CSZN27c0CmTk8f6/v7+zJ49m4iICO3neODAAezt7bVdGzM758mTJ5w5c0bb8n7o0CHS0tK0jXf+/v5MmTKF5ORkbbJ94MABfH19DfJIHwyQnD49hVM6T09P/Pz86Ny5M97e3piY6H7TqlQqpk2bplf9kydPpl27dpQpU4bo6Gg2b95MYGAg+/btIyQkhM2bN9O+fXuKFy/OuXPnGD16NI0bN9ZpjhdCCCFyTG2imS7q+96ACt0E9d+BTW3nGiwxBfjhhx+oVasWDRs2ZNOmTZw6dYo1a9bkWf1eXl6EhoYSFBRE6dKlsbOzo2XLlvj7+/PGG28wf/58KlSowL1799i1axedO3emVq1aTJkyhcjISJYtW4atrS27d++mX79+/PLLL1nWm91TnFatWlGuXDnee+895s+fT3R0tLZ/aXqLpY+PD7du3WLLli3Url2bXbt2sX379jz7LHLKx8eHH3/8kY4dO2rznGdbY3PyWL9169b4+fnRq1cv5s+fT3h4OFOnTmXYsGHaz+7UqVP07t2bgwcPUqpUKSpVqkTbtm0ZOHAgq1atIjk5meHDh/POO+9QsqSmr3X37t35+OOP6d+/PxMnTuTChQssXbqUxYsXa6+dlJTEpUuXtF/fvXuXoKAgbG1ts/zDJkf0Gvv/AlQqVY63nEwl1a9fP8XT01MxNzdXnJ2dlRYtWij79+9XFEVRbt26pTRu3FgpVqyYYmFhoZQvX14ZP358jqcjkel0hLHJVFKiKCvQU0kpima6qE8r6k4l9WmlfJtGSlEynyYRUD7//HOlVatWioWFheLl5aV89913OmVeeeWVTKeQSpfdVFKKoigJCQlKly5dFEdHR50pn6KiopQRI0YoJUuWVMzMzBQPDw+lR48eyq1bt5TDhw8rpqamytGjR3WuY29vr6xYsSLberOTPpWUubm5UrFiRWXnzp0KoOzdu1dbZvz48Urx4sUVW1tb5e2331YWL16sODg4aI+nTyX1tPfee0/p1KmTzr5nP2+embIq/XP7888/tfsOHz6sAMrjx4+1ZZo1a6ZYWVkpHh4eymefffbC013euHFDadeunWJlZaWUKFFCGTt2rJKcnJwhhtDQUO2+hw8fKu+++65ia2ur2NvbK3379lWio6N16v3rr7+Uhg0bKhYWFkqpUqWUuXPn6hxPf7/Pbk2aNMk2Xn2/31SKkr8z7ea2CT2rCYKNISoqCgcHByIjIzN0pBbCEIx1D8q9LwqC/LwPExISCA0NxdvbG0tLy9xXlJaq6YMacx9sXTV9TA3YYio0j80bNmyY7ZgSYVz6fr/l+2P9gpRkCiGEEPlCbQLejYwdRZGyfft2bG1t8fHxITg4mJEjR9KgQQNJTF8C6ucXyVuPHj3i3LlzWR4/d+4cjx8/NmBEQgghhChINm3ahK2tbaZb+tLk0dHRDBs2jIoVK9KnTx9q167Nzz//bOTIRV4w+Gj9CRMmcPbsWc6ePZvp8b59+1K7dm1WrVpl4MiEEEIIURC8/vrrOhPePy19BHnv3r11ZiYQLw+DJ6eHDx+mZ8+eWR5//fXX+eabbwwYkRBCCCEKEjs7O52J8UXRYvDH+vfu3dOuIZuZ0qVLc++enhMgCyGEEEKIl4rBk1MbG5tsR/DfvHlTVqgRQgghhCiiDJ6c1q1bl/Xr1xMdHZ3hWHR0NBs2bMh0fVwhhBBCCPHyM3hyOm7cOO7cuUP9+vXZunUrwcHBBAcHs3XrVurXr8+dO3cYP368ocMSQgghhBAFgMEHRDVr1owVK1YwcuRI3n77bZ1jZmZmfPbZZ7Rs2dLQYQkhhBBCiALA4MkpwPvvv0+HDh34/vvvCQ4OBqBChQp07dqVUqVKGSMkIYQQQghRABglOQUoVaoUo0ePNtblhRBCCPGM8PBwevXqxfHjxzEzM+PJkyfGDilbN27cwNvbmz///JNXX33V2OGIPGLwPqdCCCHEyyY1LZXT4afZfX03p8NPk5qWauyQcmXx4sWEhYURFBTE1atXjR2OyGNhYWF0796dChUqoFarGTVqlLFDypRRWk5PnDjBZ599xrVr13j48CGKougcV6lUhISEGCM0IYQQIkcCbgYw99Rc7sfd1+5ztXZlUp1JtPQsXGMoQkJCqFmzJj4+PlmWUalUhIaG4uXllSfXTEpKwtzcPE/qEtlLTEzE2dmZqVOnsnjxYmOHkyWDt5xu2LCBhg0bsm3bNhISEihTpgyenp46W3aT9AshhBAFRcDNAMYEjtFJTAEi4iIYEziGgJsB+XLdL7/8kpIlS5KWlqazv1OnTvTr148ZM2bw6quvsnbtWsqUKYOtrS1Dhw4lNTWV+fPn4+bmhouLC7Nnz9ae6+XlxbZt29iwYQMqlYo+ffrkKravvvoKDw8PrK2t6dy5M4sWLcLR0VF7PD221atX4+3tjaWlJQB79+6lYcOGODo6Urx4cTp06JChoerUqVNUr14dS0tLatWqxZ9//pmj2Hbs2IGPjw+WlpY0a9aM9evXo1KptN0XHj58yLvvvkupUqWwtramatWqfPvttzp1NG3alBEjRjBq1CicnJxwdXXlq6++IjY2lr59+2JnZ0f58uXZs2eP9pzAwEBUKhX79u2jevXqWFlZ0bx5cyIiItizZw+VKlXC3t6e7t27ExcXpz1Pn88kJ7y8vFi6dCm9e/fGwcEh1/XkN4O3nM6ePRtfX18CAgIoWbKkoS8vhBBCZElRFOJT4vUqm5qWypxTc1BQMhxL3zf31FzqutXFRG3y3PqsTK1QqVR6Xfutt95ixIgRHD58mBYtWgDw6NEj9u7dy+7duzl69CghISHs2bOHvXv3EhISQteuXbl+/ToVKlTgyJEjHD9+nH79+tGyZUvq1q3L6dOn6d27N/b29ixduhQrKyu9YnnasWPHGDx4MPPmzeP1118nICCAadOmZSgXHBzMtm3b+PHHHzEx0Xw2sbGxjBkzhmrVqhETE8NHH31E586dCQoKQq1WExMTQ4cOHWjVqhUbN24kNDSUkSNH6h1baGgoXbt2ZeTIkQwYMIA///yTcePG6ZRJSEigZs2aTJw4EXt7e3bt2kWvXr0oV66czhzs69evZ8KECZw6dYrvvvuOIUOGsH37djp37syHH37I4sWL6dWrF7du3cLa2lp73owZM/jss8+wtramW7dudOvWDQsLCzZv3kxMTAydO3dm+fLlTJw4Ua/PBKBy5crZLm7UqFEjnUS5MDB4cnrz5k0WLFggiakQQogCJz4lnrqb6+ZZfffj7lN/S329yp7sfhJrM+vnFwScnJxo164dmzdv1ianW7dupUSJEjRr1oyjR4+SlpbG2rVrsbOzw8/Pj2bNmnHlyhV2796NWq3G19eXefPmcfjwYerWrYuzszMWFhZYWVnh5uaWq/e7fPly2rVrp036KlSowPHjx/nll190yiUlJbFhwwacnZ21+7p06aJTZu3atTg7O3Pp0iWqVKnC5s2bSUtLY82aNVhaWlK5cmXu3LnDkCFD9Irtiy++wNfXlwULFgDg6+vLhQsXdFqPS5UqpZOwjhgxgn379vH999/rJKevvPIKU6dOBWDy5MnMnTuXEiVKMHDgQAA++ugjVq5cyblz56hXr572vE8++YQGDRoA0L9/fyZPnkxISAhly5YFoGvXrhw+fFibnD7vMwHYvXs3ycnJWb7v3PyRYWwGf6xfunRpEhMTDX1ZIYQQ4qXSo0cPtm3bpv2dumnTJt555x1ti5qXlxd2dnba8q6urvj5+WmPp++LiIjI9jrt2rXD1tZWu4GmtS79deXKlbVlr1y5kmGVx8xWffT09NRJTAGuXbvGu+++S9myZbG3t9f2ab116xYAf//9N9WqVdN2AwDw9/fPNvanXblyhdq1a2cbW2pqKrNmzaJq1aoUK1YMW1tb9u3bp40hXbVq1bRfm5iYULx4capWrard5+rqCpDhs336PFdXV6ytrbWJafq+p8953mcCms+yfPnyWW6FcYpOg7ecDh48mE2bNjF69GhtU74QQghREFiZWnGy+0m9yp65f4ahB4c+t9yKFiuo6VpTr2vnRMeOHVEUhV27dlG7dm2OHj2qM8jFzMxMp7xKpcp037P9Vp+1evVq4uP/6+rg4+PD7t27tUnPs3Xqw8bGJtP34+npyVdffaXtT1ulShWSkpJyXH9uLViwgKVLl7JkyRKqVq2KjY0No0aNyhDD8z7b9O4Zz362z5Z53v+HPp+JPNbPAzVr1mTbtm3UqVOHYcOG4e3tnWmS2rhxY0OHJoQQoohTqVR6P1qvX7I+rtauRMRFZNrvVIUKV2tX6pesr1ef05yytLTkzTffZNOmTQQHB+Pr60uNGjXy/DqZtbx5enpmOlrf19eX06dP6+x79nVmHj58yJUrV/jqq69o1KgRAL/99ptOmUqVKvHNN9+QkJCgbT39/fff9X0b+Pr6snv37mxjO3bsGJ06daJnz56AJrm8evUqfn5+el8nr+jzmcDL+Vjf4Mlpet8YgAEDBmTo/K0oCiqVitTUwjlHnBBCiKLBRG3CpDqTGBM4BhUqnQRVheZ328Q6E/MlMU3Xo0cPOnTowMWLF7UJlTGNGDGCxo0bs2jRIjp27MihQ4fYs2fPcwd6OTk5Ubx4cb788kvc3d25desWkyZN0inTvXt3pkyZwsCBA5k8eTI3btxg4cKFesf2/vvvs2jRIiZOnEj//v0JCgpi3bp1wH8tnT4+PmzdupXjx4/j5OTEokWLuH//vlGSU30+E9D8oZATQUFBAMTExPDgwQOCgoIwNzc3ynvMisGT06+//trQlxRCCCHyRUvPlixquijTeU4n1pmY7/OcNm/enGLFinHlyhW6d++er9fSR4MGDVi1ahUff/wxU6dOpU2bNowePZrPPvss2/PUajVbtmzhgw8+oEqVKvj6+rJs2TKaNm2qLWNra8vOnTsZPHgw1atXx8/Pj3nz5mUYNJQVb29vtm7dytixY1m6dCn+/v5MmTKFIUOGYGFhAcDUqVO5fv06bdq0wdramkGDBvHGG28QGRmZ688kt/T5THKjevXq2q/PnDnD5s2b8fT05MaNGy8WcB5SKc/OgC8yiIqKwsHBgcjISOzt7Y0djiiCjHUPyr0vCoL8vA8TEhIIDQ3VmW8zN1LTUjkbcZYHcQ9wtnamhkuNfG0xLUwGDhzI5cuXOXr0qLFDyWD27NmsWrWK27dvGzuUIkHf7zejrBAlhBBCvExM1CbUdqv9/IJFwMKFC2nVqhU2Njbs2bOH9evXs2LFCmOHBcCKFSuoXbs2xYsX59ixYyxYsIDhw4cbOyzxDINPJQVw+/Zt+vXrR+nSpTE3N+fQoUMAPHjwgH79+unVeVoIIYQQBc+pU6do1aoVVatWZdWqVSxbtowBAwbk+3UHDx6sM+XV09vgwYMBzdRMnTp1ws/Pj1mzZjF27FhmzJiR77GJnDF4y2loaCj16tUjISGBevXqERYWpj3m7OzMH3/8werVqzPMRSaEEEKIgu/77783ynVnzpyZYcWndOndQRYvXlyg15QXGgZPTqdMmYJarebChQtYWVnh4uKic7x9+/bs3LnT0GEJIYQQohBzcXHJkFOIwsngj/UDAgIYOnQoHh4emU4t4enpyZ07dwwdlhBCiCJMxgYLkf/0/T4zeHIaFRWFu7t7lseTkpJISUkxYERCCCGKqvRFYAy5CpEQRVVcXBzw/FXFDP5Y38PDg4sXL2Z5/Pfff6d8+fIGjEgIIURRZWpqirW1NQ8ePMDMzExn3XkhRN5QFIW4uDgiIiJwdHR87vL1Bk9O33zzTVatWkX//v21Lajpj/e3bdvGDz/8wMcff2zosIQQQhRBKpUKd3d3QkNDs12fXAjx4hwdHXFzc3tuOYNPwh8VFYW/vz83btygcePG7N+/n5YtWxIVFcWpU6d49dVXOXbsmN6TIa9cuZKVK1dqVzaoXLkyH330Ee3atQM0E76OHTuWLVu2kJiYSJs2bVixYgWurq45ilkmIhfGJJPwi6LMEPdhWlqaPNoXIh+ZmZk9t8U0nVFWiIqKimLatGls3ryZhw8fAppsukePHsyePTtHP3x27tyJiYkJPj4+KIrC+vXrWbBgAX/++SeVK1dmyJAh7Nq1i3Xr1uHg4MDw4cNRq9UcO3YsR/HKL2hhTJKciqJM7kMhihajL1/64MEDFEXB2dk509H7uVGsWDEWLFhA165dcXZ2ZvPmzXTt2hWAy5cvU6lSJU6cOEG9evX0qk9+MApjk+RUFGVyHwpRtBh9+VJnZ+c8qys1NZUffviB2NhY/P39OXPmDMnJybRs2VJbpmLFipQpUybb5DQxMZHExETt66ioqDyLUYiCTO59IYQQxmbQ5DQyMhIzMzOsra21+/bv38+hQ4eIjo6mZs2a9OzZE3Nz8xzVe/78efz9/UlISMDW1pbt27fj5+dHUFAQ5ubmODo66pR3dXUlPDw8y/rmzJkjg7KEYaWlws3jEHMfbF3Bsz6o9eubk5fk3hdCCGFsBklOExISePfdd9mxYwcAPXv25Ouvv2bgwIGsW7dOOymrSqVi+fLlHD16FFtbW73r9/X1JSgoiMjISLZu3cp7773HkSNHch3v5MmTGTNmjPZ1VFQUHh4eua5PiGxd2gF7J0LUvf/22ZeEtvPA73WDhiL3vhBCCGMzSHK6fPlyfv75Z2rWrImrqyubN2/G2tqadevW8f7779OmTRuSk5PZvn073377Lf/73//43//+p3f95ubm2rlRa9asyenTp1m6dClvv/02SUlJPHnyRKf19P79+9lOZWBhYYGFhcXzL1xAWrtEIXZpB3zfm1QUzlpa8MDEBOfUVGpEhWHyfW/otsGgCare974QQgiRTwySnG7evJnmzZsTEBAAwMKFC5k4cSL9+/dnxYoV2nJdu3YlMjKS7du35yg5fVZaWhqJiYnUrFkTMzMzDh48SJcuXQC4cuUKt27dwt/f/8XeVAFq7RIFkKJAWsozWyqkJv/3OiUJdo0lwNqSucWduG/637eja0oKkx4+oeXeSVDxNSO+ESGEEMKwDJKc3rx5k379+mlfd+rUiQkTJtCqVasMZdu0acOECRP0rnvy5Mm0a9eOMmXKEB0dzebNmwkMDGTfvn04ODjQv39/xowZQ7FixbC3t2fEiBH4+/vrPVI/U/+2dsEzEx1EhWn2G7i1q1BIS3sqSUvWJGrpr7UJW+pzyuTj8bQUSH02mUzJ/XElVa+PJcDaijEuJZ69k4gwMWGMS3EWRfxDy5vHofgref9/IoQQQhRABklOnzx5QvHixbWvixUrBqCz7+ljOZkIOSIigt69exMWFoaDgwPVqlVj37592sR38eLFqNVqunTpojMJf66lpWpaTDOkE/y7TwXprV1ZPeJXFFDSdFvR0lL/TZiyaGXLcFyfMvlxPOWpMqn6J4aZfl5FkMoETMxAbUqqksrc4k6aT+aZadQUlQqVojCvuBPNosMkORVCCFFkGH0qqRe1Zs2abI9bWlry+eef8/nnn+fNBW8e132Un4ECUXfhU19NIpJV4ic01JpEDbUpmJj+93VmW7bHTf4tY6b7Wm2Wy+Pp+555neF4Dq/xVBJ6+uxX3D+/LMuPRlGpCDc15WxqNL6G+L8QQgghCgCDJaexsbE8evQIQPtvdHS09ut0MTExhgopd2Lu61cu9kHO61aps0l2TJ5Kip55nePjOU388uB4pmWKxuAxRVGIiIvg2pNrXHv87/bv1/p4YO8qyakQQogiw2DJ6eDBgxk8eLDOvjfffNNQl887tq76lXttMZSupX9LncoE1Or8jV3ku5ikGIKfBHP18VWdJDQqKfeT2Tvb6HnPCSGEEC8BgySnvXv3zrOlSY3Os75mVH5UGJn3o1Rpjtd8r8i0DBZFyWnJ3Ii8oZOAXnt8jXuxmXf5MFGZUMa+DBWcKuDj6IOPkw9lHcsyYN8AIuLuZ3Un4WrtRg2XGsTGxObr+xFCCCEKCoMkp+vWrTPEZQxDbaKZLur73mjSh6fTin8T8LZzJTF9SSiKwv24+1x9fFWnNTQ0MpSULPoOu1i54OPk89/mqElELUwyzh86qc4kxgSO+fdO+u9eUv17L02sMxETuZeEEEIUIQZJTvv168f7779P3bp1DXG5/Of3uma6qEznOZ0r00gVUlFJUQQ/Ds7QGhqdHJ1peRszG8o7ltcmoD5OPlRwqoCDhYPe12zp2ZJFTRcx99Rc7sf915/Z1dqViXUm0tKz5Qu/LyGEEKIwMVjLacuWLV+e5BQ0CWjF12SFqEIoOTWZ65HXMwxQCo8Nz7S8qcoULwcvbQKavpW0KZkn3VVaerakmUczzkac5UHcA5ytnanhUkNaTIUQQhRJhX4qKaNSm4B3I2NHIbKgKAr3Yu/9l4D+m4TeiLxBipL5I3k3GzfdJNTRB28Hb8xNzPM5WjUpsWVJji5JimIJyOA4IYQQRZMkp+KlEJkYmWGEfPCTYGKTMx9IZGdmp5OA+jj5UN6pPPbm9gaOHPZeCOPjnZcIi0zQ7nN3sGR6Rz/aVnE3eDxCCCGEMRksOX1pRusLo0pMTeT6k2ceyT++RkR8RKblTdWmlHUom6FfqKu1a4G4J/deCGPIxrMZRuuHRyYwZONZVvasIQmqEEKIIsVgyemoUaOYMmWKXmVVKhUhISH5HJEoyNKUNO5G3+Xqk6s6j+RvRd0iNYt160vZlsrwSN7TwRMztZmBo9dPaprCxzsvZbcQLh/vvEQrPzcDRyaEEEIYj8GSU0VRUBT91lfXt5x4OTxKeJRh5aTgJ8HEp8RnWt7e3D5DS2h5x/LYmtsaOPKcSUtTiIhO5M7jOO48jud4yD86j/KfpQBhkQmcCn1EZeeCmWALIYQQec1gyemSJUvo3r27oS4nCqD4lHiuP7mu6Rv61GP5hwkPMy1vrjanrGPZDK2hLtYuBeKR/LPS0hT+iUnk9uN4bQL637/x3H0cT1JqWo7rjYhOkORUCCFEkSEDokSeS01L5Xb07QxTNd2KuqUz0fzTStuW1pmmqYJjBcrYl8FUXXBuUUVR+CcmiTuP455JQP/7Oikl++RTrQJ3BytKO1lhYabm16v/PPe6LnaWefUWhBBCiAKv4PzmF4WOoig8THj43yj5f5PQ60+uk5Ca+eNqJwunjKPkHctjbWZt4OgzUhSFR7FJGVo+bz/S/Hv3STwJyfoln6WcNAloaSdrSjtZ4fHvv24OlpiZaKaJSk1TaDjvEOGRCVkuX+rmYEkd72LExmS+EIAQQgjxspHkVOglLjmO4CcZV096nPg40/IWJhaUcyyn80i+glMFilsWN9ojeUVReByXnGnimd4CGp+c+WCrdCoVuNlbahNPj6cS0NJO1rg5WGJuqt8cpSZqFdM7+jFk49msFsJlekc/TNQFrwuDEEIIkV8MkpxOnz6datWqGeJS4gWlpKVwK+qW7ij5x9e4G3M300fyKlSUsS+ToV+oh52HwVc4UhSFyPjkLBPPO4/jiE3KPvkEcLW3yDTx9ChmhbuDld7Jpz7aVnFnZc8aGeY5dZN5ToUQQhRRBklOAwMDOXLkiN7lVSoVBw8ezMeIhKIoPIh/wLXH13Qmr7/+5DpJaUmZnlPcsrhOAlrBqQJlHctiZWplsLgj45O5/SjjYKP0r2MSM1/56WkudhY6j9zTE8/STta4O1hiaWbYpLptFXda+blxKvQREdEJuNhpHuVLi6kQQoiiyCDJ6ZEjRzAzM8PcXL8lIAviSOzCLDY5NsPj+GtPrhGZGJlpeStTK8o7ltfpF+rj5EMxy2L5HmtUQjJ3nmrxvP1MAhqd8Pzks4RtevJphUcxa51EtJSjlcGTT32YqFX4lytu7DCEEEIIozNIcmpqaoqiKLRs2ZK+ffvSoUMH1GpZOzyvJaclczPyZoYk9G7M3UzLq1VqytiV0Rkh7+PkQ2m70qhV+fP/E5OYokk8Hz2deP6XgEbGJz+3juI25pqE85nE08PJmlKOVliZF7zkUwghhBD6MUhyevfuXTZs2MC6devo3LkzLi4u9O7dm379+uHr62uIEF4qiqJwP+5+hrXkQyNDSU7LPLlztnLO0BJa1qEslqZ5O01RbGIKd5/EZ3j0np6IPol7fvJZLD35zGS0eyknK6zNZRyfEEII8bJSKQZejunUqVOsXbuW7777jqioKOrUqUP//v155513sLUtmCv8REVF4eDgQGRkJPb29oa9dlIUwY+fGSX/5BrRSZlPLWRtak15p/I6qyf5OPrgaOmYJ/HEJaVw95l+nk8/en8Um3l/1ac5WptpEk/H//p6pieipZyssLWQ5PNZxroHjXnvC5FO7kMhihaDJ6fpEhIS2LZtG19//TWHDx/G2tqalStX0rNnT2OEk62sfjCmpqVyNuIsD+Ie4GztTA2XGrkeoZ6cmsz1yOsZHsmHx4ZnWt5EZYKXvVeGOUNL2pZ8oUfyCcmpWSaedx/H8U/M85NPe0vTDH090wcdlXK0ws5SVjvKKUlORVEm96EQRYvRmqgsLS3p0aMHXl5eqNVqAgICuH79urHCybGAmwHMPTWX+3H3tftcrV2ZVGcSLT1bZnmeoijci72nM03TtSfXuBF5gxQl88E+rtauGUbJezt4Y26i3wCzpyUkp3LvSXymKxzdfhTPPzGJz63DzsL0qf6e/z1yT2/5dLCS5FMIIYQQuWOU5DQsLIz169ezbt06rl27RsmSJZk8eTJ9+/Y1Rjg5FnAzgDGBYzLM+xkRF8GYwDEsarqIlp4tiUyMzNAvNPhJMLHJsZnWa2tmm6FfaHnH8jhYOOgdW2JKKveeJGRY2z29D2hE9POTTxtzk39bPq0z9v0sZi3JpxBCCCHyjcGS0+TkZH7++We+/vpr9u/fj4mJCa+//jqLFy+mTZs2hWb0fmpaKnNPzc10Qvr0fRN/nYiDuQMPEh5kWoep2hRvB+8M/ULdbNyeO41WUkoaYZG6rZ1Pt4Dej07geR01rM1Nnmrt1E08S//b8inTeQkhhBDCGAySnH7wwQds3ryZx48fU7VqVT799FN69uxJsWL5P29mXjsbcVbnUX5mktKStIlpSZuSGfqFetl7YWaSeetjcmoa4ZEJmr6ezySetx/HER71/OTTysxEJ/F8dtCRk7Ukn0IIIYQomAySnH722WdYWVnx7rvvUqNGDVJSUli3bl2W5VUqFaNHjzZEaDn2IC7z1tBnDXllCL38emFnbqezPyU1jbDIBO48jsqQeN59HE9YZDxpz0k+LUzVWSaeHk5WFLMxl+RTCCGEEIWSwR7rx8fHs3nzZjZv3vzcsgU5OS1mWUKvctapFdh/PpI7j8P/HfGuSUTDIhNIfU72af5U8qnTAvrvvyVsJfkUQgghxMvJIMnp4cOHDXEZg0iN8yIt2QGVaSSZ5YeKAkqKAzN+iAf+yrQOcxM1pbRJZ8aWzxK2FqhlXXUhhBBCFEEGSU6bNGmSb3XPmTOHH3/8kcuXL2NlZUX9+vWZN2+ezspTTZs25ciRIzrnvf/++6xatSrH1/snJpnE+x2xLLURRUEnQU3vC5p4vyMlbC2p6Gaf6aAjZ0k+hRBCCCEyVeiX4jly5AjDhg2jdu3apKSk8OGHH9K6dWsuXbqEjY2NttzAgQOZOXOm9rW1tXWurudiZ0lKdBUS7vbEwnUnKrNI7TElxYHE+x1Jia7C8oE18C9XPPdvTAghhBCiCCr0yenevXt1Xq9btw4XFxfOnDlD48aNtfutra1xc3N74evV8S6Gu4Ml4ZFViI32w8Q6FJVpNEqKHalx3qhQ4+5gSR3vwjcTgRBCCCGEsRWOyUVzIDJS05L57DRVmzZtokSJElSpUoXJkycTFxeXq/pN1Cqmd/QDQIWa1LhypES9SmpcOVT/fpzTO/phIo/thRBCCCFyrNC3nD4tLS2NUaNG0aBBA6pUqaLd3717dzw9PSlZsiTnzp1j4sSJXLlyhR9//DHTehITE0lM/G8lpaioKJ3jbau4s7JnDT7eeYmwyATtfjcHS6Z39KNtFfc8fmdCGMbz7n0hhBAiv71UyemwYcO4cOECv/32m87+QYMGab+uWrUq7u7utGjRgpCQEMqVK5ehnjlz5vDxxx9ne622Vdxp5efGqdBHREQn4GKneZQvLaaiMNPn3hdCCCHyk0pRnrfeUOEwfPhwfv75Z3799Ve8vb2zLRsbG4utrS179+6lTZs2GY5n1nrk4eFBZGQk9vb2eR67EM8TFRWFg4NDvt+Dcu+LgshQ978QomAo9C2niqIwYsQItm/fTmBg4HMTU4CgoCAA3N0zf/xuYWGBhYWFzjVAHnEK40m/9/L7b0m590VBZKj7XwhRMBT65HTYsGFs3ryZn3/+GTs7O8LDwwFwcHDAysqKkJAQNm/eTPv27SlevDjnzp1j9OjRNG7cmGrVqul1jejoaAA8PDzy7X0IoY/o6GgcHBwMej2Qe18UDIa+/4UQxlHoH+tntYzn119/TZ8+fbh9+zY9e/bkwoULxMbG4uHhQefOnZk6darej4fS0tK4d+8ednZ2Ga6X/tjz9u3b8rhJvJDs7iVFUYiOjqZkyZKo1YabZCO7ex/k/hd543n3kbHufyGEcRT65NTYpC+UyCuF8V4qjDGLgkfuIyHE0+RPUCGEEEIIUWBIciqEEEIIIQoMSU5fkIWFBdOnT9cZ4SxEbhTGe6kwxiwKHrmPhBBPkz6nQgghhBCiwJCWUyGEEEIIUWBIciqEEEIIIQoMSU6FEEIIIUSBIcmpEEIIIYQoMCQ5FUIIIYQQBYYkp0IIIYQQosCQ5FQIIYQQQhQYkpwKIYQQQogCQ5JTIYQQQghRYEhyKoQQQgghCgxJToUQQgghRIEhyakQQgghhCgwJDkVQgghhBAFhqmxAygM0tLSuHfvHnZ2dqhUKmOHI4ogRVGIjo6mZMmSqNWG+5tS7n1REBjr/hdCGIckp3q4d+8eHh4exg5DCG7fvk3p0qUNdj2590VBYuj7XwhhHJKc6sHOzg7Q/GC0t7c3cjSiKIqKisLDw0N7LxqK3PuiIDDW/S+EMA5JTvWQ/jjT3t5e5xd0aloqZyPO8iDuAc7WztRwqYGJ2sRYYYoiwNCP1rO694UwBulaIkTRUKA676xcuZJq1appfxH6+/uzZ88eAG7cuIFKpcp0++GHH7Kss0+fPhnKt23b9oVjDbgZQJttbei3rx8Tj06k375+tNnWhoCbAS9ctxBCCCFEUVWgktPSpUszd+5czpw5wx9//EHz5s3p1KkTFy9exMPDg7CwMJ3t448/xtbWlnbt2mVbb9u2bXXO+/bbb18ozoCbAYwJHMP9uPs6+yPiIhgTOEYSVCGEEEKIXCpQj/U7duyo83r27NmsXLmS33//ncqVK+Pm5qZzfPv27XTr1g1bW9ts67WwsMhwbm6lpqUy99RcFJQMxxQUVKiYd2oezTyaySN+IYQQQogcKlAtp09LTU1ly5YtxMbG4u/vn+H4mTNnCAoKon///s+tKzAwEBcXF3x9fRkyZAgPHz7MtnxiYiJRUVE6W7qzEWcztJg+TUEhPC6csxFnnxuXEAVNdve+EEIIYQgFLjk9f/48tra2WFhYMHjwYLZv346fn1+GcmvWrKFSpUrUr18/2/ratm3Lhg0bOHjwIPPmzePIkSO0a9eO1NTULM+ZM2cODg4O2u3pqXQexD3Q633oW06IgiS7e18IIYQwBJWiKBmfTxtRUlISt27dIjIykq1bt7J69WqOHDmik6DGx8fj7u7OtGnTGDt2bI7qv379OuXKlSMgIIAWLVpkWiYxMZHExETt6/RpTCIjI7kSd4V++/o99zpr26yltlvtHMUmRFaioqJwcHAgMjIyX0fNZ3fvy2h9YSyGuv+FEAVDjlpO01eLSXfv3j3S0tLyNCBzc3PKly9PzZo1mTNnDq+88gpLly7VKbN161bi4uLo3bt3jusvW7YsJUqUIDg4OMsyFhYW2hkDnp1Cp4ZLDVytXVGR9ZQmNmY2vOr8ao5jE8LYsrv3hRBCCEPIUXK6f/9+unXrpn399ttvs3///jwP6mlpaWk6LTmgeaT/+uuv4+zsnOP67ty5w8OHD3F3d89VPCZqEybVmQSQZYIamxzLlGNTSEpNytU1hBBCCCGKqhwlp23btsXGxoZffvmFXbt2YW1tnSdzhqabPHkyv/76Kzdu3OD8+fNMnjyZwMBAevTooS0THBzMr7/+yoABAzKto2LFimzfvh2AmJgYxo8fz++//86NGzc4ePAgnTp1onz58rRp0ybXcbb0bMmipotwsXbR2e9m7cY7Fd/BVGXKntA9DNw/kMjEyFxfRwghhBCiqMnxVFLLli2jR48eqFQqNm3alKfBRERE0Lt3b8LCwnBwcKBatWrs27ePVq1aacusXbuW0qVL07p160zruHLlCpGRmoTQxMSEc+fOsX79ep48eULJkiVp3bo1s2bNwsLC4oVibenZkmYezTJdIaq5R3PGBI7hbMRZeu7uyYqWK/Cwk4ElQgghhBDPo/eAKG9vb+3Scffu3UOlUuHu7o6iKKhUKq5fv56vgRpTbjrjX318lWEHhxEeG04xy2J81vwzqjpXzedIxcvKWANCZCCKKAjkPhSiaNG75TQwMBCAx48f06VLF1QqFdu2bcPR0TGfQivcKjhVYFP7TQw7OIzLjy7Tb18/5jaeS4symc8QIIQQQgghcjGV1AcffIC3tzcmJiYEBwezbNmy/IqtwHiRv9pjk2MZd2Qcv939DRUqJtSeQE+/nvkUqXhZScupKMrkPhSiaMnRgKhz587xyy+/MGzYMAYPHszu3bs5f/58fsX2UrAxs2F58+W8VeEtFBTmnZ7HvFPzSE3LehEAIYQQQoiiKkfJqZOTE6tXr8bc3Bxzc3NWr16Nk5NTfsX20jBVmzKt3jRG1RgFwMa/NzL2yFjiU+KNG5gQQgghRAGTo+TUw8OD5s2ba183bdqU0qVLZ1o2NDT0xSJ7yahUKvpX7c/8xvMxU5tx8NZBBuwbwMP4h8YOTQghhBCiwMhRcqqPmzdvMnDgQCpWrJjXVb8U2nm346vWX2Fvbs+5f87Rc3dPQiMlkRdCCCGEgBwmp48fP2bx4sUMHTqUqVOncuHCBe2x+/fvM3jwYHx9fVmzZg01a9bM82BfFjVda7Kx/UZK2ZbiTswdeu3pxZn7Z4wdlhBCCCGE0ek9Wv/27dv4+/sTFhZG+ilmZmbs2LEDExMT3n77bR4/fkzjxo2ZNm0aLVq8PFMm5ddI0YfxD/ng0Aec++ccZmoz/tfwf7T1zrsVt8TLQ0bri6JM7kMhiha9W04//vhjwsLCGDVqFL/88gtLlizB1taWDz74gC5dulCmTBkOHTpEYGDgS5WY5qfiVsVZ3WY1Lcq0IDktmfG/jmfN+TXkcHYvIYQQQoiXht6T8AcEBNC9e3c+/fRT7b5ixYrRu3dvGjRoQEBAwAsvCVoUWZla8WmTT1n4x0I2/r2RJWeXcDfmLh/W/RBTdY5XlxVCCCGEKNT0bjkNCwujUaNGOvvSXw8ZMkQS0xdgojZhYp2JTKozCRUqfrj6AyMOjSA2OdbYoQkhhBBCGJTeyWlycjK2trY6+9Jfu7m55W1URVSPSj1Y3GwxliaW/Hb3N/rs7UNEXISxwxJCCCGEMJgcjdZXqVQ52i9yrkWZFqxts5ZilsW4/Ogy3Xd15+rjq8YOSwghhBDCIPQera9Wq/Hw8MDBwUG7LzU1lcuXL+Pl5YWNjY1uxSoVf/31V95GayTGGCl6O/o2QwOGciPqBrZmtixqugj/kv4GubYoeGS0vijK5D4UomjRu+W0TJkyqNVqoqOjtVtcXBxlypQhLS1NZ390dDRRUVH5GfdLz8POg43tN1LTtSYxyTEMDRjKT8E/GTssIYQQQoh8pfdw8Bs3buRjGCIzDhYOfNnqS6Yem8qe0D1MOzaNuzF3GfrKUOlKIYQQQoiXUp4vXyrylrmJOXMbzWVA1QEArPprFVOPTSU5NdnIkQkhhBBC5D29k9NOnTqxdOlSgoKC8jEckRm1Ss3IGiP5yP8jTFQm7AjZwZCAIUQlSdcJIYQQQrxccjQgKv1RsqOjI02bNqVJkyY0a9aMqlWr5muQxlaQOuMfvXOUcUfGEZcSR3nH8nze4nNK2pY0akwi/8mAKFGUyX0oRNGid3J6//59Dh8+zOHDhzly5AhXr2qmN1KpVBQrVoymTZtqt8qVK+dr0IZW0H4wXn50mWEBw4iIj6CEVQk+b/E5fsX9jB2WyEeSnIqiTO5DIYoWvZPTZ4WHhxMYGJhpslqiRAmaNm3Kd999l6fBGktB/MEYHhvO0INDufb4GlamVixsspDGpRsbOyyRTyQ5FUWZ3IdCFC25Tk6flZ6srlq1il9//RWVSkVqampeVG10BfUHY3RSNGMDx3Ii7ARqlZoP63zI2xXfNnZYIh9IciqKMrkPhSha9J5KKivBwcEcPnyYwMBAAgMDCQsLQ61Wv/T9UAsCO3M7Pm/5OTNPzOSn4J/45OQn3I25y6iao1CrZCIGIYQQQhQ+Oc5grl+/zpo1a+jVqxceHh74+voydOhQrl69yrvvvsvPP//MP//8w59//pnjYFauXEm1atWwt7fH3t4ef39/9uzZoz3etGlTVCqVzjZ48OBs61QUhY8++gh3d3esrKxo2bIl165dy3FsBZWZ2oyZ9Wcy/NXhAHx98WvGHxlPYmqikSMTQgghhMg5vVtOe/fuzZEjR7hz5w4mJibUqFGDHj160KRJExo2bIidnd0LB1O6dGnmzp2Lj48PiqKwfv16OnXqxJ9//qkdZDVw4EBmzpypPcfa2jrbOufPn8+yZctYv3493t7eTJs2jTZt2nDp0iUsLS1fOOaCQKVS8f4r71PStiQfHf+I/Tf3ExEXwbLmy3CydDJ2eEIIIYQQesvRVFJmZmb06tWLDz/8kLJly+Z3bAAUK1aMBQsW0L9/f5o2bcqrr77KkiVL9DpXURRKlizJ2LFjGTduHACRkZG4urqybt063nnnHb3qKUz9nU6FnWLU4VFEJ0fjae/JihYrKGNfxthhiRckfU5FUSb3oRBFi96P9QcNGoS3tzdr167Fx8cHPz8/hg4dyvfff094eHieB5aamsqWLVuIjY3F399fu3/Tpk2UKFGCKlWqMHnyZOLi4rKsIzQ0lPDwcFq2bKnd5+DgQN26dTlx4kSW5yUmJhIVFaWzFRZ13OvwTftvKGlTkptRN+m5uydBEUHGDksUEoX53hdCCPFy0Ds5XbVqFZcvX+bevXts3LiRxo0bc+jQId555x1KlSqFr68vgwYNYtOmTdy5cyfXAZ0/fx5bW1ssLCwYPHgw27dvx89PM4dn9+7d2bhxI4cPH2by5Ml888039OzZM8u60pNmV1dXnf2urq7ZJtRz5szBwcFBu3l4eOT6/RhDOcdybHptE37F/Xic+JgB+wdw4OYBY4clCoHCfu8LIYQo/F54KqmwsDACAwM5cuQIgYGB2sFG3t7eBAcH57i+pKQkbt26RWRkJFu3bmX16tUcOXJEm6A+7dChQ7Ro0YLg4GDKlSuX4fjx48dp0KAB9+7dw93dXbu/W7duqFSqLOdhTUxMJDHxvwFFUVFReHh4FLpHSnHJcUz4dQJH7hxBhYqxtcbS26+3dqUvUXgY6rHmy3Lvi5eLPNYXomh54fmG3N3deffddxk7diyjR4+mUaNGKIpCaGhoruozNzenfPny1KxZkzlz5vDKK6+wdOnSTMvWrVsXIMsk2M3NDdCsbvW0+/fva49lxsLCQjtjQPpWGFmbWbOk2RLe9n0bBYWFfyxkzqk5pKa9HPPPirz3stz7QgghCq9cz3MaEhKSYX5T0AxCKlu2LM2aNcuTANPS0nRacp4WFBQEoNMq+jRvb2/c3Nw4ePAgr776KqD5C/zkyZMMGTIkT+Ir6EzVpkypO4XStqX59MynfHv5W8Jiw5jXaB7WZtnPdCCEEEIIYWh6J6fXr1/XJqKBgYHcvXuX9B4BZcqUoXfv3jRr1oxmzZrlup/a5MmTadeuHWXKlCE6OprNmzcTGBjIvn37CAkJYfPmzbRv357ixYtz7tw5Ro8eTePGjalWrZq2jooVKzJnzhw6d+6MSqVi1KhRfPLJJ/j4+GinkipZsiRvvPFGrmIsjFQqFX2q9MHd1p0Pj35I4O1A+u3rx2ctPqOEVQljhyeEEEIIoaV3clq+fHlUKpV2eqbu3btrk1Fvb+88CSYiIoLevXsTFhaGg4MD1apVY9++fbRq1Yrbt28TEBDAkiVLiI2NxcPDgy5dujB16lSdOq5cuUJkZKT29YQJE4iNjWXQoEE8efKEhg0bsnfv3pdmjtOcaOPVBldrV0YcGsHFhxfpubsnK1qsoKyjYaYFE0IIIYR4Hr0HRL377rvaZNTHxye/4ypQXrbO+DejbjI0YCi3om9hZ27H0mZLqe1W29hhiWzIPKeiKDPEfZiWlkZSUlK+1C2EADMzM0xMTPQqq3fLafPmzenUqRMuLi65DkwUDJ72nmxsv5ERh0bw14O/GHRgELMazKJD2Q7GDk0IIQwuKSmJ0NBQ0tLSjB2KEC81R0dH3NzcnjtrkN4tp6ammjy2Xr16vPnmm3Tq1CnT6ZteRi9r61FCSgIf/vahdg7UEdVHMLDqQJlqqgCSllNRlOXnfagoCrdu3SI5OZmSJUuiVr/wJDZCiGcoikJcXBwRERE4OjpmOZA9nd7J6YMHD/jpp5/46aefOHjwIMnJyVSuXJnOnTvzxhtvUL169Tx5AwXRy/wLOk1JY/GZxay7uA6ALj5dmFJvCmZqM+MGJnRIciqKsvy8D5OTkwkODqZkyZI4ODjkad1CCF0PHz4kIiKCChUqZPuIX+8/EZ2dnRk4cCC7du3iwYMHbNq0CT8/P5YsWUKtWrXw8vJi9OjR/Prrr7zgvP6FR1oqhB6F81s1/xbC+UPVKjVja41lSt0pqFVqtl3bxvCDw4lJijF2aEIIke9SUzU/t83NzY0ciRAvP2trzRSWycnJ2ZbL1fMLOzs73nnnHbZs2cKDBw/YuXMnrVq14ttvv6Vp06a4uLjQr18/du7cSUJCQm4uUfBd2gFLqsD6DrCtv+bfJVU0+wuhdyq+w9JmS7EyteL4veO8t/c9wmOzXuJVCCFeJtKdSYj8p+/32Qt3rjE3N6d9+/Z89dVXhIWFceTIEXr16sWRI0d44403mD9//oteouC5tAO+7w1R93T3R4Vp9hfSBLWpR1O+bvM1xS2Lc/XxVXrs7sGVR1eMHZYQQgghipA87fmtUqlo1KgRixYtIiQkhD///JN27drl5SWMLy0V9k4EMuu68O++vZMK5SN+gMolKrPptU2UdShLRFwE7+19j2N3jxk7LCGEEEbSp0+fIrVwjTC+fB2WWK1aNWrXfsnmz7x5PGOLqQ4Fou5qyhVSpWxLsaHdBmq71SY2OZZhB4fx47UfjR2WEEIUWKlpCidCHvJz0F1OhDwkNa2IjL0QIh/oPc/pszZv3sznn3/OtWvXePjwYYbjKpWKlJSUFwquQIq5n7flCigHCwdWtVzF9OPT+eX6L0w/Pp070XcYUX2E9M0SQoin7L0Qxsc7LxEW+d8YC3cHS6Z39KNtleynzMkrSUlJMqhLvDRy1XL6ySef0KtXL0JDQ6lfvz69e/fOsPXq1SuvYy0YbF31K2fjnL9xGIC5iTn/a/g/3q/2PgBfnf+KSUcnkZQqq6gIIQRoEtMhG8/qJKYA4ZEJDNl4lr0XwvLluk2bNmX48OGMGjWKEiVK0KZNGxYtWkTVqlWxsbHBw8ODoUOHEhPz38wr69atw9HRkX379lGpUiVsbW1p27YtYWH/xZiamsqYMWNwdHSkePHiTJgwIcMMPImJiXzwwQe4uLhgaWlJw4YNOX36tPZ4YGAgKpWKffv2Ub16daysrGjevDkRERHs2bOHSpUqYW9vT/fu3YmLi8uXz0cUbrlqOV2xYgVNmzZl7969mJkVsfkwPeuDfUnN4KdM+53+6/AccPIEJy9DRZYvVCoVw6sPp5RtKWaemMnu0N1ExEWwpNkSHCxkTkAhxMtFURTik/UbM5CapjB9x8UsRyCogBk7LtGgfAlM1M9/4mRlZpKjJ1Pr169nyJAhHDumGRewZ88eli1bhre3N9evX2fo0KFMmDCBFStWaM+Ji4tj4cKFfPPNN6jVanr27Mm4cePYtGkTAJ9++inr1q1j7dq1VKpUiU8//ZTt27fTvHlzbR0TJkxg27ZtrF+/Hk9PT+bPn0+bNm0IDg6mWLFi2nIzZszgs88+w9ramm7dutGtWzcsLCzYvHkzMTExdO7cmeXLlzNx4kS937MoGvSehP9ptra2fPrpp7z//vv5EVOBk2EC6PTR+oBugqrSvDa1hJQEMLeFtnOhek94CR6FH793nDGBY4hNjsXbwZsVLVZQ2q60scMqEmQSflGU5ed9mJCQQGhoKN7e3lhaWhKXlILfR/vy9Br6ujSzDdbm+rUZNW3alKioKM6ePZtlma1btzJ48GD++ecfQNNy2rdvX4KDg7UrPK5YsYKZM2cSHq6ZOrBkyZKMHj2a8ePHA5CSkoK3tzc1a9bkp59+IjY2FicnJ9atW0f37t0BzZyVXl5ejBo1ivHjxxMYGEizZs0ICAigRYsWAMydO5fJkycTEhJC2bJlARg8eDA3btxg7969ufi0RGH07PdbVnL1WL969ercvn0718EVen6vQ7cNYP9MXyL7ktDtGxh2EsrUh6QY2DEctnSHmAfGiTUP1S9Znw3tNuBq7UpoZCg9dvfgwj8XjB2WEEIUSTVr1tR5nZ4MlipVCjs7O3r16sXDhw91Hp1bW1vrLD3u7u5OREQEAJGRkYSFhVG3bl3tcVNTU2rVqqV9HRISQnJyMg0aNNDuMzMzo06dOvz999868VSrVk37taurK9bW1trENH1f+rWFeFquHut/8skndOnShS5durzUy5Zmy+91qPiaZlR+zH1NX1TP+qD+dzmuPr/Aic/g0CdwZTfcPgWvL9OcU4hVcKrApvabGHZwGFceX6Hfvn7MazSPZmWaGTs0IYR4YVZmJlya2UavsqdCH9Hn69PPLbeub23qeBd7bjkrs6yXc8yMjY2N9usbN27QoUMHhgwZwuzZsylWrBi//fYb/fv3JykpSbsyz7Nd8VQqVb6t6vj0tVQqVabXTktLy5dri8ItV8lpkyZNWLNmDfXq1aNevXp4eXllWCNVpVKxZs2aPAmywFKbgHejrI81GAnlWsD29+H+BU0L6qs9oe0csCy8j0hdbVxZ3249Y4+M5djdY4wKHMXE2hPpXqm7sUMTQogXolKp9H603sjHGXcHS8IjEzLtd6oC3BwsaeTjrFef0xdx5swZ0tLS+PTTT1GrNQ9Fv//++xzV4eDggLu7OydPnqRx48aA5rH+mTNnqFGjBgDlypXD3NycY8eO4enpCWge658+fZpRo0bl3RsSRVquktOTJ0/y3nvvkZyczNGjRzl69GiGMkUiOdWHWxUYeAgO/w+OLYWgjRD6K3ReCV4NjR1drtmY2bC8+XJm/z6bbde2MefUHO7G3GVsrbGoVfk6fa4QQhQIJmoV0zv6MWTj2fQRB1rpqej0jn75npgClC9fnuTkZJYvX07Hjh05duwYq1atynE9I0eOZO7cufj4+FCxYkUWLVrEkydPtMdtbGwYMmQI48ePp1ixYpQpU4b58+cTFxdH//798/AdiaIsV1nEyJEjMTc35+eff+bRo0ekpaVl2FJTC+cKSfnC1AJafQx994CjJ0TegnUdYP9USE54/vkFlJnajOn+0xlZYyQAGy5tYNyRcSSkFN73JIQQOdG2ijsre9bAzUF3cIebgyUre9Yw2Dynr7zyCosWLWLevHlUqVKFTZs2MWfOnBzXM3bsWHr16sV7772Hv78/dnZ2dO7cWafM3Llz6dKlC7169aJGjRoEBwezb98+nJyc8urtiCIuV6P1ra2tmTFjBhMmTMiPmAqcPB0pmhgN+z6Esxs0r138oPMX4F4t+/MKuF3XdzHt2DSS05Kp5lyN5c2XU8zy+X2shH5ktL4oygw5Wj+3UtMUToU+IiI6ARc7S+p4FzNIi6kQhUm+jtZ3cXGRlShyy8IOXl8O727RTNQfcQm+ag5HP4W0wtva/FrZ1/iy1ZfYm9tz7sE5eu7uyY3IG8YOSwghDMJErcK/XHE6vVoK/3LFJTEV4gXkKjnt168fGzdufDmXJzUU33Yw9Heo2AHSkuHgTPi6HTy6buzIcq2WWy2+af8NpWxLcTv6Nj339OTPiD+NHZYQQgghCpFcJacNGzZErVZTr1491q5dy+HDh/n1118zbOI5bErA2xvhjZVgbge3T8LKhnBmHeTT1B75raxDWTa230iV4lWITIxkwL4B7L0hEywLIYQQQj+56nOaPk2FtpJnVj9SFAWVSvXSDIoySL+7J7fgp6Fw49+ZD3zaaB7/27nmz/XyWXxKPBN/ncjh24cBGF1zNH0r983R0nziP9LnVBRlhaHPqRDi+fT9fsvVVFJff/11rgMTWXAsA713wO8rNI/4r+2DFfWg4xLw62Ts6HLMytSKxU0Xs+CPBWz6exOLzyzmbvRdJtedjKk6V7edEEIIIYqAHGcJiYmJeHt74+7ujo+PT54Gs3LlSlauXMmNGzcAqFy5Mh999BHt2rXj0aNHTJ8+nf3793Pr1i2cnZ154403mDVrFg4ODlnW2adPH9avX6+zr02bNgVzLV+1GuoPh3LNYfsgCD8P3/eGV96FdvPAMuv3WRCZqE2YVGcSpW1LM//0fL6/+j3hceEsaLwAazNrY4cnhBBCiAIox31OTUxMaNGiBXv27MnzYEqXLs3cuXM5c+YMf/zxB82bN6dTp05cvHiRe/fuce/ePRYuXMiFCxdYt24de/fu1WvS37Zt2xIWFqbdvv322zyPPU+5+sGAQ9BoLKjU8Ne3sKI+XD9i7MhypadfTxY3XYyFiQW/3vmVPnv78CDugbHDEkIIIUQBlOPk1NTUFDc3t3xZi7djx460b98eHx8fKlSowOzZs7G1teX333+nSpUqbNu2jY4dO1KuXDmaN2/O7Nmz2blz53NnDbCwsMDNzU27FYqJgk3NocVH0HcvOHlD1B3Y8DrsnQzJ8caOLsdaeLZgbZu1FLMsxt+P/qbH7h4EPw42dlhCCCGEKGByNVr/rbfe4vvvvyctLS2v49FKTU1ly5YtxMbG4u/vn2mZ9M7xpqbZ904IDAzExcUFX19fhgwZwsOHD7Mtn5iYSFRUlM5mNGXqwuDfoGZfzevfV8CXTeFekPFiyqVqztXY2G4jXvZehMWG0XtPb06GnTR2WOIpBereF0IIUSTlKjkdMGAAcXFxtGrVip07d3L58mVu3bqVYcuN8+fPY2tri4WFBYMHD2b79u34+fllKPfPP/8wa9YsBg0alG19bdu2ZcOGDRw8eJB58+Zx5MgR2rVrl+1MAnPmzMHBwUG7eXh45Oq95BkLW83AqO4/gK0rPLgMq1vArwsgtXDNNeth78E37b6hhksNopOjGRwwmB0hO4wdlvhXgbv3hRB6U6lU/PTTT9mWuXz5MvXq1cPS0pJXX33VIHEJkWNKLqhUKkWtVmv/zWrLjcTEROXatWvKH3/8oUyaNEkpUaKEcvHiRZ0ykZGRSp06dZS2bdsqSUlJOao/JCREAZSAgIAsyyQkJCiRkZHa7fbt2wqgREZG5uo95amYfxTlu16KMt1es33VQlH+CTZ2VDmWkJKgjAscp1RZV0Wpsq6KsiJohZKWlmbssAqsyMhIg9yDBfreF0VWft7/8fHxyqVLl5T4+PgXqyg1RVGu/6oo537Q/JuakjcB5gCgbN++Pdsy3bp1U5o3b67cuHFD+eeff3J8jenTpyuvvPJK7gI0Qr0vu5s3byrt27dXrKysFGdnZ2XcuHFKcnJytuc8fPhQ6d69u2JnZ6c4ODgo/fr1U6Kjo3XK/PXXX0rDhg0VCwsLpXTp0sq8efN0jl+4cEF58803FU9PTwVQFi9erFe8+n6/5WpOn48++ijf5qs0NzenfPnyANSsWZPTp0+zdOlSvvjiCwCio6Np27YtdnZ2bN++HTMzsxzVX7ZsWUqUKEFwcDAtWrTItIyFhQUWFhYv9kbyi01xeGs9nPsedo+HO6dhVUNoPQtq9YdCMo+ohYkF8xrPo6RtSdZeWMuKoBXcjb7LdP/pmJnk7P9U5J0Cfe8LUVBd2gF7J0LUvf/22ZeEtvPA73WDhJCUlKRXuZCQEF577TU8PT0zPX7jxg28vb3zZVyJyFupqam89tpruLm5cfz4ccLCwujduzdmZmb873//y/K8Hj16EBYWxoEDB0hOTqZv374MGjSIzZs3A5p5hVu3bk3Lli1ZtWoV58+fp1+/fjg6OmqfVsfFxVG2bFneeustRo8enfdvTq9U14iaNWumvPfee4qiaP56rlevntKkSRMlNjY2V/Xdvn1bUalUys8//6z3OYZqtcqxJ7cVZV2H/1pRv3lTUSLvGTuqHPvu8ndKtfXVlCrrqij99/VXohKjjB1SgWOse7DA3vuiSCnQLacXf1aU6Q7//RzWbg6a7aL+v2tyokmTJsqwYcOUkSNHKsWLF1eaNm2qAMqKFSuUtm3bKpaWloq3t7fyww8/aM8BdLbp06dnqDc0NFTJKjX4+uuvM9Tx9ddfK4qiKI8fP1b69++vlChRQrGzs1OaNWumBAUFKYqiKBEREYqrq6sye/ZsbV3Hjh1TzMzMlICAgGzrzc7ff/+tNGjQQLGwsFAqVaqkHDhwIEPr8YQJExQfHx/FyspK8fb2VqZOnarzxDW9xXbNmjWKh4eHYmNjowwZMkRJSUlR5s2bp7i6uirOzs7KJ598onNtQFm1apXy2muvKVZWVkrFihWV48ePK9euXVOaNGmiWFtbK/7+/kpw8H9PNoODg5XXX39dcXFxUWxsbJRatWopBw4ceO77zMru3bsVtVqthIeHa/etXLlSsbe3VxITEzM959KlSwqgnD59Wrtvz549ikqlUu7evasoiqKsWLFCcXJy0qlj4sSJiq+vb6Z1enp65nnLaYFKTidNmqQcOXJECQ0NVc6dO6dMmjRJUalUyv79+5XIyEilbt26StWqVZXg4GAlLCxMu6Wk/Pf4xNfXV/nxxx8VRVGU6OhoZdy4ccqJEyeU0NBQJSAgQKlRo4bi4+OjJCQk6B1Xgf4FnZqqKCdWKMosF80PxLmeinJ+m7GjyrEjt48otTfWVqqsq6K88dMbyr3owpdk5ydJTkVRZtDkNC1NURJj9NviIxVloW8mielTCeqnFTXl9KkvB12bmjRpotja2irjx49XLl++rFy+fFkBlOLFiytfffWVcuXKFWXq1KmKiYmJcunSJUVRFCUsLEypXLmyMnbsWCUsLCzDo1xFyT45jYuLU8aOHatUrlxZ+/s3Li5OURRFadmypdKxY0fl9OnTytWrV5WxY8cqxYsXVx4+fKgoiqLs2rVLMTMzU06fPq1ERUUpZcuWVUaPHv3cerOSkpKi+Pr6Kq1atVKCgoKUo0ePKnXq1MmQnM6aNUs5duyYEhoaquzYsUNxdXXVeUQ9ffp0xdbWVunataty8eJFZceOHYq5ubnSpk0bZcSIEcrly5eVtWvXKoDy+++/a88DlFKlSinfffedcuXKFeWNN95QvLy8lObNmyt79+5VLl26pNSrV09p27at9pygoCBl1apVyvnz55WrV68qU6dOVSwtLZWbN29qy7z//vuKjY1Ntlu6adOmZegKcf36dQVQzp49m+nntmbNGsXR0VFnX3JysmJiYqLNnXr16qV06tRJp8yhQ4cUQHn06FGGOvMjOX2hpXpSU1O5fPkyjx8/znTkfuPGjXNUX0REBL179yYsLAwHBweqVavGvn37aNWqFYGBgZw8qRnZnf7YP11oaCheXl4AXLlyhcjISEAzJ+u5c+dYv349T548oWTJkrRu3ZpZs2a9PI8u1WqoN0Qzcf+PgyAsCLb2hSu7of0CsCoE02YBjUs3Zl3bdQw/OJzgJ8H02N2Dz1t8TqXilYwdmhCiKEmOg/+VzKPKFM2j/rl6Diz88B6Y2+hdu4+PD/Pnz9fZ99ZbbzFgwAAAZs2axYEDB1i+fDkrVqzAzc0NU1NTbG1tcXNz0/s66aysrLC1tdVOKZnut99+49SpU0RERGh/ty5cuJCffvqJrVu3MmjQINq3b8/AgQPp0aMHtWrVwsbGhjlz5mRbb3YOHDhASEgIgYGB2nNmz55Nq1atdMpNnTpV+7WXlxfjxo1jy5YtTJgwQbs/LS2NtWvXYmdnh5+fH82aNePKlSvs3r0btVqNr68v8+bN4/Dhw9StW1d7Xt++fenWrRsAEydOxN/fn2nTptGmTRsARo4cSd++fbXlX3nlFV555RXt61mzZrF9+3Z27NjB8OHDAZg5cybjxo3T6zMIDw/H1VV3ifP01+Hh4Vme4+LiorPP1NSUYsWKac8JDw/H29s7y3oNMR1nrpPTefPmMXfu3GynmsluRHxm1qxZk+Wxpk2b6tUH5ukyVlZW7Nu3L0cxFFrOvjAgAI7Mh6Ofwvkf4MYxeGMFlGtm7Oj04lfcj03tNzH04FCCnwTz3t73WNhkIY1L5+yPHCGEKApq1qyZYd+zUy/6+/sTFBSUbT2VK1fm5s2bwH+/Q21tbbXHGzVqlO3CO3/99RcxMTEUL15cZ398fDwhISHa1wsXLqRKlSr88MMPnDlz5oUaia5cuYKHh4dOMlunTp0M5b777juWLVtGSEgIMTExpKSkYG9vr1PGy8sLOzs77WtXV1dMTExQq9U6+yIiInTOq1atms5xgKpVq+rsS0hIICoqCnt7e2JiYpgxYwa7du0iLCyMlJQU4uPjdWY3cnFxyZA8FkW5Sk7XrFnD5MmTadKkCa1bt2bKlCmMHj0aMzMz1qxZQ9myZRk6dGhexyqex8QMmk+BCm00raiPQuCbN6DO+9ByBpgX/CVD3W3d2dBuA2MCx/B72O98cOgDPqz7Id18uxk7NCFEUWBmrWnB1MfN47Cp6/PL9dgKnvX1u3YO2Njo38qand27d5OcnAzA3bt3adq0qU5Ca2Vlle35MTExuLu7ExgYmOGYo6Oj9uuQkBDu3btHWloaN27c0Enk8sOJEyfo0aMHH3/8MW3atMHBwYEtW7bw6aef6pR7dmC1SqXKdN+zT4ifLpM+SDyzfennjRs3jgMHDrBw4ULKly+PlZUVXbt21RnMNnjwYDZu3Jjt+4qJiQHAzc2NU6dO6Ry7f/++9lhm3NzcMiTZKSkpPHr0SHuOm5ubth59681ruUpOV65cSb169Th8+DAPHz5kypQpvPbaazRv3pyRI0fy6quv5rjVVOSh0rVg8FE48BGcXg2nvoDrh6HzKiiV8S/tgsbO3I4VLVbw8YmP+TnkZ2b9Pou7MXcZWWMkalWupuYVQgj9qFT6P1ov11wzKj8qDM04ngyVaY6Xaw5qk7yMMku///47vXv31nldvXr1bM95euR++qI2z3afS2dubp7h93uNGjUIDw/H1NRU28XuWUlJSfTs2ZO3334bX19fBgwYwPnz57WthJnVmx1fX19u377N/fv3ta2Wp0+f1ilz/PhxPD09mTJlinZfeguxMRw7dow+ffrQuXNnQJNk3rhxQ6dMTh7r+/v7M3v2bCIiIrSf44EDB7C3t890fvj0c548ecKZM2e0Le+HDh0iLS1N22XB39+fKVOmkJycrE22Dxw4gK+vr8FW2MzVb/q///6bt956C/jvL4P0m8rd3Z1BgwaxdOnSPApR5Iq5Dbz2KfTcBrZu8M9VWN0KAudCarKxo3suMxMzZjWYxdBXNS3way+sZdKvk0hMTTRyZEII8S+1iWa6KACencbv39dt5xosMQX44YcfWLt2LVevXmX69OmcOnVK258xL3h5eREaGkpQUBD//PMPiYmJtGzZEn9/f9544w3279/PjRs3OH78OFOmTOGPP/4AYMqUKURGRrJs2TImTpxIhQoV6NevX7b1ZqdVq1aUK1eO9957j3PnznHs2DFt/9L0vMTHx4dbt26xZcsWQkJCWLZsGdu3b8+zzyKnfHx8+PHHHwkKCuKvv/6ie/fuGVpjXVxcKF++fLZbutatW+Pn50evXr3466+/2LdvH1OnTmXYsGHaLhOnTp2iYsWK3L17F4BKlSrRtm1bBg4cyKlTpzh27BjDhw/nnXfeoWRJTV/r7t27Y25uTv/+/bl48SLfffcdS5cuZcyYMdprJyUlERQURFBQEElJSdy9e5egoCCCg/NmWfJcJacmJibaxwnp/z69JKiXlxfXrl3Lg/DECyvfEoaegMpvgpIKgXNgTWv4p+D//6hUKoa8MoTZDWdjqjJlz409DNo/iCcJT4wdmhBCaPi9Dt02gL277n77kpr9BprnNN3HH3/Mli1bqFatGhs2bODbb7/NshUtN7p06ULbtm1p1qwZzs7OfPvtt6hUKnbv3k3jxo3p27cvFSpU4J133uHmzZu4uroSGBjIkiVL+Oabb7C3t0etVvPNN99w9OhRVq5cmWW92TExMeGnn34iJiaG2rVrM2DAAG0LqaWlJQCvv/46o0ePZvjw4bz66qscP36cadOm5dlnkVOLFi3CycmJ+vXr07FjR9q0aUONGjVyXZ+JiQm//PILJiYm+Pv707NnT3r37s3MmTO1ZeLi4rhy5Yq22wbApk2bqFixIi1atKB9+/Y0bNiQL7/8UnvcwcGB/fv3ExoaSs2aNRk7diwfffSRzoqc9+7do3r16lSvXp2wsDAWLlxI9erVtYPxXpRK0WeU0TOqVKlCx44dtSPtPD096dChA59//jmg6TPxyy+/cOfOnTwJ0tiioqJwcHAgMjIyQ0fqQuX8Vtg1BhIiwdQKWs2E2gM0I/4LuJNhJxl9eDTRydF42XuxouUKPOyKztKaxroHX5p7XxRq+XkfJiQkEBoaire3tzapyZW0VE0f1Jj7mmWmPesbtMVUaB6bN2zYkODgYMqVK2fscEQm9P1+y1VW0rhxY3bt2qV9/dZbb/HFF1/Qr18/+vTpw+rVq2nfvn1uqhb5qWpXGHICyjaDlHjYMx42vqm7qkkBVde9LhvabcDdxp0bUTfoubsn5x6cM3ZYQgihoTYB70aan7PejSQxNYDt27dz4MABbty4QUBAAIMGDaJBgwaSmL4EcpWcjhw5kmHDhhEfHw9oHiO0b9+e9evX880339CqVSvmzp2bp4GKPOJQCnr+CO0WaFpPrx+GFfU0raoFXHmn8mxqv4lKxSrxKOER/ff15+DNg8YOSwghRB7btGkTtra2mW6VK1cGNMuZDxs2jIoVK9KnTx9q167Nzz//bOTIRV7Q+7H+999/j7+/Px4eWT9KjYyMxMTERGd+tJfBS/to859rmimn7p3VvK78pmYQlXUx48b1HHHJcYw7Mo6jd4+iQsX42uPp5dfL2GHlK3msL4qyQvFYX+Sp6OjoDNMZpTMzM9OZYUAUHnn+WP/dd9/l6NGj2tdRUVHUr1+fM2fOaPc5ODi8dInpS62ED/TfD00ng8oELv4IK+tDcICxI8uWtZk1y5ovo1uFbigozD89n7mn5pKaJtOXCSHEy8DOzi7L0eqSmL789E5On21gTU5O5vfff9cuFSoKKRMzaDoJBhyA4j4QHQYbu8CusZAUa+zosmSqNmVqvamMrjkagE1/b2JM4BjiU+KNHJkQQgghXkTBH6YtDKNUTXj/V81qUqCZvH9VI7jzh3HjyoZKpaJflX4saLIAc7U5h24fot/efvwT/4+xQxNCCCFELklyKv5jbg3t50Ov7WBXUrP86ZrWcGh2gZ64v61XW75q/RUOFg5ceHiBnrt7cj3yurHDEkIIIUQuSHIqMirXHIYeh6pvaSbu/3U+rG4JD64YO7Is1XCtwab2m/Cw8+BuzF167e7FH+EFt9VXCCGEEJkzzUnhDRs28PvvvwOaEVcqlYrPPvuMn376KUNZlUolS5gWZlZO0GU1+LaHX0ZDWBB80RhaztA8+i+AE/d72nuysf1GRhwawbkH5xh0YBCfNPiE9mVlzl0hhBCisNB7Kil1DpMRlUpFaurLMXq6yE+nExUGPw+DkH/nFPVuAm+sAIfSxo0rCwkpCXz424ccuHkAgJE1RtK/Sn/tesuFkUwlJYoymUrKcMLDw+nVqxfHjx/HzMyMJ0+eGDukbN24cQNvb2/+/PNPXn31VWOHI55D3+83vVtOQ0ND8yQwUQjZu0PPbfDHGtg/DUKPwIr60H4BVOsGBSzpszS1ZGGThXz6x6dsuLSBpWeXcif6DlPrTcVUnaOHBUIIoZfUtFTORpzlQdwDnK2dqeFSA5NCuErU4sWLCQsLIygoCAcHB2OHI/JYWFgYY8eO5Y8//iA4OJgPPviAJUuWGDusDPT+TS3zihVxKhXUHqBZ+vTHQXD3D9g+CK7sgg5LCtzE/WqVmvG1x1PKthTzTs9j27VthMeF82mTT7ExszF2eEKIl0jAzQDmnprL/bj/Jo13tXZlUp1JtPRsacTIci4kJISaNWvi4+OTZRmVSkVoaCheXl55cs2kpCTMzc3zpC6RvcTERJydnZk6dSqLFy82djhZKngdB0XBVrwc9NsHzaaC2hQu/axZ/vTaAWNHlqnulbqzpOkSLE0sOXb3GH329uF+bOarjgghRE4F3AxgTOAYncQUICIugjGBYwi4mT+Lmnz55ZeULFmStLQ0nf2dOnWiX79+zJgxg1dffZW1a9dSpkwZbG1tGTp0KKmpqcyfPx83NzdcXFyYPXu29lwvLy+2bdvGhg0bUKlU9OnTJ1exffXVV3h4eGBtbU3nzp1ZtGgRjo6O2uPpsa1evVrn8e7evXtp2LAhjo6OFC9enA4dOhASEqJT96lTp6hevTqWlpbUqlWLP//8M0ex7dixAx8fHywtLWnWrBnr169HpVJpuy88fPiQd999l1KlSmFtbU3VqlX59ttvdepo2rQpI0aMYNSoUTg5OeHq6spXX31FbGwsffv21S4gsGfPHu05gYGBqFQq9u3bR/Xq1bGysqJ58+ZERESwZ88eKlWqhL29Pd27dycuLk57nj6fSU54eXmxdOlSevfuXaBbxl/oGecff/zByZMnefz4cYZvEJVKxbRp014oOFFAmZhCk/Hg0xJ+fB/+uQKbukLNvtD6E7AoWKuENSvTjK/bfs2wg8O4/OgyPXb3YEXLFVRwqmDs0IQQBYyiKHov5pGalsqcU3NQyDh0I33f3FNzqetWV69H/FamVnr3jX/rrbcYMWIEhw8fpkWLFgA8evSIvXv3snv3bo4ePUpISAh79uxh7969hISE0LVrV65fv06FChU4cuQIx48fp1+/frRs2ZK6dety+vRpevfujb29PUuXLsXKykqvWJ527NgxBg8ezLx583j99dcJCAjINBcIDg5m27Zt/Pjjj5iYaD6b2NhYxowZQ7Vq1YiJieGjjz6ic+fOBAUFoVariYmJoUOHDrRq1YqNGzcSGhrKyJEj9Y4tNDSUrl27MnLkSAYMGMCff/7JuHHjdMokJCRQs2ZNJk6ciL29Pbt27aJXr16UK1eOOnXqaMutX7+eCRMmcOrUKb777juGDBnC9u3b6dy5Mx9++CGLFy+mV69e3Lp1C2tra+15M2bM4LPPPsPa2ppu3brRrVs3LCws2Lx5MzExMXTu3Jnly5czceJEvT4TgMqVK3Pz5s0s33ejRo10EuXCQO8BUU+Lj4/nzTffZP/+/SiKgkql0q4glf61DIgqIpLj4eBM+H2F5rWTN7z5JXjUyf48I7gTfYehB4cSGhmKjZkNi5ouon7J+sYOSy8yIEoUZYYcEBWXHEfdzXXz9Br6Otn9JNZm1s8v+K833niD4sWLs2bNGkDTmvrxxx9z+/ZtZs6cyYIFCwgPD8fOzg6Atm3bcuXKFUJCQrSJTcWKFenTpw+TJk3S1uno6Mi6deuyvG52j/XfeecdYmJi+OWXX7T7evbsyS+//KJtnZwxYwb/+9//uHv3Ls7Ozlle559//sHZ2Znz589TpUoVvvzySz788EPu3LmjbW1dtWoVQ4YM0WtA1KRJk9i1axfnz5/X7ps6dSqzZ8/m8ePHOq27T+vQoQMVK1Zk4cKFgKblNDU1Vbuke2pqKg4ODrz55pts2LAB0Awsc3d358SJE9SrV4/AwECaNWtGQECA9o+JuXPnMnnyZEJCQihbtiwAgwcP5saNG+zdu1evzwTg5s2bJCdnPRe5lZUVpUqVyrC/adOmvPrqqwbtc6rvgKhcPdafOXMm+/fvZ8qUKRw+fBhFUVi/fj179uyhUaNG1K5dm0uXLuU6eFGImFlB2znQewfYl4bHobC2jSZhTUkydnQ6StuV5pt231DLtRaxybEMCxjG9mvbjR2WEELkSo8ePdi2bRuJiYkAbNq0iXfeeUebeHp5eWkTUwBXV1f8/Px0Zt9xdXUlIiIi2+u0a9cOW1tb7Qaa1rr015UrV9aWvXLlik4LI5DhNWjGsTybmF67do13332XsmXLYm9vr01+b926BcDff/9NtWrVdJIaf3//bGN/2pUrV6hdu3a2saWmpjJr1iyqVq1KsWLFsLW1Zd++fdoY0lWrVk37tYmJCcWLF6dq1arafa6urgAZPtunz3N1dcXa2lqbmKbve/qc530moPksy5cvn+WWWWJa0OXqsf7WrVt56623mDlzJg8fPgSgVKlSNG/enBYtWlC7dm3WrVvHnDlz8jRYUYCVbQJDjsGeiXBuCxz9VNMP9c0vwaWSsaPTcrBw4ItWXzDt2DR2h+7mo+MfcTfmLsNeHVaop5oSQuQNK1MrTnY/qVfZM/fPMPTg0OeWW9FiBTVda+p17Zzo2LEjiqKwa9cuateuzdGjR3UGuZiZmemUV6lUme57tlves1avXk18/H9dHXx8fNi9e7c26Xm2Tn3Y2GQcmNqxY0c8PT356quvtP1pq1SpQlKS4Ro6FixYwNKlS1myZAlVq1bFxsaGUaNGZYjheZ9t+u+TZz/bZ8s87/9Dn8/kZXysn6vk9Pbt24wZMwZA21ck/YMyNTXl3XffZeXKlZKcFjVWjvDmF1CxPewcBeHn4Ism0OIjqDe0wEzcb25iztxGcyllW4qvzn/FF+e+4F7MPT6u/zFmJjn/ISuEeHmoVCq9H63XL1kfV2tXIuIiMu13qkKFq7Ur9UvWz5dppSwtLXnzzTfZtGkTwcHB+Pr6UqNGjTy/TmYtb56enpk+1vf19eX06dM6+559nZmHDx9y5coVvvrqKxo1agTAb7/9plOmUqVKfPPNNyQkJGhbT9MXBtKHr68vu3fvzja2Y8eO0alTJ3r27AloksurV6/i5+en93Xyij6fCcDu3buf+1i/sMlVtmBnZ0dKSor2a7Vazb1797THHRwcCA8Pz3G9K1eupFq1atjb22Nvb4+/v79Otp+QkMCwYcMoXrw4tra2dOnShfv3sx95rSgKH330Ee7u7lhZWdGyZUuuXbuW49hEDvh1gqEnwKc1pCbC/imw4XV4cuv55xqISqXigxofMMN/BiYqE3Ze38n7Ae8TmRhp7NCEEIWEidqESXU0fTVV6D55SX89sc7EfJ3vtEePHuzatYu1a9fSo0ePfLuOvkaMGMHu3btZtGgR165d44svvmDPnj3PfTLl5ORE8eLF+fLLLwkODubQoUPaRrB03bt3R6VSMXDgQC5dusTu3bu1/UD18f7773P58mUmTpzI1atX+f7777V9a9Pj8/Hx4cCBAxw/fpy///6b999//7l5Rn7R5zOBnD/WDwoKIigoiJiYGB48eEBQUFCB64qZq+S0XLlyXL16FdC0nFauXJmtW7cCmmTwxx9/xMPDI8f1li5dmrlz53LmzBn++OMPmjdvTqdOnbh48SIAo0ePZufOnfzwww8cOXKEe/fu8eabb2Zb5/z581m2bBmrVq3i5MmT2NjY0KZNGxISEnIcn8gBOzfo/r1mDlQzG7hxFFY2gKDNkPMxePmmS4UurGixAhszG06Hn6b3nt7cjblr7LCEEIVES8+WLGq6CBdrF539rtauLGq6KN/nOW3evDnFihXjypUrdO/ePV+vpY8GDRqwatUqFi1axCuvvMLevXsZPXr0c1ffUqvVbNmyhTNnzlClShVGjx7NggULdMrY2tqyc+dOzp8/T/Xq1ZkyZQrz5s3TOzZvb2+2bt3Kjz/+SLVq1Vi5ciVTpkwBwMLCAtAMkKpRowZt2rShadOmuLm58cYbb+TsQ8gj+nwmuVG9enWqV6/OmTNn2Lx5M9WrV6d9+wK2zLeSC1OmTFHc3d2VlJQURVEU5fPPP1dUKpVStmxZpVy5coparVbmzp2bm6ozcHJyUlavXq08efJEMTMzU3744Qftsb///lsBlBMnTmR6blpamuLm5qYsWLBAu+/JkyeKhYWF8u233+odQ2RkpAIokZGRuX8jRdnDEEVZ3UpRpttrtm+7K0rMA2NHpePyw8tK8++bK1XWVVGabGmiXHhwwdgh6TDWPSj3vigI8vM+jI+PVy5duqTEx8e/UD0pqSnKqbBTyq6QXcqpsFNKSmpKHkVY+A0YMEBp2LChscPI1CeffKKULl3a2GEUGfp+v+Wq5XTSpEnaUfoAQ4cOZeHChTg4OODk5MT//vc/JkyY8EJJc2pqKlu2bCE2NhZ/f3/OnDlDcnIyLVv+91doxYoVKVOmDCdOnMi0jtDQUMLDw3XOcXBwoG7dulmeA5oVFKKionQ28QKKlYW+ezR9T9VmcPkXzcT9VzKfKsMYfIv5sqn9Jio4VeBhwkP67uvLkdtHjB2Wwcm9L0TumKhNqO1Wm/Zl21PbrXahXLo0ryxcuJC//vqL4OBgli9fzvr163nvvfeMHRYAK1as4PTp01y/fp1vvvmGBQsWFJjYxH9ylZza2tri6+uLqel/46nGjBnD2bNnOX36NBMnTsz1yOfz589ja2uLhYUFgwcPZvv27fj5+REeHo65uXmGechcXV2z7N+avj99Sgd9zgGYM2cODg4O2i03XRTEM9Qm0GgsDDwEzpUg9gF8+zbsGAGJ0caODgA3GzfWt11P/ZL1iU+J54PD5ngY3AAAb8hJREFUH7Dl8hZjh2VQcu8LIV7UqVOnaNWqFVWrVmXVqlUsW7aMAQMG5Pt1Bw8erDPl1dPb4MGDAc3UTJ06dcLPz49Zs2YxduxYZsyYke+xiZzJ1ST8+SkpKYlbt24RGRnJ1q1bWb16NUeOHCEoKIi+fftq53NLV6dOHZo1a5Zpv5Pjx4/ToEED7t27h7u7u3Z/t27dUKlUfPfdd5nGkJiYqHOdqKgoPDw8ZCLyvJKcAIdmwYnPAQUcPaHzF+Cp/3x1+Sk5LZlPfv+EH6/9CECfyn0YXXM0apXxZhsw1GT4cu+LgsiQk/CLwisiIiLLpz329va4uLhkekwYjr7fb7maSmr69Ols27aNCxcuZHq8atWqvP3220ydOjXHdZubm1O+fHkAatasyenTp1m6dClvv/02SUlJPHnyRKf19P79+7i5uWVaV/r++/fv6ySn9+/fz3YlCQsLC23naJEPzCyhzWzwbQfbh8CTm/B1O2gwEpp9CKbG/ezN1GbM8J9BKdtSLP9zOesuruNuzF3+1/B/WJq+3L+85N4XQhRWLi4ukoC+JHLVFLR9+3ZatWqV5fHWrVtrR++/qLS0NBITE6lZsyZmZmYcPHhQe+zKlSvcunUryxUivL29cXNz0zknKiqKkydP5mhVCZFPvBpqJu5/tQegwLEl8FVzuH/R2JGhUqkYVG0QcxrNwVRtyoGbBxi4fyCPEx4bOzQhhBDipZar5DQ0NJSKFStmedzX15fQ0NAc1zt58mR+/fVXbty4wfnz55k8eTKBgYH06NEDBwcH+vfvz5gxYzh8+DBnzpyhb9+++Pv7U69ePW0dFStWZPt2zZKUKpWKUaNG8cknn7Bjxw7Onz9P7969KVmypNGmhhDPsLSHN1bA25vAujjcvwBfNoVjSyEt1djR0aFsB75s9SV25nYEPQii5+6e3IoqOPO1CiHyRgHr4SbES0nf77Ncd6J78uRJlsceP35MamrOE4uIiAh69+6Nr68vLVq04PTp0+zbt0/bSrt48WI6dOhAly5daNy4MW5ubvz44486dVy5coXIyP8mUp8wYQIjRoxg0KBB1K5dm5iYGPbu3St9iwqaSh1g6O9QoR2kJsGBj2BdB3h8w9iRUdutNhvbbaSUbSluRd+ix+4eBEUEGTssIUQeeHaVQyFE/omLiwOev+RtrgZE1atXD7VazfHjxzMcUxSFhg0bkpiYyB9//JHTqgskQw1GEWgm6P/zG9g7GZJiwNwW2s6F6j0hlzNA5JV/4v9h+MHhXHx4EXO1OXMazaG1V2uDXNtY96Dc+6IgyM/7UFEUbt26RXJyMiVLlkRdQJZZFuJloigKcXFxRERE4OjoqDMOKDO5GhDVv39/3n//ffr06cOCBQtwdnYG4MGDB0yYMIHff/+dzz77LDdVi6JOpYIavcGrEfw0BG6dgB3D4cpu6LgMbJ2NFloJqxKsbbOWib9OJPBOIOOOjGNs7Fh6+/XO9dRpQgjjUqlUuLu7Exoays2bN40djhAvNUdHxywHsT8t11NJ9ezZk82bN2u/sQHCwsJQFIW3336bb7/9NjfVFkjSemQkaalwfDkcnq151G9dAl5fBhVfM2pYqWmpzDs9j28va+7xd3zfYVKdSfk66ba0nIqizBD3YVpamjzaFyIfmZmZabvRPM8LzXP6/fffs2nTJoKDgwGoUKECPXr0oGvXrrmtskCSX9BGFn4BfhwEEf+O4n+1J7SdoxlMZSSKovDNpW9Y+MdCFBSalm7KvMbzsDazzpfrSXIqijK5D4UoWgrcJPwFkfxgLABSEuHw/zSj+FHAoQx0XqmZjsqIDtw8wOSjk0lMTcSvuB+ft/icElYl8vw6kpyKokzuQyGKFun5LQoHUwto9TH03aNZUSrylmY0//6pmhWnjKSVZytWt16Nk4UTlx5eoseuHoQ8CTFaPEIIIURh90Itp3/88QcnT57k8ePHpKWl6VasUjFt2rQXDrAgkL/aC5jEaM1o/j+/0bx28dMsf+pezWgh3Yq6xdCDQ7kZdRM7MzuWNFtCHfc6eVa/tJyKokzuQyGKllwlp/Hx8bz55pvs378fRVFQqVTaiVXTv1apVLma67Qgkh+MBdTl3bDzA4h9AGozaDYZGoyCfByYlJ3HCY8ZeXgkf0b8ianalJn1Z9KxXMc8qVuSU1GUyX0oRNGSq8f6M2fOZP/+/UyZMoXDhw+jKArr169nz549NGrUiNq1a3Pp0qW8jlUIXRXbaybur9gB0pLh4Ez4uh08um6UcJwsnfiq9Ve09mxNSloKH/72Iav+WiUrzwghhBA5kKvkdOvWrbz11lvMnDmTKlWqAFCqVCnatGlDQEAASUlJrFu3Li/jFCJzNiXg7Y3wxkowt4PbJ2FlQzizTjOhv4FZmFiwoMkC+lbuC8DnQZ8z/fh0ktOSDR6LEEIIURjlKjm9ffs2TZo0ATIu/WZqasq7777Lli1b8ihEIZ5DpYJXu8OQY+DZEJJjYedI2Pw2RN83eDhqlZoxtcYwte5U1Co124O3MyxgGDFJMQaPRQghhChscpWc2tnZkZKSov1arVZz79497XEHBwfCw8PzJkIh9OXkCe/thNafgIk5XNsHK+rBpZ+NEs7bFd9mefPlWJlacSLsBL339iY8Vr4vhBBCiOzkKjktV64cV69eBTQtp5UrV2br1q2AZnLyH3/8EQ8Pj7yLUgh9qdVQfwQMOgJuVSH+EXzfG7YPhoRIg4fTuHRjvm77NSWsSnDt8TV67OrB5UeXDR6HEEIIUVjkKjlt2bIl27Zt047Gf//999m7dy/lypXDx8eHgIAA+vfvn6eBFkSpaQonQh7yc9BdToQ8JDVNBr4UGK5+MOAQNBoLKjX89S2sqA/Xjxg8lMrFK7Op/SbKOZQjIj6C9/a8x7G7xwwehxBCCFEY5GoqqZiYGO7evUu5cuUwNTUFYNGiRWzcuBETExO6du3KhAkTUKlUeR6wMWQ2jcneC2F8vPMSYZH/TQDv7mDJ9I5+tK3ibqxQRWZunYTtg+DxDc3rekOhxUdgZmXQMKKSohhzeAwnw09iojJhWr1pdKnQRb9zZSopUYTJfShE0SLLl+rh2R+Mey+EMWTjWZ794NJT8ZU9a0iCWtAkxmhWkzrztea1c0XNxP0lXzVoGMmpycw4MYMdITsAGFh1ICOqjyBNSeNsxFkexD3A2dqZGi41MHlqvlZJTkVRJvehEEWLJKd6ePoHo42tHQ3nHdJpMX2aCnBzsOS3ic0xUb8cLccvlav7YcdwiLkPalNoOgkajAYTU4OFoCgKK/9aycq/VgJQw6UGd2LuEBEXoS3jau3KpDqTaOnZEpDkVBRtch8KUbTkqs8pQEJCAvPnz8ff3x9XV1dcXV3x9/dn/vz5xMfH52WMBcqp0EdZJqYAChAWmcCp0EeGC0ror0JrGHICKr0OaSlw6BP4ui08DDFYCCqViqGvDmVWg1moUXM24qxOYgoQERfBmMAxBNwMMFhcQgghREGQq5bTBw8e0Lx5cy5evIi9vT1ly5YF4Pr160RFReHn58fhw4dxdnbO84CN4em/2g9fj2bklqDnnuNiZ0HVUg54lbDBu4QNZUvY4FXCBjd7S9TSomp8igLnvoPd4yExCsysofUsqNVfM2+qAaSmpdLs+2Y8Tnyc6XEVKlytXdnbZS+xMbHSciqKLLkPhShacvUsc/z48Vy6dIlFixYxdOhQzM3NAc1E/J9//jnjxo1j/PjxL+UqUS52lnqVi4hO5ODliAz7Lc3UeBXXJKzPbsVszF+aQWQFnkoFr7wDng3g56EQ+ivsGgtX9sDrn4F9/vcZPhtxNsvEFEBBITwunLMRZ/G19s33eIQQQoiCIFfJ6c6dO+nfvz+jRo3S2W9ubs7o0aO5ePEi27dvz4v4Cpw63sVwd7AkPDIhw4Ao0PQ5dbaz4NO3XuHGozhu/BNL6L/b7UdxJCSncTk8msvh0RnOtbc0fSpZtcWrhDVl//3XztIs399bkeToAb1+hlNfQMAMCA6Alf7w2iKo8ma+XvpB3AO9y0lyKoQQoqjIVXKalJREjRo1sjxeq1Ytvvvuu1wHVZCZqFVM7+jHkI1nUYFOgpre5jmzU2UaVXCm0TPnJqemcedxPKH/xBD6T9y//8Zy45847j6JJyohhb/uRPLXnYyTxZewtaDsv4mrtquAsw1lilljaWaSobzIAbUa6g2Bss00U06F/QVb+8KV/7d333FVV/8Dx1/3svdQAVFUVMSZe1BaDhSzLM2y1BxJlpj+Kq3MMleWo7Lp+FbO1GxqZWqpaeZEUXLlBidDQfbmnt8fN25eGQJe4ALvZ4/7iPv5nM8553M9wJvzOWMT9HsP7NzKpNha9sUb9lLcdEIIIURVUKrgtGPHjhw+fLjQ82FhYXTq1KnUlTJ3fVvWZvHT7fKtc+p1h3VOrSy0hp7R22Vk5xIZl0rkjVQu3Egl4noqkXH6HtcbKVncSMnkRkomoZHGE600GvB2saNhLYf/hgvUcsC3hgN13eywtCj1nLfqx6MpBG+DXfPhrw/g2HcQuQcGLIJGPUxeXDuPdnjaexKbFosqoB8+b8xpO492pKakmrx8IYQQwhyVakLUkSNH6NWrFzNnziQkJMSwEH9OTg4LFy5k1qxZbN++nTZt2pi6vhWisMH4uTpFaEQ8sckZeDjZ0snXvUyWj0rKyDYaHmB4XU8lOTOn0OsstRrq1bDH97ag1beWA55OMjGrSJcPwvrnIf7fWfydnofAGWBtb9Jitl3cxsSdEwGMAlTNv/3wC7ovILB+oCwlJao1aYdCVC+lCk579uzJ5cuXuXDhQoGz9Rs1akTdunWNC9Jo2L59e5H5zpkzhx9//JFTp05hZ2fHvffey7x58/D314+3i4yMxNfXt8Brv/32W5544okCz40aNYqVK1caHQsKCmLLli3Ful9z/cGolCIuNStfwJrX45qZoyv02ryJWbf2uOZ9LROz/pWVClunwcEv9e9rNoGBS6BOe5MWs+3iNuaGziUmLcZwzMvei8mdJss6p0Ig7VCI6qZUwWmDBg1KFbxEREQUeb5v37489dRTdOzYkZycHN544w2OHz/OyZMncXBwIDc3l+vXjSeRfP7557z33ntERUXh6OhYYL6jRo0iJiaG5cuXG47Z2Njg5la8sYSV8QejTqeISsoocJjApfg0cnWF/7M721riW8sR3xr2+NZ0NPS4VtuJWWe3wU8vQEo0aCzggdeg2ySwMN1nkavLlR2ihCiEtEMhqhez3iHq+vXreHh48Oeff3L//fcXmKZt27a0a9eOpUuXFprPqFGjSEhIYMOGDaWqR1X7wXjrxKwLtwStEddTuVbEBgOgX4ng1mECeb2vVX5iVlo8/DoRTvy7CoV3O3jsc6jpVy7FS3AqqjNph0JUL+W3Z2MpJCbqZ627u7sXeD4sLIzw8HAWLlx4x7x27tyJh4cHbm5u9OzZk9mzZ1OjRo0C02ZmZpKZmWl4n5SUVIram69bJ2b1bGp8Lj0rl4vx+kA1Iu7f/9/QB7A3UrK4npzJ9eSiJ2b51vx3qEAt/eYDdVyrwMQse3d4YgU0fVgfpF47DEu6Qe9Z0PFZ/Yz/KqCqt30hhBDmz2x7TnU6HY888ggJCQns3r27wDTjxo1j586dnDx5ssi81q1bh729Pb6+vpw/f5433ngDR0dH9u3bh4VF/t6+GTNmMHPmzHzHq/tf7Ynp+olZkXGpXLglaL3TxCwrCw0+7vb6XbL+DVrzgmMvZ9vKN7418ar+Mf+FHfr3DXvoZ/Q7e5dZkeXVcyRtX5gj6TkVonopVnDas2fPkmdcjAlQRQkJCWHz5s3s3r073+QqgPT0dGrXrs1bb73FpEmTSpT3hQsXaNSoEdu2baNXr175zhfUe+Tj4yM/GAthNDGrgB7XoiZm2VlZ/Ltuq71h8wH914642VuZb+Cq08HBL/QTpnIywNZFv3B/q8fLpLjy+uUsbV+YIwlOhaheivVY/8KFC+UaJIwfP56NGzeya9euAgNTgO+//560tDRGjBhR4vwbNmxIzZo1OXfuXIHBqY2NDTY2NiXOt7rSaDTUdLShpqMNHRsYD8HIm5hlHLSmEBmXxqX4NNKzc/knKol/ovI/Pnaxs6JBTQejzQca/vt/R5sKHpGi1ULn5/9duP95/WP+H4Lh1K/w0Af6YQCVkLR9IYQQFa1Yv+EjIyNLnPGtvS/FpZRiwoQJrF+/np07dxa6bBTA0qVLeeSRR6hVq+S751y5coW4uDhq1y77/dOrO61WQx1XO+q42tHVr6bRuexcHZfj0wocJnAtMYPE9Gz+vpzA35cT8uVby8lGv/zVrTtm1XTAp7wnZtVqAsG/6xft/3M+nPgRLu2DRz+DxoHlVw8hhBCiijD5mNOwsDCWLl3KN998Q1xcXImuHTduHGvXruWnn34yrG0K4OLigp2dneH9uXPnaNKkCZs2baJv37758mnatClz5sxh4MCBpKSkMHPmTAYNGoSXlxfnz5/ntddeIzk5mWPHjhWrl0geKZW/WydmXbiRarQJQVxqVqHXaTRQx9XOMKb11leZT8y6GgY/Pg9xZ/XvOz6rnzBlnX9HsJKS2fqiOpN2KET1YpJno/Hx8axevZply5Zx7NgxlFI0adKkxPksXrwYgO7duxsdX758OaNGjTK8X7ZsGXXr1qVPnz4F5nP69GnDTH8LCwuOHj3KypUrSUhIwNvbmz59+vD222/L40szZmdtQVMvZ5p65f9FlDcx6/YdsyJv6CdmXbmZzpWb6fx19obRdVYWGuq5298SsDrSoKY9DWs64ulsc/dDV+q0h+d3wbbpEPq5fvH+8zv0S07V7XB3eQshhBDVxF31nP72228sW7aMn3/+maysLJo0acKQIUMYNGgQLVq0MGU9K5T81V45KKW4kZJlCFQv3Ph3fOuNNCLiUskqxsSsW8e35g0VcHOwLnllzv8BG16A5Gv6hfu7TdIv3l/Khful51RUZ9IOhaheShycRkZGsmzZMlauXMmVK1eoWbMmffr0Ye3atXz33Xc89thjZVXXCiM/GCs/nU5xLTFdH6jeSDEaKnD5ZnqRO2a52FkVOEzgjhOz0m/Cplfh2Hf697Xb6HtRa/nnS5qrU4RGxBObnIGHky2dfN2x0P7XkyvBqajOpB0KUb0U+7H+mjVrWLZsGX/++ScWFhY8/PDDfPrpp/Tr14+LFy+yZs2asqynEHdFq9VQ182eum72hU7Mun2YQMSNVKL+nZgVfjmB8AImZnk42RS4okC9GvbY2LnBoC/B/0HYOBGiwuF/90PgDOj0vGHh/i3Ho5j5y0mibtmdq7aLLdP7N6dvS5m0J4QQonopdnA6fPhwGjZsyEcffcSQIUMK3V1JiMrGykJLw1qONKzlmO9celbuf9u73ja+NS41i9jkTGKTMwmNMN4xS6sB738nZjWs6U+zjt/S5+zbuEf/BVteh9ObYcAitly2IGT1YTTo6KI9hQcJxOLKwcSmhKw+zOKn20mAKoQQolopdnBqY2NDZGQkP/30E25ubjz22GNGM+iFqIrsrC1oVtuZZrWLnph1+4oCKQVMzHqdsTxt0Zg3LNdiH/EnaR91YkfuM/TRWjLdahXemv8C3GvKnVnZI5j5iy29m3uV2/0KIYQQFa3YwWlUVJRhRv7w4cMZN24cjz/+OCNHjsTbu+y2bRTCXLnYWdHax5XWPq5Gx2+dmBVxI4WIf8e5RtxI5du4IHZnteRDq8W01Z5jnvYzlBXcPuLVi3gWWX1ESDKERrShRa3STaQSQgghKptSzdY/fPgwS5cu5euvvyYxMZFatWpx/fp1vvzyS5555pmyqGeFksH4wlTyJmZFxCaSum0eQbHLKWwFK52CaGpwcMBOejRylQlRotqSdihE9VKqFcnbtWvHwoULiYqK4quvvjIsG/Xss8/Spk0bZs+ezYkTJ0xaUSGqgryJWd38a+PTpnehgSn8O25VE0fjtGPlV0EhhBCigt3Vdjk2NjYMHTqU7du3c/78ed58801u3rzJtGnTaN26tanqKESV1MwpzaTphBBCiKrAZHs5NmjQgFmzZhEZGcmmTZuq5HqnQpiS1ql4E52Km04IIYSoCky+0bhGo6Fv3758++23ps5aiKql/r3g7I2i4Gf7Cg0419GnE0IIIaoJkwenQohi0lpA33loIF+AqtDoj/Sdq08nhBBCVBMSnApRkZo/AoNXoXE2Xmhf4+wNg1fpzwshhBDVSLHXORVClJHmj0DTh+DiXkiJAUdP/aN86TEVQghRDUlwKoQ50FqAb7eKroUQQghR4SQ4LYa8fQqSkpIquCaiuspre6XYM+OuSNsX5qCi2r8QomJIcFoMycnJAPj4+FRwTUR1l5ycjIuLS7mWB9L2hXko7/YvhKgYpdq+tLrR6XRcu3YNJycnNLdt6ZOUlISPjw+XL1+WbfXEXSmqLSmlSE5OxtvbG622/OYxFtX2Qdq/MI07taOKav9CiIohPafFoNVqqVu3bpFpnJ2d5ZezMInC2lJF9BgVp+2DtH9hGkW1I+kxFaL6kD9BhRBCCCGE2ZDgVAghhBBCmA0JTu+SjY0N06dPx8bGpqKrIiq5ytiWKmOdhfmRdiSEuJVMiBJCCCGEEGZDek6FEEIIIYTZkOBUCCGEEEKYDQlOhRBCCCGE2ZDgVAghhBBCmA0JToUQQgghhNmQ4FQIIYQQQpgNCU6FEEIIIYTZkOBUCCGEEEKYDQlOhRBCCCGE2ZDgVAghhBBCmA0JToUQQgghhNmQ4FQIIYQQQpgNy4quQGWg0+m4du0aTk5OaDSaiq6OqIaUUiQnJ+Pt7Y1WW35/U0rbF+agotq/EKJiSHBaDNeuXcPHx6eiqyEEly9fpm7duuVWnrR9YU7Ku/0LISqGBKfF4OTkBOh/MDo7O1dwbUR1lJSUhI+Pj6Etlhdp+8IcVFT7F0JUDLMOThcvXszixYuJjIwEoEWLFkybNo0HH3wQgIyMDCZNmsS6devIzMwkKCiIRYsW4enpacjj0qVLhISEsGPHDhwdHRk5ciRz5szB0rL4t573ONPZ2Vl+QYsKVd6P1qXtC3MiQ0uEqB7MevBO3bp1mTt3LmFhYRw6dIiePXvy6KOPcuLECQBefvllfvnlF7777jv+/PNPrl27xmOPPWa4Pjc3l4ceeoisrCz27t3LypUrWbFiBdOmTTNJ/XJ1uRyMPsimC5s4GH2QXF2uSfIVQgghhKiuNEopVdGVKAl3d3fee+89Hn/8cWrVqsXatWt5/PHHATh16hTNmjVj3759dOnShc2bN/Pwww9z7do1Q2/qkiVLmDx5MtevX8fa2rpYZSYlJeHi4kJiYqKh92jbxW3MDZ1LTFqMIZ2nvSevd3qdwPqBJr5rUd0V1AarcrlC3EraoRDVi1n3nN4qNzeXdevWkZqaSkBAAGFhYWRnZxMY+F8g2LRpU+rVq8e+ffsA2LdvH61atTJ6zB8UFERSUpKh97UgmZmZJCUlGb1ute3iNibunGgUmALEpsUycedEtl3cZopbFqLc3antCyGEEGXN7IPTY8eO4ejoiI2NDWPHjmX9+vU0b96c6OhorK2tcXV1NUrv6elJdHQ0ANHR0UaBad75vHOFmTNnDi4uLobXrbOVc3W5zA2diyJ/h3PesXmh8+QRv6iUimr7QgghRHkw++DU39+f8PBwDhw4QEhICCNHjuTkyZNlWuaUKVNITEw0vC5fvmw4dzj2cL4e01spFNFp0Xxx7AuuJF+hko2aENVcUW1fCCGEKA9mPVsfwNramsaNGwPQvn17Dh48yMcff8yTTz5JVlYWCQkJRr2nMTExeHl5AeDl5UVoaKhRfjExMYZzhbGxscHGxqbAc9fTrher3gvDF7IwfCEOVg40cWuS72VvZV+sfIQoT0W1fSGEEKI8mH1wejudTkdmZibt27fHysqK7du3M2jQIABOnz7NpUuXCAgIACAgIIB33nmH2NhYPDw8ANi6dSvOzs40b968VOXXsq9VrHT1nOoRlRpFanYqR2KPcCT2iNF5Hycf/N389cGquz5greNYB63G7DuzhRBCCCHKjFkHp1OmTOHBBx+kXr16JCcns3btWnbu3Mlvv/2Gi4sLwcHBTJw4EXd3d5ydnZkwYQIBAQF06dIFgD59+tC8eXOGDx/O/PnziY6OZurUqbzwwgul7h1q59EOT3tPYtNiCxx3qkGDp70nPw/4GR06LiZe5PTN05y5eYbTN09zNv4ssemxXE6+zOXky2y79N/kKQcrB/xc/fB39zf0sPq5+eFg5VC6D1AIIYQQopIx6+A0NjaWESNGEBUVhYuLC/fccw+//fYbvXv3BuDDDz9Eq9UyaNAgo0X481hYWLBx40ZCQkIICAjAwcGBkSNHMmvWrFLXyUJrweudXmfizolo0BgFqBr0C0RP7jQZC60FFljQ2K0xjd0a8xAPGdLdzLipD1bj9UHrmZtnOJdwjtTsVMKvhxN+PdyoTB8nH5q4Nfmvp9WtCXWcpJdVCCGEEFVPpVvntCIUd51TL3svJneaXKp1TrN12VxMvGjoYT1z8wxn4s8Qmx5bYHp7S3v83PwMAau/u7/0slZhss6pqM6kHQpRvUhwWgyF/WDM1eVyOPYw19OuU8u+Fu082mGhtTBp2TczbnL25tn/hgbEn+Z8wnmydFkFpq/rWNcQrOb1tkova+UnwamozqQdClG9mDQ4TUtLIzIykri4uAKXULr//vtNVVS5MrcfjDm6HC4mXTQaGnD65mli04ruZTUMDXBvgp+rH47WjuVcc1FaEpyK6kzaoRDVi0nGnKalpTFx4kSWL19OTk5OvvNKKTQaDbm5sjC9KVhqLWnk2ohGro140PdBw/GEjATDGNa8ntZzN8+RlpPG39f/5u/rfxvlU8exTr5e1rpOdaWXVQghhBAVxiTB6YsvvsjSpUvp168fPXv2pEaNGqbIVpSQq60rnWp3olPtToZjObocLiVdMhoWcObmGWLSYriacpWrKVfZcXmHIb2dpZ1xL+u/E7Ckl1UIIYQQ5cEkj/Vr1qxJUFAQa9asMUWdzE5VfKSUkJHA2YSzRgHruYRzZOZmFpg+r5f11p5WHycf6WUtJ/JYX1Rn0g6FqF5M0nOakZFB9+7dTZGVKCeutq509OpIR6+OhmM5uhwuJV/iTLzx0IDo1OjCe1ld/QybCPi76VcMcLJ2qohbqtRydYrQiHhikzPwcLKlk687FlpNRVdLCCGEKHcmCU47dOjA2bNnTZGVqECWWksaujSkoUtD+vr2NRxPzEw0jGXN62k9l3CO9Jx0jt44ytEbR43y8XbwNgpY83pZTb2SQVWx5XgUM385SVRihuFYbRdbpvdvTt+WtSuwZkIIIUT5M8lj/f3799O/f382b95Mhw4dTFEvsyKPlPLL1eVyMfmiYT3WvJ7W6NToAtPbWdrR2LWx0dAAPzc/nK2r9+e55XgUIasP59trLK/PdPHT7ejbsrY81hfVmrRDIaoXk/Scfv7559StW5cuXboQEBBAw4YNsbAw7iXTaDQsXbrUFMUJM2Chtfivl7VB4b2sZ+LPGHpZj904xrEbx4zy8Xbw1gest/S0Vpde1lydYuYvJwvYBBcU+gB15i8n6d3cq5xrJoQQQlQck/ScarV3nhRTmZeSkr/a706uLpdLyf+uGBD/X+AalRpVYPrbe1nzgteq1su673wcQ77Yf8d0X4/pQotaVtJzKqotaYdCVC8m6TnV6XSmyEZUURZaC3xdfPF18c3Xy3r25lmjntazN88W2sta26G2YdJVE3d9L2s9p3qVtpc1Njnjzon+TdeillUZ10YIIYQwDyYJToUoDRcbFzp4daCD13/jlHN1uVxOvmxYKSCvp/Va6jWiUqOISo1i55WdhvS2Frb6Xlb3JkY9rS42LhVwRyXj4WRr0nRCCCFEVWDS4DQ1NZV9+/YRExNDYGAgnp6epsxeVAMWWgsauDSggUsDghoEGY4nZSVx9uZZw5qseb2sGbkZHI87zvG440b5eDl4/beJwL+Ba32n+mbVy5qRXfQwFw3g5aJfVio1Jbl8KiWEEEJUMJMFp4sXL2bKlCkkJSWh0WjYunUrnp6exMbGUq9ePT799FPGjBljquJENeNs7Ux7z/a092xvOJbXy3rrmqxn4vW9rNGp0USnRvPnlT8N6W0sbAxjWfM2EqioXtaf/77GpG/DDe81YDQxKm+2/vT+zWW9UyGEENWKSYLTH374gRdeeIFHH32U/v378+yzzxrOeXh40LdvXzZs2CDBqTCpW3tZ+zToYzienJVstCbr2ZtnOZugH8t6Iu4EJ+JOGOXjae9pCFbzelvrOdfDUls2o15W7Ytk+s8nUAoevqc2QS28eHfTP0brnHrJOqdCCCGqKZP89n3vvffo0aMH69evJy4uzig4Bf0i/V988YUpihLijpysnfL1suqU7r9e1luGBlxNuUpMWgwxaTHsurLLkN7GwoZGro0MwWpe8Ho3vaxKKT7adpaPt+s3rBjepT4zHmmBhVZDv1a1ZYcoIYQQAhMFp8eOHWPevHmFnq9duzaxsbGmKEqIUtFqtNR3rk995/r0rt/bcDw5K9mwYkDe0IC8FQNOxp3kZNxJo3w87T2NglV/N/9i9bLqdIoZv5xg1b6LALwU6MeLvfzQaPICUB2WDhew0lzH0r4W4AqYz/hYIYQQoryYJDi1sLAocjmpa9eu4eDgYIqihDApJ2sn2nm2o51nO8MxndJxJfmKIVjN62m9tZf1r6t/GdLn9bLeOiygiVsTXG1dAcjK0THx23A2Ho1Co4GZj7RgREADw/XbLm5jbuhcYtJiDMc87T15vdPrBNYPLPPPQAghhDAnJglOW7duzW+//cb//d//5Tun0+n47rvv6NixoymKEqLMaTVa6jnXo55zvXy9rOcSzhmC1dM3TxfZy+ph70EjFz/OX3XmYpQL1nbezHs0kIFtfAxptl3cxsSdE1G37RMVmxbLxJ0TWdB9gQSoQgghqhWTBKfjx49nyJAhvPXWW4wYMQLQB6WnT5/mjTfe4MSJE0U+9heiMnCydqKtR1vaerQ1HMvrZTUMC4jX//9qylVi02KJTYsFLdjV0aeffewzvr6s72X1c/Nj6bGl+QJTAIVCA8wLnUcPnx7ldIdCCCFExTPJ9qUAU6dO5d1330Wr1aLT6dBqtSilUEoxY8YMpk2bZopiKoRsnSdK6uz16wR//QtRaRHYOcbQqE4SV9MukJ6TXuK8lgUtw9/eX7YvFdWWtEMhqheTrZUze/ZsHnvsMdasWcOpU6dQSuHn58fw4cPp0KHDnTMQooo4F5vCyKVHuZboiZdzfb56ohN+nk7olI6ryVcNvay7Lv/JifiTd8zvemoM/vb+5VBzIYQQouKZdCHHdu3a0a5du3zH9+3bx19//cVrr71myuKEMDtHryQwavlB4lOzaFjTgVXBnajrZg/ox7L6OPvg4+xDr/q96KhsGF2M4LRWUgzUKuuaCyGEEOZBWx6F/PHHH0yZMqU8ihKiwuw+e4Mhn+8nPjWLe+q68N3YAENgWpB2Wgc8c3LQFDKyRqMUXjk5tLNwKqsqCyGEEGanXIJTIaq6TceiGL3iIKlZudzXuAZrx3ShhqNNwYlzMuHwV1hsf5vX424C5AtQ895PjruJhZPsEiWEEKL6KJv9GYWoRtYcuMjUDcdRCvq18uLDJ9tgY1nAAvrpCXBoGRz4H6REAxCIhgWxN5hbw40Yy/++HT1zc5kcl0CgpTvUvxdSUsvpboQQQoiKJcGpEKWklOKzP87xwdYzAAztXI+3H22Zf9vRxCuwfzGErYCsFP0xJ28IGAcOHgSuf54eaVEctrXmuoUFtXJzaZeRpd8favD/QCs7RQkhhKg+zPqx/pw5c+jYsSNOTk54eHgwYMAATp8+bZSme/fuaDQao9fYsWON0ly6dImHHnoIe3t7PDw8ePXVV8nJySnPWxFVjE6nmPnLSUNgOqFnY94ZcFtgGn0cfnwOPm4N+z7TB6YezWHAEnjxb7h3ArR+EgavwsK5Nh0zMumXmkbHjEwsnL1h8Cpo/kgF3aEQQghRMUrdcxofH1/stGlpaaUq488//+SFF16gY8eO5OTk8MYbb9CnTx9OnjxptB3qmDFjmDVrluG9vf1/k1Byc3N56KGH8PLyYu/evURFRTFixAisrKx49913S1UvUb1l5+p49bu/2RB+DYBpDzdndFdf/UmlIOJP2PMJnN/+30UNusF9L0LjQNDc1rPa/BFo+hBc3AspMeDoqX+ULz2mQgghqqFSB6c1a9ZEc/sv2UIopYqd9lZbtmwxer9ixQo8PDwICwvj/vvvNxy3t7fHy8urwDx+//13Tp48ybZt2/D09KRNmza8/fbbTJ48mRkzZmBtbV3ieonqKz0rl3Frwthx+jqWWg3vPXEPA9vWhdwcOLkB9n4CUX/rE2u00PxRuPf/oE7+JdaMaC3At1uZ118IIYQwd6UOTkeMGFGqgPNuJCYmAuDu7m50fM2aNaxevRovLy/69+/PW2+9Zeg93bdvH61atcLT09OQPigoiJCQEE6cOEHbtm25XWZmJpmZmYb3SUlJZXE7opJJSMsieOUhwi7exNZKy+Jh7enR0EE/wWnfZ5BwSZ/Q0g7aPg0BL4C7b8VWuoSk7QshhKhopQ5OV6xYYcJq3JlOp+Oll17ivvvuo2XLlobjQ4cOpX79+nh7e3P06FEmT57M6dOn+fHHHwGIjo42CkwBw/vo6OgCy5ozZw4zZ84sozsRlVF0YgYjl4VyOiYZZ1tLVj3pS5trn8NPX0K6fjko7GtAp+eh47PgUKNiK1xK0vaFEEJUtFIHp1988QUDBgygVq3y2brmhRde4Pjx4+zevdvo+HPPPWf4ulWrVtSuXZtevXpx/vx5GjVqVKqypkyZwsSJEw3vk5KS8PHxKV3FRaUXcSOVp788wNWEdNo7xvFlkwO4/fA95GToE7j5wr3jofVQsC580f3KQNq+EEKIilbq4DQkJISQkBC6dOnCY489xqOPPlrqYPBOxo8fz8aNG9m1axd169YtMm3nzp0BOHfuHI0aNcLLy4vQ0FCjNDExMQCFjlO1sbHBxqaQBdRFtXL8aiIjl4VSL+0E7zps5v6cA2hO/rtgfp32+vGkzfpXmclL0vaFEEJUtFIvJRUVFcXixYtxcXHhjTfeoEmTJtxzzz1Mnz6dI0eOmKRySinGjx/P+vXr+eOPP/D1vfP4vfDwcABq19bvqhMQEMCxY8eIjY01pNm6dSvOzs40b97cJPUUVdPec7Es/vwzFme/yXqb6TyQux8NCpr0hVGb4Nnt0GJAlQlMhRBCCHOgUaqQjb1LIDk5mV9//ZUNGzawefNmUlJS8PHxYeDAgQwcOJBu3bqVavLUuHHjWLt2LT/99BP+/v6G4y4uLtjZ2XH+/HnWrl1Lv379qFGjBkePHuXll1+mbt26/Pnnn4B+Kak2bdrg7e3N/PnziY6OZvjw4Tz77LPFXkoqKSkJFxcXEhMTcXZ2LvF9iEomJ5Njmz/H/tAiGmn0y0UprRWae57Ur03q0bTcq1RRbVDavjAH5dEOdTodWVlZZZK3EAKsrKywsCheZ45JgtNbZWVlsW3bNtavX88vv/xCbGwsNWrUoH///gwcOJDevXtja2tbvMoVEtAuX76cUaNGcfnyZZ5++mmOHz9OamqqISCeOnWq0Q+wixcvEhISws6dO3FwcGDkyJHMnTsXS8vijWqQX9DVxL/bi6bvXohd5g39IY0DVl2CsQwIAWfvCquaBKeiOivrdpiVlUVERAQ6nc7keQsh/uPq6oqXl9cdOyxNHpzeSinF7t27Wb9+PT/99BORkZFMnz6dadOmlVWRZUJ+QVdx/24vqsJWoPl3e9Eo5U5Y7SH0HfEalvauFVs/JDgV1VtZtkOlFJcuXSI7Oxtvb2+0WrPeOFGISkkpRVpaGrGxsbi6uhqGXham1BOiikOj0dCtWze6devGggULOHr0qNEaikJUqOjj+kXzj/8Auhw0wCmdD5/nPITXfU/zar+W5b6WrxCifOXk5JCWloa3t7fR7oJCCNOys7MDIDY2Fg8PjyIf8ZdpcHq7e+65pzyLEyK/QrYXPWvflncSerNT15o3+zVnzP0NK7CSQojykpubCyC7BQpRDvL+AMzOzi6f4HTt2rUsXLiQs2fPEhcXl++8RqMhJyfHVMUJUTKFbC+a2/QR3knozbIINyy0Gt5/4h4eb1/0cmVCiKpHnpIIUfaK+31mkuB09uzZTJ8+HU9PT+69917c3NxMka0Qdy8rFY6sLnB70aR2zxP803UORt7ExlLLwqHtCGzuWXR+QgghhChTJglOFy1aRPfu3dmyZQtWVlamyFKIu5NyHUL/BwcL3l40VufAiKWhnIpOxsnGki9HdqBzw8q55agQQpSlUaNGkZCQwIYNGyq6KqKaMElwmpSUxODBgyUwFRUv7jzs/RTC10Luv5Pvbtte9GJcKsOX7uNSfBo1HW1YNboTzb1lJroQovRydYrQiHhikzPwcLKlk687FloZKiBEaZgkOG3bti2XL182RVZClM7lg7DnIzj1K3DL9qL3vQhNHzbs4nTyWhIjloVyIyWTeu72fBXcifo1HCqs2kKIym/L8Shm/nKSqMQMw7HaLrZM79+cvi2LXjLHVLKysmRSl6gyTLKg2+zZs1myZInJti0Volh0Oji9GZb1haWBcGoj3L69aPNHDYFpaEQ8T36+jxspmTT1cuL7sQESmAoh7sqW41GErD5sFJgCRCdmELL6MFuOR5VJud27d2f8+PG89NJL1KxZk6CgIBYsWECrVq1wcHDAx8eHcePGkZKSYrhmxYoVuLq68ttvv9GsWTMcHR3p27cvUVH/1TE3N5eJEyfi6upKjRo1eO2117h9OfTMzEz+7//+Dw8PD2xtbenatSsHDx40nN+5cycajYbffvuNtm3bYmdnR8+ePYmNjWXz5s00a9YMZ2dnhg4dSlpaWpl8PqJyM0nP6QMPPMDSpUvp0qULXbp0oUGDBvmWCNBoNCxdutQUxYnqLicTjn6jf3x/44z+mNYKithedNvJGF5Ye5jMHB2dGrjzxcgOuNjJMBQhhDGlFOnZucVKm6tTTP/5BAXtZKMADTDj55Pc17hmsR7x21lZlGjVgJUrVxISEsKePXsA2Lx5M5988gm+vr5cuHCBcePG8dprr7Fo0SLDNWlpabz//vt89dVXaLVann76aV555RXWrFkDwAcffMCKFStYtmwZzZo144MPPmD9+vX07NnTkMdrr73GDz/8wMqVK6lfvz7z588nKCiIc+fO4e7ubkg3Y8YMPvvsM+zt7Rk8eDCDBw/GxsaGtWvXkpKSwsCBA/n000+ZPHlyse9ZVA8m2SHqwIEDBAUFkZSUVHhBGo1hPbnKRnbJMRP/bi/KgSWQEqM/ZuMMHZ6BzmML3V70u0OXef3HY+TqFIHNPPhsaDtsrYq3v6+5kB2iRHVWlu0wIyODiIgIfH19sbW1JS0rh+bTfjNpGcV1clYQ9tbF6zPq3r07SUlJHD58uNA033//PWPHjuXGDf12zCtWrOCZZ57h3LlzNGrUCNBPaJ41axbR0dEAeHt78/LLL/Pqq68C+k0KfH19ad++PRs2bCA1NRU3NzdWrFjB0KFDAf2alQ0aNOCll17i1VdfZefOnfTo0YNt27bRq1cvAObOncuUKVM4f/48DRvq15EeO3YskZGRbNmypRSflqiMbv9+K4xJek5ffPFFrK2t+emnn+jWrRuurq6myFYIvX+3FyVsBfy7vShO3hAwDtqNBNvCf1l9vus87246BcCgdnWZN6gVlhayPaEQovJr37690ftt27YxZ84cTp06RVJSEjk5OWRkZJCWlmZY/Nze3t4QmALUrl2b2NhYABITE4mKiqJz586G85aWlnTo0MHwaP/8+fNkZ2dz3333GdJYWVnRqVMn/vnnH6P63LrxjqenJ/b29obANO9YaGjo3X4MogoySXB69OhRZsyYQf/+/U2RnRB6t20vCoBHc7j3/6DlILAsfPC/Uop5W06z5M/zAIzp5suUB5uhldmzQogi2FlZcHJWULHShkbEM2r5wTumW/FMRzr5ut8xnV0Jn+g4OPw3Zj4yMpKHH36YkJAQ3nnnHdzd3dm9ezfBwcFkZWUZgtPbV9XRaDT5xpSayq1laTSaAsvW6XRlUrao3EwSnHp4eMgsQWEahWwvSoNu+pn3jQPhDmOycnJ1vLn+ON8c0q8gMblvU8Y+0FB2gBFC3JFGoyn2o/VufrWo7WJLdGJGgeNONYCXiy3d/GqV+bJSYWFh6HQ6PvjgA7Ra/dOhb7/9tkR5uLi4ULt2bQ4cOMD9998P6B/rh4WF0a5dOwAaNWqEtbU1e/bsoX79+oD+sf7Bgwd56aWXTHdDolozSXA6evRoVq9ezfjx47G0NNmOqKI6KWR7UZo/qu8prdOuWNlkZOfyf18f4feTMWg1MOexVjzZsV7Z1VsIUW1ZaDVM79+ckNWH0YBRgJoXik7v37xc1jtt3Lgx2dnZfPrpp/Tv3589e/awZMmSEufz4osvMnfuXPz8/GjatCkLFiwgISHBcN7BwYGQkBBeffVV3N3dqVevHvPnzyctLY3g4GAT3pGozkwSSXbt2pWNGzfSpUsXxo0bh6+vb77Z+oDhLzEhDLJS4fBXsH+h8fai7YZDl3Hg7lvsrJIzshmz6hD7L8Rjbanlk6fa0relVxlVXAghoG/L2ix+ul2+dU69ynmd09atW7NgwQLmzZvHlClTuP/++5kzZw4jRowoUT6TJk0iKiqKkSNHotVqGT16NAMHDiQxMdGQZu7cueh0OoYPH05ycjIdOnTgt99+k63LhcmYZLZ+3iMEQ6a3PT5VSslsfWHsDtuL4lCyrUSvJ2cyankoJ64l4WhjyRcjOhDQqOpsRyqz9UV1Vp6z9UtLdogS4s7Kdbb+8uXLTZGNqA6Ksb1oSV2OT2P40gNExqVRw8GalaM70bKOi4krLoQQhbPQaqrUH8RCVKS7Dk4zMzPx9fWldu3a+Pn5maJOoioq5vaiJXUqOokRS0OJTc6krpsdXwV3xrem7PokhBBCVFZ3HZxaWFjQq1cvPvjgAwlOhTGdDs5s0U9yurTvv+NN+uonOdW/944z74sSdjGeZ5YfJCkjB39PJ1YFd8LTufSP5YQQQghR8e46OLW0tMTLy6vM1kkTlVApthctqR2nYglZE0ZGto729d1YOrIDrvaynJkQQghR2ZlkzOkTTzzBt99+y4QJE/JNjhLVSCm3Fy2pDUeu8sp3f5OjU3T3r8XiYe2xs65c25EKIYQQomAmCU6fffZZduzYQe/evXnppZfw8/Mz7EZxq3r1ZL3JKinhsn570cMrS7y9aEkt2x3BrI0nARjQxpv3nmiNlWxHKoQQQlQZJglOW7ZsadgCbefOnYWmq6xLSYlC3MX2oiWllOKD38/w2Y5zADxzXwPeeqi5bEcqhBBCVDEmCU6nTZsmW0NWFybYXrSkcnWKt346ztoD+kX6X+nThBd6NJY2J4QQQlRBJglOZ8yYYYpshDkz0faiJZWZk8vL34Sz6Vg0Gg3MHtCSYZ3rl0lZQgghhKh4MlhPFC0rFfYvgU/bwg/B+sDU0g46PQcTDsMTK8osME3JzGH0ioNsOhaNtYWWhUPbSWAqhBCF0Gg0bNiwocg0p06dokuXLtja2tKmTZtyqZcQJWXS4DQ3N5cTJ06we/dudu3ale8lKpGU6/DHbFjQHLZM1u97b18Dur8BL5+Afu+VaN/7kopLyWToF/vZcy4Oe2sLlo3qSL9W5bNHtRBClJguFyL+gmPf6/+vM885FtOnT8fBwYHTp0+zffv2O19wmxkzZpRJUFtW+VZ1ly5d4qGHHsLe3h4PDw9effVVcnJyirwmPj6eYcOG4ezsjKurK8HBwaSkpBilOXr0KN26dcPW1hYfHx/mz59vdP7EiRMMGjSIBg0aoNFo+Oijj0x6XyZ5rA8wb9485s6dS1JSUqFpSjohas6cOfz444+cOnUKOzs77r33XubNm4e/v78hTUZGBpMmTWLdunVkZmYSFBTEokWL8PT0NKS5dOkSISEh7NixA0dHR0aOHMmcOXOwtDTZ7VcdZbC9aEldTUhn+NIDXLieiruDNctHdaS1j2uZlyuEEKVy8mf9H/FJ1/475uwNfedB80fKpQpZWVnFSnf+/Hkeeugh6tcv+ClUZGQkvr6+snZ5JZCbm8tDDz2El5cXe/fuJSoqihEjRmBlZcW7775b6HXDhg0jKiqKrVu3kp2dzTPPPMNzzz3H2rVrAUhKSqJPnz4EBgayZMkSjh07xujRo3F1deW5554DIC0tjYYNG/LEE0/w8ssvm/7mlAl8+eWXSqPRqO7du6t3331XaTQaNXHiRDV58mRVs2ZN1alTJ7VixYoS5xsUFKSWL1+ujh8/rsLDw1W/fv1UvXr1VEpKiiHN2LFjlY+Pj9q+fbs6dOiQ6tKli7r33nsN53NyclTLli1VYGCgOnLkiNq0aZOqWbOmmjJlSrHrkZiYqACVmJhY4nuoNC6FKvX1UKWmuyg13Vn/+ryHUic2KJWbU27VOBOdpDq/s03Vn7xRBby7TZ2NSS63ss1ZRbXBatH2hdkry3aYnp6uTp48qdLT00uXwYmfjH9uGl4u+teJn0xY2/888MAD6oUXXlAvvviiqlGjhurevbsC1KJFi1Tfvn2Vra2t8vX1Vd99953hGvR7Rxte06dPz5dvRESEKiw0WL58eb48li9frpRS6ubNmyo4OFjVrFlTOTk5qR49eqjw8HCllFKxsbHK09NTvfPOO4a89uzZo6ysrNS2bduKzLco//zzj7rvvvuUjY2Natasmdq6dasC1Pr16w1pXnvtNeXn56fs7OyUr6+vmjp1qsrKyjKcnz59umrdurVaunSp8vHxUQ4ODiokJETl5OSoefPmKU9PT1WrVi01e/Zso7IBtWTJEvXQQw8pOzs71bRpU7V371519uxZ9cADDyh7e3sVEBCgzp07Z7jm3Llz6pFHHlEeHh7KwcFBdejQQW3duvWO91mYTZs2Ka1Wq6Kjow3HFi9erJydnVVmZmaB15w8eVIB6uDBg4ZjmzdvVhqNRl29elUppdSiRYuUm5ubUR6TJ09W/v7+BeZZv3599eGHHxarzsX9fjNJcNq+fXsVEBCglFLqxo0bSqPRqO3btyullLp27Zry8PBQS5cuvetyYmNjFaD+/PNPpZRSCQkJysrKyuib759//lGA2rdvn1KqdP94t6uyv6Bzc5X651ellgYZ/1BdM1ipiN1K6XTlWp2wi/Gq9czfVP3JG1WvD3aqawlp5Vq+OZPgVFRn5Rqc6nRKZaYU75WeqNT7/gUEprcEqB801acrTn4l+Jn7wAMPKEdHR/Xqq6+qU6dOqVOnTilA1ahRQ33xxRfq9OnTaurUqcrCwkKdPHlSKaVUVFSUatGihZo0aZKKiopSycn5//gvKjhNS0tTkyZNUi1atFBRUVEqKipKpaXpf04HBgaq/v37q4MHD6ozZ86oSZMmqRo1aqi4uDillFK//vqrsrKyUgcPHlRJSUmqYcOG6uWXX75jvoXJyclR/v7+qnfv3io8PFz99ddfqlOnTvmC07ffflvt2bNHRUREqJ9//ll5enqqefPmGc5Pnz5dOTo6qscff1ydOHFC/fzzz8ra2loFBQWpCRMmqFOnTqlly5YpQO3fv99wHaDq1KmjvvnmG3X69Gk1YMAA1aBBA9WzZ0+1ZcsWdfLkSdWlSxfVt29fwzXh4eFqyZIl6tixY+rMmTNq6tSpytbWVl28eNGQ5vnnn1cODg5FvvK89dZbqnXr1kafy4ULFxSgDh8+XODntnTpUuXq6mp0LDs7W1lYWKgff/xRKaXU8OHD1aOPPmqU5o8//lCAio+Pz5dnWQSnJnmu/c8//zB79mwAw/I+eY/wa9euzXPPPcfHH3/M6NGj76qcxMREANzd3QEICwsjOzubwMBAQ5qmTZtSr1499u3bR5cuXdi3bx+tWrUyeswfFBRESEgIJ06coG3btndVp0qpHLYXLak/z1xn7FdhpGfn0sbHleWjOuLmINuRCiHKWXYavGua3exA6R/1z/UpXvI3roG1Q7Fz9/PzyzcW8IknnuDZZ58F4O2332br1q18+umnLFq0CC8vLywtLXF0dMTLy6vY5eSxs7PD0dHRsG15nt27dxMaGkpsbCw2NjYAvP/++2zYsIHvv/+e5557jn79+jFmzBiGDRtGhw4dcHBwYM6cOUXmW5StW7dy/vx5du7cabjmnXfeoXfv3kbppk6davi6QYMGvPLKK6xbt47XXnvNcFyn07Fs2TKcnJxo3rw5PXr04PTp02zatAmtVou/vz/z5s1jx44ddO7c2XDdM888w+DBgwGYPHkyAQEBvPXWWwQFBQHw4osv8swzzxjSt27dmtatWxvev/3226xfv56ff/6Z8ePHAzBr1ixeeeWVYn0G0dHRRrENYHgfHR1d6DUeHh5GxywtLXF3dzdcEx0dja+v8ZySW/N1c3MrVv3uhkmCUwsLCxwc9N9Qef+Pi4sznG/QoAFnz569qzJ0Oh0vvfQS9913Hy1btgT0H5K1tTWurq5GaT09PY0+5JL+42VmZpKZmWl4X9Q42kql0O1FR/+7vWjFTDj6+e9rTPo2nOxcRTe/mix5uj0ONjIeuCJU2bYvRBXUvn37fMcCAgLyvQ8PDy8ynxYtWnDx4kUAw1hTR0dHw/lu3bqxefPmQq//+++/SUlJoUaNGkbH09PTOX/+vOH9+++/T8uWLfnuu+8ICwszBLKlcfr0aXx8fIyC2U6dOuVL98033/DJJ59w/vx5UlJSyMnJwdnZeNfCBg0a4OTkZHjv6emJhYWF0Xbsnp6exMbGGl13zz33GJ0HaNWqldGxjIwMkpKScHZ2JiUlhRkzZvDrr78SFRVFTk4O6enpXLp0yXCNh4dHvuCxOjJJBFCvXj0iIiIAsLGxwcfHh7/++ounnnoKgIMHDxp6O0vrhRde4Pjx4+zevfuu63snc+bMYebMmWVeTrkpx+1FS2rVvkim/3wCpeDhe2qzYHAbrC1lhbOKUuXavhAlZWWv78Esjot7Yc3jd0437Huof2/xyi6BvM6gu7Vp0yays7MBuHr1Kt27dzcKaO3s7Iq8PiUlhdq1axe4Q+StnUfnz5/n2rVr6HQ6IiMjjQK5srBv3z6GDRvGzJkzCQoKwsXFhXXr1vHBBx8YpbOysjJ6r9FoCjym0+kKvS7vqXFBx/Kue+WVV9i6dSvvv/8+jRs3xs7Ojscff9xoMtvYsWNZvXp1kfeVN7Pey8uL0NBQo3MxMTGGcwXx8vLKF2Tn5OQQHx9vuMbLy8uQT3HzNTWTBKf3338/v/76q6GL/oknnuCjjz4iPT0dnU7H6tWr7+qR/vjx49m4cSO7du2ibt26huNeXl5kZWWRkJBg9A0QExNj9CGX9B9vypQpTJw40fA+KSkJH59iPpYxJ+W4vWhJKaX4aNtZPt6u71Ef3qU+Mx5pgYVsR1qhqkzbF6K0NJriP1pv1FM/Kz8pCv08nnyZ6c836glaC1PWslD79+9nxIgRRu/vNHzt1pn7eavYNG7cuMC01tbW+VbeadeuHdHR0VhaWtKgQYMCr8vKyuLpp5/mySefxN/fn2effZZjx44ZegkLyrco/v7+XL58mZiYGEOv5cGDB43S7N27l/r16/Pmm28ajuX1EFeEPXv2MGrUKAYOHAjog8zIyEijNCV5rB8QEMA777xDbGys4XPcunUrzs7ONG/evNBrEhISCAsLM/S8//HHH+h0OsOQhYCAAN58802ys7MNwfbWrVvx9/cvl0f6YKLg9MUXX6R169akp6djZ2fHzJkzOXPmDCtXrgSgT58+zJ07t8T5KqWYMGEC69evZ+fOnfnGQLRv3x4rKyu2b9/OoEGDAH1X/6VLlwyPNkrzj2djY3NXjxsqlGF70Y/h/B//HS/D7UVLSqdTzPjlBKv26X9IvBTox4u9/GQ7UjNQqdu+EOVNa6FfLurbEYAG4wD1359nfeeWW2AK8N1339GhQwe6du3KmjVrCA0NZenSpSbLv0GDBkRERBAeHk7dunVxcnIiMDCQgIAABgwYwPz582nSpAnXrl3j119/ZeDAgXTo0IE333yTxMREPvnkExwdHdm0aROjR49m48aNheZb1M+i3r1706hRI0aOHMn8+fNJTk42jC/N+13i5+fHpUuXWLduHR07duTXX39l/fr1JvssSsrPz48ff/yR/v37o9FoeOutt/L1xpbksX6fPn1o3rw5w4cPZ/78+URHRzN16lReeOEFw2cXGhrKiBEj2L59O3Xq1KFZs2b07duXMWPGsGTJErKzsxk/fjxPPfUU3t76sdZDhw5l5syZBAcHM3nyZI4fP87HH3/Mhx9+aCg7KyuLkydPGr6+evUq4eHhODo6FvqHTYkUa3pVAb755ht16dKlItMkJCQUOBuwuEJCQpSLi4vauXOnYQbf7bP4xo4dq+rVq6f++OMPdejQIRUQEGBYOUCp/5aS6tOnjwoPD1dbtmxRtWrVqnpLSeVkK3X0O6UWd/1vpugMV6W+HanUlbCKrp1BZnauemFNmKo/eaNq8PpGtXJvREVXqVKQ2fqiOjPrpaSU0i8X9UFT45n6HzQrs2WklNLP1n/xxReNjgFq4cKFqnfv3srGxkY1aNBAffPNN0ZpWrduXeASUnmKmq2vlFIZGRlq0KBBytXV1WjJp6SkJDVhwgTl7e2trKyslI+Pjxo2bJi6dOmS2rFjh7K0tFR//fWXUTnOzs5q0aJFReZblLylpKytrVXTpk3VL7/8ogC1ZcsWQ5pXX31V1ahRQzk6Oqonn3xSffjhh8rFxcVwPm8pqVuNHDky32z12z9vblsVIO9zO3LkiOHYjh07FKBu3rxpSNOjRw9lZ2enfHx81GeffVbgv2NJREZGqgcffFDZ2dmpmjVrqkmTJqns7Ox8dYiIiDAci4uLU0OGDFGOjo7K2dlZPfPMM/litb///lt17dpV2djYqDp16qi5c+canc+739tfDzzwQJH1Le73m0ap0q20a2FhwVdffcXQoUMB/eO/vn378umnnxY4SLs0CutJW758OaNGjQL+W4T/66+/NlqE/9ZH9hcvXiQkJISdO3fi4ODAyJEjmTt3brEX4U9KSsLFxYXExMR8A6krXFYqHP4K9i/U7+IE+u1F2w2HLuPKdBenkkrNzGHs6jD+OnsDKwsNHwxuwyOtTTUrtmqrqDZo1m1fVBtl2Q4zMjKIiIjA19cXW1vb0meky9WPQU2JAUdP/RjTcuwxFfrH5l27duXcuXM0atSooqsjClDc77dSP9a/PabNzs5m//79huWeTKE4cbOtrS0LFy5k4cKFhaapX78+mzZtMlm9zELKdQj9H4R+ARkJ+mP2NaDT89DxWXCoUeTl5e1mahbPrDhI+OUE7Kws+N/w9tzfpFZFV0sIIUxDawG+3Sq6FtXK+vXrcXR0xM/Pj3PnzvHiiy9y3333SWBaBch6PZWNGWwvWlLXEtIZsSyUc7EpuNpbsXxUR9rWK59B1UIIISqfNWvW8Pzzzxd4rn79+pw4cYLk5GQmT57MpUuXqFmzJoGBgflm4ovKSYLTyuJyqH6S06lfMQy6r9NeP8mp6cNm+/joXGwKI5Ye4FpiBl7OtnwV3Ak/T6c7XyiEEKLaeuSRR4wWvL9V3gzyESNGGK1MIKoOCU7NmU4HZ7bol4O6tO+/40366peDqn9vhc+8L8rRKwmMWn6Q+NQsGtZ0YFVwJ+q6mV/PrhBCCPPi5ORktDC+qF7uKjhdtWoV+/fvB/SDXDUaDZ999hkbNmzIl1aj0fDxxx/fTXHVhxluL1pSu8/e4PmvDpGalcs9dV1YPqojNRxliSIhhBBCFO2ugtPff/+d33//3ehYQYEpSHBaLOk3/91e9H9mtb1oSW06FsVL68LJytVxX+Ma/G94BxxlO1IhhBBCFEOpI4a87UqFCZjx9qIltebARaZuOI5S0K+VFx8+2QYbS/McDyuEEEII81Pq4PTW7c5EKZnx9qIlpZTisz/O8cFW/TCEoZ3r8fajLWU7UiGEEEKUiDxrLW+VYHvRktLpFLM2nmTF3kgAJvRszMTeTWQ7UiGEEEKUmEmD00OHDnHgwAFu3ryZb7/YvH1kq63cHDi5QR+URh/VH9Noofmj+p7SOu0qtHqllZ2r49Xv/mZD+DUApj3cnNFdzWdXKiGEEMUXHR3N8OHD2bt3L1ZWViQkJFR0lYoUGRmJr68vR44coU2bNhVdHWEiJglO09PTeeyxx/j9999RSqHRaAy7O+V9XW2D00q0vWhJpWflErImjJ2nr2Op1fDeE/cwsG3diq6WEEKUu1xdLodjD3M97Tq17GvRzqMdFma6/nRRPvzwQ6KioggPD8fFxaWiqyNMLCoqikmTJnHo0CHOnTvH//3f//HRRx9VdLXyMUlwOmvWLH7//XfefPNNevXqRY8ePVi5ciUeHh7MmTOH9PR0Vq1aZYqizEtReymnxELo55Vme9GSSkjLInjlIcIu3sTWSsviYe3p0dSjoqslhBDlbtvFbcwNnUtMWozhmKe9J693ep3A+oEVWLOSO3/+PO3bt8fPz6/QNBqNhoiICBo0aGCSMrOysrC2rjxzLCqzzMxMatWqxdSpU/nwww8rujqF0poik++//54nnniCWbNm0bJlSwDq1KlDUFAQ27ZtIysrixUrVpiiKPNx8mf4qCWsfBh+CNb//6OWcOBz+OVF+LAl7HpPH5i6+cJDC+DlE9B9cqUPTKMTM3jyf/sJu3gTZ1tLVgd3lsBUCFEtbbu4jYk7JxoFpgCxabFM3DmRbRe3lUm5n3/+Od7e3vmG0D366KOMHj2aGTNm0KZNG5YtW0a9evVwdHRk3Lhx5ObmMn/+fLy8vPDw8OCdd94xXNugQQN++OEHVq1ahUajYdSoUaWq2xdffIGPjw/29vYMHDiQBQsW4OrqajifV7cvv/wSX19fbG1tAdiyZQtdu3bF1dWVGjVq8PDDD3P+/HmjvENDQ2nbti22trZ06NCBI0eOlKhuP//8M35+ftja2ho60jQajWH4QlxcHEOGDKFOnTrY29vTqlUrvv76a6M8unfvzoQJE3jppZdwc3PD09OTL774gtTUVJ555hmcnJxo3LgxmzdvNlyzc+dONBoNv/32G23btsXOzo6ePXsSGxvL5s2badasGc7OzgwdOpS0tDTDdcX5TEqiQYMGfPzxx4wYMcKse8ZNEpxevnyZBx54AAALC33PYVZWFgCWlpYMGTKEdevWmaIo83DyZ/h2BCRdMz6edA02vwphK/T73tdpD4NXwYQw6BgMVnYVUl1TiriRyqDFezkdk4yHkw3fjg2gQwP3iq6WEEKYhFKKtOy0Yr2SM5OZEzoHlbel9K35/Pvf3NC5JGcmFyu/vOFwxfHEE08QFxfHjh07DMfi4+PZsmULw4YNA/S9oJs3b2bLli18/fXXLF26lIceeogrV67w559/Mm/ePKZOncqBAwcAOHjwIH379mXw4MFERUWVam3yPXv2MHbsWF588UXCw8Pp3bu3UQCc59y5c/zwww/8+OOPhIeHA5CamsrEiRM5dOgQ27dvR6vVMnDgQEMAnpKSwsMPP0zz5s0JCwtjxowZvPLKK8WuW0REBI8//jgDBgzg77//5vnnn+fNN980SpORkUH79u359ddfOX78OM899xzDhw8nNDTUKN3KlSupWbMmoaGhTJgwgZCQEJ544gnuvfdeDh8+TJ8+fRg+fLhRoAn6wPyzzz5j7969XL58mcGDB/PRRx+xdu1afv31V37//Xc+/fRTQ/o7fSYALVq0wNHRsdDXgw8+WOzPyFyY5LG+k5MTOTk5hq+1Wi3Xrv0XuLm4uBAdHW2KoiqeLhe2TIYCfhgZWNrC0O/At1ulm3lflONXExm5LJS41Cwa1LDnq+DO+LjLdqRCiKojPSedzmsL3tO9NGLSYrh33b3FSntg6AHsrYr3M9XNzY0HH3yQtWvX0qtXL0D/FLNmzZr06NGDv/76C51Ox7Jly3BycqJ58+b06NGD06dPs2nTJrRaLf7+/sybN48dO3bQuXNnatWqhY2NDXZ2dnh5eZXqfj/99FMefPBBQ9DYpEkT9u7dy8aNG43SZWVlsWrVKmrVqmU4NmjQIKM0y5Yto1atWpw8eZKWLVuydu1adDodS5cuxdbWlhYtWnDlyhVCQkKKVbf//e9/+Pv789577wHg7+/P8ePHjYLnOnXqGAW8EyZM4LfffuPbb7+lU6dOhuOtW7dm6tSpAEyZMoW5c+dSs2ZNxowZA8C0adNYvHgxR48epUuXLobrZs+ezX333QdAcHAwU6ZM4fz58zRs2BCAxx9/nB07djB58uRifSYAmzZtIjs7u9D7trOrfB1jJuk5bdSoEWfO6Ne3tLCwoEWLFnz//feA/q/QH3/8ER8fH1MUVfEu7s3fY3q7nAx9UFqFAtO952/w1Of7iUvNooW3M9+H3CuBqRBCVKBhw4bxww8/kJmZCcCaNWt46qmn0Gr1v9obNGhgtD+9p6cnzZs3N5zPOxYbG1tkOQ8++KBRTxwY99a1aNHCkPb06dNGQRyQ7z3o10q/NTAFOHv2LEOGDKFhw4Y4OzsbxrReuqSfTPzPP/9wzz33GIYBAAQEBBRZ91udPn2ajh07Flm33Nxc3n77bVq1aoW7uzuOjo789ttvhjrkueeeewxfW1hYUKNGDVq1amU45unpCZDvs731Ok9PT+zt7Q2Bad6xW6+502cC+s+ycePGhb7q1KlTrM/HnJik5zQwMJBly5bx0UcfYWFhwfPPP8/48eNp1KiRYeD0u+++a4qiKl5KzJ3TlCRdJbDleDT/9/URsnJ1dPZ158uRHXCytaroagkhhMnZWdpxYOiBYqUNiwlj3PZxd0y3qNci2nu2L1bZJdG/f3+UUvz666907NiRv/76y2iSi5WV8c9pjUZT4LHbx63e7ssvvyQ9Pd3w3s/Pj02bNhmCntvzLA4HB4cC76d+/fp88cUXhvG0LVu2NAwTLA/vvfceH3/8MR999BGtWrXCwcGBl156KV8d7vTZ5q3zfftne3uaO/17FOczadGiBRcvXiz0nrp162Y0/rUyMElw+vrrrzN8+HDDeJlx48aRkZHB6tWrsbCwYMyYMbz22mumKKriOXqaNp2Z++bgJab8eAydgj7NPflkSFtsrSrf8ihCCFEcGo2m2I/W7/W+F097T2LTYgscd6pBg6e9J/d631smy0rZ2try2GOPsWbNGs6dO4e/vz/t2pl+zeyCet7q169f4Gx9f39/Dh48aHTs9vcFiYuL4/Tp03zxxRd069YNgN27dxuladasGV999RUZGRmG3tP9+/cX9zbw9/dn06ZNRdZtz549PProozz99NOAPrg8c+YMzZs3L3Y5plKczwSq5mN9kwSnjo6O+Pv7Gx2bOHEiEydONEX25qX+veDsDUlRFDzuVKM/X794Y4zMlVKKJX9eYN6WUwA82cGHdwa2xNLCJCNBhBCi0rPQWvB6p9eZuHMiGjRGAaoGfc/Z5E6Ty3S902HDhvHwww9z4sQJQ0BVkSZMmMD999/PggUL6N+/P3/88QebN2++446Bbm5u1KhRg88//5zatWtz6dIlXn/9daM0Q4cO5c0332TMmDFMmTKFyMhI3n///WLX7fnnn2fBggVMnjyZ4OBgwsPDDSsJ5dXPz8+P77//nr179+Lm5saCBQuIiYmpkOC0OJ8JlHw7+bwJaCkpKVy/fp3w8HCsra0r5B4LI5FGSWktoO+8f9/c/s327/u+c/9b77QS0ukU7276xxCYjn2gEXMHtZLAVAghbhNYP5AF3RfgYW+8nJ6nvScLui8o83VOe/bsibu7O6dPn2bo0KFlWlZx3HfffSxZsoQFCxbQunVrtmzZwssvv2w0TrQgWq2WdevWERYWRsuWLXn55ZcNE5fyODo68ssvv3Ds2DHatm3Lm2++ybx58wrJMT9fX1++//57fvzxR+655x4WL15smK1vY2MDwNSpU2nXrh1BQUF0794dLy8vBgwYULIPwUSK85mURtu2bWnbti1hYWGsXbuWtm3b0q9fPxPU2HQ0qiRrVxRi+vTp/PDDDxw/frzA861ateLJJ580zGyrbJKSknBxcSExMRFnZ2f9wZM/62ft3zo5yrmOPjBt/kjFVNQEsnN1vP7DMX44fAWAN/s1Y8z9De9wlShrBbbBKlyuELcqy3aYkZFBRESE0XqbpVFVdogqC2PGjOHUqVP89ddfFV2VfN555x2WLFnC5cuXK7oq1UJxv99M8lh//fr19O7du9Dzffr04fvvv6+0wWmBmj8CTR8qfIeoSigjO5cX1hxm+6lYLLQa5g26h8fby3akQghxJxZaCzp6dbxzwmrg/fffp3fv3jg4OLB582ZWrlzJokWLKrpaACxatIiOHTtSo0YN9uzZw3vvvcf48eMrulriNiYJTiMiImjatGmh5/39/fnyyy9NUZR50Vro1zKtAhLTs3l25UEORt7ExlLLwqHtCGxeNSZ1CSGEKD+hoaHMnz+f5ORkGjZsyCeffMKzzz5b5uWOHTuW1atXF3ju6aefZsmSJZw9e5bZs2cTHx9PvXr1mDRpElOmTCnzuomSMUlwChi2/irIzZs3yc3NNVVRwsRikzIYsSyUU9HJONlYsnRURzr5yq5PQgghSu7bb7+tkHJnzZpV6I5RecNBPvzwQ7PeU17omSQ4bdGiBT/99JNhR4NbKaX4+eefi+xZFRXnYlwqw5eGcik+jZqONqwa3Ynm3jK2UAghROXi4eGBh4fHnRMKs2eS6dfBwcHs37+fUaNGcf36dcPx69evM3r0aPbv309wcLApihImdPJaEoMW7+NSfBr13O35ISRAAlMhhBBCVCiT9JyOGTOGP//8k1WrVvHVV19Ru3ZtAKKiolBK8eSTTxZ771tRPkIj4gleeZDkjByaejmxanQnPJxLP1NVCCEqMxMsXCOEuIM77UaWx2RjTlevXs0jjzxi2KkCoGPHjgwbNozHH3/cVMUIE9h2MoYX1h4mM0dHpwbufDGyAy52sh2pEKL6sbKyQqPRcP36dWrVqnXHxeKFECWnlCIrK4vr16+j1WqxtrYuMr3JglOAwYMHM3jwYJPlt2vXLt577z3CwsKIiopi/fr1Rovhjho1ipUrVxpdExQUxJYtWwzv4+PjmTBhAr/88gtarZZBgwbx8ccf4+joaLJ6VibfHbrM6z8eI1enCGzmwWdD28l2pEKIasvCwoK6dety5coVIiMjK7o6QlRp9vb21KtXD6226FGlJg1OTS01NZXWrVszevRoHnvssQLT9O3bl+XLlxve5+3ykGfYsGFERUWxdetWsrOzeeaZZ3juuedYu3ZtmdbdHH2+6zzvbtLv+jSoXV3mya5PQgiBo6Mjfn5+Re5PLoS4OxYWFlhaWhbr6YRJg9NDhw5x4MABbt68mW9cgUaj4a233ipRfg8++CAPPvhgkWlsbGzw8vIq8Nw///zDli1bOHjwIB06dADg008/pV+/frz//vt4e3uXqD6VlVKKuVtO8b8/LwAwppsvUx5shlYrj6+EEAL0vzgtLOQpkhDmwCTBaXp6Oo899hi///47Sik0Go1hcHne16UJTotj586deHh44ObmRs+ePZk9ezY1atQAYN++fbi6uhoCU4DAwEC0Wi0HDhxg4MCBJq+PucnJ1fHm+uN8c0i/Ndvkvk0Z+0BDGVclhBBCCLNkkuB01qxZ/P7777z55pv06tWLHj16sHLlSjw8PJgzZw7p6emsWrXKFEUZ6du3L4899hi+vr6cP3+eN954gwcffJB9+/ZhYWFBdHR0vjXPLC0tcXd3Jzo6utB8MzMzyczMNLxPSkoyed3LQ0Z2Lv/39RF+PxmDVgNzHmvFkx3rVXS1hBmrKm1fCCFE5WWSAYfff/89TzzxBLNmzaJly5YA1KlTh6CgILZt20ZWVhYrVqwwRVFGnnrqKR555BFatWrFgAED2LhxIwcPHmTnzp13le+cOXNwcXExvHx8fExT4XKUnJHNqOWh/H4yBmtLLYuGtZfAVNxRVWj7QgghKjeTBKeXL1/mgQceADCM2cnKygL0PZVDhgxh3bp1piiqSA0bNqRmzZqGpay8vLyIjY01SpOTk0N8fHyh41QBpkyZQmJiouF1+fLlMq23qV1PzuSpz/ez/0I8jjaWrHymE31bFn6/QuSp7G1fCCFE5WeSx/pOTk7k5OQYvtZqtVy7ds1w3sXFpcjH6KZy5coV4uLiDJsABAQEkJCQQFhYGO3btwfgjz/+QKfT0blz50LzsbGxyTfrv7K4HJ/G8KUHiIxLo4aDNStHd6JlHZeKrpaoJCpz2xdCCFE1mKTntFGjRpw5cwbQ95y2aNGC77//HtDPFP/xxx9L9XgwJSWF8PBwwsPDAYiIiCA8PJxLly6RkpLCq6++yv79+4mMjGT79u08+uijNG7cmKCgIACaNWtG3759GTNmDKGhoezZs4fx48fz1FNPVcmZ+qeikxi0eC+RcWnUdbPj+5B7JTAVQgghRKVikuA0MDCQH374gdzcXACef/55tmzZQqNGjfDz82Pbtm0EBweXON9Dhw7Rtm1b2rZtC8DEiRNp27Yt06ZNw8LCgqNHj/LII4/QpEkTgoODad++PX/99ZdRz8+aNWto2rQpvXr1ol+/fnTt2pXPP//cFLdtVg5FxjN4yT5ikzPx93Tih5B78a3pUNHVEkIIIYQoEY0ywYbCKSkpXL16lUaNGmFpqR8psGDBAlavXo2FhQWPP/44r732WqVdvigpKQkXFxcSExNxdnau6Orks+NULCFrwsjI1tG+vhtLR3bA1b7orcFE5VJRbdDc276oHqQdClG9mCQ4rerM+QfjhiNXeeW7v8nRKbr712LxsPbYWctC0lWNBKeiOpN2KET1Ytbbl4qiLdsdwayNJwEY0Mab955ojZVsRyqEEEKISsxkkUxGRgbz588nICAAT09PPD09CQgIYP78+aSnp5uqGIF+ktn7v502BKbP3NeABYPbSGAqhBBCiErPJD2n169fp2fPnpw4cQJnZ2caNmwI6Pe2P3DgAKtWrWLHjh3UqlXLFMVVa7k6xVs/HWftgUsAvNKnCS/0aFxpx/MKIYQQQtzKJF1tr776KidPnmTBggXExsZy+PBhDh8+TGxsLB988AH//PMPr776qimKqtYyc3KZ8PVh1h64hEYD7wxsyfiefhKYCiGEEKLKMEnP6S+//EJwcDAvvfSS0XFra2tefvllTpw4wfr1601RVLWVkpnDc6sOsfd8HNYWWj56qg39WtWu6GoJIYQQQpiUSXpOs7KyaNeuXaHnO3ToYNjOVJRcXEomQ7/Yz97zcdhbW7BsVEcJTIUQQghRJZmk57Rjx44cPny40PNhYWF06tTJFEVVO1cT0hm+9AAXrqfi7mDN8lEdae3jWtHVEkIIIYQoEyYJTj/44AN69epFq1atCAkJMSzEn5OTw8KFC/nxxx/Zvn27KYqqVs7GJDN8aSjRSRl4u9iyKrgzjT0cK7paQgghhBBlxiSL8Pfs2ZPLly9z4cIFo9n6Fy5cICkpiUaNGlG3bl3jgjWaShOwVsQC0Icv3WT0ioMkpGXT2MORr4I7UdvFrlzKFuZHFuEX1Zm0QyGqF5P0nF64cAGNRkO9evUAiI+PB8DV1RVXV1eys7OJiIgwRVHVwp9nrjP2qzDSs3Np4+PK8lEdcXOQ7UiFEEIIUfWZJDiNjIw0RTYC+Pnva0z6NpzsXEU3v5osebo9DjaykZcQQgghqgeJeszIqn2RTP/5BErBw/fUZsHgNlhbyq5PQgghhKg+JDg1A0opPtp2lo+3nwVgeJf6zHikBRZaWVxfCCGEENVLqYLTnj17lviayjQBqjzpdIoZv5xg1b6LALwU6MeLvWTXJyGEEEJUT6UKTvMmQIm7k5WjY+K34Ww8GoVGAzMfacGIgAYVXS0hhBBCiApTquC0NBOgMjMzS1NUlZWamcPY1WH8dfYGVhYaPhjchkdae1d0tYQQQgghKlSZz7YJCwtj3LhxeHtL4JXnZmoWw748wF9nb2BnZcHSkR0lMBVCCCGEoIwmRMXHx7N69WqWLVvGsWPHUErRpEmTsiiq0rmWkM6IZaGci03B1d6K5aM60raeW0VXSwghhBDCLJi05/S3337jySefpE6dOrz88stkZmYyffp0jh07xqlTp0xZVKV0LjaFxxfv5VxsCl7Otnz3fIAEpkIIIYQQt7jrntPIyEiWLVvGypUruXLlCjVr1uTxxx9n7dq1vPPOOzz22GOmqGel9/flBEYtD+VmWjYNazqwKrgTdd3sK7paQgghhBBmpdQ9p2vWrKFXr140btyYefPm0aFDB9avX8/Vq1eZMWMGSilT1rNS2332BkO/2M/NtGzuqevCd2MDJDAVQgghhChAqXtOhw8fTsOGDfnoo48YMmQINWrUMGW9qoxNx6J4aV04Wbk67mtcg/8N74CjbEcqhBBCCFGgUvec2tjYEBkZyU8//cSWLVtIT083Zb2qhDUHLvLC2sNk5ero18qLZaM6SmAqhBBCCFGEUgenUVFRfPTRR8TFxTF8+HC8vLwIDg5m165d1f6RvlKKT7ef5c31x1EKhnaux6dD2mFjaVHRVRNCCCGEMGulDk5dXV0ZP348hw8f5tChQzz99NOsX7+eHj160LVrVzQaDYmJiaasa6Wg0ylm/nKSD7aeAWBCz8a8M6AlFlrZUUsIIYQQ4k5MspRUu3btWLhwIVFRUXz11Ve0aNECgGeffZY2bdowe/ZsTpw4YYqizEquTrHvfBw/hV9l3/k4MrJzmfhtOCv2RgIw7eHmTOrjL1u9CiGEEEIUk0nXObWxsWHo0KFs376d8+fP8+abb3Lz5k2mTZtG69atS5zfrl276N+/P97e3mg0GjZs2GB0XinFtGnTqF27NnZ2dgQGBnL27FmjNPHx8QwbNgxnZ2dcXV0JDg4mJSXlbm4TgC3Ho+g67w+GfLGfF9eFM+SL/bSe+Tsbwq9hqdXw4ZOtGd3V967LEUIIIYSoTsps+9IGDRowa9YsIiMj2bRpU6nWO01NTaV169YsXLiwwPPz58/nk08+YcmSJRw4cAAHBweCgoLIyMgwpBk2bBgnTpxg69atbNy4kV27dvHcc8+V+r5AH5iGrD5MVGKG0fHMHB0Azz/QkIFt695VGUIIIYQQ1ZFGVZLZSxqNhvXr1zNgwABA32vq7e3NpEmTeOWVVwBITEzE09OTFStW8NRTT/HPP//QvHlzDh48SIcOHQDYsmUL/fr148qVK3h7F28/+6SkJFxcXEhMTMTB0Ymu8/7IF5jeqraLLbsn95RxpsJkbm2Dzs7OVb5cIW4l7VCI6qXMek7LWkREBNHR0QQGBhqOubi40LlzZ/bt2wfAvn37cHV1NQSmAIGBgWi1Wg4cOFCqckMj4osMTAGiEjMIjYgvVf5CCCGEENVZpV10Mzo6GgBPT0+j456enoZz0dHReHh4GJ23tLTE3d3dkKYgmZmZZGZmGt4nJSUZvo5NLjowLWk6IcxJUW1fCCGEKA+Vtue0LM2ZMwcXFxfDy8fHx3DOw8m2WHkUN50Q5qSoti+EEEKUh0obnHp5eQEQExNjdDwmJsZwzsvLi9jYWKPzOTk5xMfHG9IUZMqUKSQmJhpely9fNpzr5OtObRdbChtNqkE/5rSTr3vJb0qIClZU2xdCCCHKQ6UNTn19ffHy8mL79u2GY0lJSRw4cICAgAAAAgICSEhIICwszJDmjz/+QKfT0blz50LztrGxwdnZ2eiVx0KrYXr/5gD5AtS899P7N5fJUKJSKqrtCyGEEOXBrIPTlJQUwsPDCQ8PB/SToMLDw7l06RIajYaXXnqJ2bNn8/PPP3Ps2DFGjBiBt7e3YUZ/s2bN6Nu3L2PGjCE0NJQ9e/Ywfvx4nnrqqWLP1C9I35a1Wfx0O7xcjB/de7nYsvjpdvRtWbvUeQshhBBCVGdmPSHq0KFD9OjRw/B+4sSJAIwcOZIVK1bw2muvkZqaynPPPUdCQgJdu3Zly5Yt2Nr+FzSuWbOG8ePH06tXL7RaLYMGDeKTTz6567r1bVmb3s29CI2IJzY5Aw8n/aN86TEVQgghhCi9SrPOaUVKTEzE1dWVy5cvy2NOUSGSkpLw8fEhISEBFxeXcitX2r4wBxXV/oUQFcOse07NRXJyMoDMXBYVLjk5uVx/OUvbF+akvNu/EKJiSM9pMeh0Oq5du4aTkxMajfFj+7y/6KVnSdytotqSUork5GS8vb3RastvqHhRbR+k/QvTuFM7qqj2L4SoGNJzWgxarZa6desWmUZmNgtTKawtVUSPUXHaPkj7F6ZRVDuSHlMhqg/5E1QIIYQQQpgNCU6FEEIIIYTZkOD0LtnY2DB9+nRsbGwquiqikquMbaky1lmYH2lHQohbyYQoIYQQQghhNqTnVAghhBBCmA0JToUQQgghhNmQ4FQIIYQQQpgNCU6FEEIIIYTZkOAU2LVrF/3798fb2xuNRsOGDRuMziulmDZtGrVr18bOzo7AwEDOnj1rlCY+Pp5hw4bh7OyMq6srwcHBpKSkGKU5evQo3bp1w9bWFh8fH+bPn1/WtybKkDm1m++++46mTZtia2tLq1at2LRpU6W8D1F5mFO7uZv2L4QwPxKcAqmpqbRu3ZqFCxcWeH7+/Pl88sknLFmyhAMHDuDg4EBQUBAZGRmGNMOGDePEiRNs3bqVjRs3smvXLp577jnD+aSkJPr06UP9+vUJCwvjvffeY8aMGXz++edlfn+ibJhLu9m7dy9DhgwhODiYI0eOMGDAAAYMGMDx48cr1X2IysVc2s3dtn8hhBlSwgig1q9fb3iv0+mUl5eXeu+99wzHEhISlI2Njfr666+VUkqdPHlSAergwYOGNJs3b1YajUZdvXpVKaXUokWLlJubm8rMzDSkmTx5svL39y/jOxLloSLbzeDBg9VDDz1kVJ/OnTur559/vlLdh6i8qkr7F0KYB+k5vYOIiAiio6MJDAw0HHNxcaFz587s27cPgH379uHq6kqHDh0MaQIDA9FqtRw4cMCQ5v7778fa2tqQJigoiNOnT3Pz5s1yuhtRXsqz3ezbt8+onLw0eeVUlvsQVUdVaf9CiIohwekdREdHA+Dp6Wl03NPT03AuOjoaDw8Po/OWlpa4u7sbpSkoj1vLEFVHebabwtKYol1J+xelUVXavxCiYkhwKoQQQgghzIYEp3fg5eUFQExMjNHxmJgYwzkvLy9iY2ONzufk5BAfH2+UpqA8bi1DVB3l2W4KS2OKdiXtX5RGVWn/QoiKIcHpHfj6+uLl5cX27dsNx5KSkjhw4AABAQEABAQEkJCQQFhYmCHNH3/8gU6no3PnzoY0u3btIjs725Bm69at+Pv74+bmVk53I8pLebabgIAAo3Ly0uSVU1nuQ1QdVaX9CyEqSEXPyDIHycnJ6siRI+rIkSMKUAsWLFBHjhxRFy9eVEopNXfuXOXq6qp++ukndfToUfXoo48qX19flZ6ebsijb9++qm3bturAgQNq9+7dys/PTw0ZMsRwPiEhQXl6eqrhw4er48ePq3Xr1il7e3v1v//9r9zvV5iGubSbPXv2KEtLS/X++++rf/75R02fPl1ZWVmpY8eOVar7EJWLubSbu23/QgjzI8GpUmrHjh0KyPcaOXKkUkq/LMpbb72lPD09lY2NjerVq5c6ffq0UR5xcXFqyJAhytHRUTk7O6tnnnlGJScnG6X5+++/VdeuXZWNjY2qU6eOmjt3bnndoigD5tRuvv32W9WkSRNlbW2tWrRooX799ddKeR+i8jCndnM37V8IYX40SilVfv20QgghhBBCFE7GnAohhBBCCLMhwakQQgghhDAbEpwKIYQQQgizIcGpEEIIIYQwGxKcCiGEEEIIsyHBqRBCCCGEMBsSnAohhBBCCLMhwakQQgghhDAbEpwKIYQQQgizIcGpEEIIIYQwGxKcCiGEEEIIsyHBqRBCCCGEMBv/D7/Ehj99ccX9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 700x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yaxis_type = 'abs'\n",
    "# yaxis_type = 'delta_random'\n",
    "assert(yaxis_type in ['delta_random', 'abs'])\n",
    "datasets = ['flan_v2', 'dolly', 'stanford_alpaca', 'oasst1', 'ultrachat200kv2', 'wizardlmv2', 'sharegptv2']\n",
    "task_names = ['nonchat', 'MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*',]\n",
    "\n",
    "datasets = ['dolly', 'sharegpt50k']\n",
    "task_names = []\n",
    "task_names += ['nonchat']\n",
    "# task_names += [f'MTBench({mtbench_judge})/Turn-1',  f'MTBench({mtbench_judge})/Turn-2', f'MTBench({mtbench_judge})/Rating']\n",
    "task_names += [f'MTBench({mtbench_judge})/Rating']\n",
    "task_names += [f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/Len']\n",
    "\n",
    "\n",
    "\n",
    "w = 2\n",
    "ncols = len(datasets)\n",
    "nrows = len(task_names)\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(w*ncols+3,w*nrows), sharey='row', sharex=True)\n",
    "\n",
    "xs_possible = []\n",
    "\n",
    "for axi, task_name in enumerate(task_names):\n",
    "    d = D[task_name]\n",
    "    for axj, dataset in enumerate(datasets):\n",
    "        ax = axs.reshape(nrows, ncols)[axi, axj]\n",
    "        sort_by_types = sorted(set(x.sort_by_type for x in d.keys()))\n",
    "\n",
    "        for i, sort_by_type in enumerate(sort_by_types):\n",
    "            xs = sorted([x.subset_size for x in d.keys()\n",
    "                         if x.dataset == dataset and x.sort_by_type==sort_by_type])\n",
    "            xs_possible += list(set(xs) - set(xs_possible))\n",
    "            ys = [d[DKey(sort_by_type, dataset, x)] for x in xs]\n",
    "            if yaxis_type == 'delta_random':\n",
    "                if not all(DKey('random', dataset, x) in d for x in xs): continue\n",
    "                ys = [y-d[DKey('random', dataset, x)] for x, y in zip(xs, ys)] \n",
    "            if 'random' in sort_by_type:\n",
    "                marker_style = 'o-'\n",
    "            else:\n",
    "                marker_style = 'o-'\n",
    "            ax.plot(xs, ys, marker_style, label=sort_by_type)\n",
    "        \n",
    "#         ax.set_yscale('log')\n",
    "            \n",
    "for axi, task_name in enumerate(task_names):\n",
    "    task_name_shortened = task_name.replace(f'({mtbench_judge})', '').replace(f'({alpacafarm_judge})', '')\n",
    "    axs.reshape(nrows, ncols)[axi, 0].set_ylabel('△ '+task_name_shortened if yaxis_type.startswith('delta') else task_name_shortened, fontsize=13)\n",
    "    axs.reshape(nrows, ncols)[axi, -1].legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "for axj, dataset in enumerate(datasets):\n",
    "    axs.reshape(nrows, ncols)[0, axj].set_title(dataset, fontsize=15)\n",
    "    axs.reshape(nrows, ncols)[0, axj].set_xticks(xs_possible, xs_possible)\n",
    "\n",
    "space = 0.05\n",
    "fig.subplots_adjust(wspace=space, hspace=space)  # Adjust the value as needed\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5fdb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e2f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39874bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
