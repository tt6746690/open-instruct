{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3da1794b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'x86_64', 'cluster': 'ccc', 'queue': 'alt_7d'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info, move_lsf_job_summary_to_save_dir\n",
    "from note_pruning_analysis import open_instruct_dir, assets_dir\n",
    "os.makedirs(assets_dir, exist_ok=True)\n",
    "\n",
    "import getpass\n",
    "\n",
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "info = get_host_info()\n",
    "info.update({'queue': queue})\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "os.environ['TORCHELASTIC_ERROR_FILE'] = os.path.join(os.getcwd(), 'torchelastic_error_file') \n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = os.path.normpath(os.path.join(os.getcwd(), '../../../../mitibm2023/cache')) \\\n",
    "    if arch == 'ppc64le' else '/dccstor/data-pruning/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True, mode=0o777)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##\n",
    "##\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c421d1",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c09b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/oi2/llama-7b_sharegptv2_ep=2 using 6 GPUs, 1 batch size per GPU, 1 gradient accumulation steps, 30 effective batch size.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp7pssto1b', 'job_id': 1354503}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2p8zx1bt', 'job_id': 1354504}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2_mpillk', 'job_id': 1354505}]\n"
     ]
    }
   ],
   "source": [
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm\n",
    "debug = False\n",
    "if debug:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'INFO'\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "else:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'WARNING'\n",
    "    os.environ['NCCL_DEBUG'] = ''\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  650 if arch == 'ppc64le' else 64\n",
    "\n",
    "preprocessing_num_workers = 32\n",
    "report_to = 'wandb'\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16'\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float32'\n",
    "gradient_checkpointing = True\n",
    "use_fast_tokenizer = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "resume_from_checkpoint = True # resume from latest checkpoint if exists, otherwise train from scratch\n",
    "num_train_epochs = 2\n",
    "checkpointing_steps = 300 # (50_000 / 32) * 2 / 6 ~= 500 (data size of 50k, bsz=32, ep=2, total save 6 times at most)\n",
    "max_train_steps = None\n",
    "subsample_inds_file_list = [None]\n",
    "dataloader_sampler = 'RandomSampler'\n",
    "overwrite_cache = True\n",
    "\n",
    "\n",
    "# #####\n",
    "# job_name = 'dpo1'\n",
    "\n",
    "# # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-410m-deduped'; max_seq_length = 2048; abbr_model_name = 'pythia-410m'\n",
    "# # model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "\n",
    "# train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "# train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "train_file = 'data/processed/hh_rlhf/hh_rlhf_data.jsonl'; dataset = 'hh_rlhf'\n",
    "\n",
    "# M = 60_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 50_000; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "    (10_000, 10), # 1k\n",
    "#     (30_000, 3),  # 10k\n",
    "#     (60_000, 3),  # 20k\n",
    "]]\n",
    "\n",
    "gen_output_md = 'llama7br512p4096'\n",
    "# gen_output_md = 'llama7b+sharegptv2ep2+r512p4096'\n",
    "# gen_output_model_name = 'all-mpnet-base-v2'\n",
    "\n",
    "scoring_fn_list = []\n",
    "scoring_fn_list += ['random_s=0']\n",
    "# scoring_fn_list += ['random_s=1']\n",
    "scoring_fn_list += [ \n",
    "#     f'dppmap_k=vmf_gamma=1_kmd={gen_output_md}_kemb=grad+rp+loraB',\n",
    "#     f'dppmap_k=rbf_gamma=1e-3_kmd={gen_output_md}_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1_kmd=mpnet_kemb=text+embedding',\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'dpo2_{dataset}:{abbr_model_name}'\n",
    "    \n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12\n",
    "# nodes = 2; num_gpus = 1; gpu_type = 'v100'; job_duration = 6; cpu_mem = 100; num_cpus = 32; max_train_steps = 5; checkpointing_steps = 2; report_to = 'tensorboard'\n",
    "    \n",
    "#####\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs = 1 # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "use_deepspeed = True\n",
    "deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate.conf'\n",
    "\n",
    "per_device_train_batch_size = 1; total_batch_size = 32\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"{effective_batch_size} effective batch size.\")\n",
    "\n",
    "# reference: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n",
    "launcher = f\"\"\"accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines {nodes} \\\n",
    "    --num_processes {num_gpus*nodes} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''} \\\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''} \\\n",
    "    {'--main_process_ip $master_addr' if use_deepspeed else ''} \\\n",
    "    {'--main_process_port $master_port' if use_deepspeed else ''} \\\n",
    "    {'--machine_rank $SLURM_PROCID' if use_deepspeed else ''} \\\n",
    "    {'--rdzv_backend c10d' if use_deepspeed and nodes>1 else ''} \\\n",
    "    {'--deepspeed_multinode_launcher standard' if use_deepspeed and nodes>1 else ''} \\\n",
    "\"\"\"\n",
    "\n",
    "cmds = []\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_inds_file_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (subsample_inds_file,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{dataset}\"\n",
    "    if any(job_name == y for y in ['dpo1']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "\n",
    "    if subsample_inds_file:\n",
    "        assert(num_train_epochs==1)\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                s = f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "    if subsample_inds_file is not None:\n",
    "        assert(dataloader_sampler=='SequentialSampler')\n",
    "        assert(num_train_epochs==1)\n",
    "    else:\n",
    "        assert(dataloader_sampler=='RandomSampler')\n",
    "\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    wandb_run_name = output_dir.replace('results/', '')\n",
    "\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {f'cd .. && CUDA_VISIBLE_DEVICES={os.environ[\"CUDA_VISIBLE_DEVICES\"]} ' if test_run else ''}{launcher}\n",
    "        open_instruct/dpo_tune.py \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --tokenizer_name {model_name_or_path} \\\n",
    "        {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        --train_file {train_file} \\\n",
    "        --max_seq_length {max_seq_length} \\\n",
    "        {'--subsample_inds_file '+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        --dataloader_sampler {dataloader_sampler} \\\n",
    "        --preprocessing_num_workers {preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size {per_device_train_batch_size} \\d\n",
    "        --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "        --learning_rate 5e-7 \\\n",
    "        --lr_scheduler_type linear \\\n",
    "        --warmup_ratio 0.1 \\\n",
    "        --weight_decay 0. \\\n",
    "        --num_train_epochs {num_train_epochs} \\\n",
    "        --with_tracking \\\n",
    "        {'--report_to \"'+str(report_to)+'\"' if report_to else ''} \\\n",
    "        --checkpointing_steps {checkpointing_steps} \\\n",
    "        {'--max_train_steps '+str(max_train_steps) if max_train_steps else ''} \\\n",
    "        {'--resume_from_checkpoint' if resume_from_checkpoint else ''} \\\n",
    "        {'--low_cpu_mem_usage' if not use_deepspeed else ''} \\\n",
    "        {'--overwrite_cache' if overwrite_cache else ''} \\\n",
    "        --logging_steps 1 \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    # if test_run:\n",
    "    #     print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    if test_run:\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prune: {1k@10, 10k@3}, datasets={dolly, stanford_alpaca}, scoring={random, dppmapx2}\n",
    "# need to gen curriculum for 50k sft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b79b9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_dpo.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce604bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=5\n",
      "+ cd ..\n",
      "+ CUDA_VISIBLE_DEVICES=2,5\n",
      "+ accelerate launch --mixed_precision fp16 --num_machines 1 --num_processes 2 --use_deepspeed --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py --model_name_or_path results/baselines/huggyllama/llama-7b --tokenizer_name results/baselines/huggyllama/llama-7b --gradient_checkpointing --train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl --max_seq_length 2048 --preprocessing_num_workers 32 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --learning_rate 5e-7 --lr_scheduler_type linear --warmup_ratio 0.1 --weight_decay 0. --num_train_epochs 2 --with_tracking --report_to tensorboard --checkpointing_steps 500 --logging_steps 1 --output_dir results/dpo1/jpt_llama-7b_ultrafeedback\n",
      "[2024-01-08 20:41:08,547] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-01-08 20:41:17,396] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-08 20:41:17,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:662:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,611] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 251.17it/s]\n",
      "01/08/2024 20:41:25 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-201ebfceee303348/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 252.78it/s]\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:26,493] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.11s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:43,556] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.48B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "01/08/2024 20:42:20 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.\n",
      "[2024-01-08 20:42:20,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.1+23a11a39, git-hash=23a11a39, git-branch=master\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "[2024-01-08 20:42:20,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-01-08 20:42:20,779] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-01-08 20:42:20,891] [INFO] [utils.py:786:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 14.16 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.9 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:118:__init__] Reduce bucket size 16777216\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:119:__init__] Prefetch bucket size 15099494\n",
      "[2024-01-08 20:42:20,995] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 13.64 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2024-01-08 20:42:21,139] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.76 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:21,240] [INFO] [utils.py:786:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.4 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:23,090] [INFO] [utils.py:786:see_memory_usage] After creating fp16 partitions: 4\n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.4 GB         CA 14.6 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.99 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,190] [INFO] [utils.py:786:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.39 GB         CA 14.6 GB         Max_CA 15 GB \n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.81 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,742] [INFO] [utils.py:786:see_memory_usage] After creating fp32 partitions\n",
      "[2024-01-08 20:42:23,743] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 26.61 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,744] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 116.15 GB, percent = 16.7%\n",
      "[2024-01-08 20:42:23,836] [INFO] [utils.py:786:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 25.94 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 117.28 GB, percent = 16.8%\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 1; 31.75 GiB total capacity; 25.93 GiB already allocated; 3.34 GiB free; 27.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 0; 31.75 GiB total capacity; 25.94 GiB already allocated; 3.21 GiB free; 27.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 409110) of binary: /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/python3.10\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/accelerate\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 964, in launch_command\r\n",
      "    deepspeed_launcher(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 687, in deepspeed_launcher\r\n",
      "    distrib_run.run(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "open_instruct/dpo_tune.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "[1]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 1 (local_rank: 1)\r\n",
      "  exitcode  : 1 (pid: 409111)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 409110)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_dpo.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "f5886d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'id', 'messages'],\n",
       "    num_rows: 96913\n",
       "})"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from note_pruning_analysis import get_dataset\n",
    "ds = get_dataset('super_ni')\n",
    "ds\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19aebd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"log_prob_neg\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"grad_loraB_l2n\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"numtoks_total_neg\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=10_kmd=llama7br512p4096_kemb=text+embedding\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_theta=0.1_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_theta=0.3_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_theta=0.6_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_theta=0.9_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/baselines/huggyllama/llama-7b using 8 GPUs, 1 batch size per GPU, 16 gradient accumulation steps, Effective batch size 128\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=random:s=0_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/random_s=0/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=random:s=0_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=random:s=0_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807221}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=log:prob:neg_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/log_prob_neg/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=log:prob:neg_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=log:prob:neg_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807222}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=grad:loraB:l2n_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/grad_loraB_l2n/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=grad:loraB:l2n_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=grad:loraB:l2n_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807224}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=numtoks:total:neg_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/numtoks_total_neg/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=numtoks:total:neg_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=numtoks:total:neg_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807225}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807226}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807227}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=vmf_gamma=10_kmd=llama7br512p4096_kemb=text+embedding/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807228}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=vmf_gamma=1_theta=0.1_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807229}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.3:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=vmf_gamma=1_theta=0.3_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.3:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.3:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807230}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.6:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=vmf_gamma=1_theta=0.6_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.6:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.6:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807231}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi6_mix_all50k:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": null,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': 'jbsub -queue alt_7d -name oi6_mix_all50k:llama-7b -mem 768g -cores 1x128+8 -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/mix/mix_all50k_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-05 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.9:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/mix_all50k/dppmap_k=vmf_gamma=1_theta=0.9_kmd=llama7br512p4096_kemb=grad+rp+loraB_q=numtoks+output_qmd=llama7br512p4096/inds_prune_size=30000_ep=3.pkl --dataloader_sampler SequentialSampler --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.9:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi6_mix_all50k:llama-7b/llama-7b_mix_all50k_score=dppmap:k=vmf:gamma=1:theta=0.9:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3 ||:; # some job may still be in RUN state due to daemon compiling binaries for the job not exited yet.; # https://www.ibm.com/support/pages/job-kept-run-status-after-job-finished; lsf_job_output=$(bjobs -l $LSB_JOBID 2>&1); if echo \"$lsf_job_output\" | grep -q \"Done successfully.\"; then;   echo \"Job $LSB_JOBID is finished\"; else;   echo \"Job $LSB_JOBID not finished\";   echo \"Output of bjobs -l:\";   echo \"$lsf_job_output\"; fi\\'', 'job_id': 2807232}]\n"
     ]
    }
   ],
   "source": [
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "add_hardwarespec_to_dirname = False\n",
    "save_strategy = 'steps'\n",
    "save_steps = 200 if getpass.getuser() in ('PTFMqngp', 'wpq') else 1_000_000\n",
    "save_total_limit = 1\n",
    "preprocessing_num_workers = 32\n",
    "evaluation_strategy = 'no' # set do_eval=False\n",
    "eval_steps = save_steps\n",
    "report_to = 'tensorboard wandb'\n",
    "suffix = None\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.03\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "subsample_inds_file_list = [None]\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "scoring_fn_and_pacing_fn = None\n",
    "learning_rate = 2e-5\n",
    "seed = None\n",
    "\n",
    "# ########### sft baselines\n",
    "\n",
    "\n",
    "# job_name = 'oi2'; num_train_epochs_list = [3] # submission.\n",
    "# job_name = 'oi3'; num_train_epochs_list = [3] # rebuttal. since model deleted, need to re-train.\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048; learning_rate = 2e-6\n",
    "# # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048; learning_rate = 2e-5\n",
    "\n",
    "# # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; \n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# # train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'\n",
    "# # train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2'; max_train_samples_list=[100_000]\n",
    "\n",
    "# ## 50k sft datasets\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca50k_data.jsonl'; abbr_train_file = 'stanford_alpaca50k'; \n",
    "# # train_file = 'data/processed/sharegpt/sharegpt50k_data.jsonl'; abbr_train_file = 'sharegpt50k'\n",
    "# train_file = 'data/processed/ultrachat/ultrachat50k_train_data.jsonl'; abbr_train_file = 'ultrachat50k'\n",
    "\n",
    "# ## additional ones : {wizardlm50k, oasst2, lima, gpt4_alpaca50k, flan_v250k}\n",
    "# # train_file = 'data/processed/wizardlm/wizardlm50k_data.jsonl'; abbr_train_file = 'wizardlm50k'\n",
    "# # train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# # train_file = 'data/processed/flan_v2/flan_v250k_data.jsonl'; abbr_train_file = 'flan_v250k';\n",
    "# # train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca50k_data.jsonl'; abbr_train_file = 'gpt4_alpaca'; \n",
    "# # train_file = 'data/processed/lima/lima_data.jsonl'; abbr_train_file = 'lima'; num_train_epochs_list = [3]\n",
    "\n",
    "# # mix\n",
    "# # train_file = 'data/processed/mix/mix_all50k_data.jsonl'; abbr_train_file = 'mix_all50k'; num_train_epochs_list = [2]\n",
    "\n",
    "# ## additional ones \n",
    "# # train_file = '/dccstor/data-pruning/data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'; max_train_samples_list=[50_000]\n",
    "# # train_file = '/dccstor/data-pruning/data/processed/self_instruct/self_instruct50k_data.jsonl'; abbr_train_file = 'self_instruct50k'\n",
    "\n",
    "\n",
    "# # train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2'; max_train_samples_list=[100_000]\n",
    "# ###########\n",
    "\n",
    "\n",
    "############ pruning runs\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048; kmd = gen_output_md = 'mistral7br512p4096'; learning_rate = 2e-6\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048; kmd = gen_output_md = 'llama7br512p4096'; learning_rate = 2e-5\n",
    "# \n",
    "\n",
    "\n",
    "# # 50k sft datasets\n",
    "# dataset = 'flan_v250k'; train_file = 'data/processed/flan_v2/flan_v250k_data.jsonl'; abbr_train_file = 'flan_v250k';\n",
    "# dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# dataset = 'stanford_alpaca50k'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca50k_data.jsonl'; abbr_train_file = 'stanford_alpaca50k'; \n",
    "# dataset = 'oasst2'; train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# dataset = 'wizardlm50k'; train_file = 'data/processed/wizardlm/wizardlm50k_data.jsonl'; abbr_train_file = 'wizardlm50k'\n",
    "# dataset = 'sharegpt50k'; train_file = 'data/processed/sharegpt/sharegpt50k_data.jsonl'; abbr_train_file = 'sharegpt50k'\n",
    "# dataset = 'ultrachat50k'; train_file = 'data/processed/ultrachat/ultrachat50k_train_data.jsonl'; abbr_train_file = 'ultrachat50k'\n",
    "dataset = 'mix_all50k'; train_file = 'data/processed/mix/mix_all50k_data.jsonl'; abbr_train_file = 'mix_all50k'; \n",
    "\n",
    "\n",
    "# dataset = 'flan_v2'; train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# dataset = 'stanford_alpaca'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca';\n",
    "# dataset = 'oasst2'; train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# dataset = 'wizardlmv2'; train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2';\n",
    "# dataset = 'sharegptv2'; train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2';\n",
    "# dataset = 'ultrachat200kv2'; train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2';\n",
    "\n",
    "\n",
    "# dataset = 'oasst1'; train_file = 'data/processed/oasst/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# dataset = 'open_orca_slim'; train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; \n",
    "# dataset = 'tulu_v2'; train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2';\n",
    "        \n",
    "# M = 80_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 40_000\n",
    "# M = 40_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 20_000\n",
    "# M = 30_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "#         (10_000, 10), # -> 1k, 2%\n",
    "#         (20_000, 4),  # -> 5k, 10% # not used in sweeps, but might be another data point.\n",
    "        (30_000, 3),  # -> 10k, 20%\n",
    "#         (60_000, 3),  # -> 20k, 40%\n",
    "#         (90_000, 3),  # -> 30k, 60%\n",
    "#         (120_000, 3),  # -> 40k, 80%\n",
    "    ]\n",
    "]\n",
    "#         + [running] 10k prune size. 4 datasets, compare {vmf,rbf} x {text,grad}\n",
    "\n",
    "## for ultrachat everything is in oi6, \n",
    "# for stanford alpaca, mostly the q+div in oi6\n",
    "job_name = f'oi6_{dataset}:{abbr_model_name}'\n",
    "\n",
    "scoring_fn_list = []\n",
    "scoring_fn_list += [\n",
    "    'random_s=0',\n",
    "#     'random_s=1',\n",
    "    'log_prob_neg', \n",
    "    # 'el2n_agg=mean', \n",
    "    'grad_loraB_l2n',\n",
    "#     'ifd_neg', # 'log_pmi_neg',\n",
    "#     'numtoks_input_neg', 'numtoks_output_neg', \n",
    "    'numtoks_total_neg',\n",
    "#     'alpagasus_rating',\n",
    "    ## dedup\n",
    "#     f'dedup_dist=cd_md=mpnet_emb=text+embedding',\n",
    "#     f'dedup_dist=cd_md={kmd}_emb=text+embedding',\n",
    "#     f'dedup_dist=cd_md={kmd}_emb=grad+rp+loraB',\n",
    "    ##\n",
    "#     f'dppmap_k=vmf_gamma=1_kmd=mpnet_kemb=text+embedding',\n",
    "    f'dppmap_k=rbf_gamma=1e-3_kmd={kmd}_kemb=text+embedding',\n",
    "    f'dppmap_k=vmf_gamma=1_kmd={kmd}_kemb=grad+rp+loraB',\n",
    "    f'dppmap_k=vmf_gamma=10_kmd={kmd}_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1e-2_kmd={kmd}_kemb=grad+rp+loraB',\n",
    "    ## arccos kernel\n",
    "#     'dppmap_k=acos0_kmd=llama7br512p4096_kemb=grad+rp+loraB',\n",
    "#     'dppmap_k=acos0_kmd=llama7br512p4096_kemb=text+embedding',\n",
    "#     'dppmap_k=acos1_kmd=llama7br512p4096_kemb=grad+rp+loraB',\n",
    "]\n",
    "\n",
    "# ablate with facility location\n",
    "# gammas = [10]\n",
    "# scoring_fn_list += [f'fl_k=vmf_gamma={gamma}_kmd=mpnet_kemb=text+embedding' for gamma in [1]]\n",
    "# scoring_fn_list += [f'fl_k=vmf_gamma={gamma}_kmd={kmd}_kemb=grad+rp+loraB' for gamma in [1]]\n",
    "# scoring_fn_list += [f'fl_k=vmf_gamma={gamma}_kmd={kmd}_kemb=text+embedding' for gamma in [10]]\n",
    "\n",
    "\n",
    "# job_name = f'oi6_{dataset}:{abbr_model_name}'\n",
    "# scoring_fn_list = []\n",
    "# scoring_fn_list += ['numtoks_output_neg']\n",
    "scoring_fn_list += [ ## alpaga w. numtoks\n",
    "    f'dppmap_k=vmf_gamma=1_theta={theta}_kmd={kmd}_kemb=grad+rp+loraB_q=numtoks+output_qmd={kmd}' for theta in [.1, .3, .6, .9] # .9 [0.1, 0.4, 0.6, 0.8,] \n",
    "#     f'dppmap_k=vmf_gamma=1_theta={theta}_kmd={kmd}_kemb=grad+rp+loraB_q=log+prob_qmd={kmd}' for theta in [.3, .6, .9]\n",
    "    ## ablate \\gamma\n",
    "#     f'dppmap_k=vmf_gamma={gamma}_theta={theta}_kmd={kmd}_kemb=grad+rp+loraB_q=numtoks+output_qmd={kmd}' for theta in [.1] for gamma in [0.03] # [0.03,0.1,1,10]\n",
    "#      f'dppmap_k=vmf_gamma={gamma}_kmd={kmd}_kemb=grad+rp+loraB' for gamma in [0.1,10]\n",
    "]\n",
    "\n",
    "# scoring_fn_list = []\n",
    "# # scoring_fn_list += ['alpagasus_rating_neg']\n",
    "# scoring_fn_list += [ ## alpaca w. alpagasus rating\n",
    "#     f'dppmap_k=vmf_gamma=1_theta={theta}_kmd={kmd}_kemb=grad+rp+loraB_q=alpagasus+rating_qmd={kmd}'\n",
    "#     for theta in [0.9] # [0.1, 0.3, 0.6]\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scoring_fn_list += [ # vary kernel embedding model \n",
    "#     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd={kmd}_kemb=grad+rp+loraB'\n",
    "#     for kmd in ['llama7br256p4096', 'llama7br512p4096', 'pythia1br512p4096']\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "############ \n",
    "\n",
    "\n",
    "    \n",
    "# add_hardwarespec_to_dirname = True\n",
    "# job_name += '_debug' # wpq debug\n",
    "# max_train_samples_list=[128*2]\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "if arch == 'x86_64':\n",
    "#     nodes = 1; num_gpus = 8; gpu_type = 'a100_80gb'; job_duration = 6\n",
    "#     nodes = 1; num_gpus = 4; gpu_type = 'a100_80gb'; job_duration = 6\n",
    "    nodes = 1; num_gpus = 8; gpu_type = None; job_duration = 6 \n",
    "#     nodes = 1; num_gpus = 4; gpu_type = None; job_duration = 6 \n",
    "#     nodes = 1; num_gpus = 2; gpu_type = 'a100_80gb'; job_duration = 6 \n",
    "\n",
    "    num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus); preprocessing_num_workers = 128 # tok takes quite a bit.\n",
    "    per_device_train_batch_size = 1\n",
    "    gradient_checkpointing = False\n",
    "    mixed_precision = 'bf16'; torch_dtype = 'float32'; use_flash_attn = False; use_tf32 = True \n",
    "    save_model_torch_dtype = 'bfloat16' # typically save fp32 weights, but for disk space sake, convert to bf16.\n",
    "else:\n",
    "    nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_checkpointing = True\n",
    "    mixed_precision = 'fp16'; torch_dtype = 'float32'; use_flash_attn = False; use_tf32 = False\n",
    "    save_model_torch_dtype = None\n",
    "\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "total_batch_size = 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "optimizer = 'adamw_hf'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\" \n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 \n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "load_in_8bit = False\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nnodes 1 --nproc_per_node={num_gpus} --rdzv_backend=c10d --rdzv_endpoint=localhost:0\" # assigns random port. https://github.com/pytorch/pytorch/issues/73320\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=${'SLURM_JOB_ID' if arch == 'ppcle64' else 'LSB_JOBID'} --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file = os.path.join(open_instruct_dir, 'scripts', 'error_file')\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    raise ValueError(f'train_file={train_file} does not exists')\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "            \n",
    "    if any(job_name == y for y in ['oi2', 'oi3']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "    \n",
    "#     if learning_rate != 2e-5:\n",
    "#         output_dirname += f'_lr={learning_rate}'\n",
    "        \n",
    "    if subsample_inds_file:\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            else:\n",
    "                return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            ('_dtype='+torch_dtype if torch_dtype is not None else '')+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)+\\\n",
    "            '_ngpus='+str(num_gpus)+\\\n",
    "            ('_fa2' if use_flash_attn else '')\n",
    "    if suffix:\n",
    "        output_dirname += suffix\n",
    "    if seed is not None:\n",
    "        output_dirname += f'_seed={seed}'\n",
    "    output_dir = os.path.join(open_instruct_dir, 'results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join(open_instruct_dir, 'results', job_name), exist_ok=True)\n",
    "    if arch == 'x86_64':\n",
    "        wandb_run_name = 'ccc'+output_dir[output_dir.find('results'):][7:] # e.g., ccc/oi2/run_name\n",
    "    else:\n",
    "        wandb_run_name = output_dir.replace('results/', '') # e.g., oi2/run_name\n",
    "    \n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {'cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''} \\\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers={preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate={learning_rate} \\\n",
    "        {'--seed='+str(seed) if seed else ''} \\\n",
    "        --lr_scheduler_type={lr_scheduler_type} \\\n",
    "        --warmup_ratio={warmup_ratio} \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy={evaluation_strategy} \\\n",
    "        {'--eval_steps='+str(eval_steps) if eval_steps else ''} \\\n",
    "        {'--report_to '+str(report_to) if report_to else ''} \\\n",
    "        --run_name {wandb_run_name} \\\n",
    "        --logging_strategy=steps \\\n",
    "        --logging_first_step \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        --ddp_timeout=7200 \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''} \\\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        {'--torch_dtype='+str(torch_dtype) if torch_dtype else ''} \\\n",
    "        {'--save_model_torch_dtype='+str(save_model_torch_dtype) if save_model_torch_dtype else ''} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {f'--tf32=True' if use_tf32 else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --use_flash_attn {'True' if use_flash_attn else 'False'} \\\n",
    "        --low_cpu_mem_usage \\\n",
    "        --overwrite_cache \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #  --overwrite_cache   # if delete a dataset and need to refresh cache\n",
    "\n",
    "    if test_run:\n",
    "        print()\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64':\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        queue=queue,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c05af07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313300 data/processed/mix/mix_all50k_data.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "# !find /dccstor/data-pruning/curriculum -type d -exec chmod 777 {} \\; && find /dccstor/data-pruning/curriculum -type f -exec chmod 777 {} \\;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12fc2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_sft.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed0c2894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ cd ..\n",
      "+ TORCHELASTIC_ERROR_FILE=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/error_file\n",
      "+ TORCH_CPP_LOG_LEVEL=INFO\n",
      "+ NCCL_DEBUG=INFO\n",
      "+ LOGLEVEL=INFO\n",
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/oasst1/oasst1_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=32 --per_device_train_batch_size=1 --gradient_accumulation_steps=128 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=200 --report_to tensorboard wandb --run_name /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=200 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --overwrite_output_dir --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/oasst1/random_s=0/inds_prune_size=10000_ep=10.pkl --dataloader_sampler SequentialSampler --use_flash_attn True --low_cpu_mem_usage --overwrite_cache --output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10\n",
      "[2024-01-19 02:04:37,834] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Saving args dict to /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10.args.json\n",
      "01/19/2024 02:04:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "01/19/2024 02:04:39 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_sampler=SequentialSampler,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=7200,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=200.0,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10/runs/Jan19_02-04-39_cccxc552,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=200,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Using custom data configuration default-b03eccd42e843020\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Using custom data configuration default-b03eccd42e843020\n",
      "Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset info from data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "[INFO|configuration_utils.py:715] 2024-01-19 02:04:39,135 >> loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-01-19 02:04:39,136 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3118] 2024-01-19 02:04:39,229 >> loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1222] 2024-01-19 02:04:39,229 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|modeling_utils.py:1304] 2024-01-19 02:04:39,230 >> You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:791] 2024-01-19 02:04:39,230 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "[INFO|modeling_utils.py:3950] 2024-01-19 02:04:41,778 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3958] 2024-01-19 02:04:41,778 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-01-19 02:04:41,781 >> loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-01-19 02:04:41,781 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "01/19/2024 02:04:41 - INFO - __main__ - [wpq] model.dtype=torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1648] 2024-01-19 02:04:41,845 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "Process #16 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #16 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "Process #17 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #17 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "Process #18 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #18 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "Process #19 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #19 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "Process #20 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #20 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "Process #21 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #21 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "Process #22 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #22 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "Process #23 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #23 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "Process #24 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #24 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "Process #25 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #25 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "Process #26 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #26 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "Process #27 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #27 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "Process #28 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #28 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "Process #29 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #29 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "Process #30 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #30 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "Process #31 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #31 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spawning 32 processes\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Spawning 32 processes\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   0%| | 0/33717 [00:Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   0%| | 22/33717 [00Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   1%| | 361/33717 [0Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=32):   3%| | 1001/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   5%| | 1719/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   7%| | 2251/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=32):  12%| | 4021/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):  14%|▏| 4841/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):  18%|▏| 5957/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "01/19/2024 02:04:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32): 100%|█| 33717/33717 \n",
      "Concatenating 32 shards\n",
      "01/19/2024 02:04:55 - INFO - datasets.arrow_dataset - Concatenating 32 shards\n",
      "Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00000_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00000_of_00016.arrow\n",
      "Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00001_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00001_of_00016.arrow\n",
      "Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00002_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00002_of_00016.arrow\n",
      "Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00003_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00003_of_00016.arrow\n",
      "Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00004_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00004_of_00016.arrow\n",
      "Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00005_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00005_of_00016.arrow\n",
      "Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00006_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00006_of_00016.arrow\n",
      "Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00007_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00007_of_00016.arrow\n",
      "Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00008_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00008_of_00016.arrow\n",
      "Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00009_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00009_of_00016.arrow\n",
      "Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00010_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00010_of_00016.arrow\n",
      "Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00011_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00011_of_00016.arrow\n",
      "Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00012_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00012_of_00016.arrow\n",
      "Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00013_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00013_of_00016.arrow\n",
      "Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00014_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00014_of_00016.arrow\n",
      "Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00015_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_*_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "01/19/2024 02:04:56 - INFO - __main__ - Subsample dataset according to indices: /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/oasst1/random_s=0/inds_prune_size=10000_ep=10.pkl\n",
      "01/19/2024 02:04:56 - INFO - __main__ - subsample_inds_file has 10000 indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wpq] Example 0 of train_dataset: \r\n",
      "{'dataset': 'oasst1', 'id': 'oasst1_20480', 'messages': [{'role': 'user', 'content': 'Cómo manejar un carro manual'}, {'role': 'assistant', 'content': 'Lo primero que tienes que hacer, si nunca has conducido un coche manual, es familiarizarte con el embrague y palanca de cambios. Si conduces habitualmente un coche automático, estarás acostumbrado a no utilizar para nada el pie izquierdo ni la palanca del cambio. Encontrarás tres pedales, siendo el embrague el que está situado a la izquierda y el que tendrás que pisar cada vez que cambies de marcha. Por otro lado, la palanca del cambio se ubica siempre en la consola central.\\n\\nPara arrancar un coche manual, es necesario seguir una serie de pasos que, al principio, pueden parecer muchos, pero que, con el tiempo, acabarás haciéndolos sin darte cuenta:\\n\\n1) Comprueba que la palanca del cambio está en punto muerto\\n2) Coloca el pie derecho en el pedal del freno\\n3) Arranca el motor\\n4) Pisa el embrague con el pie izquierdo\\n5) Coloca la palanca del cambio en la primera marcha, sin levantar el pedal del freno\\n6) Suelta el freno de mano\\n7) Suelta el pedal del freno\\nYa estás listo para iniciar la marcha, soltando suavemente el embrague, a medida que aceleras.\\n\\nUna vez que ya estás en marcha, debes hacer un uso correcto del cambio manual para cambiar las marchas de forma correcta. Un uso incorrecto de la caja de cambios manual puede repercutir negativamente en tu seguridad y también afectar gravemente al embrague y a la transmisión, lo que se traduce en serias averías de coste muy elevado. Para evitarlo, te explicamos cómo debes proceder:\\n\\nUna vez que hayas arrancado, pisa el acelerador muy lentamente. Notarás que el régimen del motor aumenta. En ese momento, comienza a soltar suavemente el pedal del embrague. Verás que el motor vuelve a bajar de vueltas. En ese momento, puedes presionar un poco más el acelerador y el coche comenzará a avanzar.\\n\\nAhora llega el momento de meter la segunda marcha. Dependiendo del tipo de coche y combustible, podrás circular a un régimen de giro más bajo o alto. El régimen de giro en coche de gasolina, por lo general, oscila entre loas 2.500 y 3.000 vueltas. Si el motor está sobrealimentado por turbo, te permitirá circular por debajo de ese rango, ya que algunos coches turbos modernos entregan la totalidad de su par motor, incluso por debajo de las 2.000 vueltas.\\n\\nUn coche con motor turbodiésel te permite circular a un régimen muy bajo, por debajo de las 2.000 vueltas, ya que la entrega de par se produce antes que en un motor de gasolina.\\n\\nCuando el coche alcance un régimen de vueltas apropiado, suelta el pedal del acelerador y vuelve a pisar el embrague. Coge la palanca del cambio y baja para meter segunda. Suelta el embrague y presiona nuevamente el acelerador. A partir de aquí, cada vez que quieras cambiar de marcha, deberás repetir el mismo proceso: soltar el acelerador, pisar embrague, meter la marcha, soltar embrague y volver a acelerar.\\n\\n¡Buen viaje!'}], 'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29907, 29980,  4346,\r\n",
      "          767, 29872,  4758,   443,  1559,   307, 12219,    13, 29966, 29989,\r\n",
      "          465, 22137, 29989, 29958,    13,  3410,  1903,  1489,   712,   260,\r\n",
      "          819,   267,   712, 14557, 29892,  1354, 28456,   756, 13417, 13321,\r\n",
      "          443,  1302,  1173, 12219, 29892,   831,  9985,   466, 11908,   378,\r\n",
      "          560,  7232,  1431,   434,   343,  5112,   273,  1113,   316, 10625,\r\n",
      "         2363, 29889,  6101, 13417,   778,  4760, 14162,   443,  1302,  1173,\r\n",
      "         3345, 22054, 29892, 23673,  1569,  1274,   520,   398,  1182,   912,\r\n",
      "          263,   694, 11824,   279,  1702, 25801,   560,  5036,  5951, 16026,\r\n",
      "         1867,  6836,   425,  5112,   273,  1113,   628, 26007, 29889,  1174,\r\n",
      "         9996,   279,  1569,  9941,  8939,  2122, 29892, 18200,   560,  7232,\r\n",
      "         1431,   434,   560,   712,  7919,  2990,   912,   263,   425,  5951,\r\n",
      "        16026,  1388,   343,   560,   712, 10331, 11964,   712, 20066,   279,\r\n",
      "         9747,  7763,   712, 10625,   583,   316,  8575, 29874, 29889,  7102,\r\n",
      "        16994, 19931, 29892,   425,  5112,   273,  1113,   628, 26007,   409,\r\n",
      "        13069,   983, 26692,   427,   425,  1136,  2963,  6555, 29889,    13,\r\n",
      "           13,  2177, 29874,   564,   661,  4287,   443,  1302,  1173, 12219,\r\n",
      "        29892,   831, 16632,  2628,  7025,   381,  1185,  7080,   316,  2331,\r\n",
      "          359,   712, 29892,   394,  3420,   601, 29892, 19796,  9541,  2265,\r\n",
      "        24312, 29892,  7046,   712, 29892,   378,   560, 13924, 29892, 22998,\r\n",
      "          279,  1569,   447,   455, 21183,   324,   359,  4457,  5424,   371,\r\n",
      "        21052, 29901,    13,    13, 29896, 29897,   422,   558,   434,  2291,\r\n",
      "          712,   425,  5112,   273,  1113,   628, 26007,  7919,   427, 15978,\r\n",
      "          286, 15009,    13, 29906, 29897,  1530,  6400,   560,  5036, 14923,\r\n",
      "         1859,   427,   560,  8939,   284,   628,   285, 26155,    13, 29941,\r\n",
      "        29897,   826,   661,  1113,   560, 10992,    13, 29946, 29897,   349,\r\n",
      "         8069,   560,  7232,  1431,   434,   378,   560,  5036,  5951, 16026,\r\n",
      "         1867,    13, 29945, 29897,  1530,  6400,   425,  5112,   273,  1113,\r\n",
      "          628, 26007,   427,   425,  8633,  8575, 29874, 29892,  4457, 14453,\r\n",
      "          424,   279,   560,  8939,   284,   628,   285, 26155,    13, 29953,\r\n",
      "        29897,  2166,  2554,   560,   285, 26155,   316, 24318,    13, 29955,\r\n",
      "        29897,  2166,  2554,   560,  8939,   284,   628,   285, 26155,    13,\r\n",
      "        29979, 29874,   707,  1569,  1051, 29877,  1702, 21855,   279,   425,\r\n",
      "         8575, 29874, 29892,   899, 29873,  1743,   480,   485,  9936,   560,\r\n",
      "         7232,  1431,   434, 29892,   263,  1612,  1458,   712,  1274,  7367,\r\n",
      "          294, 29889,    13,    13, 29965,  1056,  7763,   712,  9343,   707,\r\n",
      "         1569,   427,  8575, 29874, 29892,  2553,   267, 14557,   443, 17448,\r\n",
      "         1959, 29877,   628, 26007, 12219,  1702, 10625,  4447,  1869,  8575,\r\n",
      "          294,   316,  5954,  1959, 29874, 29889,   853, 17448, 10240, 29877,\r\n",
      "          316,   425,   274,  9919,   316, 10625,  2363, 12219, 11493,   337,\r\n",
      "          546,  7582,   381,  3480,  1926,  2503,   427,  5291,  2377,   332,\r\n",
      "         2368,   343,  6196,  2511,   522,   279,  8310,  9936,   394,  7232,\r\n",
      "         1431,   434,   343,   263,   425, 18750, 11861, 29892,   658,   712,\r\n",
      "          409,  3534, 24551,   427,   724,  3173,  4759,  8577,   316,  3438,\r\n",
      "        29872, 12287, 11858,   912, 29889, 12994,  3415,  3673,   417, 29892,\r\n",
      "          734, 28117, 14054, 28810,  4346,  2553,   267,  6449,   261, 29901,\r\n",
      "           13,    13, 29965,  1056,  7763,   712, 14842,   294,   564,   661,\r\n",
      "        29883,   912, 29892,   282,  8069,   560,  1274,  7367,  3136, 12287,\r\n",
      "          301,   296,  2503, 29889,  2216,   279,  1569,   712,   560,  6367,\r\n",
      "        19933,   628, 10992, 19291, 29874, 29889,  1174, 15371, 14341, 29892,\r\n",
      "          419, 24880,   263,   899, 12637,   480,   485,  9936,   560,  8939,\r\n",
      "          284,   628,  7232,  1431,   434, 29889,  1798,  1569,   712,   560,\r\n",
      "        10992, 22126,   345,   263,   289,  1175,   279,   316, 18679,  2152,\r\n",
      "          294, 29889,  1174, 15371, 14341, 29892,  2653, 11696,  2225,   291,\r\n",
      "          279,   443, 14534,  3627,   560,  1274,  7367,  3136,   343,   560,\r\n",
      "         1302,  1173, 19487, 20484,   263,  1029,  4096,   279, 29889,    13,\r\n",
      "           13, 29909, 15255, 10953, 29874,   560, 14341,   316, 11134,   425,\r\n",
      "        17329,  8575, 29874, 29889, 10034,   355, 17008,   628, 13306,   316,\r\n",
      "         1302,  1173,   343,  4145,   504,  1821, 29892,  2532, 11964, 19308,\r\n",
      "          263,   443,  6367, 19933,   316,   330,  3350,  3627, 13085,   288,\r\n",
      "        20478, 29889,  1260,  6367, 19933,   316,   330,  3350,   427,  1302,\r\n",
      "         1173,   316, 10489,   324,  1099, 29892,  1277,   658,  2498, 29892,\r\n",
      "        15199,  4233,  2637,   658,   294, 29871, 29906, 29889, 29945, 29900,\r\n",
      "        29900,   343, 29871, 29941, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29889,  6101,   560, 10992,  7919,  4166,   284,  2073,   912,\r\n",
      "         1277,  7013,   833, 29892,   734, 14257,   381, 29976, 19308,  1277,\r\n",
      "         2553,  7069,   316, 15371,   364,  4524, 29892,  9343,   712, 20071,\r\n",
      "         1302,  6609,  7013, 27737,  5400,   359,   875,  1727,   273,   425,\r\n",
      "         3001,  2368,   316,   480,   610, 10992, 29892,  1343, 10648,  1277,\r\n",
      "         2553,  7069,   316,  1869, 29871, 29906, 29889, 29900, 29900, 29900,\r\n",
      "        18679,  2152,   294, 29889,    13,    13,  2525,  1302,  1173,   378,\r\n",
      "        10992,  7013, 29890, 12143,   743,   295,   734,  3635,   568, 19308,\r\n",
      "          263,   443,  6367, 19933, 12287, 13085, 29892,  1277,  2553,  7069,\r\n",
      "          316,  1869, 29871, 29906, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29892,  9343,   712,   425,   875,  1727, 29874,   316,   610,\r\n",
      "          409,  7738, 12971,   712,   427,   443, 10992,   316, 10489,   324,\r\n",
      "         1099, 29889,    13,    13, 29907, 29884,  1743,   560,  1302,  1173,\r\n",
      "        10747,   749,   443,  6367, 19933,   316, 18679,  2152,   294, 11712,\r\n",
      "         1631,   912, 29892,   480,  2554,   560,  8939,   284,   628,  1274,\r\n",
      "         7367,  3136,   343, 22126,   345,   263, 20066,   279,   560,  7232,\r\n",
      "         1431,   434, 29889,   315, 21317,   425,  5112,   273,  1113,   628,\r\n",
      "        26007,   343,   289,  9919,  1702, 11134, 17329, 29889,  2166,  2554,\r\n",
      "          560,  7232,  1431,   434,   343,  2225, 16017,  8005, 29894,  2503,\r\n",
      "          560,  1274,  7367,  3136, 29889,   319,  8019,   316, 10592, 29983,\r\n",
      "        29892,  9747,  7763,   712,   439,   631,   294, 10625,  4447,   316,\r\n",
      "         8575, 29874, 29892,   316,   495,  1569, 21159,   381,   560, 11329,\r\n",
      "        14177, 29877, 29901,   899, 12637,   560,  1274,  7367,  3136, 29892,\r\n",
      "        20066,   279,  7232,  1431,   434, 29892, 11134,   425,  8575, 29874,\r\n",
      "        29892,   899, 12637,  7232,  1431,   434,   343,  1700,   369,   263,\r\n",
      "         1274,  7367,   279, 29889,    13,    13, 30180,  3727,   264,  3025,\r\n",
      "         1324, 29991,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\r\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\r\n",
      "         -100,  -100,  -100,  -100,  -100,  3410,  1903,  1489,   712,   260,\r\n",
      "          819,   267,   712, 14557, 29892,  1354, 28456,   756, 13417, 13321,\r\n",
      "          443,  1302,  1173, 12219, 29892,   831,  9985,   466, 11908,   378,\r\n",
      "          560,  7232,  1431,   434,   343,  5112,   273,  1113,   316, 10625,\r\n",
      "         2363, 29889,  6101, 13417,   778,  4760, 14162,   443,  1302,  1173,\r\n",
      "         3345, 22054, 29892, 23673,  1569,  1274,   520,   398,  1182,   912,\r\n",
      "          263,   694, 11824,   279,  1702, 25801,   560,  5036,  5951, 16026,\r\n",
      "         1867,  6836,   425,  5112,   273,  1113,   628, 26007, 29889,  1174,\r\n",
      "         9996,   279,  1569,  9941,  8939,  2122, 29892, 18200,   560,  7232,\r\n",
      "         1431,   434,   560,   712,  7919,  2990,   912,   263,   425,  5951,\r\n",
      "        16026,  1388,   343,   560,   712, 10331, 11964,   712, 20066,   279,\r\n",
      "         9747,  7763,   712, 10625,   583,   316,  8575, 29874, 29889,  7102,\r\n",
      "        16994, 19931, 29892,   425,  5112,   273,  1113,   628, 26007,   409,\r\n",
      "        13069,   983, 26692,   427,   425,  1136,  2963,  6555, 29889,    13,\r\n",
      "           13,  2177, 29874,   564,   661,  4287,   443,  1302,  1173, 12219,\r\n",
      "        29892,   831, 16632,  2628,  7025,   381,  1185,  7080,   316,  2331,\r\n",
      "          359,   712, 29892,   394,  3420,   601, 29892, 19796,  9541,  2265,\r\n",
      "        24312, 29892,  7046,   712, 29892,   378,   560, 13924, 29892, 22998,\r\n",
      "          279,  1569,   447,   455, 21183,   324,   359,  4457,  5424,   371,\r\n",
      "        21052, 29901,    13,    13, 29896, 29897,   422,   558,   434,  2291,\r\n",
      "          712,   425,  5112,   273,  1113,   628, 26007,  7919,   427, 15978,\r\n",
      "          286, 15009,    13, 29906, 29897,  1530,  6400,   560,  5036, 14923,\r\n",
      "         1859,   427,   560,  8939,   284,   628,   285, 26155,    13, 29941,\r\n",
      "        29897,   826,   661,  1113,   560, 10992,    13, 29946, 29897,   349,\r\n",
      "         8069,   560,  7232,  1431,   434,   378,   560,  5036,  5951, 16026,\r\n",
      "         1867,    13, 29945, 29897,  1530,  6400,   425,  5112,   273,  1113,\r\n",
      "          628, 26007,   427,   425,  8633,  8575, 29874, 29892,  4457, 14453,\r\n",
      "          424,   279,   560,  8939,   284,   628,   285, 26155,    13, 29953,\r\n",
      "        29897,  2166,  2554,   560,   285, 26155,   316, 24318,    13, 29955,\r\n",
      "        29897,  2166,  2554,   560,  8939,   284,   628,   285, 26155,    13,\r\n",
      "        29979, 29874,   707,  1569,  1051, 29877,  1702, 21855,   279,   425,\r\n",
      "         8575, 29874, 29892,   899, 29873,  1743,   480,   485,  9936,   560,\r\n",
      "         7232,  1431,   434, 29892,   263,  1612,  1458,   712,  1274,  7367,\r\n",
      "          294, 29889,    13,    13, 29965,  1056,  7763,   712,  9343,   707,\r\n",
      "         1569,   427,  8575, 29874, 29892,  2553,   267, 14557,   443, 17448,\r\n",
      "         1959, 29877,   628, 26007, 12219,  1702, 10625,  4447,  1869,  8575,\r\n",
      "          294,   316,  5954,  1959, 29874, 29889,   853, 17448, 10240, 29877,\r\n",
      "          316,   425,   274,  9919,   316, 10625,  2363, 12219, 11493,   337,\r\n",
      "          546,  7582,   381,  3480,  1926,  2503,   427,  5291,  2377,   332,\r\n",
      "         2368,   343,  6196,  2511,   522,   279,  8310,  9936,   394,  7232,\r\n",
      "         1431,   434,   343,   263,   425, 18750, 11861, 29892,   658,   712,\r\n",
      "          409,  3534, 24551,   427,   724,  3173,  4759,  8577,   316,  3438,\r\n",
      "        29872, 12287, 11858,   912, 29889, 12994,  3415,  3673,   417, 29892,\r\n",
      "          734, 28117, 14054, 28810,  4346,  2553,   267,  6449,   261, 29901,\r\n",
      "           13,    13, 29965,  1056,  7763,   712, 14842,   294,   564,   661,\r\n",
      "        29883,   912, 29892,   282,  8069,   560,  1274,  7367,  3136, 12287,\r\n",
      "          301,   296,  2503, 29889,  2216,   279,  1569,   712,   560,  6367,\r\n",
      "        19933,   628, 10992, 19291, 29874, 29889,  1174, 15371, 14341, 29892,\r\n",
      "          419, 24880,   263,   899, 12637,   480,   485,  9936,   560,  8939,\r\n",
      "          284,   628,  7232,  1431,   434, 29889,  1798,  1569,   712,   560,\r\n",
      "        10992, 22126,   345,   263,   289,  1175,   279,   316, 18679,  2152,\r\n",
      "          294, 29889,  1174, 15371, 14341, 29892,  2653, 11696,  2225,   291,\r\n",
      "          279,   443, 14534,  3627,   560,  1274,  7367,  3136,   343,   560,\r\n",
      "         1302,  1173, 19487, 20484,   263,  1029,  4096,   279, 29889,    13,\r\n",
      "           13, 29909, 15255, 10953, 29874,   560, 14341,   316, 11134,   425,\r\n",
      "        17329,  8575, 29874, 29889, 10034,   355, 17008,   628, 13306,   316,\r\n",
      "         1302,  1173,   343,  4145,   504,  1821, 29892,  2532, 11964, 19308,\r\n",
      "          263,   443,  6367, 19933,   316,   330,  3350,  3627, 13085,   288,\r\n",
      "        20478, 29889,  1260,  6367, 19933,   316,   330,  3350,   427,  1302,\r\n",
      "         1173,   316, 10489,   324,  1099, 29892,  1277,   658,  2498, 29892,\r\n",
      "        15199,  4233,  2637,   658,   294, 29871, 29906, 29889, 29945, 29900,\r\n",
      "        29900,   343, 29871, 29941, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29889,  6101,   560, 10992,  7919,  4166,   284,  2073,   912,\r\n",
      "         1277,  7013,   833, 29892,   734, 14257,   381, 29976, 19308,  1277,\r\n",
      "         2553,  7069,   316, 15371,   364,  4524, 29892,  9343,   712, 20071,\r\n",
      "         1302,  6609,  7013, 27737,  5400,   359,   875,  1727,   273,   425,\r\n",
      "         3001,  2368,   316,   480,   610, 10992, 29892,  1343, 10648,  1277,\r\n",
      "         2553,  7069,   316,  1869, 29871, 29906, 29889, 29900, 29900, 29900,\r\n",
      "        18679,  2152,   294, 29889,    13,    13,  2525,  1302,  1173,   378,\r\n",
      "        10992,  7013, 29890, 12143,   743,   295,   734,  3635,   568, 19308,\r\n",
      "          263,   443,  6367, 19933, 12287, 13085, 29892,  1277,  2553,  7069,\r\n",
      "          316,  1869, 29871, 29906, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29892,  9343,   712,   425,   875,  1727, 29874,   316,   610,\r\n",
      "          409,  7738, 12971,   712,   427,   443, 10992,   316, 10489,   324,\r\n",
      "         1099, 29889,    13,    13, 29907, 29884,  1743,   560,  1302,  1173,\r\n",
      "        10747,   749,   443,  6367, 19933,   316, 18679,  2152,   294, 11712,\r\n",
      "         1631,   912, 29892,   480,  2554,   560,  8939,   284,   628,  1274,\r\n",
      "         7367,  3136,   343, 22126,   345,   263, 20066,   279,   560,  7232,\r\n",
      "         1431,   434, 29889,   315, 21317,   425,  5112,   273,  1113,   628,\r\n",
      "        26007,   343,   289,  9919,  1702, 11134, 17329, 29889,  2166,  2554,\r\n",
      "          560,  7232,  1431,   434,   343,  2225, 16017,  8005, 29894,  2503,\r\n",
      "          560,  1274,  7367,  3136, 29889,   319,  8019,   316, 10592, 29983,\r\n",
      "        29892,  9747,  7763,   712,   439,   631,   294, 10625,  4447,   316,\r\n",
      "         8575, 29874, 29892,   316,   495,  1569, 21159,   381,   560, 11329,\r\n",
      "        14177, 29877, 29901,   899, 12637,   560,  1274,  7367,  3136, 29892,\r\n",
      "        20066,   279,  7232,  1431,   434, 29892, 11134,   425,  8575, 29874,\r\n",
      "        29892,   899, 12637,  7232,  1431,   434,   343,  1700,   369,   263,\r\n",
      "         1274,  7367,   279, 29889,    13,    13, 30180,  3727,   264,  3025,\r\n",
      "         1324, 29991,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:593] 2024-01-19 02:04:57,333 >> Using auto half precision backend\n",
      "[INFO|trainer.py:738] 2024-01-19 02:04:57,494 >> The following columns in the training set don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: messages, id, dataset. If messages, id, dataset are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1723] 2024-01-19 02:04:57,514 >> ***** Running training *****\n",
      "[INFO|trainer.py:1724] 2024-01-19 02:04:57,514 >>   Num examples = 10,000\n",
      "[INFO|trainer.py:1725] 2024-01-19 02:04:57,514 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1726] 2024-01-19 02:04:57,514 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1729] 2024-01-19 02:04:57,514 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1730] 2024-01-19 02:04:57,514 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1731] 2024-01-19 02:04:57,514 >>   Total optimization steps = 78\n",
      "[INFO|trainer.py:1732] 2024-01-19 02:04:57,515 >>   Number of trainable parameters = 6,738,423,808\n",
      "[INFO|integration_utils.py:718] 2024-01-19 02:04:57,519 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
      "  0%|                                                    | 0/78 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-01-19 02:05:01,563 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,569 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,572 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,576 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,577 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.6425, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7168, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.03}        \n",
      "  3%|█▏                                          | 2/78 [00:42<26:54, 21.24s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/spawnbase.py:372\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash gen_cmds_sft.sh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:177\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Ensure the subprocess really is terminated\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m         \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# add isalive check, to ensure exitstatus is set:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m child\u001b[38;5;241m.\u001b[39misalive()\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:646\u001b[0m, in \u001b[0;36mspawn.terminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill(signal\u001b[38;5;241m.\u001b[39mSIGCONT)\n\u001b[0;32m--> 646\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayafterterminate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misalive():\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_sft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "499d6f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mmlu_s=0', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('humaneval', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('humaneval_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('humaneval', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('humaneval_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('humaneval', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('humaneval_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi6_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "#cmds:  54 \n",
      "\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_1h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gen_cmds_utils import remove_all_symlinks, create_unique_symlinks, get_chat_formatting_function, get_resource_for_task, OPENAI_MODEL_LIST\n",
    "\n",
    "\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "use_slow_tokenizer = True\n",
    "launch_one_job_per_model = True\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    'mmlu_s=5', \n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "#     'alpacafarm_ann=gpt35:turbo:1106',\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "task_names_mtbench = ['mtbench_ann=gpt:4:1106:preview_chatfmt'] # ann=gpt:4, ann=gpt:3.5:turbo:1106, gpt:4:1106:preview (gpt4-turbo)\n",
    "# task_names_mtbench = ['mtbench_ann=gpt:3.5:turbo:1106_chatfmt'] # for debug sake, prefer gpt4\n",
    "# task_names_mtbench = ['mtbench_ann=gpt:4_chatfmt']\n",
    "# task_names_alpacafarm = ['alpacafarm_ann=chatgpt_chatfmt']\n",
    "task_names_alpacafarm = ['alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt']\n",
    "task_names_chateval = task_names_mtbench + task_names_alpacafarm\n",
    "\n",
    "\n",
    "# # ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "# #     'huggyllama/llama-7b', \n",
    "# #     'mistralai/Mistral-7B-v0.1',\n",
    "# #     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "# #     'NousResearch/Llama-2-7b-hf',\n",
    "# #     'NousResearch/Llama-2-7b-chat-hf',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "# #     'HuggingFaceH4/zephyr-7b-alpha',\n",
    "#     'HuggingFaceH4/zephyr-7b-beta',\n",
    "# #     'codellama/CodeLlama-7b-hf',\n",
    "# #     'codellama/CodeLlama-7b-Python-hf',\n",
    "# #     'codellama/CodeLlama-7b-Instruct-hf',\n",
    "# ]]\n",
    "# # task_names = task_names + task_names_chatfmt\n",
    "# task_names = task_names_mtbench\n",
    "\n",
    "# # oi5\n",
    "# exp_dir = 'results/oi2'\n",
    "# exp_dir = 'results/oi5_flan_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_dolly:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlm50k:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegpt50k:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b_debug'\n",
    "# exp_dir = 'results/oi5_stanford_alpaca:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlmv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegptv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat200kv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_tulu_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_open_orca_slim:llama-7b'\n",
    "# exp_dir = 'results/dpo1'\n",
    "# exp_dir = 'results/dpo2_ultrafeedback:llama-7b+sharegptv2ep2'\n",
    "exp_dirs = [\n",
    "#     'results/baselines/huggyllama',\n",
    "#     'results/oi2',\n",
    "#     'results/oi3',\n",
    "#     'results/oi5_dolly:llama-7b',\n",
    "#     'results/oi5_flan_v250k:llama-7b',\n",
    "#     'results/oi5_oasst2:llama-7b',\n",
    "#     'results/oi5_wizardlm50k:llama-7b',\n",
    "#     'results/oi5_sharegpt50k:llama-7b',\n",
    "#     'results/oi5_ultrachat50k:llama-7b',\n",
    "#     'results/oi5_stanford_alpaca50k:llama-7b',\n",
    "    'results/oi6_stanford_alpaca50k:llama-7b', # diversity+quality\n",
    "#     'results/oi6_ultrachat50k:llama-7b', # redo- oi5 counterpart because i deleted the model weights \n",
    "#     'results/oi6_stanford_alpaca50k:mistral-7b',\n",
    "#     'results/oi6_ultrachat50k:mistral-7b',\n",
    "]\n",
    "# exp_dirs = [\n",
    "#     'results/dpo2_ultrafeedback:llama-7b+sharegptv2ep2',\n",
    "# ]\n",
    "\n",
    "# subdir_filter_fn = lambda x: 'ultrachat' in x and 'size=30000' in x and 'acos' not in x and 'random' in x #'dpp' in x and 'vmf' in x\n",
    "# subdir_filter_fn = lambda x: 'stanford' in x and 'size=30000' in x and 'acos' not in x\n",
    "# subdir_filter_fn = lambda x: 'sharegpt' in x and 'size=60000' in x and 'acos' not in x\n",
    "# subdir_filter_fn = lambda x: 'qmd' in x and '0.9' not in x\n",
    "# subdir_filter_fn = lambda x: 'dppmap:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=' in x\n",
    "# subdir_filter_fn = lambda x: 'size=30' in x and ('acos' not in x) #and 'random' in x\n",
    "subdir_filter_fn = lambda x: 'alpagasus' not in x and 'acos' not in x\n",
    "task_names = task_names + task_names_chatfmt;\n",
    "# task_names = ['alpacafarm_ann=alpaca:eval:gpt4_chatfmt']\n",
    "# task_names = ['alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt'] # length-adj \n",
    "# task_names = ['mtbench_ann=gpt:4_chatfmt'] \n",
    "# task_names = task_names_alpacafarm;\n",
    "# task_names = task_names_mtbench\n",
    "# task_names = task_names + task_names_chatfmt + task_names_mtbench\n",
    "# task_names = task_names\n",
    "# task_names = ['alpacafarm_ann=gpt35:turbo:1106_chatfmt']\n",
    "\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "\n",
    "num_gpus = 1\n",
    "if arch == 'x86_64': # ccc\n",
    "    gpu_type = 'a100_80gb'; num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus)\n",
    "    use_vllm = True; torch_dtype = 'bfloat16'\n",
    "else:\n",
    "    gpu_type = 'v100'\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    use_vllm = False; torch_dtype = 'float16'\n",
    "    \n",
    "    \n",
    "if len(subdir_path_list)==0:\n",
    "    subdir_path_list = []\n",
    "    for exp_dir in exp_dirs:\n",
    "        if create_symlinks:\n",
    "            remove_all_symlinks(exp_dir)\n",
    "        subdirs = list(os.listdir(exp_dir))\n",
    "        subdirs = filter(subdir_filter_fn, subdirs)\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(exp_dir, subdir)\n",
    "            if include_checkpoints:\n",
    "                subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "            if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "                continue\n",
    "            subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "    \n",
    "dfo = pd.DataFrame(options_list, columns=['task_name', 'model_name_or_path'])\n",
    "model_and_task_list = dfo.groupby('model_name_or_path')['task_name'].agg(list).to_dict()\n",
    "\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for model_name_or_path, task_name_list in model_and_task_list.items():\n",
    "    num_tasks = len(task_name_list)\n",
    "    cmds_per_model = []\n",
    "    for i, task_name in enumerate(task_name_list):\n",
    "        queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else \\\n",
    "            ('alt_7d' if task_name.startswith('mtbench') else 'alt_1h')\n",
    "\n",
    "        use_chat_format = 'chatfmt' in task_name\n",
    "        chat_formatting_function = get_chat_formatting_function(model_name_or_path)\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "                ft_args = json.load(f)\n",
    "            # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "            # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "            if 'model_args' in ft_args:\n",
    "                ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "            else:\n",
    "                ft_args_model_name_or_path = ft_args['model_name_or_path']\n",
    "        except:\n",
    "            ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "        batch_size, job_duration = get_resource_for_task(\n",
    "            task_name, ft_args_model_name_or_path)\n",
    "\n",
    "        job_name = f'eval.{task_name}'\n",
    "        save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "\n",
    "        if task_name.startswith('mmlu'):\n",
    "            match = re.search(r's=(\\d+)', task_name)\n",
    "            n_shot = int(match.group(1))\n",
    "            assert(n_shot <= 5)\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.mmlu.run_eval \\\n",
    "                --data_dir data/eval/mmlu \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size {batch_size} \\\n",
    "                --ntrain {n_shot} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('gsm'):\n",
    "            match = re.search(r's=(\\d+)', task_name)\n",
    "            n_shot = int(match.group(1))\n",
    "            assert(n_shot <= 8)\n",
    "            # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.gsm.run_eval \\\n",
    "                --data_dir data/eval/gsm/ \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size {batch_size} \\\n",
    "                --max_num_examples 500 \\\n",
    "                --n_shot {n_shot} \\\n",
    "                --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "                {'--use_vllm' if use_vllm else ''} \\\n",
    "                {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('bbh'):\n",
    "            max_num_examples_per_task = 40\n",
    "            match = re.search(r's=(\\d+)', task_name)\n",
    "            n_shot = int(match.group(1))\n",
    "            assert(n_shot <= 3)\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.bbh.run_eval \\\n",
    "                --data_dir data/eval/bbh/ \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size {batch_size} \\\n",
    "                --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "                --n_shot {n_shot} \\\n",
    "                {'--use_vllm' if use_vllm else ''} \\\n",
    "                {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('humaneval'):\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.codex_humaneval.run_eval \\\n",
    "                --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size {batch_size} \\\n",
    "                --max_new_tokens 512 \\\n",
    "                --eval_pass_at_ks 1 \\\n",
    "                --unbiased_sampling_size_n 1 \\\n",
    "                --temperature 0.1 \\\n",
    "                {'--use_vllm' if use_vllm else ''} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('tydiqa'):\n",
    "            no_context = 'cb' in task_name\n",
    "            match = re.search(r's=(\\d+)', task_name)\n",
    "            n_shot = int(match.group(1))\n",
    "            assert(n_shot in [0,1])\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.tydiqa.run_eval \\\n",
    "                --data_dir data/eval/tydiqa \\\n",
    "                --n_shot {n_shot} \\\n",
    "                --max_num_examples_per_lang 100 \\\n",
    "                --max_context_length 512 \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size {batch_size} \\\n",
    "                {'--use_vllm' if use_vllm else ''} \\\n",
    "                {'--no_context' if no_context else ''} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('toxigen'):\n",
    "            # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "            # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.toxigen.run_eval \\\n",
    "                --data_dir data/eval/toxigen \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size 3 \\\n",
    "                --max_prompts_per_group 200 \\\n",
    "                {'--use_vllm' if use_vllm else ''} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('alpacafarm'):\n",
    "            match = re.search(r'ann=([^_]+)', task_name)\n",
    "            annotators_config = match.group(1)\n",
    "            annotators_config = annotators_config.replace(':', '_')\n",
    "            if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314', 'gpt35_turbo_1106', 'alpaca_eval_gpt4_turbo_fn', 'alpaca_eval_gpt4', 'weighted_alpaca_eval_gpt4_turbo']:\n",
    "                raise ValueError('Just support 2 annotators_config.')\n",
    "            cmd = f\"\"\"\n",
    "            python -m eval.alpaca_farm.run_eval \\\n",
    "                --reference_path alpaca_eval_data \\\n",
    "                --model_name_or_path \"{model_name_or_path}\" \\\n",
    "                --max_new_tokens 2048 \\\n",
    "                --save_dir \"{save_dir}\" \\\n",
    "                --eval_batch_size {batch_size} \\\n",
    "                --annotators_config {annotators_config} \\\n",
    "                {'--use_vllm' if use_vllm else ''} \\\n",
    "                {'--use_chat_format' if use_chat_format else ''} \\\n",
    "                --chat_formatting_function {chat_formatting_function} \\\n",
    "                {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "                --torch_dtype {torch_dtype} \\\n",
    "            \"\"\"\n",
    "        elif task_name.startswith('mtbench'):\n",
    "            assert('chatfmt' in task_name)\n",
    "            match = re.search(r'ann=([^_]+)', task_name)\n",
    "            judge_model = match.group(1).replace(':', '-')\n",
    "            if not judge_model in OPENAI_MODEL_LIST:\n",
    "                raise ValueError('fastchat does not support the judge model.')\n",
    "            os.makedirs(save_dir, exist_ok=True) # since not using python file, make the directory now.\n",
    "            fastchat_mtbench_data_dir = os.path.normpath(os.path.join(open_instruct_dir, '../FastChat/fastchat/llm_judge/data'))\n",
    "            question_file = os.path.join(fastchat_mtbench_data_dir, 'mt_bench/question.jsonl')\n",
    "            rating_file = os.path.join(save_dir, f'{judge_model}_single.jsonl')\n",
    "            question_begin, question_end = (0, 1) if False else (None, None)\n",
    "            model_id = os.path.basename(model_name_or_path) if 'results/baselines' in save_dir else 'tulu'\n",
    "            cmd = \"\"\n",
    "            cmd += f\"\"\"\n",
    "                python -m fastchat.llm_judge.gen_model_answer \\\n",
    "                    --model-path {model_name_or_path} \\\n",
    "                    --model-id {model_id} \\\n",
    "                    --bench-name mt_bench \\\n",
    "                    --question-file {question_file} \\\n",
    "                    {'--question-begin '+str(question_begin) if question_begin else ''} \\\n",
    "                    {'--question-end '+str(question_end) if question_end else ''} \\\n",
    "                    --max-new-token 2048 \\\n",
    "                    --answer-file {os.path.join(save_dir, 'model_answer.jsonl')} \\\n",
    "                    --dtype {torch_dtype} \\\n",
    "                && \\\n",
    "            \"\"\"\n",
    "            cmd += f\"\"\"\n",
    "                python -m fastchat.llm_judge.gen_judgment \\\n",
    "                    --bench-name mt_bench \\\n",
    "                    --judge-file {os.path.join(fastchat_mtbench_data_dir, 'judge_prompts.jsonl')} \\\n",
    "                    --judge-model {judge_model} \\\n",
    "                    --mode single \\\n",
    "                    --question-file {question_file} \\\n",
    "                    --answer-dir {save_dir} \\\n",
    "                    --ref-answer-dir {os.path.join(fastchat_mtbench_data_dir, 'mt_bench/reference_answer')} \\\n",
    "                    --output-file {rating_file} \\\n",
    "                && \\\n",
    "                python -m fastchat.llm_judge.show_result \\\n",
    "                    --bench-name mt_bench \\\n",
    "                    --input-file {rating_file} \\\n",
    "                    --mode single \\\n",
    "                    --save-to-json\n",
    "            \"\"\"\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'{task_name} not supported.')\n",
    "            \n",
    "        if task_name.startswith('alpacafarm') and (getpass.getuser() not in ('PTFMqngp', 'wpq')):\n",
    "            queue = 'alt_6h'\n",
    "\n",
    "        if test_run:\n",
    "            print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd.strip())]))\n",
    "\n",
    "        cmd = multiline_to_singleline(cmd)\n",
    "        cmds.append(cmd)\n",
    "        cmds_per_model.append(cmd)\n",
    "        \n",
    "        if launch_one_job_per_model:\n",
    "            shell_scripts = shell_scripts_template.format(\n",
    "                conda_env='open-instruct',\n",
    "                cwd=os.path.dirname(os.getcwd()),\n",
    "                cmd=cmd,\n",
    "                log_dir=os.getcwd(),\n",
    "                save_dir=save_dir,\n",
    "            )\n",
    "            if arch == 'x86_64': # ccc\n",
    "                shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "            out = submit_job(\n",
    "                shell_scripts, \n",
    "                job_name=job_name,\n",
    "                num_cpus=num_cpus,\n",
    "                cpu_mem=cpu_mem,\n",
    "                num_gpus=num_gpus,\n",
    "                gpu_type=gpu_type,\n",
    "                test_run=test_run,\n",
    "                job_duration=job_duration,\n",
    "                queue=queue,\n",
    "            )\n",
    "        else:\n",
    "            if i + 1 == num_tasks:\n",
    "                assert(len(cmds_per_model) == num_tasks)\n",
    "                cmd = ' && '.join(cmds_per_model)\n",
    "                if test_run:\n",
    "                    print(cmd)\n",
    "                shell_scripts = shell_scripts_template.format(\n",
    "                    conda_env='open-instruct',\n",
    "                    cwd=os.path.dirname(os.getcwd()),\n",
    "                    cmd=cmd,\n",
    "                    log_dir=os.getcwd(),\n",
    "                    save_dir=os.getcwd(), # just delete afterwards.\n",
    "                )\n",
    "                if arch == 'x86_64': # ccc\n",
    "                    shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "                out = submit_job(\n",
    "                    shell_scripts, \n",
    "                    job_name=f'eval.{os.path.basename(model_name_or_path)}',\n",
    "                    num_cpus=num_cpus,\n",
    "                    cpu_mem=cpu_mem,\n",
    "                    num_gpus=num_gpus,\n",
    "                    gpu_type=gpu_type,\n",
    "                    test_run=test_run,\n",
    "                    job_duration=6,\n",
    "                    queue=None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_6b',\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "ea7ac978",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_eval.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b3939309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06 --max_new_tokens 2048 --save_dir results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt --eval_batch_size 10 --annotators_config weighted_alpaca_eval_gpt4_turbo --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "[2024-03-26 18:42:00,142] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading data and model...\n",
      "INFO 03-26 18:42:05 llm_engine.py:73] Initializing an LLM engine with config: model='results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06', tokenizer='results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "MegaBlocks not found. Please install it by `pip install megablocks`. Note that MegaBlocks depends on mosaicml-turbo, which only supports Python 3.10 for now.\n",
      "STK not found: please see https://github.com/stanford-futuredata/stk\n",
      "INFO 03-26 18:42:32 llm_engine.py:222] # GPU blocks: 27701, # CPU blocks: 2048\n",
      "Processed prompts: 100%|██████████████████████| 805/805 [00:45<00:00, 17.63it/s]\n",
      "INFO:root:Evaluating the mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06-greedy-long outputs.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "WARNING:root:Saving_path is given but not 'auto', make sure that it's different for different seeds.\n",
      "Annotation chunk:   0%|                                   | 0/7 [00:00<?, ?it/s]INFO:root:Annotating 128 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 128 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/128 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/128 [00:00<01:18,  1.62it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▎                         | 6/128 [00:01<00:17,  6.95it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▍                         | 7/128 [00:02<00:57,  2.12it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  16%|████                      | 20/128 [00:02<00:11,  9.12it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|████▉                     | 24/128 [00:03<00:13,  7.97it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  22%|█████▋                    | 28/128 [00:03<00:10,  9.56it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▎                   | 31/128 [00:04<00:09, 10.25it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  26%|██████▋                   | 33/128 [00:04<00:10,  9.27it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  27%|███████                   | 35/128 [00:05<00:17,  5.32it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  34%|████████▋                 | 43/128 [00:06<00:13,  6.19it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  39%|██████████▏               | 50/128 [00:06<00:08,  8.83it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  41%|██████████▌               | 52/128 [00:06<00:08,  9.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  42%|██████████▉               | 54/128 [00:07<00:07,  9.70it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  44%|███████████▍              | 56/128 [00:07<00:06, 10.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▊              | 58/128 [00:07<00:08,  8.34it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  48%|████████████▍             | 61/128 [00:09<00:18,  3.55it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  56%|██████████████▋           | 72/128 [00:10<00:07,  7.43it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  60%|███████████████▋          | 77/128 [00:10<00:06,  8.48it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  62%|████████████████          | 79/128 [00:10<00:06,  7.61it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  63%|████████████████▍         | 81/128 [00:10<00:05,  8.48it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|████████████████▊         | 83/128 [00:12<00:10,  4.38it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  71%|██████████████████▍       | 91/128 [00:12<00:04,  8.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  73%|██████████████████▉       | 93/128 [00:14<00:08,  4.00it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|████████████████████▌    | 105/128 [00:14<00:02,  8.62it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  84%|█████████████████████    | 108/128 [00:15<00:03,  5.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  91%|██████████████████████▊  | 117/128 [00:16<00:01,  8.75it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  95%|███████████████████████▋ | 121/128 [00:16<00:00,  8.99it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  96%|████████████████████████ | 123/128 [00:16<00:00,  9.36it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  98%|████████████████████████▍| 125/128 [00:16<00:00,  9.88it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 128/128 [00:17<00:00,  7.36it/s]\u001b[A\n",
      "INFO:root:Completed 128 examples in 17.5 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  14%|███▊                       | 1/7 [00:17<01:45, 17.66s/it]INFO:root:Annotating 126 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 126 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/126 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/126 [00:00<01:06,  1.88it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▍                          | 2/126 [00:00<00:41,  2.96it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   3%|▊                          | 4/126 [00:01<00:34,  3.56it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   7%|█▉                         | 9/126 [00:01<00:13,  8.79it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   9%|██▎                       | 11/126 [00:02<00:18,  6.10it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  11%|██▉                       | 14/126 [00:02<00:14,  7.81it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  13%|███▎                      | 16/126 [00:02<00:14,  7.59it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  14%|███▋                      | 18/126 [00:03<00:19,  5.44it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|████▉                     | 24/126 [00:03<00:10,  9.98it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  21%|█████▎                    | 26/126 [00:03<00:12,  8.26it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  22%|█████▊                    | 28/126 [00:03<00:10,  9.08it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▏                   | 30/126 [00:04<00:15,  6.30it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  28%|███████▏                  | 35/126 [00:04<00:10,  8.61it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  29%|███████▋                  | 37/126 [00:05<00:16,  5.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  35%|█████████                 | 44/126 [00:06<00:10,  7.96it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  37%|█████████▋                | 47/126 [00:06<00:10,  7.84it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  40%|██████████▎               | 50/126 [00:06<00:08,  9.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  41%|██████████▋               | 52/126 [00:07<00:08,  8.74it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  43%|███████████▏              | 54/126 [00:08<00:15,  4.67it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▊              | 57/126 [00:09<00:18,  3.78it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  53%|█████████████▊            | 67/126 [00:09<00:07,  8.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  55%|██████████████▏           | 69/126 [00:10<00:08,  7.10it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  57%|██████████████▊           | 72/126 [00:10<00:06,  8.00it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  60%|███████████████▍          | 75/126 [00:10<00:05,  8.64it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  61%|███████████████▉          | 77/126 [00:10<00:05,  8.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  63%|████████████████▌         | 80/126 [00:11<00:04,  9.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|████████████████▉         | 82/126 [00:11<00:05,  8.30it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  66%|█████████████████▏        | 83/126 [00:11<00:05,  8.47it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  68%|█████████████████▋        | 86/126 [00:11<00:05,  7.58it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  70%|██████████████████▏       | 88/126 [00:12<00:07,  4.77it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|███████████████████▊      | 96/126 [00:12<00:02, 10.84it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  79%|████████████████████▍     | 99/126 [00:13<00:02,  9.88it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  81%|████████████████████▏    | 102/126 [00:13<00:02,  9.99it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  83%|████████████████████▋    | 104/126 [00:13<00:02,  9.59it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  84%|█████████████████████    | 106/126 [00:14<00:02,  9.34it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  86%|█████████████████████▍   | 108/126 [00:14<00:01,  9.25it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  88%|██████████████████████   | 111/126 [00:14<00:01,  8.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  90%|██████████████████████▌  | 114/126 [00:14<00:01,  9.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  92%|███████████████████████  | 116/126 [00:15<00:00, 10.07it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  94%|███████████████████████▍ | 118/126 [00:15<00:00, 10.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  95%|███████████████████████▊ | 120/126 [00:15<00:00, 10.04it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  97%|████████████████████████▏| 122/126 [00:16<00:00,  5.77it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "prompt_batches: 100%|█████████████████████████| 126/126 [00:16<00:00,  7.74it/s]\n",
      "INFO:root:Completed 126 examples in 16.5 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  29%|███████▋                   | 2/7 [00:34<01:25, 17.03s/it]INFO:root:Annotating 126 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 126 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/126 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/126 [00:00<01:00,  2.05it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▍                          | 2/126 [00:00<00:37,  3.34it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▋                          | 3/126 [00:00<00:37,  3.25it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▎                         | 6/126 [00:01<00:15,  7.56it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:   7%|█▉                         | 9/126 [00:01<00:16,  7.29it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   9%|██▎                       | 11/126 [00:02<00:27,  4.25it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  13%|███▌                      | 17/126 [00:02<00:14,  7.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  16%|████▏                     | 20/126 [00:02<00:11,  9.31it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▌                     | 22/126 [00:03<00:13,  7.99it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|████▉                     | 24/126 [00:03<00:18,  5.54it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  23%|█████▉                    | 29/126 [00:04<00:10,  9.06it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  25%|██████▍                   | 31/126 [00:04<00:10,  9.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  26%|██████▊                   | 33/126 [00:04<00:10,  9.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  28%|███████▏                  | 35/126 [00:05<00:14,  6.50it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  29%|███████▋                  | 37/126 [00:05<00:18,  4.80it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  35%|█████████                 | 44/126 [00:06<00:09,  8.98it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  37%|█████████▋                | 47/126 [00:06<00:09,  8.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  39%|██████████                | 49/126 [00:06<00:08,  9.29it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  41%|██████████▋               | 52/126 [00:07<00:08,  8.45it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  44%|███████████▎              | 55/126 [00:08<00:18,  3.93it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  53%|█████████████▊            | 67/126 [00:09<00:06,  9.13it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  55%|██████████████▏           | 69/126 [00:09<00:06,  8.62it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  57%|██████████████▊           | 72/126 [00:09<00:05,  9.32it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  59%|███████████████▎          | 74/126 [00:09<00:05,  9.83it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  60%|███████████████▋          | 76/126 [00:10<00:07,  6.36it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  63%|████████████████▌         | 80/126 [00:10<00:05,  8.97it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  66%|█████████████████▏        | 83/126 [00:10<00:04, 10.62it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  67%|█████████████████▌        | 85/126 [00:11<00:04,  8.31it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  69%|█████████████████▉        | 87/126 [00:11<00:04,  9.20it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  71%|██████████████████▎       | 89/126 [00:11<00:04,  7.45it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  75%|███████████████████▍      | 94/126 [00:12<00:04,  7.68it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  77%|████████████████████      | 97/126 [00:12<00:03,  7.32it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  79%|███████████████████▊     | 100/126 [00:13<00:04,  5.48it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  85%|█████████████████████▏   | 107/126 [00:14<00:02,  9.38it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_batches:  87%|█████████████████████▋   | 109/126 [00:14<00:01,  9.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  88%|██████████████████████   | 111/126 [00:15<00:02,  5.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  90%|██████████████████████▍  | 113/126 [00:15<00:02,  4.79it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  95%|███████████████████████▊ | 120/126 [00:16<00:00,  8.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  98%|████████████████████████▍| 123/126 [00:16<00:00,  9.08it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  99%|████████████████████████▊| 125/126 [00:16<00:00,  9.44it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "prompt_batches: 100%|█████████████████████████| 126/126 [00:16<00:00,  7.61it/s]\n",
      "INFO:root:Completed 126 examples in 16.7 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  43%|███████████▌               | 3/7 [00:51<01:07, 16.94s/it]INFO:root:Annotating 127 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 127 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/127 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/127 [00:00<01:10,  1.79it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▎                         | 6/127 [00:00<00:16,  7.13it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   6%|█▍                         | 7/127 [00:03<01:08,  1.74it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  20%|█████                     | 25/127 [00:03<00:11,  8.96it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  22%|█████▋                    | 28/127 [00:03<00:10,  9.66it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▏                   | 30/127 [00:04<00:11,  8.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  25%|██████▌                   | 32/127 [00:07<00:31,  3.02it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  42%|██████████▊               | 53/127 [00:07<00:07,  9.54it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▋              | 57/127 [00:08<00:08,  8.18it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  47%|████████████▎             | 60/127 [00:08<00:08,  8.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  49%|████████████▋             | 62/127 [00:08<00:08,  7.80it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  51%|█████████████▎            | 65/127 [00:09<00:07,  8.13it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  53%|█████████████▋            | 67/127 [00:09<00:08,  7.38it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  55%|██████████████▎           | 70/127 [00:09<00:06,  9.18it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  57%|██████████████▉           | 73/127 [00:10<00:10,  5.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  63%|████████████████▍         | 80/127 [00:11<00:05,  8.19it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  66%|█████████████████▏        | 84/127 [00:11<00:04,  9.76it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  68%|█████████████████▌        | 86/127 [00:12<00:05,  7.67it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  72%|██████████████████▋       | 91/127 [00:12<00:03,  9.08it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  73%|███████████████████       | 93/127 [00:12<00:03,  9.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  75%|███████████████████▍      | 95/127 [00:12<00:03,  8.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|███████████████████▋      | 96/127 [00:13<00:03,  8.21it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  77%|████████████████████      | 98/127 [00:17<00:18,  1.58it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 127/127 [00:17<00:00,  7.32it/s]\u001b[A\n",
      "INFO:root:Completed 127 examples in 17.5 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  57%|███████████████▍           | 4/7 [01:08<00:51, 17.22s/it]INFO:root:Annotating 125 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 125 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/125 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/125 [00:00<01:04,  1.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▍                          | 2/125 [00:00<00:34,  3.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▋                          | 3/125 [00:01<00:42,  2.88it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   6%|█▌                         | 7/125 [00:01<00:15,  7.76it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   8%|██                        | 10/125 [00:01<00:12,  9.15it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  10%|██▍                       | 12/125 [00:01<00:14,  7.76it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  12%|███                       | 15/125 [00:02<00:11,  9.93it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  14%|███▌                      | 17/125 [00:02<00:19,  5.50it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▎                     | 21/125 [00:02<00:12,  8.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|████▉                     | 24/125 [00:03<00:10, 10.00it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  21%|█████▍                    | 26/125 [00:03<00:11,  8.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  22%|█████▊                    | 28/125 [00:04<00:19,  4.99it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  26%|██████▊                   | 33/125 [00:04<00:13,  6.68it/s]WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  31%|████████                  | 39/125 [00:05<00:09,  9.38it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  33%|████████▌                 | 41/125 [00:05<00:09,  8.44it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  35%|█████████▏                | 44/125 [00:05<00:08,  9.58it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  37%|█████████▌                | 46/125 [00:05<00:08,  9.03it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  38%|█████████▉                | 48/125 [00:06<00:07,  9.94it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  40%|██████████▍               | 50/125 [00:06<00:11,  6.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  43%|███████████▏              | 54/125 [00:07<00:11,  6.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  47%|████████████▎             | 59/125 [00:07<00:07,  8.44it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  50%|████████████▉             | 62/125 [00:07<00:06, 10.05it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  51%|█████████████▎            | 64/125 [00:08<00:06,  8.95it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  53%|█████████████▋            | 66/125 [00:08<00:06,  9.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  54%|██████████████▏           | 68/125 [00:08<00:06,  8.65it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  56%|██████████████▌           | 70/125 [00:08<00:05,  9.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  58%|███████████████▏          | 73/125 [00:09<00:05,  8.98it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  59%|███████████████▍          | 74/125 [00:09<00:07,  7.27it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  62%|████████████████▏         | 78/125 [00:09<00:05,  8.69it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  64%|████████████████▋         | 80/125 [00:09<00:04,  9.96it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  66%|█████████████████         | 82/125 [00:10<00:05,  8.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  66%|█████████████████▎        | 83/125 [00:11<00:08,  4.77it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  72%|██████████████████▋       | 90/125 [00:11<00:03, 10.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_batches:  74%|███████████████████▏      | 92/125 [00:11<00:03,  8.67it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|███████████████████▊      | 95/125 [00:11<00:02, 10.95it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  78%|████████████████████▏     | 97/125 [00:11<00:02, 10.40it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  79%|████████████████████▌     | 99/125 [00:12<00:02, 10.61it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  81%|████████████████████▏    | 101/125 [00:12<00:02,  8.61it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|████████████████████▌    | 103/125 [00:12<00:02,  8.76it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  85%|█████████████████████▏   | 106/125 [00:12<00:02,  8.59it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  87%|█████████████████████▊   | 109/125 [00:13<00:01,  9.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  89%|██████████████████████▏  | 111/125 [00:13<00:02,  6.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  93%|███████████████████████▏ | 116/125 [00:15<00:01,  5.09it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 125/125 [00:15<00:00,  8.07it/s]\u001b[A\n",
      "INFO:root:Completed 125 examples in 15.7 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  71%|███████████████████▎       | 5/7 [01:24<00:33, 16.71s/it]INFO:root:Annotating 119 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 119 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/119 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/119 [00:00<01:16,  1.55it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▎                         | 6/119 [00:00<00:14,  7.56it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   7%|█▊                         | 8/119 [00:01<00:13,  8.34it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:   8%|██▏                       | 10/119 [00:01<00:11,  9.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  10%|██▌                       | 12/119 [00:01<00:14,  7.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  11%|██▊                       | 13/119 [00:02<00:31,  3.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  14%|███▋                      | 17/119 [00:04<00:41,  2.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  30%|███████▊                  | 36/119 [00:06<00:12,  6.80it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  36%|█████████▍                | 43/119 [00:06<00:08,  9.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  38%|█████████▊                | 45/119 [00:06<00:07,  9.48it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  39%|██████████▎               | 47/119 [00:06<00:07,  9.69it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  41%|██████████▋               | 49/119 [00:06<00:07,  9.04it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  43%|███████████▏              | 51/119 [00:07<00:06,  9.87it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▌              | 53/119 [00:07<00:06, 10.70it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  46%|████████████              | 55/119 [00:07<00:08,  7.66it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  48%|████████████▍             | 57/119 [00:08<00:09,  6.45it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  50%|█████████████             | 60/119 [00:08<00:07,  7.98it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  52%|█████████████▌            | 62/119 [00:08<00:06,  8.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  54%|█████████████▉            | 64/119 [00:08<00:06,  8.81it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  55%|██████████████▍           | 66/119 [00:09<00:06,  8.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  58%|███████████████           | 69/119 [00:09<00:04, 10.74it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  60%|███████████████▌          | 71/119 [00:11<00:15,  3.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  72%|██████████████████▊       | 86/119 [00:11<00:03, 10.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|███████████████████▋      | 90/119 [00:11<00:02, 10.31it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  78%|████████████████████▎     | 93/119 [00:12<00:02,  9.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  81%|████████████████████▉     | 96/119 [00:12<00:02,  9.79it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|█████████████████████▍    | 98/119 [00:12<00:02,  9.09it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  84%|█████████████████████    | 100/119 [00:12<00:01,  9.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  86%|█████████████████████▍   | 102/119 [00:13<00:02,  7.18it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  88%|██████████████████████   | 105/119 [00:13<00:01,  9.16it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  90%|██████████████████████▍  | 107/119 [00:13<00:01,  8.88it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  92%|██████████████████████▉  | 109/119 [00:13<00:01,  9.39it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  93%|███████████████████████▎ | 111/119 [00:14<00:00,  8.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  95%|███████████████████████▋ | 113/119 [00:14<00:00,  8.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  97%|████████████████████████▏| 115/119 [00:14<00:00,  8.45it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  97%|████████████████████████▎| 116/119 [00:14<00:00,  8.17it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 119/119 [00:15<00:00,  7.88it/s]\u001b[A\n",
      "INFO:root:Completed 119 examples in 15.3 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  86%|███████████████████████▏   | 6/7 [01:39<00:16, 16.26s/it]INFO:root:Annotating 37 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 37 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                    | 0/37 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:   3%|▊                           | 1/37 [00:00<00:16,  2.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▌                          | 2/37 [00:00<00:09,  3.65it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  16%|████▌                       | 6/37 [00:02<00:11,  2.78it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  30%|████████                   | 11/37 [00:02<00:04,  5.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  46%|████████████▍              | 17/37 [00:02<00:02,  9.59it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  51%|█████████████▊             | 19/37 [00:03<00:02,  7.22it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  59%|████████████████           | 22/37 [00:03<00:01,  8.81it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|█████████████████▌         | 24/37 [00:03<00:01,  7.71it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  78%|█████████████████████▏     | 29/37 [00:04<00:01,  6.96it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  86%|███████████████████████▎   | 32/37 [00:04<00:00,  6.80it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|███████████████████████████| 37/37 [00:05<00:00,  7.05it/s]\u001b[A\n",
      "INFO:root:Completed 37 examples in 5.4 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation chunk: 100%|███████████████████████████| 7/7 [01:45<00:00, 15.06s/it]\n",
      "INFO:root:Saving all results to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo\n",
      "Price (per-example / total) = 0.0051 / 4.04\n",
      "Time  (per-example / total) = 0.1327 / 104.61\n",
      "                                                     model  win_rate  standard_error       mode  avg_length  n_wins  n_wins_base  n_draws  n_total  discrete_win_rate  length_controlled_winrate  avg_output_tok_length  price\n",
      "0  mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06-greedy-long     35.61            1.52  community         425     282          506       17      805              36.09                      39.10                 110.06   4.04\n",
      "ann_file=results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/annotations.json does not exist.\n",
      "=1 ann_files found in sub-directories. ann_file=results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=2e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/annotations.json\n",
      "Map:   0%|                                       | 0/805 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2315 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|███████████████████████████| 805/805 [00:00<00:00, 1594.11 examples/s]\n",
      "/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/note_pruning_analysis.py:1008: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  win_rate = (n_wins/n_total + .5*n_draws/n_total)\n",
      "Filter (num_proc=4): 100%|███████████| 805/805 [00:00<00:00, 3081.56 examples/s]\n",
      "Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 143.24ba/s]\n",
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06 --max_new_tokens 2048 --save_dir results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt --eval_batch_size 10 --annotators_config weighted_alpaca_eval_gpt4_turbo --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "[2024-03-26 18:45:16,399] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading data and model...\n",
      "INFO 03-26 18:45:19 llm_engine.py:73] Initializing an LLM engine with config: model='results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06', tokenizer='results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "MegaBlocks not found. Please install it by `pip install megablocks`. Note that MegaBlocks depends on mosaicml-turbo, which only supports Python 3.10 for now.\n",
      "STK not found: please see https://github.com/stanford-futuredata/stk\n",
      "INFO 03-26 18:45:40 llm_engine.py:222] # GPU blocks: 27701, # CPU blocks: 2048\n",
      "Processed prompts: 100%|██████████████████████| 805/805 [00:42<00:00, 18.90it/s]\n",
      "INFO:root:Evaluating the mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06-greedy-long outputs.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "WARNING:root:Saving_path is given but not 'auto', make sure that it's different for different seeds.\n",
      "Annotation chunk:   0%|                                   | 0/7 [00:00<?, ?it/s]INFO:root:Annotating 127 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 127 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/127 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/127 [00:00<01:15,  1.66it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▋                          | 3/127 [00:01<01:17,  1.59it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  10%|██▋                       | 13/127 [00:02<00:14,  7.77it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  14%|███▋                      | 18/127 [00:02<00:11,  9.15it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  16%|████                      | 20/127 [00:02<00:10, 10.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▌                     | 22/127 [00:02<00:09, 10.75it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|████▉                     | 24/127 [00:03<00:09, 10.75it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  20%|█████▎                    | 26/127 [00:03<00:12,  8.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  22%|█████▋                    | 28/127 [00:06<00:42,  2.31it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  38%|█████████▊                | 48/127 [00:06<00:07, 10.22it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  43%|███████████               | 54/127 [00:06<00:07, 10.09it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  46%|███████████▊              | 58/127 [00:07<00:06, 10.85it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  49%|████████████▋             | 62/127 [00:07<00:06,  9.54it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  51%|█████████████▎            | 65/127 [00:08<00:07,  8.71it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  54%|██████████████▏           | 69/127 [00:08<00:05, 10.00it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  56%|██████████████▌           | 71/127 [00:08<00:06,  8.89it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  57%|██████████████▉           | 73/127 [00:08<00:05,  9.64it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  59%|███████████████▎          | 75/127 [00:09<00:08,  6.42it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  64%|████████████████▌         | 81/127 [00:10<00:05,  8.90it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|████████████████▉         | 83/127 [00:10<00:04,  9.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  68%|█████████████████▌        | 86/127 [00:10<00:04,  8.87it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  70%|██████████████████▏       | 89/127 [00:10<00:04,  8.72it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  72%|██████████████████▋       | 91/127 [00:11<00:03,  9.16it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  75%|███████████████████▍      | 95/127 [00:11<00:03, 10.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|███████████████████▊      | 97/127 [00:12<00:04,  6.59it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  77%|████████████████████      | 98/127 [00:12<00:04,  6.73it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  80%|████████████████████     | 102/127 [00:12<00:02,  8.70it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|████████████████████▍    | 104/127 [00:12<00:02,  9.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  83%|████████████████████▊    | 106/127 [00:12<00:02,  9.85it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  85%|█████████████████████▎   | 108/127 [00:13<00:02,  8.05it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  87%|█████████████████████▋   | 110/127 [00:13<00:02,  8.45it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  89%|██████████████████████▏  | 113/127 [00:13<00:01,  8.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  91%|██████████████████████▊  | 116/127 [00:13<00:01, 10.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  93%|███████████████████████▏ | 118/127 [00:14<00:01,  8.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  94%|███████████████████████▌ | 120/127 [00:14<00:00,  7.42it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  96%|████████████████████████ | 122/127 [00:14<00:00,  7.66it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  98%|████████████████████████▌| 125/127 [00:15<00:00,  9.02it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 127/127 [00:15<00:00,  8.26it/s]\u001b[A\n",
      "INFO:root:Completed 127 examples in 15.5 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  14%|███▊                       | 1/7 [00:15<01:33, 15.62s/it]INFO:root:Annotating 126 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 126 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/126 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/126 [00:00<01:53,  1.10it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▎                         | 6/126 [00:01<00:23,  5.02it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   7%|█▉                         | 9/126 [00:01<00:15,  7.75it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   9%|██▎                       | 11/126 [00:01<00:13,  8.62it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  10%|██▋                       | 13/126 [00:02<00:17,  6.39it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  12%|███                       | 15/126 [00:02<00:14,  7.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_batches:  13%|███▌                      | 17/126 [00:02<00:12,  8.42it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  15%|███▉                      | 19/126 [00:03<00:25,  4.21it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  21%|█████▌                    | 27/126 [00:03<00:10,  9.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  23%|█████▉                    | 29/126 [00:04<00:10,  8.86it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  25%|██████▌                   | 32/126 [00:04<00:10,  9.02it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  27%|███████                   | 34/126 [00:04<00:09, 10.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  29%|███████▍                  | 36/126 [00:04<00:08, 10.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  30%|███████▊                  | 38/126 [00:04<00:08, 10.10it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  32%|████████▎                 | 40/126 [00:05<00:10,  8.36it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  33%|████████▋                 | 42/126 [00:05<00:09,  9.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  35%|█████████                 | 44/126 [00:06<00:19,  4.27it/s]WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  40%|██████████▌               | 51/126 [00:07<00:11,  6.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  43%|███████████▏              | 54/126 [00:07<00:08,  8.00it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▊              | 57/126 [00:07<00:07,  8.74it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  47%|████████████▏             | 59/126 [00:08<00:09,  7.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  49%|████████████▊             | 62/126 [00:08<00:07,  8.90it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  51%|█████████████▏            | 64/126 [00:08<00:07,  8.64it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  53%|█████████████▊            | 67/126 [00:08<00:05, 10.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  55%|██████████████▏           | 69/126 [00:08<00:05, 10.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  56%|██████████████▋           | 71/126 [00:09<00:06,  8.54it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  58%|███████████████           | 73/126 [00:09<00:08,  6.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  63%|████████████████▎         | 79/126 [00:10<00:05,  8.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  64%|████████████████▋         | 81/126 [00:10<00:04,  9.32it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  66%|█████████████████▏        | 83/126 [00:10<00:04, 10.27it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  67%|█████████████████▌        | 85/126 [00:10<00:04,  9.38it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  69%|█████████████████▉        | 87/126 [00:11<00:05,  7.65it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  72%|██████████████████▊       | 91/126 [00:11<00:03, 10.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  74%|███████████████████▏      | 93/126 [00:11<00:03,  8.46it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  75%|███████████████████▌      | 95/126 [00:12<00:04,  7.07it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  77%|████████████████████      | 97/126 [00:12<00:04,  7.16it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  79%|████████████████████▍     | 99/126 [00:12<00:03,  7.85it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  81%|████████████████████▏    | 102/126 [00:12<00:02, 10.15it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  83%|████████████████████▋    | 104/126 [00:15<00:10,  2.03it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  93%|███████████████████████▏ | 117/126 [00:17<00:02,  4.45it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  94%|███████████████████████▌ | 119/126 [00:18<00:01,  4.22it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 126/126 [00:22<00:00,  5.57it/s]\u001b[A\n",
      "INFO:root:Completed 126 examples in 22.8 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  29%|███████▋                   | 2/7 [00:38<01:39, 19.91s/it]INFO:root:Annotating 126 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 126 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/126 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:   1%|▏                          | 1/126 [00:00<01:05,  1.90it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▍                          | 2/126 [00:00<00:40,  3.09it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▎                         | 6/126 [00:01<00:18,  6.47it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   6%|█▌                         | 7/126 [00:01<00:20,  5.69it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   6%|█▋                         | 8/126 [00:02<00:40,  2.95it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  13%|███▎                      | 16/126 [00:02<00:13,  8.30it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  16%|████▏                     | 20/126 [00:02<00:10, 10.16it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▌                     | 22/126 [00:03<00:10,  9.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|████▉                     | 24/126 [00:03<00:11,  8.83it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  21%|█████▎                    | 26/126 [00:04<00:16,  5.96it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▏                   | 30/126 [00:04<00:12,  7.81it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  26%|██████▊                   | 33/126 [00:04<00:10,  9.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  28%|███████▏                  | 35/126 [00:04<00:09, 10.06it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  29%|███████▋                  | 37/126 [00:04<00:09,  9.79it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  31%|████████                  | 39/126 [00:05<00:10,  8.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  33%|████████▋                 | 42/126 [00:05<00:10,  8.22it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  36%|█████████▎                | 45/126 [00:06<00:10,  7.71it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  37%|█████████▋                | 47/126 [00:06<00:09,  7.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  40%|██████████▎               | 50/126 [00:07<00:13,  5.78it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▊              | 57/126 [00:13<00:36,  1.87it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  80%|████████████████████     | 101/126 [00:13<00:02,  9.64it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  84%|█████████████████████    | 106/126 [00:14<00:02,  9.52it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  88%|██████████████████████   | 111/126 [00:14<00:01,  9.96it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  90%|██████████████████████▍  | 113/126 [00:14<00:01, 10.34it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  91%|██████████████████████▊  | 115/126 [00:14<00:01, 10.04it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  93%|███████████████████████▏ | 117/126 [00:15<00:00,  9.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  95%|███████████████████████▊ | 120/126 [00:15<00:00,  9.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  97%|████████████████████████▏| 122/126 [00:15<00:00, 10.26it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  98%|████████████████████████▌| 124/126 [00:15<00:00, 10.61it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 126/126 [00:16<00:00,  7.81it/s]\u001b[A\n",
      "INFO:root:Completed 126 examples in 16.3 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  43%|███████████▌               | 3/7 [00:54<01:13, 18.31s/it]INFO:root:Annotating 125 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 125 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/125 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/125 [00:00<01:21,  1.53it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▋                          | 3/125 [00:01<00:55,  2.21it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   8%|██                        | 10/125 [00:01<00:12,  9.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  10%|██▋                       | 13/125 [00:01<00:10, 11.01it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  13%|███▎                      | 16/125 [00:02<00:13,  8.03it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  15%|███▉                      | 19/125 [00:02<00:11,  9.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▎                     | 21/125 [00:02<00:09, 10.54it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  18%|████▊                     | 23/125 [00:02<00:11,  8.82it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  20%|█████▏                    | 25/125 [00:03<00:11,  8.74it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  22%|█████▌                    | 27/125 [00:03<00:11,  8.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  23%|██████                    | 29/125 [00:03<00:10,  9.15it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  26%|██████▋                   | 32/125 [00:03<00:10,  8.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  30%|███████▋                  | 37/125 [00:05<00:14,  6.15it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  31%|████████                  | 39/125 [00:05<00:12,  6.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  34%|████████▉                 | 43/125 [00:05<00:08,  9.35it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  36%|█████████▎                | 45/125 [00:05<00:11,  7.21it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  39%|██████████▏               | 49/125 [00:06<00:11,  6.70it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  45%|███████████▋              | 56/125 [00:06<00:06, 11.48it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  47%|████████████▎             | 59/125 [00:07<00:07,  9.00it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  50%|████████████▉             | 62/125 [00:07<00:06,  9.81it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  51%|█████████████▎            | 64/125 [00:07<00:06,  9.52it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  54%|█████████████▉            | 67/125 [00:07<00:05, 10.69it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  55%|██████████████▎           | 69/125 [00:08<00:06,  8.88it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  57%|██████████████▊           | 71/125 [00:08<00:05, 10.17it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  58%|███████████████▏          | 73/125 [00:08<00:07,  7.33it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  62%|████████████████▏         | 78/125 [00:10<00:08,  5.40it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  64%|████████████████▋         | 80/125 [00:10<00:07,  5.77it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  69%|█████████████████▉        | 86/125 [00:10<00:05,  7.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  71%|██████████████████▌       | 89/125 [00:12<00:07,  4.53it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  80%|████████████████████     | 100/125 [00:12<00:02,  8.68it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|████████████████████▍    | 102/125 [00:13<00:02,  8.55it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  84%|█████████████████████    | 105/125 [00:13<00:02,  9.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  86%|█████████████████████▍   | 107/125 [00:13<00:02,  8.66it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  87%|█████████████████████▊   | 109/125 [00:13<00:01,  9.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  90%|██████████████████████▍  | 112/125 [00:14<00:01,  8.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  92%|███████████████████████  | 115/125 [00:14<00:01,  9.87it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  94%|███████████████████████▍ | 117/125 [00:14<00:00,  9.93it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  95%|███████████████████████▊ | 119/125 [00:14<00:00, 10.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  97%|████████████████████████▏| 121/125 [00:14<00:00, 11.14it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  98%|████████████████████████▌| 123/125 [00:15<00:00,  7.98it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 125/125 [00:15<00:00,  8.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Completed 125 examples in 15.5 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  57%|███████████████▍           | 4/7 [01:10<00:51, 17.26s/it]INFO:root:Annotating 126 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 126 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/126 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/126 [00:00<01:22,  1.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   4%|█                          | 5/126 [00:00<00:15,  7.85it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   6%|█▌                         | 7/126 [00:01<00:21,  5.49it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   9%|██▎                       | 11/126 [00:01<00:15,  7.22it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  11%|██▉                       | 14/126 [00:01<00:11,  9.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  13%|███▎                      | 16/126 [00:02<00:18,  5.91it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▎                     | 21/126 [00:02<00:12,  8.27it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  18%|████▋                     | 23/126 [00:03<00:12,  8.21it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  21%|█████▎                    | 26/126 [00:03<00:11,  8.41it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  23%|█████▉                    | 29/126 [00:03<00:12,  7.77it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  25%|██████▌                   | 32/126 [00:04<00:09,  9.89it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  27%|███████                   | 34/126 [00:04<00:11,  7.68it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  30%|███████▊                  | 38/126 [00:04<00:07, 11.04it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  32%|████████▎                 | 40/126 [00:05<00:10,  7.84it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  35%|█████████                 | 44/126 [00:05<00:08,  9.65it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  37%|█████████▍                | 46/126 [00:05<00:10,  7.97it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  39%|██████████                | 49/126 [00:05<00:07, 10.05it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  40%|██████████▌               | 51/126 [00:06<00:09,  7.71it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  42%|██████████▉               | 53/126 [00:06<00:08,  8.93it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  44%|███████████▌              | 56/126 [00:06<00:08,  8.51it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  46%|███████████▉              | 58/126 [00:07<00:06,  9.85it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  48%|████████████▍             | 60/126 [00:08<00:15,  4.39it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  51%|█████████████▏            | 64/126 [00:08<00:08,  6.98it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  54%|██████████████            | 68/126 [00:08<00:06,  9.32it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  56%|██████████████▍           | 70/126 [00:09<00:08,  6.86it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  59%|███████████████▎          | 74/126 [00:09<00:05,  8.80it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  60%|███████████████▋          | 76/126 [00:09<00:05,  9.14it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  62%|████████████████          | 78/126 [00:09<00:04, 10.07it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  63%|████████████████▌         | 80/126 [00:09<00:04,  9.54it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|████████████████▉         | 82/126 [00:10<00:05,  8.75it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  67%|█████████████████▎        | 84/126 [00:10<00:04, 10.22it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  68%|█████████████████▋        | 86/126 [00:10<00:04,  9.92it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  70%|██████████████████▏       | 88/126 [00:10<00:05,  7.13it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  71%|██████████████████▌       | 90/126 [00:11<00:04,  8.23it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  73%|██████████████████▉       | 92/126 [00:11<00:03,  9.19it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  75%|███████████████████▍      | 94/126 [00:11<00:04,  7.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  77%|████████████████████      | 97/126 [00:11<00:02, 10.29it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  79%|████████████████████▍     | 99/126 [00:12<00:03,  8.80it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|████████████████████▍    | 103/126 [00:12<00:02, 10.18it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  83%|████████████████████▊    | 105/126 [00:12<00:02, 10.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  86%|█████████████████████▍   | 108/126 [00:13<00:02,  8.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_batches:  88%|██████████████████████   | 111/126 [00:13<00:01,  8.43it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  90%|██████████████████████▍  | 113/126 [00:13<00:01,  7.86it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  93%|███████████████████████▏ | 117/126 [00:14<00:00,  9.25it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  94%|███████████████████████▌ | 119/126 [00:14<00:00,  7.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  98%|████████████████████████▍| 123/126 [00:14<00:00,  9.82it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 126/126 [00:15<00:00,  8.09it/s]\u001b[A\n",
      "INFO:root:Completed 126 examples in 15.7 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  71%|███████████████████▎       | 5/7 [01:26<00:33, 16.75s/it]INFO:root:Annotating 119 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 119 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/119 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/119 [00:00<01:18,  1.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   2%|▍                          | 2/119 [00:02<02:29,  1.28s/it]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  13%|███▍                      | 16/119 [00:02<00:11,  8.86it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  17%|████▎                     | 20/119 [00:02<00:11,  8.97it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  19%|█████                     | 23/119 [00:03<00:11,  8.64it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  21%|█████▍                    | 25/119 [00:03<00:10,  8.94it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  23%|█████▉                    | 27/119 [00:03<00:09,  9.30it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▎                   | 29/119 [00:04<00:10,  8.47it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  28%|███████▏                  | 33/119 [00:04<00:07, 11.15it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  29%|███████▋                  | 35/119 [00:04<00:09,  8.69it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  32%|████████▎                 | 38/119 [00:06<00:22,  3.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  39%|██████████                | 46/119 [00:06<00:10,  7.19it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  44%|███████████▎              | 52/119 [00:06<00:06, 10.41it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  46%|████████████              | 55/119 [00:07<00:06,  9.30it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  49%|████████████▋             | 58/119 [00:07<00:07,  8.04it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  52%|█████████████▌            | 62/119 [00:08<00:05,  9.53it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  54%|█████████████▉            | 64/119 [00:08<00:06,  9.05it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  56%|██████████████▋           | 67/119 [00:08<00:05,  9.02it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  59%|███████████████▎          | 70/119 [00:08<00:04, 10.32it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  61%|███████████████▋          | 72/119 [00:09<00:04,  9.55it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  62%|████████████████▏         | 74/119 [00:09<00:04,  9.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|████████████████▊         | 77/119 [00:10<00:05,  7.24it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  69%|█████████████████▉        | 82/119 [00:10<00:03,  9.46it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  71%|██████████████████▎       | 84/119 [00:10<00:03,  9.55it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  72%|██████████████████▊       | 86/119 [00:10<00:03, 10.60it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  74%|███████████████████▏      | 88/119 [00:10<00:03,  9.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|███████████████████▉      | 91/119 [00:11<00:02, 11.05it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  78%|████████████████████▎     | 93/119 [00:11<00:02,  9.55it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  80%|████████████████████▊     | 95/119 [00:11<00:02, 10.54it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  82%|█████████████████████▏    | 97/119 [00:12<00:03,  7.12it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  83%|█████████████████████▋    | 99/119 [00:12<00:02,  7.94it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  85%|█████████████████████▏   | 101/119 [00:15<00:09,  1.94it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|█████████████████████████| 119/119 [00:16<00:00,  7.15it/s]\u001b[A\n",
      "INFO:root:Completed 119 examples in 16.8 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk:  86%|███████████████████████▏   | 6/7 [01:43<00:16, 16.81s/it]INFO:root:Annotating 37 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 37 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                    | 0/37 [00:00<?, ?it/s]\u001b[AWARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   3%|▊                           | 1/37 [00:00<00:22,  1.57it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   8%|██▎                         | 3/37 [00:00<00:07,  4.62it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  16%|████▌                       | 6/37 [00:01<00:04,  6.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▊                     | 9/37 [00:02<00:08,  3.25it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  51%|█████████████▊             | 19/37 [00:02<00:02,  8.51it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  57%|███████████████▎           | 21/37 [00:03<00:01,  8.97it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  65%|█████████████████▌         | 24/37 [00:03<00:01,  8.33it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  76%|████████████████████▍      | 28/37 [00:03<00:00, 10.28it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  81%|█████████████████████▉     | 30/37 [00:04<00:00,  8.36it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/dccstor/data-pruning/wpq/github/mitibm2023/external/alpaca_eval/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  89%|████████████████████████   | 33/37 [00:04<00:00,  9.94it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  95%|█████████████████████████▌ | 35/37 [00:04<00:00,  7.72it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches: 100%|███████████████████████████| 37/37 [00:05<00:00,  7.35it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Completed 37 examples in 5.2 seconds.\n",
      "INFO:root:Saving all annotations to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "INFO:root:Loading all annotations from results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk: 100%|███████████████████████████| 7/7 [01:48<00:00, 15.52s/it]\n",
      "INFO:root:Saving all results to results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo\n",
      "Price (per-example / total) = 0.0050 / 3.96\n",
      "Time  (per-example / total) = 0.1372 / 107.80\n",
      "                                                     model  win_rate  standard_error       mode  avg_length  n_wins  n_wins_base  n_draws  n_total  discrete_win_rate  length_controlled_winrate  avg_output_tok_length  price\n",
      "0  mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06-greedy-long     34.63            1.51  community         379     263          521       21      805              33.98                      39.09                  97.41   3.96\n",
      "ann_file=results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/annotations.json does not exist.\n",
      "=1 ann_files found in sub-directories. ann_file=results/oi3/mistral-7b_stanford_alpaca50k_ep=3_lr=4e-06/eval/alpacafarm_ann=weighted:alpaca:eval:gpt4:turbo_chatfmt/weighted_alpaca_eval_gpt4_turbo/annotations.json\n",
      "Map:  49%|█████████████              | 391/805 [00:00<00:00, 1932.31 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2091 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|███████████████████████████| 805/805 [00:00<00:00, 1876.10 examples/s]\n",
      "Filter (num_proc=4): 100%|███████████| 805/805 [00:00<00:00, 3118.91 examples/s]\n",
      "Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 197.02ba/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_eval.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859d6d6",
   "metadata": {},
   "source": [
    "# Visualize Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1b033ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move ./1397708.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/bbh_s=3_cot_chatfmt/1397708.out.lsf\n",
      "Job ./1388986.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=10000:ep=10/eval/mmlu_s=0_chatfmt\n",
      "Job ./1389187.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=0_chatfmt\n",
      "Move ./1397603.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=60000:ep=3/eval/mmlu_s=5/1397603.out.lsf\n",
      "Move ./1397594.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=5_chatfmt/1397594.out.lsf\n",
      "Move ./1398282.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp/1398282.out.lsf\n",
      "Job ./1397600.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb_chatfmt\n",
      "Move ./1397652.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/bbh_s=3/1397652.out.lsf\n",
      "Job ./1394502.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=0\n",
      "Job ./1396996.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt\n",
      "./1394831.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1396995.out exited with error code. --save_dir=results/oi5_oasst2:llama-7b/llama-7b_oasst2_score=random:s=0_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt\n",
      "Move ./1398502.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt/1398502.out.lsf\n",
      "Move ./1397699.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/bbh_s=3_cot/1397699.out.lsf\n",
      "Move ./1397702.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/tydiqa_s=1_gp/1397702.out.lsf\n",
      "Move ./1398283.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb/1398283.out.lsf\n",
      "Job ./1397663.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/humaneval_chatfmt\n",
      "Job ./1388963.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/bbh_s=3\n",
      "Job ./1389159.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp\n",
      "Job ./1397561.out exited with error code. --save_dir=results/oi5_ultrachat50k:llama-7b/llama-7b_ultrachat50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/gsm_s=8_cot\n",
      "./1388921.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1388458.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/mmlu_s=0\n",
      "./1388192.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1397164.out exited with error code. --save_dir=results/oi5_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/mmlu_s=5\n",
      "Move ./1397678.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=10000:ep=10/eval/humaneval_chatfmt/1397678.out.lsf\n",
      "Job ./1397644.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot_chatfmt\n",
      "Job ./1397642.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot_chatfmt\n",
      "./1394835.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1393671.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/humaneval\n",
      "Job ./1397168.out exited with error code. --save_dir=results/oi5_stanford_alpaca50k:llama-7b/llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot\n",
      "Move ./1397655.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/tydiqa_s=1_cb/1397655.out.lsf\n",
      "Job ./1397003.out exited with error code. --save_dir=results/oi5_ultrachat50k:llama-7b/llama-7b_ultrachat50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt\n",
      "Job ./1396671.out exited with error code. --save_dir=results/oi5_flan_v250k:llama-7b/llama-7b_flan_v250k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8_chatfmt\n",
      "./1388919.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1397615.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=60000:ep=3/eval/bbh_s=3_chatfmt\n",
      "Job ./1388463.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/mmlu_s=5_chatfmt\n",
      "Job ./1397072.out exited with error code. --save_dir=results/oi5_flan_v250k:llama-7b/llama-7b_flan_v250k_score=random:s=0_pace=prune:size=60000:ep=3/eval/humaneval\n",
      "Job ./1388948.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb\n",
      "./1396744.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1389161.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=30000:ep=3/eval/mmlu_s=0_chatfmt\n",
      "Job ./1388971.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot_chatfmt\n",
      "Job ./1393673.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/tydiqa_s=1_gp\n",
      "Move ./1397671.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp/1397671.out.lsf\n",
      "Move ./1397689.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3_chatfmt/1397689.out.lsf\n",
      "Job ./1394503.out exited with error code. --save_dir=results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/mmlu_s=0\n",
      "Move ./1397648.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/mmlu_s=0/1397648.out.lsf\n",
      "./1396354.out does not have `--save_dir` specified. Probably still running.\n",
      "Job ./1396997.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt\n",
      "Move ./1398299.out -> /dccstor/data-pruning/results/oi5_oasst2:llama-7b/llama-7b_oasst2_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt/1398299.out.lsf\n",
      "Job ./1396413.out exited with error code. --save_dir=results/oi5_ultrachat50k:llama-7b/llama-7b_ultrachat50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp\n",
      "Job ./1393687.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/gsm_s=8_cot\n",
      "Move ./1397703.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/mmlu_s=0_chatfmt/1397703.out.lsf\n",
      "Job ./1397014.out exited with error code. --save_dir=results/oi5_oasst2:llama-7b/llama-7b_oasst2_score=random:s=0_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt\n",
      "./1397179.out does not have `--save_dir` specified. Probably still running.\n",
      "Move ./1397700.out -> /dccstor/data-pruning/results/oi5_wizardlm50k:llama-7b/llama-7b_wizardlm50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/humaneval/1397700.out.lsf\n",
      "Job ./1388947.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/humaneval\n",
      "Job ./1389097.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot\n",
      "Job ./1388991.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=10000:ep=10/eval/bbh_s=3_cot_chatfmt\n",
      "Job ./1397093.out exited with error code. --save_dir=results/oi5_oasst2:llama-7b/llama-7b_oasst2_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=60000:ep=3/eval/gsm_s=8\n",
      "Job ./1388976.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1396720.out exited with error code. --save_dir=results/oi5_flan_v250k:llama-7b/llama-7b_flan_v250k_score=random:s=0_pace=prune:size=60000:ep=3/eval/humaneval\n",
      "Job ./1388969.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=5_chatfmt\n",
      "Job ./1396437.out exited with error code. --save_dir=results/oi5_ultrachat50k:llama-7b/llama-7b_ultrachat50k_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot_chatfmt\n",
      "Job ./1397004.out exited with error code. --save_dir=results/oi5_ultrachat50k:llama-7b/llama-7b_ultrachat50k_score=random:s=0_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt\n",
      "Job ./1393700.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=random:s=0_pace=prune:size=60000:ep=3/eval/tydiqa_s=1_cb_chatfmt\n",
      "Job ./1389178.out exited with error code. --save_dir=results/oi5_sharegpt50k:llama-7b/llama-7b_sharegpt50k_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/mmlu_s=5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/model_outputs/sft/llama-7b+lora:r=512:a=11585+proj=4096;/1389415.out.lsf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/shutil.py:816\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './1389415.out' -> '/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/model_outputs/sft/llama-7b+lora:r=512:a=11585+proj=4096;/1389415.out.lsf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if job successfully ran, lsf system will generate a summary in log_dir,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# call this function to move lsf summary to save_dir if job is successful.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmove_lsf_job_summary_to_save_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/wpq/github/mitibm2023/src/llm/submit.py:464\u001b[0m, in \u001b[0;36mmove_lsf_job_summary_to_save_dir\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m    462\u001b[0m     target_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, save_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(out_file)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.lsf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    463\u001b[0m     target_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(target_path)\n\u001b[0;32m--> 464\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMove \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExited with exit code\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m t:\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/shutil.py:836\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m         rmtree(src)\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 836\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/shutil.py:434\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    433\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 434\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/shutil.py:256\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m    258\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    259\u001b[0m                 \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/model_outputs/sft/llama-7b+lora:r=512:a=11585+proj=4096;/1389415.out.lsf'"
     ]
    }
   ],
   "source": [
    "# if job successfully ran, lsf system will generate a summary in log_dir,\n",
    "# call this function to move lsf summary to save_dir if job is successful.\n",
    "move_lsf_job_summary_to_save_dir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4392e9ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=mix\n",
      "model=llama, dataset=stanford_alpaca50k, N=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_55123 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_55123_row0_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_55123_row0_col1, #T_55123_row0_col2, #T_55123_row0_col3, #T_55123_row0_col8, #T_55123_row0_col9, #T_55123_row0_col10, #T_55123_row0_col11, #T_55123_row0_col12, #T_55123_row0_col13, #T_55123_row0_col14, #T_55123_row0_col15, #T_55123_row0_col16, #T_55123_row0_col17, #T_55123_row0_col18, #T_55123_row0_col19, #T_55123_row0_col20, #T_55123_row0_col21, #T_55123_row0_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55123_row0_col4, #T_55123_row0_col5, #T_55123_row0_col6, #T_55123_row0_col7, #T_55123_row0_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_55123\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_55123_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_55123_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_55123_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_55123_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_55123_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_55123_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_55123_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_55123_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_55123_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_55123_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_55123_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_55123_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_55123_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_55123_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_55123_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_55123_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_55123_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_55123_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_55123_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_55123_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_55123_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_55123_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_55123_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_55123_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_55123_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_55123_row0_col0\" class=\"data row0 col0\" >llama-7b</td>\n",
       "      <td id=\"T_55123_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_55123_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_55123_row0_col3\" class=\"data row0 col3\" >22.8</td>\n",
       "      <td id=\"T_55123_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_55123_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_55123_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "      <td id=\"T_55123_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
       "      <td id=\"T_55123_row0_col8\" class=\"data row0 col8\" >31.8</td>\n",
       "      <td id=\"T_55123_row0_col9\" class=\"data row0 col9\" >34.5</td>\n",
       "      <td id=\"T_55123_row0_col10\" class=\"data row0 col10\" >5.4</td>\n",
       "      <td id=\"T_55123_row0_col11\" class=\"data row0 col11\" >11.8</td>\n",
       "      <td id=\"T_55123_row0_col12\" class=\"data row0 col12\" >30.6</td>\n",
       "      <td id=\"T_55123_row0_col13\" class=\"data row0 col13\" >32.7</td>\n",
       "      <td id=\"T_55123_row0_col14\" class=\"data row0 col14\" >9.6</td>\n",
       "      <td id=\"T_55123_row0_col15\" class=\"data row0 col15\" >38.4</td>\n",
       "      <td id=\"T_55123_row0_col16\" class=\"data row0 col16\" >10.2</td>\n",
       "      <td id=\"T_55123_row0_col17\" class=\"data row0 col17\" >33.1</td>\n",
       "      <td id=\"T_55123_row0_col18\" class=\"data row0 col18\" >8.6</td>\n",
       "      <td id=\"T_55123_row0_col19\" class=\"data row0 col19\" >31.7</td>\n",
       "      <td id=\"T_55123_row0_col20\" class=\"data row0 col20\" >24.0</td>\n",
       "      <td id=\"T_55123_row0_col21\" class=\"data row0 col21\" >10.2</td>\n",
       "      <td id=\"T_55123_row0_col22\" class=\"data row0 col22\" >22.8</td>\n",
       "      <td id=\"T_55123_row0_col23\" class=\"data row0 col23\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca2e116e00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8a2a td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_e8a2a_row0_col0, #T_e8a2a_row1_col0, #T_e8a2a_row2_col0, #T_e8a2a_row3_col0, #T_e8a2a_row4_col0, #T_e8a2a_row5_col0, #T_e8a2a_row6_col0, #T_e8a2a_row7_col0, #T_e8a2a_row8_col0, #T_e8a2a_row9_col0, #T_e8a2a_row10_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8a2a_row0_col1, #T_e8a2a_row0_col2, #T_e8a2a_row0_col12, #T_e8a2a_row1_col1, #T_e8a2a_row1_col2, #T_e8a2a_row1_col4, #T_e8a2a_row1_col5, #T_e8a2a_row2_col1, #T_e8a2a_row2_col2, #T_e8a2a_row3_col1, #T_e8a2a_row3_col2, #T_e8a2a_row3_col16, #T_e8a2a_row3_col21, #T_e8a2a_row4_col1, #T_e8a2a_row4_col2, #T_e8a2a_row4_col6, #T_e8a2a_row4_col7, #T_e8a2a_row4_col16, #T_e8a2a_row4_col21, #T_e8a2a_row4_col23, #T_e8a2a_row5_col1, #T_e8a2a_row5_col2, #T_e8a2a_row6_col1, #T_e8a2a_row6_col2, #T_e8a2a_row7_col1, #T_e8a2a_row7_col2, #T_e8a2a_row8_col1, #T_e8a2a_row8_col2, #T_e8a2a_row8_col10, #T_e8a2a_row9_col1, #T_e8a2a_row9_col2, #T_e8a2a_row9_col9, #T_e8a2a_row9_col14, #T_e8a2a_row9_col15, #T_e8a2a_row9_col17, #T_e8a2a_row9_col20, #T_e8a2a_row10_col1, #T_e8a2a_row10_col2, #T_e8a2a_row10_col3, #T_e8a2a_row10_col8, #T_e8a2a_row10_col11, #T_e8a2a_row10_col13, #T_e8a2a_row10_col18, #T_e8a2a_row10_col19, #T_e8a2a_row10_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col4, #T_e8a2a_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row0_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row0_col6, #T_e8a2a_row0_col7, #T_e8a2a_row0_col18, #T_e8a2a_row1_col3, #T_e8a2a_row1_col8, #T_e8a2a_row1_col9, #T_e8a2a_row1_col17, #T_e8a2a_row2_col4, #T_e8a2a_row2_col13, #T_e8a2a_row2_col22, #T_e8a2a_row2_col23, #T_e8a2a_row3_col14, #T_e8a2a_row3_col15, #T_e8a2a_row3_col18, #T_e8a2a_row3_col20, #T_e8a2a_row4_col5, #T_e8a2a_row5_col11, #T_e8a2a_row6_col16, #T_e8a2a_row6_col21, #T_e8a2a_row8_col12, #T_e8a2a_row8_col19, #T_e8a2a_row9_col10, #T_e8a2a_row9_col16, #T_e8a2a_row9_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e46e56;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col9, #T_e8a2a_row3_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col10, #T_e8a2a_row1_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row0_col11, #T_e8a2a_row1_col19, #T_e8a2a_row1_col20, #T_e8a2a_row3_col3, #T_e8a2a_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col14, #T_e8a2a_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col15, #T_e8a2a_row1_col14, #T_e8a2a_row5_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col16, #T_e8a2a_row0_col21, #T_e8a2a_row3_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row0_col17, #T_e8a2a_row1_col23, #T_e8a2a_row8_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col19, #T_e8a2a_row6_col8, #T_e8a2a_row6_col18, #T_e8a2a_row9_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col22, #T_e8a2a_row6_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row0_col23, #T_e8a2a_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e4d9d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row1_col12, #T_e8a2a_row4_col4, #T_e8a2a_row10_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row1_col13, #T_e8a2a_row4_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #cd423b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row1_col16, #T_e8a2a_row1_col21, #T_e8a2a_row9_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row1_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row1_col22, #T_e8a2a_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col5, #T_e8a2a_row3_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #b8122a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col6, #T_e8a2a_row2_col9, #T_e8a2a_row3_col10, #T_e8a2a_row5_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col7, #T_e8a2a_row7_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row2_col8, #T_e8a2a_row5_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row2_col10, #T_e8a2a_row4_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row2_col11, #T_e8a2a_row4_col9, #T_e8a2a_row4_col11, #T_e8a2a_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col12, #T_e8a2a_row4_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row2_col14, #T_e8a2a_row6_col10, #T_e8a2a_row7_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col15, #T_e8a2a_row2_col18, #T_e8a2a_row4_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col16, #T_e8a2a_row2_col21, #T_e8a2a_row7_col16, #T_e8a2a_row7_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row2_col17, #T_e8a2a_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #b50927;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row2_col20, #T_e8a2a_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row3_col5, #T_e8a2a_row3_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row3_col8, #T_e8a2a_row8_col16, #T_e8a2a_row8_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row3_col11, #T_e8a2a_row4_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row3_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row4_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row4_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row4_col19, #T_e8a2a_row5_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row4_col22, #T_e8a2a_row5_col18, #T_e8a2a_row6_col15, #T_e8a2a_row6_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row5_col4, #T_e8a2a_row5_col5, #T_e8a2a_row5_col6, #T_e8a2a_row5_col7, #T_e8a2a_row5_col23, #T_e8a2a_row6_col4, #T_e8a2a_row6_col5, #T_e8a2a_row6_col6, #T_e8a2a_row6_col7, #T_e8a2a_row6_col23, #T_e8a2a_row7_col4, #T_e8a2a_row7_col5, #T_e8a2a_row7_col6, #T_e8a2a_row7_col7, #T_e8a2a_row7_col23, #T_e8a2a_row8_col4, #T_e8a2a_row8_col5, #T_e8a2a_row8_col6, #T_e8a2a_row8_col7, #T_e8a2a_row8_col23, #T_e8a2a_row9_col4, #T_e8a2a_row9_col5, #T_e8a2a_row9_col6, #T_e8a2a_row9_col7, #T_e8a2a_row9_col23, #T_e8a2a_row10_col4, #T_e8a2a_row10_col5, #T_e8a2a_row10_col6, #T_e8a2a_row10_col7, #T_e8a2a_row10_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row5_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row5_col13, #T_e8a2a_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row5_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row5_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row5_col16, #T_e8a2a_row5_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row5_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row6_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row6_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row6_col13, #T_e8a2a_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f49a7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row6_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row6_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row6_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row6_col22, #T_e8a2a_row9_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row7_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row7_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #d7dce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row7_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row7_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row7_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row7_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row7_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row7_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row7_col22, #T_e8a2a_row10_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row8_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row8_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row8_col11, #T_e8a2a_row9_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row8_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row8_col15, #T_e8a2a_row8_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row8_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row8_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row8_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #7093f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row9_col3, #T_e8a2a_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row9_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #df634e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row9_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row10_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #4e68d8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row10_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row10_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row10_col16, #T_e8a2a_row10_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e8a2a_row10_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e8a2a_row10_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8a2a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8a2a_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_e8a2a_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_e8a2a_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_e8a2a_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_e8a2a_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_e8a2a_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_e8a2a_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_e8a2a_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_e8a2a_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_e8a2a_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_e8a2a_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_e8a2a_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_e8a2a_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_e8a2a_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_e8a2a_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_e8a2a_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_e8a2a_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_e8a2a_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_e8a2a_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_e8a2a_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_e8a2a_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_e8a2a_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_e8a2a_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_e8a2a_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e8a2a_row0_col0\" class=\"data row0 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:theta=0.4:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_e8a2a_row0_col1\" class=\"data row0 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row0_col2\" class=\"data row0 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row0_col3\" class=\"data row0 col3\" >23.2</td>\n",
       "      <td id=\"T_e8a2a_row0_col4\" class=\"data row0 col4\" >42.5</td>\n",
       "      <td id=\"T_e8a2a_row0_col5\" class=\"data row0 col5\" >263.6</td>\n",
       "      <td id=\"T_e8a2a_row0_col6\" class=\"data row0 col6\" >32.4</td>\n",
       "      <td id=\"T_e8a2a_row0_col7\" class=\"data row0 col7\" >27.0</td>\n",
       "      <td id=\"T_e8a2a_row0_col8\" class=\"data row0 col8\" >36.8</td>\n",
       "      <td id=\"T_e8a2a_row0_col9\" class=\"data row0 col9\" >37.1</td>\n",
       "      <td id=\"T_e8a2a_row0_col10\" class=\"data row0 col10\" >5.8</td>\n",
       "      <td id=\"T_e8a2a_row0_col11\" class=\"data row0 col11\" >11.0</td>\n",
       "      <td id=\"T_e8a2a_row0_col12\" class=\"data row0 col12\" >30.9</td>\n",
       "      <td id=\"T_e8a2a_row0_col13\" class=\"data row0 col13\" >31.3</td>\n",
       "      <td id=\"T_e8a2a_row0_col14\" class=\"data row0 col14\" >9.2</td>\n",
       "      <td id=\"T_e8a2a_row0_col15\" class=\"data row0 col15\" >38.1</td>\n",
       "      <td id=\"T_e8a2a_row0_col16\" class=\"data row0 col16\" >8.5</td>\n",
       "      <td id=\"T_e8a2a_row0_col17\" class=\"data row0 col17\" >37.0</td>\n",
       "      <td id=\"T_e8a2a_row0_col18\" class=\"data row0 col18\" >8.4</td>\n",
       "      <td id=\"T_e8a2a_row0_col19\" class=\"data row0 col19\" >31.1</td>\n",
       "      <td id=\"T_e8a2a_row0_col20\" class=\"data row0 col20\" >23.6</td>\n",
       "      <td id=\"T_e8a2a_row0_col21\" class=\"data row0 col21\" >8.5</td>\n",
       "      <td id=\"T_e8a2a_row0_col22\" class=\"data row0 col22\" >25.9</td>\n",
       "      <td id=\"T_e8a2a_row0_col23\" class=\"data row0 col23\" >-38.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e8a2a_row1_col0\" class=\"data row1 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td id=\"T_e8a2a_row1_col1\" class=\"data row1 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row1_col2\" class=\"data row1 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row1_col3\" class=\"data row1 col3\" >23.5</td>\n",
       "      <td id=\"T_e8a2a_row1_col4\" class=\"data row1 col4\" >37.6</td>\n",
       "      <td id=\"T_e8a2a_row1_col5\" class=\"data row1 col5\" >186.6</td>\n",
       "      <td id=\"T_e8a2a_row1_col6\" class=\"data row1 col6\" >30.8</td>\n",
       "      <td id=\"T_e8a2a_row1_col7\" class=\"data row1 col7\" >25.5</td>\n",
       "      <td id=\"T_e8a2a_row1_col8\" class=\"data row1 col8\" >38.9</td>\n",
       "      <td id=\"T_e8a2a_row1_col9\" class=\"data row1 col9\" >37.6</td>\n",
       "      <td id=\"T_e8a2a_row1_col10\" class=\"data row1 col10\" >5.8</td>\n",
       "      <td id=\"T_e8a2a_row1_col11\" class=\"data row1 col11\" >9.0</td>\n",
       "      <td id=\"T_e8a2a_row1_col12\" class=\"data row1 col12\" >32.2</td>\n",
       "      <td id=\"T_e8a2a_row1_col13\" class=\"data row1 col13\" >31.6</td>\n",
       "      <td id=\"T_e8a2a_row1_col14\" class=\"data row1 col14\" >9.4</td>\n",
       "      <td id=\"T_e8a2a_row1_col15\" class=\"data row1 col15\" >38.9</td>\n",
       "      <td id=\"T_e8a2a_row1_col16\" class=\"data row1 col16\" >7.9</td>\n",
       "      <td id=\"T_e8a2a_row1_col17\" class=\"data row1 col17\" >38.2</td>\n",
       "      <td id=\"T_e8a2a_row1_col18\" class=\"data row1 col18\" >7.4</td>\n",
       "      <td id=\"T_e8a2a_row1_col19\" class=\"data row1 col19\" >31.9</td>\n",
       "      <td id=\"T_e8a2a_row1_col20\" class=\"data row1 col20\" >24.2</td>\n",
       "      <td id=\"T_e8a2a_row1_col21\" class=\"data row1 col21\" >7.9</td>\n",
       "      <td id=\"T_e8a2a_row1_col22\" class=\"data row1 col22\" >25.4</td>\n",
       "      <td id=\"T_e8a2a_row1_col23\" class=\"data row1 col23\" >-38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e8a2a_row2_col0\" class=\"data row2 col0\" >\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_e8a2a_row2_col1\" class=\"data row2 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row2_col2\" class=\"data row2 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row2_col3\" class=\"data row2 col3\" >23.3</td>\n",
       "      <td id=\"T_e8a2a_row2_col4\" class=\"data row2 col4\" >46.1</td>\n",
       "      <td id=\"T_e8a2a_row2_col5\" class=\"data row2 col5\" >287.0</td>\n",
       "      <td id=\"T_e8a2a_row2_col6\" class=\"data row2 col6\" >32.0</td>\n",
       "      <td id=\"T_e8a2a_row2_col7\" class=\"data row2 col7\" >24.5</td>\n",
       "      <td id=\"T_e8a2a_row2_col8\" class=\"data row2 col8\" >35.5</td>\n",
       "      <td id=\"T_e8a2a_row2_col9\" class=\"data row2 col9\" >36.3</td>\n",
       "      <td id=\"T_e8a2a_row2_col10\" class=\"data row2 col10\" >6.0</td>\n",
       "      <td id=\"T_e8a2a_row2_col11\" class=\"data row2 col11\" >10.6</td>\n",
       "      <td id=\"T_e8a2a_row2_col12\" class=\"data row2 col12\" >31.7</td>\n",
       "      <td id=\"T_e8a2a_row2_col13\" class=\"data row2 col13\" >32.5</td>\n",
       "      <td id=\"T_e8a2a_row2_col14\" class=\"data row2 col14\" >8.9</td>\n",
       "      <td id=\"T_e8a2a_row2_col15\" class=\"data row2 col15\" >38.9</td>\n",
       "      <td id=\"T_e8a2a_row2_col16\" class=\"data row2 col16\" >9.1</td>\n",
       "      <td id=\"T_e8a2a_row2_col17\" class=\"data row2 col17\" >35.9</td>\n",
       "      <td id=\"T_e8a2a_row2_col18\" class=\"data row2 col18\" >8.3</td>\n",
       "      <td id=\"T_e8a2a_row2_col19\" class=\"data row2 col19\" >32.1</td>\n",
       "      <td id=\"T_e8a2a_row2_col20\" class=\"data row2 col20\" >23.9</td>\n",
       "      <td id=\"T_e8a2a_row2_col21\" class=\"data row2 col21\" >9.1</td>\n",
       "      <td id=\"T_e8a2a_row2_col22\" class=\"data row2 col22\" >26.0</td>\n",
       "      <td id=\"T_e8a2a_row2_col23\" class=\"data row2 col23\" >-38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e8a2a_row3_col0\" class=\"data row3 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:theta=0.8:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_e8a2a_row3_col1\" class=\"data row3 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row3_col2\" class=\"data row3 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row3_col3\" class=\"data row3 col3\" >23.3</td>\n",
       "      <td id=\"T_e8a2a_row3_col4\" class=\"data row3 col4\" >44.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col5\" class=\"data row3 col5\" >286.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col6\" class=\"data row3 col6\" >30.5</td>\n",
       "      <td id=\"T_e8a2a_row3_col7\" class=\"data row3 col7\" >23.7</td>\n",
       "      <td id=\"T_e8a2a_row3_col8\" class=\"data row3 col8\" >35.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col9\" class=\"data row3 col9\" >37.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col10\" class=\"data row3 col10\" >6.6</td>\n",
       "      <td id=\"T_e8a2a_row3_col11\" class=\"data row3 col11\" >10.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col12\" class=\"data row3 col12\" >31.9</td>\n",
       "      <td id=\"T_e8a2a_row3_col13\" class=\"data row3 col13\" >32.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col14\" class=\"data row3 col14\" >9.7</td>\n",
       "      <td id=\"T_e8a2a_row3_col15\" class=\"data row3 col15\" >39.3</td>\n",
       "      <td id=\"T_e8a2a_row3_col16\" class=\"data row3 col16\" >7.3</td>\n",
       "      <td id=\"T_e8a2a_row3_col17\" class=\"data row3 col17\" >36.2</td>\n",
       "      <td id=\"T_e8a2a_row3_col18\" class=\"data row3 col18\" >8.4</td>\n",
       "      <td id=\"T_e8a2a_row3_col19\" class=\"data row3 col19\" >32.0</td>\n",
       "      <td id=\"T_e8a2a_row3_col20\" class=\"data row3 col20\" >24.5</td>\n",
       "      <td id=\"T_e8a2a_row3_col21\" class=\"data row3 col21\" >7.3</td>\n",
       "      <td id=\"T_e8a2a_row3_col22\" class=\"data row3 col22\" >25.7</td>\n",
       "      <td id=\"T_e8a2a_row3_col23\" class=\"data row3 col23\" >-39.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e8a2a_row4_col0\" class=\"data row4 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.6$)</td>\n",
       "      <td id=\"T_e8a2a_row4_col1\" class=\"data row4 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row4_col2\" class=\"data row4 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row4_col3\" class=\"data row4 col3\" >23.1</td>\n",
       "      <td id=\"T_e8a2a_row4_col4\" class=\"data row4 col4\" >42.4</td>\n",
       "      <td id=\"T_e8a2a_row4_col5\" class=\"data row4 col5\" >288.5</td>\n",
       "      <td id=\"T_e8a2a_row4_col6\" class=\"data row4 col6\" >29.1</td>\n",
       "      <td id=\"T_e8a2a_row4_col7\" class=\"data row4 col7\" >23.4</td>\n",
       "      <td id=\"T_e8a2a_row4_col8\" class=\"data row4 col8\" >36.1</td>\n",
       "      <td id=\"T_e8a2a_row4_col9\" class=\"data row4 col9\" >36.5</td>\n",
       "      <td id=\"T_e8a2a_row4_col10\" class=\"data row4 col10\" >6.0</td>\n",
       "      <td id=\"T_e8a2a_row4_col11\" class=\"data row4 col11\" >10.6</td>\n",
       "      <td id=\"T_e8a2a_row4_col12\" class=\"data row4 col12\" >31.7</td>\n",
       "      <td id=\"T_e8a2a_row4_col13\" class=\"data row4 col13\" >31.7</td>\n",
       "      <td id=\"T_e8a2a_row4_col14\" class=\"data row4 col14\" >9.1</td>\n",
       "      <td id=\"T_e8a2a_row4_col15\" class=\"data row4 col15\" >38.6</td>\n",
       "      <td id=\"T_e8a2a_row4_col16\" class=\"data row4 col16\" >7.3</td>\n",
       "      <td id=\"T_e8a2a_row4_col17\" class=\"data row4 col17\" >36.3</td>\n",
       "      <td id=\"T_e8a2a_row4_col18\" class=\"data row4 col18\" >8.3</td>\n",
       "      <td id=\"T_e8a2a_row4_col19\" class=\"data row4 col19\" >31.7</td>\n",
       "      <td id=\"T_e8a2a_row4_col20\" class=\"data row4 col20\" >23.8</td>\n",
       "      <td id=\"T_e8a2a_row4_col21\" class=\"data row4 col21\" >7.3</td>\n",
       "      <td id=\"T_e8a2a_row4_col22\" class=\"data row4 col22\" >25.2</td>\n",
       "      <td id=\"T_e8a2a_row4_col23\" class=\"data row4 col23\" >-42.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e8a2a_row5_col0\" class=\"data row5 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$)</td>\n",
       "      <td id=\"T_e8a2a_row5_col1\" class=\"data row5 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row5_col2\" class=\"data row5 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row5_col3\" class=\"data row5 col3\" >23.0</td>\n",
       "      <td id=\"T_e8a2a_row5_col4\" class=\"data row5 col4\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row5_col5\" class=\"data row5 col5\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row5_col6\" class=\"data row5 col6\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row5_col7\" class=\"data row5 col7\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row5_col8\" class=\"data row5 col8\" >37.8</td>\n",
       "      <td id=\"T_e8a2a_row5_col9\" class=\"data row5 col9\" >36.8</td>\n",
       "      <td id=\"T_e8a2a_row5_col10\" class=\"data row5 col10\" >4.8</td>\n",
       "      <td id=\"T_e8a2a_row5_col11\" class=\"data row5 col11\" >11.2</td>\n",
       "      <td id=\"T_e8a2a_row5_col12\" class=\"data row5 col12\" >31.5</td>\n",
       "      <td id=\"T_e8a2a_row5_col13\" class=\"data row5 col13\" >31.2</td>\n",
       "      <td id=\"T_e8a2a_row5_col14\" class=\"data row5 col14\" >8.4</td>\n",
       "      <td id=\"T_e8a2a_row5_col15\" class=\"data row5 col15\" >35.9</td>\n",
       "      <td id=\"T_e8a2a_row5_col16\" class=\"data row5 col16\" >8.9</td>\n",
       "      <td id=\"T_e8a2a_row5_col17\" class=\"data row5 col17\" >37.3</td>\n",
       "      <td id=\"T_e8a2a_row5_col18\" class=\"data row5 col18\" >8.0</td>\n",
       "      <td id=\"T_e8a2a_row5_col19\" class=\"data row5 col19\" >31.3</td>\n",
       "      <td id=\"T_e8a2a_row5_col20\" class=\"data row5 col20\" >22.2</td>\n",
       "      <td id=\"T_e8a2a_row5_col21\" class=\"data row5 col21\" >8.9</td>\n",
       "      <td id=\"T_e8a2a_row5_col22\" class=\"data row5 col22\" >23.0</td>\n",
       "      <td id=\"T_e8a2a_row5_col23\" class=\"data row5 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e8a2a_row6_col0\" class=\"data row6 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=acos0:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_e8a2a_row6_col1\" class=\"data row6 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row6_col2\" class=\"data row6 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row6_col3\" class=\"data row6 col3\" >22.9</td>\n",
       "      <td id=\"T_e8a2a_row6_col4\" class=\"data row6 col4\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row6_col5\" class=\"data row6 col5\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row6_col6\" class=\"data row6 col6\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row6_col7\" class=\"data row6 col7\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row6_col8\" class=\"data row6 col8\" >36.6</td>\n",
       "      <td id=\"T_e8a2a_row6_col9\" class=\"data row6 col9\" >37.4</td>\n",
       "      <td id=\"T_e8a2a_row6_col10\" class=\"data row6 col10\" >6.4</td>\n",
       "      <td id=\"T_e8a2a_row6_col11\" class=\"data row6 col11\" >9.2</td>\n",
       "      <td id=\"T_e8a2a_row6_col12\" class=\"data row6 col12\" >31.2</td>\n",
       "      <td id=\"T_e8a2a_row6_col13\" class=\"data row6 col13\" >29.3</td>\n",
       "      <td id=\"T_e8a2a_row6_col14\" class=\"data row6 col14\" >7.9</td>\n",
       "      <td id=\"T_e8a2a_row6_col15\" class=\"data row6 col15\" >37.7</td>\n",
       "      <td id=\"T_e8a2a_row6_col16\" class=\"data row6 col16\" >10.4</td>\n",
       "      <td id=\"T_e8a2a_row6_col17\" class=\"data row6 col17\" >37.0</td>\n",
       "      <td id=\"T_e8a2a_row6_col18\" class=\"data row6 col18\" >7.8</td>\n",
       "      <td id=\"T_e8a2a_row6_col19\" class=\"data row6 col19\" >30.2</td>\n",
       "      <td id=\"T_e8a2a_row6_col20\" class=\"data row6 col20\" >22.8</td>\n",
       "      <td id=\"T_e8a2a_row6_col21\" class=\"data row6 col21\" >10.4</td>\n",
       "      <td id=\"T_e8a2a_row6_col22\" class=\"data row6 col22\" >22.9</td>\n",
       "      <td id=\"T_e8a2a_row6_col23\" class=\"data row6 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e8a2a_row7_col0\" class=\"data row7 col0\" >DPP (Llama Emb)</td>\n",
       "      <td id=\"T_e8a2a_row7_col1\" class=\"data row7 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row7_col2\" class=\"data row7 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row7_col3\" class=\"data row7 col3\" >20.7</td>\n",
       "      <td id=\"T_e8a2a_row7_col4\" class=\"data row7 col4\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row7_col5\" class=\"data row7 col5\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row7_col7\" class=\"data row7 col7\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row7_col8\" class=\"data row7 col8\" >32.1</td>\n",
       "      <td id=\"T_e8a2a_row7_col9\" class=\"data row7 col9\" >36.6</td>\n",
       "      <td id=\"T_e8a2a_row7_col10\" class=\"data row7 col10\" >6.2</td>\n",
       "      <td id=\"T_e8a2a_row7_col11\" class=\"data row7 col11\" >7.4</td>\n",
       "      <td id=\"T_e8a2a_row7_col12\" class=\"data row7 col12\" >32.0</td>\n",
       "      <td id=\"T_e8a2a_row7_col13\" class=\"data row7 col13\" >30.1</td>\n",
       "      <td id=\"T_e8a2a_row7_col14\" class=\"data row7 col14\" >5.9</td>\n",
       "      <td id=\"T_e8a2a_row7_col15\" class=\"data row7 col15\" >27.0</td>\n",
       "      <td id=\"T_e8a2a_row7_col16\" class=\"data row7 col16\" >9.1</td>\n",
       "      <td id=\"T_e8a2a_row7_col17\" class=\"data row7 col17\" >34.3</td>\n",
       "      <td id=\"T_e8a2a_row7_col18\" class=\"data row7 col18\" >6.8</td>\n",
       "      <td id=\"T_e8a2a_row7_col19\" class=\"data row7 col19\" >31.1</td>\n",
       "      <td id=\"T_e8a2a_row7_col20\" class=\"data row7 col20\" >16.4</td>\n",
       "      <td id=\"T_e8a2a_row7_col21\" class=\"data row7 col21\" >9.1</td>\n",
       "      <td id=\"T_e8a2a_row7_col22\" class=\"data row7 col22\" >20.7</td>\n",
       "      <td id=\"T_e8a2a_row7_col23\" class=\"data row7 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e8a2a_row8_col0\" class=\"data row8 col0\" >DPP (Llama Emb Not Norm.)</td>\n",
       "      <td id=\"T_e8a2a_row8_col1\" class=\"data row8 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row8_col2\" class=\"data row8 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row8_col3\" class=\"data row8 col3\" >19.9</td>\n",
       "      <td id=\"T_e8a2a_row8_col4\" class=\"data row8 col4\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row8_col5\" class=\"data row8 col5\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row8_col6\" class=\"data row8 col6\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row8_col7\" class=\"data row8 col7\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row8_col8\" class=\"data row8 col8\" >25.7</td>\n",
       "      <td id=\"T_e8a2a_row8_col9\" class=\"data row8 col9\" >33.5</td>\n",
       "      <td id=\"T_e8a2a_row8_col10\" class=\"data row8 col10\" >3.8</td>\n",
       "      <td id=\"T_e8a2a_row8_col11\" class=\"data row8 col11\" >6.0</td>\n",
       "      <td id=\"T_e8a2a_row8_col12\" class=\"data row8 col12\" >33.2</td>\n",
       "      <td id=\"T_e8a2a_row8_col13\" class=\"data row8 col13\" >31.0</td>\n",
       "      <td id=\"T_e8a2a_row8_col14\" class=\"data row8 col14\" >7.0</td>\n",
       "      <td id=\"T_e8a2a_row8_col15\" class=\"data row8 col15\" >29.7</td>\n",
       "      <td id=\"T_e8a2a_row8_col16\" class=\"data row8 col16\" >9.6</td>\n",
       "      <td id=\"T_e8a2a_row8_col17\" class=\"data row8 col17\" >29.6</td>\n",
       "      <td id=\"T_e8a2a_row8_col18\" class=\"data row8 col18\" >4.9</td>\n",
       "      <td id=\"T_e8a2a_row8_col19\" class=\"data row8 col19\" >32.1</td>\n",
       "      <td id=\"T_e8a2a_row8_col20\" class=\"data row8 col20\" >18.3</td>\n",
       "      <td id=\"T_e8a2a_row8_col21\" class=\"data row8 col21\" >9.6</td>\n",
       "      <td id=\"T_e8a2a_row8_col22\" class=\"data row8 col22\" >19.9</td>\n",
       "      <td id=\"T_e8a2a_row8_col23\" class=\"data row8 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e8a2a_row9_col0\" class=\"data row9 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=acos0:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_e8a2a_row9_col1\" class=\"data row9 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row9_col2\" class=\"data row9 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row9_col3\" class=\"data row9 col3\" >19.2</td>\n",
       "      <td id=\"T_e8a2a_row9_col4\" class=\"data row9 col4\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row9_col5\" class=\"data row9 col5\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row9_col7\" class=\"data row9 col7\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row9_col8\" class=\"data row9 col8\" >26.7</td>\n",
       "      <td id=\"T_e8a2a_row9_col9\" class=\"data row9 col9\" >27.6</td>\n",
       "      <td id=\"T_e8a2a_row9_col10\" class=\"data row9 col10\" >7.0</td>\n",
       "      <td id=\"T_e8a2a_row9_col11\" class=\"data row9 col11\" >8.8</td>\n",
       "      <td id=\"T_e8a2a_row9_col12\" class=\"data row9 col12\" >31.4</td>\n",
       "      <td id=\"T_e8a2a_row9_col13\" class=\"data row9 col13\" >30.8</td>\n",
       "      <td id=\"T_e8a2a_row9_col14\" class=\"data row9 col14\" >5.6</td>\n",
       "      <td id=\"T_e8a2a_row9_col15\" class=\"data row9 col15\" >24.6</td>\n",
       "      <td id=\"T_e8a2a_row9_col16\" class=\"data row9 col16\" >10.4</td>\n",
       "      <td id=\"T_e8a2a_row9_col17\" class=\"data row9 col17\" >27.2</td>\n",
       "      <td id=\"T_e8a2a_row9_col18\" class=\"data row9 col18\" >7.9</td>\n",
       "      <td id=\"T_e8a2a_row9_col19\" class=\"data row9 col19\" >31.1</td>\n",
       "      <td id=\"T_e8a2a_row9_col20\" class=\"data row9 col20\" >15.1</td>\n",
       "      <td id=\"T_e8a2a_row9_col21\" class=\"data row9 col21\" >10.4</td>\n",
       "      <td id=\"T_e8a2a_row9_col22\" class=\"data row9 col22\" >19.2</td>\n",
       "      <td id=\"T_e8a2a_row9_col23\" class=\"data row9 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8a2a_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_e8a2a_row10_col0\" class=\"data row10 col0\" >Random</td>\n",
       "      <td id=\"T_e8a2a_row10_col1\" class=\"data row10 col1\" >10000.0</td>\n",
       "      <td id=\"T_e8a2a_row10_col2\" class=\"data row10 col2\" >1000.0</td>\n",
       "      <td id=\"T_e8a2a_row10_col3\" class=\"data row10 col3\" >18.7</td>\n",
       "      <td id=\"T_e8a2a_row10_col4\" class=\"data row10 col4\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row10_col5\" class=\"data row10 col5\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row10_col6\" class=\"data row10 col6\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row10_col7\" class=\"data row10 col7\" >nan</td>\n",
       "      <td id=\"T_e8a2a_row10_col8\" class=\"data row10 col8\" >25.2</td>\n",
       "      <td id=\"T_e8a2a_row10_col9\" class=\"data row10 col9\" >30.3</td>\n",
       "      <td id=\"T_e8a2a_row10_col10\" class=\"data row10 col10\" >4.0</td>\n",
       "      <td id=\"T_e8a2a_row10_col11\" class=\"data row10 col11\" >5.6</td>\n",
       "      <td id=\"T_e8a2a_row10_col12\" class=\"data row10 col12\" >32.2</td>\n",
       "      <td id=\"T_e8a2a_row10_col13\" class=\"data row10 col13\" >19.7</td>\n",
       "      <td id=\"T_e8a2a_row10_col14\" class=\"data row10 col14\" >7.6</td>\n",
       "      <td id=\"T_e8a2a_row10_col15\" class=\"data row10 col15\" >34.2</td>\n",
       "      <td id=\"T_e8a2a_row10_col16\" class=\"data row10 col16\" >9.3</td>\n",
       "      <td id=\"T_e8a2a_row10_col17\" class=\"data row10 col17\" >27.8</td>\n",
       "      <td id=\"T_e8a2a_row10_col18\" class=\"data row10 col18\" >4.8</td>\n",
       "      <td id=\"T_e8a2a_row10_col19\" class=\"data row10 col19\" >26.0</td>\n",
       "      <td id=\"T_e8a2a_row10_col20\" class=\"data row10 col20\" >20.9</td>\n",
       "      <td id=\"T_e8a2a_row10_col21\" class=\"data row10 col21\" >9.3</td>\n",
       "      <td id=\"T_e8a2a_row10_col22\" class=\"data row10 col22\" >18.7</td>\n",
       "      <td id=\"T_e8a2a_row10_col23\" class=\"data row10 col23\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca2e13c1c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6be59 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_6be59_row0_col0, #T_6be59_row1_col0, #T_6be59_row2_col0, #T_6be59_row3_col0, #T_6be59_row4_col0, #T_6be59_row5_col0, #T_6be59_row6_col0, #T_6be59_row7_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_6be59_row0_col1, #T_6be59_row0_col2, #T_6be59_row0_col14, #T_6be59_row1_col1, #T_6be59_row1_col2, #T_6be59_row1_col10, #T_6be59_row1_col12, #T_6be59_row1_col15, #T_6be59_row1_col20, #T_6be59_row2_col1, #T_6be59_row2_col2, #T_6be59_row2_col16, #T_6be59_row2_col21, #T_6be59_row3_col1, #T_6be59_row3_col2, #T_6be59_row4_col1, #T_6be59_row4_col2, #T_6be59_row5_col1, #T_6be59_row5_col2, #T_6be59_row5_col3, #T_6be59_row5_col8, #T_6be59_row5_col9, #T_6be59_row5_col13, #T_6be59_row5_col16, #T_6be59_row5_col17, #T_6be59_row5_col19, #T_6be59_row5_col21, #T_6be59_row6_col1, #T_6be59_row6_col2, #T_6be59_row7_col1, #T_6be59_row7_col2, #T_6be59_row7_col4, #T_6be59_row7_col5, #T_6be59_row7_col6, #T_6be59_row7_col7, #T_6be59_row7_col11, #T_6be59_row7_col12, #T_6be59_row7_col18, #T_6be59_row7_col22, #T_6be59_row7_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col4, #T_6be59_row0_col23, #T_6be59_row1_col4, #T_6be59_row1_col23, #T_6be59_row2_col4, #T_6be59_row2_col23, #T_6be59_row3_col4, #T_6be59_row3_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row0_col5, #T_6be59_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c5ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col6, #T_6be59_row0_col7, #T_6be59_row0_col12, #T_6be59_row0_col17, #T_6be59_row2_col9, #T_6be59_row3_col8, #T_6be59_row3_col13, #T_6be59_row3_col19, #T_6be59_row4_col11, #T_6be59_row4_col18, #T_6be59_row5_col10, #T_6be59_row5_col15, #T_6be59_row5_col20, #T_6be59_row6_col3, #T_6be59_row6_col4, #T_6be59_row6_col5, #T_6be59_row6_col14, #T_6be59_row6_col16, #T_6be59_row6_col19, #T_6be59_row6_col21, #T_6be59_row6_col22, #T_6be59_row6_col23, #T_6be59_row7_col16, #T_6be59_row7_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row0_col9, #T_6be59_row2_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row0_col10, #T_6be59_row2_col10, #T_6be59_row3_col10, #T_6be59_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col13, #T_6be59_row2_col8, #T_6be59_row3_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col15, #T_6be59_row2_col5, #T_6be59_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c1d4f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col16, #T_6be59_row0_col21, #T_6be59_row3_col16, #T_6be59_row3_col21, #T_6be59_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row0_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row0_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #e7d7ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col3, #T_6be59_row2_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #d5dbe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #cd423b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #b6cefa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col13, #T_6be59_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #e0dbd8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col14, #T_6be59_row2_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col16, #T_6be59_row1_col21, #T_6be59_row4_col8, #T_6be59_row4_col16, #T_6be59_row4_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col17, #T_6be59_row6_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row1_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #8caffe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row1_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row2_col6, #T_6be59_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row2_col15, #T_6be59_row3_col15, #T_6be59_row6_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row2_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row2_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #b8122a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row3_col7, #T_6be59_row4_col17, #T_6be59_row6_col6, #T_6be59_row6_col9, #T_6be59_row6_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row3_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row3_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row3_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row4_col4, #T_6be59_row4_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col9, #T_6be59_row5_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row4_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col14, #T_6be59_row4_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #ed8366;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row4_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row4_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row5_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d7dce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row5_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row5_col18, #T_6be59_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row5_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row5_col23, #T_6be59_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row6_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row6_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row6_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row6_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row7_col8, #T_6be59_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row7_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row7_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row7_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6be59_row7_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row7_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6be59_row7_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #e9d5cb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6be59\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6be59_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_6be59_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_6be59_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_6be59_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_6be59_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_6be59_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_6be59_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_6be59_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_6be59_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_6be59_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_6be59_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_6be59_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_6be59_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_6be59_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_6be59_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_6be59_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_6be59_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_6be59_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_6be59_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_6be59_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_6be59_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_6be59_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_6be59_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_6be59_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6be59_row0_col0\" class=\"data row0 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.3$)</td>\n",
       "      <td id=\"T_6be59_row0_col1\" class=\"data row0 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row0_col2\" class=\"data row0 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row0_col3\" class=\"data row0 col3\" >22.9</td>\n",
       "      <td id=\"T_6be59_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_6be59_row0_col5\" class=\"data row0 col5\" >153.7</td>\n",
       "      <td id=\"T_6be59_row0_col6\" class=\"data row0 col6\" >35.6</td>\n",
       "      <td id=\"T_6be59_row0_col7\" class=\"data row0 col7\" >32.3</td>\n",
       "      <td id=\"T_6be59_row0_col8\" class=\"data row0 col8\" >36.6</td>\n",
       "      <td id=\"T_6be59_row0_col9\" class=\"data row0 col9\" >37.7</td>\n",
       "      <td id=\"T_6be59_row0_col10\" class=\"data row0 col10\" >4.8</td>\n",
       "      <td id=\"T_6be59_row0_col11\" class=\"data row0 col11\" >10.2</td>\n",
       "      <td id=\"T_6be59_row0_col12\" class=\"data row0 col12\" >33.9</td>\n",
       "      <td id=\"T_6be59_row0_col13\" class=\"data row0 col13\" >33.1</td>\n",
       "      <td id=\"T_6be59_row0_col14\" class=\"data row0 col14\" >7.2</td>\n",
       "      <td id=\"T_6be59_row0_col15\" class=\"data row0 col15\" >34.7</td>\n",
       "      <td id=\"T_6be59_row0_col16\" class=\"data row0 col16\" >7.9</td>\n",
       "      <td id=\"T_6be59_row0_col17\" class=\"data row0 col17\" >37.1</td>\n",
       "      <td id=\"T_6be59_row0_col18\" class=\"data row0 col18\" >7.5</td>\n",
       "      <td id=\"T_6be59_row0_col19\" class=\"data row0 col19\" >33.5</td>\n",
       "      <td id=\"T_6be59_row0_col20\" class=\"data row0 col20\" >20.9</td>\n",
       "      <td id=\"T_6be59_row0_col21\" class=\"data row0 col21\" >7.9</td>\n",
       "      <td id=\"T_6be59_row0_col22\" class=\"data row0 col22\" >24.9</td>\n",
       "      <td id=\"T_6be59_row0_col23\" class=\"data row0 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6be59_row1_col0\" class=\"data row1 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td id=\"T_6be59_row1_col1\" class=\"data row1 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row1_col2\" class=\"data row1 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row1_col3\" class=\"data row1 col3\" >22.5</td>\n",
       "      <td id=\"T_6be59_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
       "      <td id=\"T_6be59_row1_col5\" class=\"data row1 col5\" >138.2</td>\n",
       "      <td id=\"T_6be59_row1_col6\" class=\"data row1 col6\" >33.8</td>\n",
       "      <td id=\"T_6be59_row1_col7\" class=\"data row1 col7\" >31.9</td>\n",
       "      <td id=\"T_6be59_row1_col8\" class=\"data row1 col8\" >36.4</td>\n",
       "      <td id=\"T_6be59_row1_col9\" class=\"data row1 col9\" >37.5</td>\n",
       "      <td id=\"T_6be59_row1_col10\" class=\"data row1 col10\" >3.8</td>\n",
       "      <td id=\"T_6be59_row1_col11\" class=\"data row1 col11\" >8.8</td>\n",
       "      <td id=\"T_6be59_row1_col12\" class=\"data row1 col12\" >32.4</td>\n",
       "      <td id=\"T_6be59_row1_col13\" class=\"data row1 col13\" >33.4</td>\n",
       "      <td id=\"T_6be59_row1_col14\" class=\"data row1 col14\" >7.8</td>\n",
       "      <td id=\"T_6be59_row1_col15\" class=\"data row1 col15\" >33.5</td>\n",
       "      <td id=\"T_6be59_row1_col16\" class=\"data row1 col16\" >9.1</td>\n",
       "      <td id=\"T_6be59_row1_col17\" class=\"data row1 col17\" >37.0</td>\n",
       "      <td id=\"T_6be59_row1_col18\" class=\"data row1 col18\" >6.3</td>\n",
       "      <td id=\"T_6be59_row1_col19\" class=\"data row1 col19\" >32.9</td>\n",
       "      <td id=\"T_6be59_row1_col20\" class=\"data row1 col20\" >20.6</td>\n",
       "      <td id=\"T_6be59_row1_col21\" class=\"data row1 col21\" >9.1</td>\n",
       "      <td id=\"T_6be59_row1_col22\" class=\"data row1 col22\" >24.4</td>\n",
       "      <td id=\"T_6be59_row1_col23\" class=\"data row1 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6be59_row2_col0\" class=\"data row2 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=0.1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=20000:ep=4</td>\n",
       "      <td id=\"T_6be59_row2_col1\" class=\"data row2 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row2_col2\" class=\"data row2 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row2_col3\" class=\"data row2 col3\" >22.6</td>\n",
       "      <td id=\"T_6be59_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
       "      <td id=\"T_6be59_row2_col5\" class=\"data row2 col5\" >131.7</td>\n",
       "      <td id=\"T_6be59_row2_col6\" class=\"data row2 col6\" >33.0</td>\n",
       "      <td id=\"T_6be59_row2_col7\" class=\"data row2 col7\" >31.3</td>\n",
       "      <td id=\"T_6be59_row2_col8\" class=\"data row2 col8\" >34.6</td>\n",
       "      <td id=\"T_6be59_row2_col9\" class=\"data row2 col9\" >38.2</td>\n",
       "      <td id=\"T_6be59_row2_col10\" class=\"data row2 col10\" >4.8</td>\n",
       "      <td id=\"T_6be59_row2_col11\" class=\"data row2 col11\" >8.2</td>\n",
       "      <td id=\"T_6be59_row2_col12\" class=\"data row2 col12\" >33.7</td>\n",
       "      <td id=\"T_6be59_row2_col13\" class=\"data row2 col13\" >34.1</td>\n",
       "      <td id=\"T_6be59_row2_col14\" class=\"data row2 col14\" >7.7</td>\n",
       "      <td id=\"T_6be59_row2_col15\" class=\"data row2 col15\" >35.1</td>\n",
       "      <td id=\"T_6be59_row2_col16\" class=\"data row2 col16\" >7.3</td>\n",
       "      <td id=\"T_6be59_row2_col17\" class=\"data row2 col17\" >36.4</td>\n",
       "      <td id=\"T_6be59_row2_col18\" class=\"data row2 col18\" >6.5</td>\n",
       "      <td id=\"T_6be59_row2_col19\" class=\"data row2 col19\" >33.9</td>\n",
       "      <td id=\"T_6be59_row2_col20\" class=\"data row2 col20\" >21.4</td>\n",
       "      <td id=\"T_6be59_row2_col21\" class=\"data row2 col21\" >7.3</td>\n",
       "      <td id=\"T_6be59_row2_col22\" class=\"data row2 col22\" >24.4</td>\n",
       "      <td id=\"T_6be59_row2_col23\" class=\"data row2 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6be59_row3_col0\" class=\"data row3 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.6$)</td>\n",
       "      <td id=\"T_6be59_row3_col1\" class=\"data row3 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row3_col2\" class=\"data row3 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row3_col3\" class=\"data row3 col3\" >23.2</td>\n",
       "      <td id=\"T_6be59_row3_col4\" class=\"data row3 col4\" >nan</td>\n",
       "      <td id=\"T_6be59_row3_col5\" class=\"data row3 col5\" >163.8</td>\n",
       "      <td id=\"T_6be59_row3_col6\" class=\"data row3 col6\" >35.5</td>\n",
       "      <td id=\"T_6be59_row3_col7\" class=\"data row3 col7\" >31.0</td>\n",
       "      <td id=\"T_6be59_row3_col8\" class=\"data row3 col8\" >36.9</td>\n",
       "      <td id=\"T_6be59_row3_col9\" class=\"data row3 col9\" >36.6</td>\n",
       "      <td id=\"T_6be59_row3_col10\" class=\"data row3 col10\" >4.8</td>\n",
       "      <td id=\"T_6be59_row3_col11\" class=\"data row3 col11\" >11.4</td>\n",
       "      <td id=\"T_6be59_row3_col12\" class=\"data row3 col12\" >33.1</td>\n",
       "      <td id=\"T_6be59_row3_col13\" class=\"data row3 col13\" >35.2</td>\n",
       "      <td id=\"T_6be59_row3_col14\" class=\"data row3 col14\" >8.0</td>\n",
       "      <td id=\"T_6be59_row3_col15\" class=\"data row3 col15\" >35.1</td>\n",
       "      <td id=\"T_6be59_row3_col16\" class=\"data row3 col16\" >7.9</td>\n",
       "      <td id=\"T_6be59_row3_col17\" class=\"data row3 col17\" >36.8</td>\n",
       "      <td id=\"T_6be59_row3_col18\" class=\"data row3 col18\" >8.1</td>\n",
       "      <td id=\"T_6be59_row3_col19\" class=\"data row3 col19\" >34.1</td>\n",
       "      <td id=\"T_6be59_row3_col20\" class=\"data row3 col20\" >21.6</td>\n",
       "      <td id=\"T_6be59_row3_col21\" class=\"data row3 col21\" >7.9</td>\n",
       "      <td id=\"T_6be59_row3_col22\" class=\"data row3 col22\" >25.0</td>\n",
       "      <td id=\"T_6be59_row3_col23\" class=\"data row3 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6be59_row4_col0\" class=\"data row4 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks)</td>\n",
       "      <td id=\"T_6be59_row4_col1\" class=\"data row4 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row4_col2\" class=\"data row4 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row4_col3\" class=\"data row4 col3\" >23.2</td>\n",
       "      <td id=\"T_6be59_row4_col4\" class=\"data row4 col4\" >41.1</td>\n",
       "      <td id=\"T_6be59_row4_col5\" class=\"data row4 col5\" >175.0</td>\n",
       "      <td id=\"T_6be59_row4_col6\" class=\"data row4 col6\" >33.1</td>\n",
       "      <td id=\"T_6be59_row4_col7\" class=\"data row4 col7\" >30.0</td>\n",
       "      <td id=\"T_6be59_row4_col8\" class=\"data row4 col8\" >35.9</td>\n",
       "      <td id=\"T_6be59_row4_col9\" class=\"data row4 col9\" >36.5</td>\n",
       "      <td id=\"T_6be59_row4_col10\" class=\"data row4 col10\" >5.2</td>\n",
       "      <td id=\"T_6be59_row4_col11\" class=\"data row4 col11\" >12.2</td>\n",
       "      <td id=\"T_6be59_row4_col12\" class=\"data row4 col12\" >32.7</td>\n",
       "      <td id=\"T_6be59_row4_col13\" class=\"data row4 col13\" >33.7</td>\n",
       "      <td id=\"T_6be59_row4_col14\" class=\"data row4 col14\" >8.0</td>\n",
       "      <td id=\"T_6be59_row4_col15\" class=\"data row4 col15\" >35.5</td>\n",
       "      <td id=\"T_6be59_row4_col16\" class=\"data row4 col16\" >9.1</td>\n",
       "      <td id=\"T_6be59_row4_col17\" class=\"data row4 col17\" >36.2</td>\n",
       "      <td id=\"T_6be59_row4_col18\" class=\"data row4 col18\" >8.7</td>\n",
       "      <td id=\"T_6be59_row4_col19\" class=\"data row4 col19\" >33.2</td>\n",
       "      <td id=\"T_6be59_row4_col20\" class=\"data row4 col20\" >21.8</td>\n",
       "      <td id=\"T_6be59_row4_col21\" class=\"data row4 col21\" >9.1</td>\n",
       "      <td id=\"T_6be59_row4_col22\" class=\"data row4 col22\" >26.1</td>\n",
       "      <td id=\"T_6be59_row4_col23\" class=\"data row4 col23\" >-36.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_6be59_row5_col0\" class=\"data row5 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$)</td>\n",
       "      <td id=\"T_6be59_row5_col1\" class=\"data row5 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row5_col2\" class=\"data row5 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row5_col3\" class=\"data row5 col3\" >21.9</td>\n",
       "      <td id=\"T_6be59_row5_col4\" class=\"data row5 col4\" >27.0</td>\n",
       "      <td id=\"T_6be59_row5_col5\" class=\"data row5 col5\" >116.4</td>\n",
       "      <td id=\"T_6be59_row5_col6\" class=\"data row5 col6\" >30.1</td>\n",
       "      <td id=\"T_6be59_row5_col7\" class=\"data row5 col7\" >29.8</td>\n",
       "      <td id=\"T_6be59_row5_col8\" class=\"data row5 col8\" >32.7</td>\n",
       "      <td id=\"T_6be59_row5_col9\" class=\"data row5 col9\" >34.8</td>\n",
       "      <td id=\"T_6be59_row5_col10\" class=\"data row5 col10\" >5.6</td>\n",
       "      <td id=\"T_6be59_row5_col11\" class=\"data row5 col11\" >7.8</td>\n",
       "      <td id=\"T_6be59_row5_col12\" class=\"data row5 col12\" >33.3</td>\n",
       "      <td id=\"T_6be59_row5_col13\" class=\"data row5 col13\" >31.6</td>\n",
       "      <td id=\"T_6be59_row5_col14\" class=\"data row5 col14\" >7.7</td>\n",
       "      <td id=\"T_6be59_row5_col15\" class=\"data row5 col15\" >36.4</td>\n",
       "      <td id=\"T_6be59_row5_col16\" class=\"data row5 col16\" >7.3</td>\n",
       "      <td id=\"T_6be59_row5_col17\" class=\"data row5 col17\" >33.8</td>\n",
       "      <td id=\"T_6be59_row5_col18\" class=\"data row5 col18\" >6.7</td>\n",
       "      <td id=\"T_6be59_row5_col19\" class=\"data row5 col19\" >32.5</td>\n",
       "      <td id=\"T_6be59_row5_col20\" class=\"data row5 col20\" >22.1</td>\n",
       "      <td id=\"T_6be59_row5_col21\" class=\"data row5 col21\" >7.3</td>\n",
       "      <td id=\"T_6be59_row5_col22\" class=\"data row5 col22\" >23.7</td>\n",
       "      <td id=\"T_6be59_row5_col23\" class=\"data row5 col23\" >-49.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_6be59_row6_col0\" class=\"data row6 col0\" >\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_6be59_row6_col1\" class=\"data row6 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row6_col2\" class=\"data row6 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row6_col3\" class=\"data row6 col3\" >23.5</td>\n",
       "      <td id=\"T_6be59_row6_col4\" class=\"data row6 col4\" >43.2</td>\n",
       "      <td id=\"T_6be59_row6_col5\" class=\"data row6 col5\" >191.8</td>\n",
       "      <td id=\"T_6be59_row6_col6\" class=\"data row6 col6\" >33.0</td>\n",
       "      <td id=\"T_6be59_row6_col7\" class=\"data row6 col7\" >29.3</td>\n",
       "      <td id=\"T_6be59_row6_col8\" class=\"data row6 col8\" >36.7</td>\n",
       "      <td id=\"T_6be59_row6_col9\" class=\"data row6 col9\" >37.3</td>\n",
       "      <td id=\"T_6be59_row6_col10\" class=\"data row6 col10\" >4.8</td>\n",
       "      <td id=\"T_6be59_row6_col11\" class=\"data row6 col11\" >10.8</td>\n",
       "      <td id=\"T_6be59_row6_col12\" class=\"data row6 col12\" >33.2</td>\n",
       "      <td id=\"T_6be59_row6_col13\" class=\"data row6 col13\" >35.0</td>\n",
       "      <td id=\"T_6be59_row6_col14\" class=\"data row6 col14\" >8.2</td>\n",
       "      <td id=\"T_6be59_row6_col15\" class=\"data row6 col15\" >35.8</td>\n",
       "      <td id=\"T_6be59_row6_col16\" class=\"data row6 col16\" >9.8</td>\n",
       "      <td id=\"T_6be59_row6_col17\" class=\"data row6 col17\" >37.0</td>\n",
       "      <td id=\"T_6be59_row6_col18\" class=\"data row6 col18\" >7.8</td>\n",
       "      <td id=\"T_6be59_row6_col19\" class=\"data row6 col19\" >34.1</td>\n",
       "      <td id=\"T_6be59_row6_col20\" class=\"data row6 col20\" >22.0</td>\n",
       "      <td id=\"T_6be59_row6_col21\" class=\"data row6 col21\" >9.8</td>\n",
       "      <td id=\"T_6be59_row6_col22\" class=\"data row6 col22\" >26.4</td>\n",
       "      <td id=\"T_6be59_row6_col23\" class=\"data row6 col23\" >-33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6be59_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_6be59_row7_col0\" class=\"data row7 col0\" >Random</td>\n",
       "      <td id=\"T_6be59_row7_col1\" class=\"data row7 col1\" >20000</td>\n",
       "      <td id=\"T_6be59_row7_col2\" class=\"data row7 col2\" >5000</td>\n",
       "      <td id=\"T_6be59_row7_col3\" class=\"data row7 col3\" >22.4</td>\n",
       "      <td id=\"T_6be59_row7_col4\" class=\"data row7 col4\" >22.0</td>\n",
       "      <td id=\"T_6be59_row7_col5\" class=\"data row7 col5\" >90.8</td>\n",
       "      <td id=\"T_6be59_row7_col6\" class=\"data row7 col6\" >26.3</td>\n",
       "      <td id=\"T_6be59_row7_col7\" class=\"data row7 col7\" >27.6</td>\n",
       "      <td id=\"T_6be59_row7_col8\" class=\"data row7 col8\" >35.6</td>\n",
       "      <td id=\"T_6be59_row7_col9\" class=\"data row7 col9\" >37.1</td>\n",
       "      <td id=\"T_6be59_row7_col10\" class=\"data row7 col10\" >4.2</td>\n",
       "      <td id=\"T_6be59_row7_col11\" class=\"data row7 col11\" >6.8</td>\n",
       "      <td id=\"T_6be59_row7_col12\" class=\"data row7 col12\" >32.4</td>\n",
       "      <td id=\"T_6be59_row7_col13\" class=\"data row7 col13\" >32.7</td>\n",
       "      <td id=\"T_6be59_row7_col14\" class=\"data row7 col14\" >7.4</td>\n",
       "      <td id=\"T_6be59_row7_col15\" class=\"data row7 col15\" >35.4</td>\n",
       "      <td id=\"T_6be59_row7_col16\" class=\"data row7 col16\" >9.8</td>\n",
       "      <td id=\"T_6be59_row7_col17\" class=\"data row7 col17\" >36.4</td>\n",
       "      <td id=\"T_6be59_row7_col18\" class=\"data row7 col18\" >5.5</td>\n",
       "      <td id=\"T_6be59_row7_col19\" class=\"data row7 col19\" >32.5</td>\n",
       "      <td id=\"T_6be59_row7_col20\" class=\"data row7 col20\" >21.4</td>\n",
       "      <td id=\"T_6be59_row7_col21\" class=\"data row7 col21\" >9.8</td>\n",
       "      <td id=\"T_6be59_row7_col22\" class=\"data row7 col22\" >23.1</td>\n",
       "      <td id=\"T_6be59_row7_col23\" class=\"data row7 col23\" >-55.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca27f5d720>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b9c24 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_b9c24_row0_col0, #T_b9c24_row1_col0, #T_b9c24_row2_col0, #T_b9c24_row3_col0, #T_b9c24_row4_col0, #T_b9c24_row5_col0, #T_b9c24_row6_col0, #T_b9c24_row7_col0, #T_b9c24_row8_col0, #T_b9c24_row9_col0, #T_b9c24_row10_col0, #T_b9c24_row11_col0, #T_b9c24_row12_col0, #T_b9c24_row13_col0, #T_b9c24_row14_col0, #T_b9c24_row15_col0, #T_b9c24_row16_col0, #T_b9c24_row17_col0, #T_b9c24_row18_col0, #T_b9c24_row19_col0, #T_b9c24_row20_col0, #T_b9c24_row21_col0, #T_b9c24_row22_col0, #T_b9c24_row23_col0, #T_b9c24_row24_col0, #T_b9c24_row25_col0, #T_b9c24_row26_col0, #T_b9c24_row27_col0, #T_b9c24_row28_col0, #T_b9c24_row29_col0, #T_b9c24_row30_col0, #T_b9c24_row31_col0, #T_b9c24_row32_col0, #T_b9c24_row33_col0, #T_b9c24_row34_col0, #T_b9c24_row35_col0, #T_b9c24_row36_col0, #T_b9c24_row37_col0, #T_b9c24_row38_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b9c24_row0_col1, #T_b9c24_row0_col2, #T_b9c24_row1_col1, #T_b9c24_row1_col2, #T_b9c24_row2_col1, #T_b9c24_row2_col2, #T_b9c24_row3_col1, #T_b9c24_row3_col2, #T_b9c24_row4_col1, #T_b9c24_row4_col2, #T_b9c24_row5_col1, #T_b9c24_row5_col2, #T_b9c24_row6_col1, #T_b9c24_row6_col2, #T_b9c24_row6_col3, #T_b9c24_row6_col8, #T_b9c24_row6_col9, #T_b9c24_row6_col17, #T_b9c24_row7_col1, #T_b9c24_row7_col2, #T_b9c24_row8_col1, #T_b9c24_row8_col2, #T_b9c24_row9_col1, #T_b9c24_row9_col2, #T_b9c24_row10_col1, #T_b9c24_row10_col2, #T_b9c24_row11_col1, #T_b9c24_row11_col2, #T_b9c24_row12_col1, #T_b9c24_row12_col2, #T_b9c24_row12_col10, #T_b9c24_row13_col1, #T_b9c24_row13_col2, #T_b9c24_row14_col1, #T_b9c24_row14_col2, #T_b9c24_row15_col1, #T_b9c24_row15_col2, #T_b9c24_row16_col1, #T_b9c24_row16_col2, #T_b9c24_row16_col20, #T_b9c24_row16_col23, #T_b9c24_row17_col1, #T_b9c24_row17_col2, #T_b9c24_row18_col1, #T_b9c24_row18_col2, #T_b9c24_row18_col11, #T_b9c24_row18_col16, #T_b9c24_row18_col21, #T_b9c24_row19_col1, #T_b9c24_row19_col2, #T_b9c24_row19_col12, #T_b9c24_row20_col1, #T_b9c24_row20_col2, #T_b9c24_row21_col1, #T_b9c24_row21_col2, #T_b9c24_row22_col1, #T_b9c24_row22_col2, #T_b9c24_row23_col1, #T_b9c24_row23_col2, #T_b9c24_row24_col1, #T_b9c24_row24_col2, #T_b9c24_row24_col22, #T_b9c24_row25_col1, #T_b9c24_row25_col2, #T_b9c24_row25_col14, #T_b9c24_row25_col20, #T_b9c24_row26_col1, #T_b9c24_row26_col2, #T_b9c24_row27_col1, #T_b9c24_row27_col2, #T_b9c24_row27_col15, #T_b9c24_row28_col1, #T_b9c24_row28_col2, #T_b9c24_row29_col1, #T_b9c24_row29_col2, #T_b9c24_row30_col1, #T_b9c24_row30_col2, #T_b9c24_row31_col1, #T_b9c24_row31_col2, #T_b9c24_row31_col11, #T_b9c24_row31_col18, #T_b9c24_row32_col1, #T_b9c24_row32_col2, #T_b9c24_row32_col4, #T_b9c24_row32_col5, #T_b9c24_row32_col6, #T_b9c24_row32_col7, #T_b9c24_row32_col13, #T_b9c24_row32_col19, #T_b9c24_row33_col1, #T_b9c24_row33_col2, #T_b9c24_row34_col1, #T_b9c24_row34_col2, #T_b9c24_row34_col10, #T_b9c24_row35_col1, #T_b9c24_row35_col2, #T_b9c24_row36_col1, #T_b9c24_row36_col2, #T_b9c24_row37_col1, #T_b9c24_row37_col2, #T_b9c24_row38_col1, #T_b9c24_row38_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row0_col3, #T_b9c24_row3_col17, #T_b9c24_row9_col9, #T_b9c24_row36_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col4, #T_b9c24_row0_col23, #T_b9c24_row1_col4, #T_b9c24_row1_col23, #T_b9c24_row2_col4, #T_b9c24_row2_col23, #T_b9c24_row8_col4, #T_b9c24_row8_col23, #T_b9c24_row9_col4, #T_b9c24_row9_col23, #T_b9c24_row11_col4, #T_b9c24_row11_col23, #T_b9c24_row14_col3, #T_b9c24_row14_col4, #T_b9c24_row14_col8, #T_b9c24_row14_col9, #T_b9c24_row14_col13, #T_b9c24_row14_col17, #T_b9c24_row14_col19, #T_b9c24_row14_col23, #T_b9c24_row20_col4, #T_b9c24_row20_col23, #T_b9c24_row21_col3, #T_b9c24_row21_col4, #T_b9c24_row21_col9, #T_b9c24_row21_col17, #T_b9c24_row21_col23, #T_b9c24_row23_col4, #T_b9c24_row23_col23, #T_b9c24_row24_col3, #T_b9c24_row24_col4, #T_b9c24_row24_col8, #T_b9c24_row24_col9, #T_b9c24_row24_col11, #T_b9c24_row24_col12, #T_b9c24_row24_col13, #T_b9c24_row24_col14, #T_b9c24_row24_col15, #T_b9c24_row24_col16, #T_b9c24_row24_col17, #T_b9c24_row24_col18, #T_b9c24_row24_col19, #T_b9c24_row24_col20, #T_b9c24_row24_col21, #T_b9c24_row24_col23, #T_b9c24_row25_col4, #T_b9c24_row25_col23, #T_b9c24_row29_col4, #T_b9c24_row29_col23, #T_b9c24_row30_col4, #T_b9c24_row30_col23, #T_b9c24_row31_col4, #T_b9c24_row31_col23, #T_b9c24_row33_col5, #T_b9c24_row33_col6, #T_b9c24_row33_col7, #T_b9c24_row33_col23, #T_b9c24_row34_col5, #T_b9c24_row34_col6, #T_b9c24_row34_col7, #T_b9c24_row34_col23, #T_b9c24_row35_col5, #T_b9c24_row35_col6, #T_b9c24_row35_col7, #T_b9c24_row35_col23, #T_b9c24_row36_col4, #T_b9c24_row36_col5, #T_b9c24_row36_col6, #T_b9c24_row36_col7, #T_b9c24_row36_col23, #T_b9c24_row37_col4, #T_b9c24_row37_col5, #T_b9c24_row37_col6, #T_b9c24_row37_col7, #T_b9c24_row37_col23, #T_b9c24_row38_col5, #T_b9c24_row38_col6, #T_b9c24_row38_col7, #T_b9c24_row38_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row0_col5, #T_b9c24_row10_col18, #T_b9c24_row12_col6, #T_b9c24_row12_col19, #T_b9c24_row19_col8, #T_b9c24_row22_col13, #T_b9c24_row33_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col6, #T_b9c24_row0_col7, #T_b9c24_row0_col11, #T_b9c24_row0_col16, #T_b9c24_row0_col18, #T_b9c24_row0_col21, #T_b9c24_row3_col22, #T_b9c24_row3_col23, #T_b9c24_row5_col5, #T_b9c24_row7_col19, #T_b9c24_row10_col4, #T_b9c24_row10_col13, #T_b9c24_row20_col12, #T_b9c24_row23_col10, #T_b9c24_row28_col3, #T_b9c24_row28_col8, #T_b9c24_row28_col9, #T_b9c24_row28_col17, #T_b9c24_row30_col14, #T_b9c24_row30_col15, #T_b9c24_row30_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row0_col8, #T_b9c24_row1_col12, #T_b9c24_row2_col8, #T_b9c24_row14_col14, #T_b9c24_row17_col20, #T_b9c24_row21_col12, #T_b9c24_row26_col5, #T_b9c24_row38_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col9, #T_b9c24_row12_col4, #T_b9c24_row16_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #e7d7ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col10, #T_b9c24_row2_col10, #T_b9c24_row9_col10, #T_b9c24_row13_col20, #T_b9c24_row18_col8, #T_b9c24_row20_col10, #T_b9c24_row21_col5, #T_b9c24_row22_col10, #T_b9c24_row27_col10, #T_b9c24_row29_col6, #T_b9c24_row32_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #d4dbe6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col12, #T_b9c24_row10_col12, #T_b9c24_row12_col14, #T_b9c24_row17_col7, #T_b9c24_row20_col3, #T_b9c24_row20_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col13, #T_b9c24_row9_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row0_col14, #T_b9c24_row2_col22, #T_b9c24_row3_col14, #T_b9c24_row8_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row0_col15, #T_b9c24_row26_col11, #T_b9c24_row28_col7, #T_b9c24_row29_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col17, #T_b9c24_row11_col9, #T_b9c24_row12_col3, #T_b9c24_row13_col15, #T_b9c24_row26_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d9dce1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row0_col20, #T_b9c24_row22_col5, #T_b9c24_row28_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #d2dbe8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row0_col22, #T_b9c24_row35_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row1_col3, #T_b9c24_row11_col18, #T_b9c24_row22_col18, #T_b9c24_row23_col5, #T_b9c24_row26_col18, #T_b9c24_row36_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d5dbe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col5, #T_b9c24_row12_col7, #T_b9c24_row13_col7, #T_b9c24_row14_col6, #T_b9c24_row15_col13, #T_b9c24_row33_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col6, #T_b9c24_row5_col22, #T_b9c24_row10_col19, #T_b9c24_row13_col18, #T_b9c24_row23_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row1_col7, #T_b9c24_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row1_col8, #T_b9c24_row17_col17, #T_b9c24_row30_col5, #T_b9c24_row36_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col9, #T_b9c24_row13_col23, #T_b9c24_row20_col14, #T_b9c24_row25_col6, #T_b9c24_row26_col14, #T_b9c24_row36_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col10, #T_b9c24_row4_col10, #T_b9c24_row5_col8, #T_b9c24_row5_col17, #T_b9c24_row6_col10, #T_b9c24_row7_col17, #T_b9c24_row9_col3, #T_b9c24_row19_col13, #T_b9c24_row19_col22, #T_b9c24_row20_col7, #T_b9c24_row22_col6, #T_b9c24_row26_col13, #T_b9c24_row27_col13, #T_b9c24_row30_col10, #T_b9c24_row37_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col11, #T_b9c24_row2_col17, #T_b9c24_row4_col3, #T_b9c24_row33_col11, #T_b9c24_row38_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col13, #T_b9c24_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row1_col14, #T_b9c24_row4_col9, #T_b9c24_row5_col9, #T_b9c24_row10_col8, #T_b9c24_row16_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col15, #T_b9c24_row25_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col16, #T_b9c24_row1_col21, #T_b9c24_row3_col12, #T_b9c24_row5_col12, #T_b9c24_row7_col16, #T_b9c24_row7_col21, #T_b9c24_row8_col19, #T_b9c24_row12_col12, #T_b9c24_row17_col13, #T_b9c24_row23_col17, #T_b9c24_row27_col16, #T_b9c24_row27_col21, #T_b9c24_row35_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f49a7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col17, #T_b9c24_row20_col5, #T_b9c24_row25_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col18, #T_b9c24_row2_col14, #T_b9c24_row5_col3, #T_b9c24_row13_col22, #T_b9c24_row16_col8, #T_b9c24_row26_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row1_col20, #T_b9c24_row3_col20, #T_b9c24_row19_col4, #T_b9c24_row30_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #c1d4f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row1_col22, #T_b9c24_row3_col18, #T_b9c24_row6_col7, #T_b9c24_row6_col19, #T_b9c24_row7_col7, #T_b9c24_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row2_col3, #T_b9c24_row6_col23, #T_b9c24_row27_col3, #T_b9c24_row29_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e0dbd8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col5, #T_b9c24_row3_col11, #T_b9c24_row4_col11, #T_b9c24_row21_col8, #T_b9c24_row27_col17, #T_b9c24_row34_col11, #T_b9c24_row36_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row2_col7, #T_b9c24_row9_col13, #T_b9c24_row29_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row2_col9, #T_b9c24_row3_col10, #T_b9c24_row7_col10, #T_b9c24_row8_col10, #T_b9c24_row13_col10, #T_b9c24_row15_col10, #T_b9c24_row26_col6, #T_b9c24_row34_col18, #T_b9c24_row34_col22, #T_b9c24_row38_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col11, #T_b9c24_row3_col9, #T_b9c24_row4_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col12, #T_b9c24_row18_col9, #T_b9c24_row24_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col13, #T_b9c24_row22_col12, #T_b9c24_row31_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row2_col15, #T_b9c24_row11_col3, #T_b9c24_row17_col8, #T_b9c24_row30_col13, #T_b9c24_row31_col20, #T_b9c24_row33_col15, #T_b9c24_row34_col15, #T_b9c24_row35_col8, #T_b9c24_row35_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col16, #T_b9c24_row2_col21, #T_b9c24_row3_col16, #T_b9c24_row3_col21, #T_b9c24_row4_col8, #T_b9c24_row12_col16, #T_b9c24_row12_col21, #T_b9c24_row21_col11, #T_b9c24_row23_col3, #T_b9c24_row23_col11, #T_b9c24_row25_col10, #T_b9c24_row29_col17, #T_b9c24_row33_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col18, #T_b9c24_row8_col22, #T_b9c24_row30_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row2_col19, #T_b9c24_row10_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row2_col20, #T_b9c24_row17_col9, #T_b9c24_row23_col14, #T_b9c24_row31_col22, #T_b9c24_row38_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row3_col3, #T_b9c24_row23_col22, #T_b9c24_row26_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row3_col5, #T_b9c24_row20_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row3_col6, #T_b9c24_row10_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row3_col8, #T_b9c24_row8_col17, #T_b9c24_row11_col16, #T_b9c24_row11_col21, #T_b9c24_row14_col5, #T_b9c24_row14_col16, #T_b9c24_row14_col21, #T_b9c24_row15_col22, #T_b9c24_row21_col19, #T_b9c24_row23_col16, #T_b9c24_row23_col21, #T_b9c24_row26_col16, #T_b9c24_row26_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row3_col13, #T_b9c24_row5_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row3_col15, #T_b9c24_row6_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row3_col19, #T_b9c24_row28_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col4, #T_b9c24_row7_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col5, #T_b9c24_row31_col9, #T_b9c24_row32_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #df634e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col6, #T_b9c24_row7_col13, #T_b9c24_row30_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col7, #T_b9c24_row6_col13, #T_b9c24_row13_col19, #T_b9c24_row25_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col12, #T_b9c24_row11_col15, #T_b9c24_row21_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #799cf8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col13, #T_b9c24_row6_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row4_col15, #T_b9c24_row7_col15, #T_b9c24_row15_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col16, #T_b9c24_row4_col21, #T_b9c24_row6_col16, #T_b9c24_row6_col21, #T_b9c24_row15_col18, #T_b9c24_row17_col16, #T_b9c24_row17_col21, #T_b9c24_row22_col16, #T_b9c24_row22_col21, #T_b9c24_row25_col16, #T_b9c24_row25_col21, #T_b9c24_row31_col16, #T_b9c24_row31_col21, #T_b9c24_row32_col16, #T_b9c24_row32_col21, #T_b9c24_row33_col16, #T_b9c24_row33_col21, #T_b9c24_row36_col16, #T_b9c24_row36_col21, #T_b9c24_row37_col16, #T_b9c24_row37_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #8caffe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row4_col18, #T_b9c24_row6_col11, #T_b9c24_row7_col11, #T_b9c24_row13_col13, #T_b9c24_row19_col11, #T_b9c24_row28_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col19, #T_b9c24_row17_col12, #T_b9c24_row24_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row4_col20, #T_b9c24_row13_col8, #T_b9c24_row32_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col22, #T_b9c24_row7_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row4_col23, #T_b9c24_row7_col9, #T_b9c24_row8_col8, #T_b9c24_row33_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row5_col6, #T_b9c24_row6_col6, #T_b9c24_row18_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row5_col7, #T_b9c24_row35_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #e46e56;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row5_col10, #T_b9c24_row5_col16, #T_b9c24_row5_col21, #T_b9c24_row10_col10, #T_b9c24_row16_col16, #T_b9c24_row16_col21, #T_b9c24_row20_col16, #T_b9c24_row20_col21, #T_b9c24_row34_col16, #T_b9c24_row34_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row5_col11, #T_b9c24_row7_col6, #T_b9c24_row9_col19, #T_b9c24_row13_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #cd423b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row5_col13, #T_b9c24_row7_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row5_col14, #T_b9c24_row6_col22, #T_b9c24_row7_col3, #T_b9c24_row18_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row5_col15, #T_b9c24_row16_col18, #T_b9c24_row19_col15, #T_b9c24_row25_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row5_col18, #T_b9c24_row7_col18, #T_b9c24_row32_col15, #T_b9c24_row32_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row5_col19, #T_b9c24_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b70d28;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row5_col20, #T_b9c24_row23_col15, #T_b9c24_row26_col4, #T_b9c24_row30_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row5_col23, #T_b9c24_row28_col19, #T_b9c24_row28_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row6_col4, #T_b9c24_row32_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row6_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row6_col12, #T_b9c24_row11_col17, #T_b9c24_row18_col19, #T_b9c24_row23_col12, #T_b9c24_row32_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dbdcde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row6_col14, #T_b9c24_row8_col11, #T_b9c24_row8_col14, #T_b9c24_row22_col11, #T_b9c24_row34_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #b6cefa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row6_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row7_col8, #T_b9c24_row9_col17, #T_b9c24_row12_col5, #T_b9c24_row17_col6, #T_b9c24_row19_col7, #T_b9c24_row19_col16, #T_b9c24_row19_col21, #T_b9c24_row31_col12, #T_b9c24_row37_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row7_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row7_col14, #T_b9c24_row29_col22, #T_b9c24_row34_col17, #T_b9c24_row35_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row7_col20, #T_b9c24_row12_col11, #T_b9c24_row14_col11, #T_b9c24_row19_col23, #T_b9c24_row29_col11, #T_b9c24_row30_col11, #T_b9c24_row38_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row8_col3, #T_b9c24_row19_col3, #T_b9c24_row32_col12, #T_b9c24_row38_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col5, #T_b9c24_row33_col4, #T_b9c24_row34_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col7, #T_b9c24_row11_col6, #T_b9c24_row17_col19, #T_b9c24_row25_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row8_col9, #T_b9c24_row37_col8, #T_b9c24_row38_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col12, #T_b9c24_row22_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col15, #T_b9c24_row11_col20, #T_b9c24_row38_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col16, #T_b9c24_row8_col21, #T_b9c24_row10_col16, #T_b9c24_row10_col21, #T_b9c24_row16_col4, #T_b9c24_row29_col16, #T_b9c24_row29_col21, #T_b9c24_row31_col3, #T_b9c24_row35_col16, #T_b9c24_row35_col21, #T_b9c24_row37_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col18, #T_b9c24_row11_col8, #T_b9c24_row24_col7, #T_b9c24_row36_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row8_col20, #T_b9c24_row15_col14, #T_b9c24_row16_col3, #T_b9c24_row17_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col5, #T_b9c24_row20_col8, #T_b9c24_row30_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row9_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row9_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f39577;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col11, #T_b9c24_row13_col5, #T_b9c24_row21_col6, #T_b9c24_row22_col22, #T_b9c24_row23_col6, #T_b9c24_row27_col11, #T_b9c24_row35_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col12, #T_b9c24_row14_col7, #T_b9c24_row23_col9, #T_b9c24_row26_col12, #T_b9c24_row28_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col14, #T_b9c24_row34_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col15, #T_b9c24_row13_col3, #T_b9c24_row15_col16, #T_b9c24_row15_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col16, #T_b9c24_row9_col21, #T_b9c24_row15_col5, #T_b9c24_row28_col16, #T_b9c24_row28_col21, #T_b9c24_row29_col3, #T_b9c24_row36_col20, #T_b9c24_row37_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col18, #T_b9c24_row12_col8, #T_b9c24_row19_col9, #T_b9c24_row23_col19, #T_b9c24_row27_col9, #T_b9c24_row27_col18, #T_b9c24_row33_col18, #T_b9c24_row36_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row9_col20, #T_b9c24_row27_col23, #T_b9c24_row28_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col3, #T_b9c24_row15_col9, #T_b9c24_row35_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #b8122a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row10_col7, #T_b9c24_row27_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col9, #T_b9c24_row18_col7, #T_b9c24_row20_col13, #T_b9c24_row22_col9, #T_b9c24_row37_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col11, #T_b9c24_row33_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col14, #T_b9c24_row32_col9, #T_b9c24_row33_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row10_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col17, #T_b9c24_row15_col8, #T_b9c24_row16_col13, #T_b9c24_row18_col20, #T_b9c24_row18_col22, #T_b9c24_row28_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row10_col20, #T_b9c24_row27_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col5, #T_b9c24_row13_col4, #T_b9c24_row25_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col7, #T_b9c24_row29_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col10, #T_b9c24_row17_col10, #T_b9c24_row18_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row11_col11, #T_b9c24_row16_col5, #T_b9c24_row26_col3, #T_b9c24_row35_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #e1dad6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col12, #T_b9c24_row19_col17, #T_b9c24_row27_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col13, #T_b9c24_row12_col13, #T_b9c24_row16_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col14, #T_b9c24_row23_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col19, #T_b9c24_row12_col9, #T_b9c24_row12_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row11_col22, #T_b9c24_row16_col7, #T_b9c24_row18_col13, #T_b9c24_row26_col9, #T_b9c24_row36_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row12_col15, #T_b9c24_row12_col18, #T_b9c24_row14_col15, #T_b9c24_row31_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #6282ea;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row12_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b497;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row12_col20, #T_b9c24_row14_col18, #T_b9c24_row22_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row12_col23, #T_b9c24_row27_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row13_col6, #T_b9c24_row20_col9, #T_b9c24_row22_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row13_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row13_col12, #T_b9c24_row38_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row13_col14, #T_b9c24_row18_col17, #T_b9c24_row27_col6, #T_b9c24_row34_col9, #T_b9c24_row36_col17, #T_b9c24_row36_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row13_col16, #T_b9c24_row13_col21, #T_b9c24_row16_col12, #T_b9c24_row19_col19, #T_b9c24_row29_col7, #T_b9c24_row31_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row13_col17, #T_b9c24_row19_col20, #T_b9c24_row30_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row14_col10, #T_b9c24_row18_col23, #T_b9c24_row21_col10, #T_b9c24_row31_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row14_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row14_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row14_col22, #T_b9c24_row17_col11, #T_b9c24_row31_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row15_col3, #T_b9c24_row22_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b3cdfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row15_col4, #T_b9c24_row17_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row15_col6, #T_b9c24_row15_col7, #T_b9c24_row18_col4, #T_b9c24_row33_col17, #T_b9c24_row33_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row15_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row15_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row15_col15, #T_b9c24_row17_col4, #T_b9c24_row38_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row15_col17, #T_b9c24_row19_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c1a9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row15_col19, #T_b9c24_row17_col14, #T_b9c24_row27_col22, #T_b9c24_row30_col22, #T_b9c24_row38_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row15_col20, #T_b9c24_row27_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row16_col9, #T_b9c24_row18_col15, #T_b9c24_row21_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row16_col10, #T_b9c24_row26_col20, #T_b9c24_row28_col10, #T_b9c24_row35_col15, #T_b9c24_row36_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row16_col11, #T_b9c24_row20_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row16_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row16_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row16_col22, #T_b9c24_row22_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row17_col5, #T_b9c24_row29_col19, #T_b9c24_row35_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row17_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row17_col22, #T_b9c24_row19_col5, #T_b9c24_row25_col5, #T_b9c24_row25_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row17_col23, #T_b9c24_row37_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #516ddb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row18_col3, #T_b9c24_row37_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row18_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row18_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row18_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row19_col10, #T_b9c24_row21_col22, #T_b9c24_row26_col10, #T_b9c24_row37_col10, #T_b9c24_row38_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row19_col14, #T_b9c24_row37_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row19_col18, #T_b9c24_row25_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row20_col15, #T_b9c24_row21_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row20_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row20_col19, #T_b9c24_row34_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row20_col20, #T_b9c24_row23_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row20_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row21_col13, #T_b9c24_row29_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row21_col16, #T_b9c24_row21_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row21_col18, #T_b9c24_row26_col22, #T_b9c24_row29_col15, #T_b9c24_row29_col18, #T_b9c24_row29_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row21_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row22_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row22_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row22_col15, #T_b9c24_row37_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row22_col19, #T_b9c24_row33_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #ed8366;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row22_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row23_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e4d9d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row23_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row24_col5, #T_b9c24_row28_col6, #T_b9c24_row36_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row25_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row25_col12, #T_b9c24_row35_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row25_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row25_col22, #T_b9c24_row27_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e9d5cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row26_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row26_col15, #T_b9c24_row26_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row27_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c5ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row27_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row28_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row28_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row28_col18, #T_b9c24_row32_col18, #T_b9c24_row35_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row28_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row29_col9, #T_b9c24_row33_col9, #T_b9c24_row34_col8, #T_b9c24_row36_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row29_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #d7dce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row30_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #b50927;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row30_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row30_col16, #T_b9c24_row30_col21, #T_b9c24_row31_col6, #T_b9c24_row38_col16, #T_b9c24_row38_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row30_col19, #T_b9c24_row32_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row31_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row31_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row31_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row31_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #7093f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row32_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row32_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row33_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row34_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row34_col14, #T_b9c24_row35_col17, #T_b9c24_row37_col22, #T_b9c24_row38_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row34_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row35_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #8badfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row37_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c24_row37_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row38_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c24_row38_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #94b6ff;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b9c24\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b9c24_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_b9c24_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_b9c24_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_b9c24_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_b9c24_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_b9c24_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_b9c24_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_b9c24_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_b9c24_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_b9c24_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_b9c24_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_b9c24_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_b9c24_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_b9c24_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_b9c24_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_b9c24_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_b9c24_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_b9c24_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_b9c24_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_b9c24_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_b9c24_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_b9c24_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_b9c24_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_b9c24_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b9c24_row0_col0\" class=\"data row0 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=0.03:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row0_col1\" class=\"data row0 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row0_col2\" class=\"data row0 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row0_col3\" class=\"data row0 col3\" >23.3</td>\n",
       "      <td id=\"T_b9c24_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row0_col5\" class=\"data row0 col5\" >118.5</td>\n",
       "      <td id=\"T_b9c24_row0_col6\" class=\"data row0 col6\" >34.2</td>\n",
       "      <td id=\"T_b9c24_row0_col7\" class=\"data row0 col7\" >33.7</td>\n",
       "      <td id=\"T_b9c24_row0_col8\" class=\"data row0 col8\" >32.0</td>\n",
       "      <td id=\"T_b9c24_row0_col9\" class=\"data row0 col9\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row0_col10\" class=\"data row0 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row0_col11\" class=\"data row0 col11\" >10.2</td>\n",
       "      <td id=\"T_b9c24_row0_col12\" class=\"data row0 col12\" >34.0</td>\n",
       "      <td id=\"T_b9c24_row0_col13\" class=\"data row0 col13\" >33.0</td>\n",
       "      <td id=\"T_b9c24_row0_col14\" class=\"data row0 col14\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row0_col15\" class=\"data row0 col15\" >36.6</td>\n",
       "      <td id=\"T_b9c24_row0_col16\" class=\"data row0 col16\" >14.0</td>\n",
       "      <td id=\"T_b9c24_row0_col17\" class=\"data row0 col17\" >34.1</td>\n",
       "      <td id=\"T_b9c24_row0_col18\" class=\"data row0 col18\" >7.5</td>\n",
       "      <td id=\"T_b9c24_row0_col19\" class=\"data row0 col19\" >33.5</td>\n",
       "      <td id=\"T_b9c24_row0_col20\" class=\"data row0 col20\" >22.6</td>\n",
       "      <td id=\"T_b9c24_row0_col21\" class=\"data row0 col21\" >14.0</td>\n",
       "      <td id=\"T_b9c24_row0_col22\" class=\"data row0 col22\" >25.2</td>\n",
       "      <td id=\"T_b9c24_row0_col23\" class=\"data row0 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b9c24_row1_col0\" class=\"data row1 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=0.1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row1_col1\" class=\"data row1 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row1_col2\" class=\"data row1 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row1_col3\" class=\"data row1 col3\" >22.6</td>\n",
       "      <td id=\"T_b9c24_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row1_col5\" class=\"data row1 col5\" >118.9</td>\n",
       "      <td id=\"T_b9c24_row1_col6\" class=\"data row1 col6\" >33.5</td>\n",
       "      <td id=\"T_b9c24_row1_col7\" class=\"data row1 col7\" >32.8</td>\n",
       "      <td id=\"T_b9c24_row1_col8\" class=\"data row1 col8\" >31.1</td>\n",
       "      <td id=\"T_b9c24_row1_col9\" class=\"data row1 col9\" >36.4</td>\n",
       "      <td id=\"T_b9c24_row1_col10\" class=\"data row1 col10\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row1_col11\" class=\"data row1 col11\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row1_col12\" class=\"data row1 col12\" >32.7</td>\n",
       "      <td id=\"T_b9c24_row1_col13\" class=\"data row1 col13\" >34.1</td>\n",
       "      <td id=\"T_b9c24_row1_col14\" class=\"data row1 col14\" >8.0</td>\n",
       "      <td id=\"T_b9c24_row1_col15\" class=\"data row1 col15\" >36.5</td>\n",
       "      <td id=\"T_b9c24_row1_col16\" class=\"data row1 col16\" >12.2</td>\n",
       "      <td id=\"T_b9c24_row1_col17\" class=\"data row1 col17\" >33.8</td>\n",
       "      <td id=\"T_b9c24_row1_col18\" class=\"data row1 col18\" >6.3</td>\n",
       "      <td id=\"T_b9c24_row1_col19\" class=\"data row1 col19\" >33.4</td>\n",
       "      <td id=\"T_b9c24_row1_col20\" class=\"data row1 col20\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row1_col21\" class=\"data row1 col21\" >12.2</td>\n",
       "      <td id=\"T_b9c24_row1_col22\" class=\"data row1 col22\" >24.5</td>\n",
       "      <td id=\"T_b9c24_row1_col23\" class=\"data row1 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b9c24_row2_col0\" class=\"data row2 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td id=\"T_b9c24_row2_col1\" class=\"data row2 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row2_col2\" class=\"data row2 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row2_col3\" class=\"data row2 col3\" >22.8</td>\n",
       "      <td id=\"T_b9c24_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row2_col5\" class=\"data row2 col5\" >117.2</td>\n",
       "      <td id=\"T_b9c24_row2_col6\" class=\"data row2 col6\" >33.2</td>\n",
       "      <td id=\"T_b9c24_row2_col7\" class=\"data row2 col7\" >32.4</td>\n",
       "      <td id=\"T_b9c24_row2_col8\" class=\"data row2 col8\" >32.0</td>\n",
       "      <td id=\"T_b9c24_row2_col9\" class=\"data row2 col9\" >36.2</td>\n",
       "      <td id=\"T_b9c24_row2_col10\" class=\"data row2 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row2_col11\" class=\"data row2 col11\" >8.2</td>\n",
       "      <td id=\"T_b9c24_row2_col12\" class=\"data row2 col12\" >33.4</td>\n",
       "      <td id=\"T_b9c24_row2_col13\" class=\"data row2 col13\" >35.0</td>\n",
       "      <td id=\"T_b9c24_row2_col14\" class=\"data row2 col14\" >8.1</td>\n",
       "      <td id=\"T_b9c24_row2_col15\" class=\"data row2 col15\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row2_col16\" class=\"data row2 col16\" >11.6</td>\n",
       "      <td id=\"T_b9c24_row2_col17\" class=\"data row2 col17\" >34.1</td>\n",
       "      <td id=\"T_b9c24_row2_col18\" class=\"data row2 col18\" >6.5</td>\n",
       "      <td id=\"T_b9c24_row2_col19\" class=\"data row2 col19\" >34.2</td>\n",
       "      <td id=\"T_b9c24_row2_col20\" class=\"data row2 col20\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row2_col21\" class=\"data row2 col21\" >11.6</td>\n",
       "      <td id=\"T_b9c24_row2_col22\" class=\"data row2 col22\" >24.6</td>\n",
       "      <td id=\"T_b9c24_row2_col23\" class=\"data row2 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b9c24_row3_col0\" class=\"data row3 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.3$)</td>\n",
       "      <td id=\"T_b9c24_row3_col1\" class=\"data row3 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row3_col2\" class=\"data row3 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row3_col3\" class=\"data row3 col3\" >23.7</td>\n",
       "      <td id=\"T_b9c24_row3_col4\" class=\"data row3 col4\" >39.0</td>\n",
       "      <td id=\"T_b9c24_row3_col5\" class=\"data row3 col5\" >130.9</td>\n",
       "      <td id=\"T_b9c24_row3_col6\" class=\"data row3 col6\" >33.3</td>\n",
       "      <td id=\"T_b9c24_row3_col7\" class=\"data row3 col7\" >31.0</td>\n",
       "      <td id=\"T_b9c24_row3_col8\" class=\"data row3 col8\" >34.9</td>\n",
       "      <td id=\"T_b9c24_row3_col9\" class=\"data row3 col9\" >37.9</td>\n",
       "      <td id=\"T_b9c24_row3_col10\" class=\"data row3 col10\" >5.0</td>\n",
       "      <td id=\"T_b9c24_row3_col11\" class=\"data row3 col11\" >8.6</td>\n",
       "      <td id=\"T_b9c24_row3_col12\" class=\"data row3 col12\" >34.6</td>\n",
       "      <td id=\"T_b9c24_row3_col13\" class=\"data row3 col13\" >35.9</td>\n",
       "      <td id=\"T_b9c24_row3_col14\" class=\"data row3 col14\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row3_col15\" class=\"data row3 col15\" >36.0</td>\n",
       "      <td id=\"T_b9c24_row3_col16\" class=\"data row3 col16\" >11.6</td>\n",
       "      <td id=\"T_b9c24_row3_col17\" class=\"data row3 col17\" >36.4</td>\n",
       "      <td id=\"T_b9c24_row3_col18\" class=\"data row3 col18\" >6.8</td>\n",
       "      <td id=\"T_b9c24_row3_col19\" class=\"data row3 col19\" >35.3</td>\n",
       "      <td id=\"T_b9c24_row3_col20\" class=\"data row3 col20\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row3_col21\" class=\"data row3 col21\" >11.6</td>\n",
       "      <td id=\"T_b9c24_row3_col22\" class=\"data row3 col22\" >26.4</td>\n",
       "      <td id=\"T_b9c24_row3_col23\" class=\"data row3 col23\" >-31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b9c24_row4_col0\" class=\"data row4 col0\" >$\\norm{\\nabla_{\\theta} \\ell}_2$ ($\\downarrow$)</td>\n",
       "      <td id=\"T_b9c24_row4_col1\" class=\"data row4 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row4_col2\" class=\"data row4 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row4_col3\" class=\"data row4 col3\" >22.7</td>\n",
       "      <td id=\"T_b9c24_row4_col4\" class=\"data row4 col4\" >37.0</td>\n",
       "      <td id=\"T_b9c24_row4_col5\" class=\"data row4 col5\" >136.1</td>\n",
       "      <td id=\"T_b9c24_row4_col6\" class=\"data row4 col6\" >32.8</td>\n",
       "      <td id=\"T_b9c24_row4_col7\" class=\"data row4 col7\" >30.9</td>\n",
       "      <td id=\"T_b9c24_row4_col8\" class=\"data row4 col8\" >36.4</td>\n",
       "      <td id=\"T_b9c24_row4_col9\" class=\"data row4 col9\" >37.3</td>\n",
       "      <td id=\"T_b9c24_row4_col10\" class=\"data row4 col10\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row4_col11\" class=\"data row4 col11\" >8.6</td>\n",
       "      <td id=\"T_b9c24_row4_col12\" class=\"data row4 col12\" >31.2</td>\n",
       "      <td id=\"T_b9c24_row4_col13\" class=\"data row4 col13\" >34.9</td>\n",
       "      <td id=\"T_b9c24_row4_col14\" class=\"data row4 col14\" >7.5</td>\n",
       "      <td id=\"T_b9c24_row4_col15\" class=\"data row4 col15\" >34.4</td>\n",
       "      <td id=\"T_b9c24_row4_col16\" class=\"data row4 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row4_col17\" class=\"data row4 col17\" >36.9</td>\n",
       "      <td id=\"T_b9c24_row4_col18\" class=\"data row4 col18\" >6.9</td>\n",
       "      <td id=\"T_b9c24_row4_col19\" class=\"data row4 col19\" >33.1</td>\n",
       "      <td id=\"T_b9c24_row4_col20\" class=\"data row4 col20\" >20.9</td>\n",
       "      <td id=\"T_b9c24_row4_col21\" class=\"data row4 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row4_col22\" class=\"data row4 col22\" >25.4</td>\n",
       "      <td id=\"T_b9c24_row4_col23\" class=\"data row4 col23\" >-44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b9c24_row5_col0\" class=\"data row5 col0\" >\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_b9c24_row5_col1\" class=\"data row5 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row5_col2\" class=\"data row5 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row5_col3\" class=\"data row5 col3\" >23.4</td>\n",
       "      <td id=\"T_b9c24_row5_col4\" class=\"data row5 col4\" >39.4</td>\n",
       "      <td id=\"T_b9c24_row5_col5\" class=\"data row5 col5\" >151.7</td>\n",
       "      <td id=\"T_b9c24_row5_col6\" class=\"data row5 col6\" >32.8</td>\n",
       "      <td id=\"T_b9c24_row5_col7\" class=\"data row5 col7\" >30.9</td>\n",
       "      <td id=\"T_b9c24_row5_col8\" class=\"data row5 col8\" >35.2</td>\n",
       "      <td id=\"T_b9c24_row5_col9\" class=\"data row5 col9\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row5_col10\" class=\"data row5 col10\" >4.4</td>\n",
       "      <td id=\"T_b9c24_row5_col11\" class=\"data row5 col11\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row5_col12\" class=\"data row5 col12\" >34.6</td>\n",
       "      <td id=\"T_b9c24_row5_col13\" class=\"data row5 col13\" >36.2</td>\n",
       "      <td id=\"T_b9c24_row5_col14\" class=\"data row5 col14\" >8.2</td>\n",
       "      <td id=\"T_b9c24_row5_col15\" class=\"data row5 col15\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row5_col16\" class=\"data row5 col16\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row5_col17\" class=\"data row5 col17\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row5_col18\" class=\"data row5 col18\" >7.1</td>\n",
       "      <td id=\"T_b9c24_row5_col19\" class=\"data row5 col19\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row5_col20\" class=\"data row5 col20\" >21.8</td>\n",
       "      <td id=\"T_b9c24_row5_col21\" class=\"data row5 col21\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row5_col22\" class=\"data row5 col22\" >26.1</td>\n",
       "      <td id=\"T_b9c24_row5_col23\" class=\"data row5 col23\" >-36.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b9c24_row6_col0\" class=\"data row6 col0\" >\\#Total Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_b9c24_row6_col1\" class=\"data row6 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row6_col2\" class=\"data row6 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row6_col3\" class=\"data row6 col3\" >20.4</td>\n",
       "      <td id=\"T_b9c24_row6_col4\" class=\"data row6 col4\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row6_col5\" class=\"data row6 col5\" >149.2</td>\n",
       "      <td id=\"T_b9c24_row6_col6\" class=\"data row6 col6\" >32.7</td>\n",
       "      <td id=\"T_b9c24_row6_col7\" class=\"data row6 col7\" >29.7</td>\n",
       "      <td id=\"T_b9c24_row6_col8\" class=\"data row6 col8\" >23.8</td>\n",
       "      <td id=\"T_b9c24_row6_col9\" class=\"data row6 col9\" >26.3</td>\n",
       "      <td id=\"T_b9c24_row6_col10\" class=\"data row6 col10\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row6_col11\" class=\"data row6 col11\" >9.2</td>\n",
       "      <td id=\"T_b9c24_row6_col12\" class=\"data row6 col12\" >33.1</td>\n",
       "      <td id=\"T_b9c24_row6_col13\" class=\"data row6 col13\" >34.0</td>\n",
       "      <td id=\"T_b9c24_row6_col14\" class=\"data row6 col14\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row6_col15\" class=\"data row6 col15\" >36.0</td>\n",
       "      <td id=\"T_b9c24_row6_col16\" class=\"data row6 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row6_col17\" class=\"data row6 col17\" >25.1</td>\n",
       "      <td id=\"T_b9c24_row6_col18\" class=\"data row6 col18\" >7.2</td>\n",
       "      <td id=\"T_b9c24_row6_col19\" class=\"data row6 col19\" >33.5</td>\n",
       "      <td id=\"T_b9c24_row6_col20\" class=\"data row6 col20\" >21.7</td>\n",
       "      <td id=\"T_b9c24_row6_col21\" class=\"data row6 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row6_col22\" class=\"data row6 col22\" >23.6</td>\n",
       "      <td id=\"T_b9c24_row6_col23\" class=\"data row6 col23\" >-46.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b9c24_row7_col0\" class=\"data row7 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.6$)</td>\n",
       "      <td id=\"T_b9c24_row7_col1\" class=\"data row7 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row7_col2\" class=\"data row7 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row7_col3\" class=\"data row7 col3\" >23.6</td>\n",
       "      <td id=\"T_b9c24_row7_col4\" class=\"data row7 col4\" >40.1</td>\n",
       "      <td id=\"T_b9c24_row7_col5\" class=\"data row7 col5\" >139.2</td>\n",
       "      <td id=\"T_b9c24_row7_col6\" class=\"data row7 col6\" >32.5</td>\n",
       "      <td id=\"T_b9c24_row7_col7\" class=\"data row7 col7\" >29.7</td>\n",
       "      <td id=\"T_b9c24_row7_col8\" class=\"data row7 col8\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row7_col9\" class=\"data row7 col9\" >37.2</td>\n",
       "      <td id=\"T_b9c24_row7_col10\" class=\"data row7 col10\" >5.0</td>\n",
       "      <td id=\"T_b9c24_row7_col11\" class=\"data row7 col11\" >9.2</td>\n",
       "      <td id=\"T_b9c24_row7_col12\" class=\"data row7 col12\" >35.5</td>\n",
       "      <td id=\"T_b9c24_row7_col13\" class=\"data row7 col13\" >35.6</td>\n",
       "      <td id=\"T_b9c24_row7_col14\" class=\"data row7 col14\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row7_col15\" class=\"data row7 col15\" >34.4</td>\n",
       "      <td id=\"T_b9c24_row7_col16\" class=\"data row7 col16\" >12.2</td>\n",
       "      <td id=\"T_b9c24_row7_col17\" class=\"data row7 col17\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row7_col18\" class=\"data row7 col18\" >7.1</td>\n",
       "      <td id=\"T_b9c24_row7_col19\" class=\"data row7 col19\" >35.5</td>\n",
       "      <td id=\"T_b9c24_row7_col20\" class=\"data row7 col20\" >21.1</td>\n",
       "      <td id=\"T_b9c24_row7_col21\" class=\"data row7 col21\" >12.2</td>\n",
       "      <td id=\"T_b9c24_row7_col22\" class=\"data row7 col22\" >26.2</td>\n",
       "      <td id=\"T_b9c24_row7_col23\" class=\"data row7 col23\" >-34.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b9c24_row8_col0\" class=\"data row8 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row8_col1\" class=\"data row8 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row8_col2\" class=\"data row8 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row8_col3\" class=\"data row8 col3\" >22.5</td>\n",
       "      <td id=\"T_b9c24_row8_col4\" class=\"data row8 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row8_col5\" class=\"data row8 col5\" >101.7</td>\n",
       "      <td id=\"T_b9c24_row8_col6\" class=\"data row8 col6\" >29.0</td>\n",
       "      <td id=\"T_b9c24_row8_col7\" class=\"data row8 col7\" >29.4</td>\n",
       "      <td id=\"T_b9c24_row8_col8\" class=\"data row8 col8\" >35.0</td>\n",
       "      <td id=\"T_b9c24_row8_col9\" class=\"data row8 col9\" >37.0</td>\n",
       "      <td id=\"T_b9c24_row8_col10\" class=\"data row8 col10\" >5.0</td>\n",
       "      <td id=\"T_b9c24_row8_col11\" class=\"data row8 col11\" >6.8</td>\n",
       "      <td id=\"T_b9c24_row8_col12\" class=\"data row8 col12\" >33.2</td>\n",
       "      <td id=\"T_b9c24_row8_col13\" class=\"data row8 col13\" >33.1</td>\n",
       "      <td id=\"T_b9c24_row8_col14\" class=\"data row8 col14\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row8_col15\" class=\"data row8 col15\" >35.2</td>\n",
       "      <td id=\"T_b9c24_row8_col16\" class=\"data row8 col16\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row8_col17\" class=\"data row8 col17\" >36.0</td>\n",
       "      <td id=\"T_b9c24_row8_col18\" class=\"data row8 col18\" >5.9</td>\n",
       "      <td id=\"T_b9c24_row8_col19\" class=\"data row8 col19\" >33.2</td>\n",
       "      <td id=\"T_b9c24_row8_col20\" class=\"data row8 col20\" >21.3</td>\n",
       "      <td id=\"T_b9c24_row8_col21\" class=\"data row8 col21\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row8_col22\" class=\"data row8 col22\" >23.7</td>\n",
       "      <td id=\"T_b9c24_row8_col23\" class=\"data row8 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b9c24_row9_col0\" class=\"data row9 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=10:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row9_col1\" class=\"data row9 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row9_col2\" class=\"data row9 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row9_col3\" class=\"data row9 col3\" >23.2</td>\n",
       "      <td id=\"T_b9c24_row9_col4\" class=\"data row9 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row9_col5\" class=\"data row9 col5\" >138.5</td>\n",
       "      <td id=\"T_b9c24_row9_col6\" class=\"data row9 col6\" >31.8</td>\n",
       "      <td id=\"T_b9c24_row9_col7\" class=\"data row9 col7\" >29.2</td>\n",
       "      <td id=\"T_b9c24_row9_col8\" class=\"data row9 col8\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row9_col9\" class=\"data row9 col9\" >37.5</td>\n",
       "      <td id=\"T_b9c24_row9_col10\" class=\"data row9 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row9_col11\" class=\"data row9 col11\" >8.0</td>\n",
       "      <td id=\"T_b9c24_row9_col12\" class=\"data row9 col12\" >34.4</td>\n",
       "      <td id=\"T_b9c24_row9_col13\" class=\"data row9 col13\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row9_col14\" class=\"data row9 col14\" >7.8</td>\n",
       "      <td id=\"T_b9c24_row9_col15\" class=\"data row9 col15\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row9_col16\" class=\"data row9 col16\" >10.4</td>\n",
       "      <td id=\"T_b9c24_row9_col17\" class=\"data row9 col17\" >36.5</td>\n",
       "      <td id=\"T_b9c24_row9_col18\" class=\"data row9 col18\" >6.4</td>\n",
       "      <td id=\"T_b9c24_row9_col19\" class=\"data row9 col19\" >34.9</td>\n",
       "      <td id=\"T_b9c24_row9_col20\" class=\"data row9 col20\" >21.6</td>\n",
       "      <td id=\"T_b9c24_row9_col21\" class=\"data row9 col21\" >10.4</td>\n",
       "      <td id=\"T_b9c24_row9_col22\" class=\"data row9 col22\" >24.6</td>\n",
       "      <td id=\"T_b9c24_row9_col23\" class=\"data row9 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_b9c24_row10_col0\" class=\"data row10 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks)</td>\n",
       "      <td id=\"T_b9c24_row10_col1\" class=\"data row10 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row10_col2\" class=\"data row10 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row10_col3\" class=\"data row10 col3\" >23.5</td>\n",
       "      <td id=\"T_b9c24_row10_col4\" class=\"data row10 col4\" >40.4</td>\n",
       "      <td id=\"T_b9c24_row10_col5\" class=\"data row10 col5\" >150.2</td>\n",
       "      <td id=\"T_b9c24_row10_col6\" class=\"data row10 col6\" >31.8</td>\n",
       "      <td id=\"T_b9c24_row10_col7\" class=\"data row10 col7\" >29.2</td>\n",
       "      <td id=\"T_b9c24_row10_col8\" class=\"data row10 col8\" >35.1</td>\n",
       "      <td id=\"T_b9c24_row10_col9\" class=\"data row10 col9\" >38.1</td>\n",
       "      <td id=\"T_b9c24_row10_col10\" class=\"data row10 col10\" >4.4</td>\n",
       "      <td id=\"T_b9c24_row10_col11\" class=\"data row10 col11\" >8.8</td>\n",
       "      <td id=\"T_b9c24_row10_col12\" class=\"data row10 col12\" >34.0</td>\n",
       "      <td id=\"T_b9c24_row10_col13\" class=\"data row10 col13\" >36.5</td>\n",
       "      <td id=\"T_b9c24_row10_col14\" class=\"data row10 col14\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row10_col15\" class=\"data row10 col15\" >36.0</td>\n",
       "      <td id=\"T_b9c24_row10_col16\" class=\"data row10 col16\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row10_col17\" class=\"data row10 col17\" >36.6</td>\n",
       "      <td id=\"T_b9c24_row10_col18\" class=\"data row10 col18\" >6.6</td>\n",
       "      <td id=\"T_b9c24_row10_col19\" class=\"data row10 col19\" >35.2</td>\n",
       "      <td id=\"T_b9c24_row10_col20\" class=\"data row10 col20\" >22.3</td>\n",
       "      <td id=\"T_b9c24_row10_col21\" class=\"data row10 col21\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row10_col22\" class=\"data row10 col22\" >26.1</td>\n",
       "      <td id=\"T_b9c24_row10_col23\" class=\"data row10 col23\" >-35.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_b9c24_row11_col0\" class=\"data row11 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row11_col1\" class=\"data row11 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row11_col2\" class=\"data row11 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row11_col3\" class=\"data row11 col3\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row11_col4\" class=\"data row11 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row11_col5\" class=\"data row11 col5\" >101.0</td>\n",
       "      <td id=\"T_b9c24_row11_col6\" class=\"data row11 col6\" >28.7</td>\n",
       "      <td id=\"T_b9c24_row11_col7\" class=\"data row11 col7\" >28.7</td>\n",
       "      <td id=\"T_b9c24_row11_col8\" class=\"data row11 col8\" >33.3</td>\n",
       "      <td id=\"T_b9c24_row11_col9\" class=\"data row11 col9\" >35.3</td>\n",
       "      <td id=\"T_b9c24_row11_col10\" class=\"data row11 col10\" >4.0</td>\n",
       "      <td id=\"T_b9c24_row11_col11\" class=\"data row11 col11\" >7.6</td>\n",
       "      <td id=\"T_b9c24_row11_col12\" class=\"data row11 col12\" >34.3</td>\n",
       "      <td id=\"T_b9c24_row11_col13\" class=\"data row11 col13\" >31.2</td>\n",
       "      <td id=\"T_b9c24_row11_col14\" class=\"data row11 col14\" >8.3</td>\n",
       "      <td id=\"T_b9c24_row11_col15\" class=\"data row11 col15\" >34.5</td>\n",
       "      <td id=\"T_b9c24_row11_col16\" class=\"data row11 col16\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row11_col17\" class=\"data row11 col17\" >34.3</td>\n",
       "      <td id=\"T_b9c24_row11_col18\" class=\"data row11 col18\" >5.8</td>\n",
       "      <td id=\"T_b9c24_row11_col19\" class=\"data row11 col19\" >32.7</td>\n",
       "      <td id=\"T_b9c24_row11_col20\" class=\"data row11 col20\" >21.4</td>\n",
       "      <td id=\"T_b9c24_row11_col21\" class=\"data row11 col21\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row11_col22\" class=\"data row11 col22\" >23.4</td>\n",
       "      <td id=\"T_b9c24_row11_col23\" class=\"data row11 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_b9c24_row12_col0\" class=\"data row12 col0\" >Perplexity ($\\downarrow$)</td>\n",
       "      <td id=\"T_b9c24_row12_col1\" class=\"data row12 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row12_col2\" class=\"data row12 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row12_col3\" class=\"data row12 col3\" >22.7</td>\n",
       "      <td id=\"T_b9c24_row12_col4\" class=\"data row12 col4\" >25.3</td>\n",
       "      <td id=\"T_b9c24_row12_col5\" class=\"data row12 col5\" >105.9</td>\n",
       "      <td id=\"T_b9c24_row12_col6\" class=\"data row12 col6\" >27.5</td>\n",
       "      <td id=\"T_b9c24_row12_col7\" class=\"data row12 col7\" >28.5</td>\n",
       "      <td id=\"T_b9c24_row12_col8\" class=\"data row12 col8\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row12_col9\" class=\"data row12 col9\" >39.2</td>\n",
       "      <td id=\"T_b9c24_row12_col10\" class=\"data row12 col10\" >3.4</td>\n",
       "      <td id=\"T_b9c24_row12_col11\" class=\"data row12 col11\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row12_col12\" class=\"data row12 col12\" >34.6</td>\n",
       "      <td id=\"T_b9c24_row12_col13\" class=\"data row12 col13\" >31.2</td>\n",
       "      <td id=\"T_b9c24_row12_col14\" class=\"data row12 col14\" >8.1</td>\n",
       "      <td id=\"T_b9c24_row12_col15\" class=\"data row12 col15\" >33.8</td>\n",
       "      <td id=\"T_b9c24_row12_col16\" class=\"data row12 col16\" >11.6</td>\n",
       "      <td id=\"T_b9c24_row12_col17\" class=\"data row12 col17\" >37.7</td>\n",
       "      <td id=\"T_b9c24_row12_col18\" class=\"data row12 col18\" >4.7</td>\n",
       "      <td id=\"T_b9c24_row12_col19\" class=\"data row12 col19\" >32.9</td>\n",
       "      <td id=\"T_b9c24_row12_col20\" class=\"data row12 col20\" >21.0</td>\n",
       "      <td id=\"T_b9c24_row12_col21\" class=\"data row12 col21\" >11.6</td>\n",
       "      <td id=\"T_b9c24_row12_col22\" class=\"data row12 col22\" >23.8</td>\n",
       "      <td id=\"T_b9c24_row12_col23\" class=\"data row12 col23\" >-48.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_b9c24_row13_col0\" class=\"data row13 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$)</td>\n",
       "      <td id=\"T_b9c24_row13_col1\" class=\"data row13 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row13_col2\" class=\"data row13 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row13_col3\" class=\"data row13 col3\" >21.7</td>\n",
       "      <td id=\"T_b9c24_row13_col4\" class=\"data row13 col4\" >26.3</td>\n",
       "      <td id=\"T_b9c24_row13_col5\" class=\"data row13 col5\" >103.8</td>\n",
       "      <td id=\"T_b9c24_row13_col6\" class=\"data row13 col6\" >28.3</td>\n",
       "      <td id=\"T_b9c24_row13_col7\" class=\"data row13 col7\" >28.5</td>\n",
       "      <td id=\"T_b9c24_row13_col8\" class=\"data row13 col8\" >27.3</td>\n",
       "      <td id=\"T_b9c24_row13_col9\" class=\"data row13 col9\" >30.3</td>\n",
       "      <td id=\"T_b9c24_row13_col10\" class=\"data row13 col10\" >5.0</td>\n",
       "      <td id=\"T_b9c24_row13_col11\" class=\"data row13 col11\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row13_col12\" class=\"data row13 col12\" >34.8</td>\n",
       "      <td id=\"T_b9c24_row13_col13\" class=\"data row13 col13\" >33.4</td>\n",
       "      <td id=\"T_b9c24_row13_col14\" class=\"data row13 col14\" >7.8</td>\n",
       "      <td id=\"T_b9c24_row13_col15\" class=\"data row13 col15\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row13_col16\" class=\"data row13 col16\" >9.6</td>\n",
       "      <td id=\"T_b9c24_row13_col17\" class=\"data row13 col17\" >28.8</td>\n",
       "      <td id=\"T_b9c24_row13_col18\" class=\"data row13 col18\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row13_col19\" class=\"data row13 col19\" >34.1</td>\n",
       "      <td id=\"T_b9c24_row13_col20\" class=\"data row13 col20\" >22.6</td>\n",
       "      <td id=\"T_b9c24_row13_col21\" class=\"data row13 col21\" >9.6</td>\n",
       "      <td id=\"T_b9c24_row13_col22\" class=\"data row13 col22\" >23.2</td>\n",
       "      <td id=\"T_b9c24_row13_col23\" class=\"data row13 col23\" >-45.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_b9c24_row14_col0\" class=\"data row14 col0\" >llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row14_col1\" class=\"data row14 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row14_col2\" class=\"data row14 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row14_col3\" class=\"data row14 col3\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col4\" class=\"data row14 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col5\" class=\"data row14 col5\" >102.6</td>\n",
       "      <td id=\"T_b9c24_row14_col6\" class=\"data row14 col6\" >27.6</td>\n",
       "      <td id=\"T_b9c24_row14_col7\" class=\"data row14 col7\" >28.2</td>\n",
       "      <td id=\"T_b9c24_row14_col8\" class=\"data row14 col8\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col9\" class=\"data row14 col9\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col10\" class=\"data row14 col10\" >3.8</td>\n",
       "      <td id=\"T_b9c24_row14_col11\" class=\"data row14 col11\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row14_col12\" class=\"data row14 col12\" >35.0</td>\n",
       "      <td id=\"T_b9c24_row14_col13\" class=\"data row14 col13\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col14\" class=\"data row14 col14\" >7.6</td>\n",
       "      <td id=\"T_b9c24_row14_col15\" class=\"data row14 col15\" >33.8</td>\n",
       "      <td id=\"T_b9c24_row14_col16\" class=\"data row14 col16\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row14_col17\" class=\"data row14 col17\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col18\" class=\"data row14 col18\" >4.9</td>\n",
       "      <td id=\"T_b9c24_row14_col19\" class=\"data row14 col19\" >nan</td>\n",
       "      <td id=\"T_b9c24_row14_col20\" class=\"data row14 col20\" >20.7</td>\n",
       "      <td id=\"T_b9c24_row14_col21\" class=\"data row14 col21\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row14_col22\" class=\"data row14 col22\" >19.1</td>\n",
       "      <td id=\"T_b9c24_row14_col23\" class=\"data row14 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_b9c24_row15_col0\" class=\"data row15 col0\" >DPP (Llama Emb Not Norm.)</td>\n",
       "      <td id=\"T_b9c24_row15_col1\" class=\"data row15 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row15_col2\" class=\"data row15 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row15_col3\" class=\"data row15 col3\" >22.1</td>\n",
       "      <td id=\"T_b9c24_row15_col4\" class=\"data row15 col4\" >21.7</td>\n",
       "      <td id=\"T_b9c24_row15_col5\" class=\"data row15 col5\" >92.6</td>\n",
       "      <td id=\"T_b9c24_row15_col6\" class=\"data row15 col6\" >26.2</td>\n",
       "      <td id=\"T_b9c24_row15_col7\" class=\"data row15 col7\" >27.5</td>\n",
       "      <td id=\"T_b9c24_row15_col8\" class=\"data row15 col8\" >35.6</td>\n",
       "      <td id=\"T_b9c24_row15_col9\" class=\"data row15 col9\" >38.3</td>\n",
       "      <td id=\"T_b9c24_row15_col10\" class=\"data row15 col10\" >5.0</td>\n",
       "      <td id=\"T_b9c24_row15_col11\" class=\"data row15 col11\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row15_col12\" class=\"data row15 col12\" >31.3</td>\n",
       "      <td id=\"T_b9c24_row15_col13\" class=\"data row15 col13\" >31.9</td>\n",
       "      <td id=\"T_b9c24_row15_col14\" class=\"data row15 col14\" >7.1</td>\n",
       "      <td id=\"T_b9c24_row15_col15\" class=\"data row15 col15\" >35.8</td>\n",
       "      <td id=\"T_b9c24_row15_col16\" class=\"data row15 col16\" >8.7</td>\n",
       "      <td id=\"T_b9c24_row15_col17\" class=\"data row15 col17\" >36.9</td>\n",
       "      <td id=\"T_b9c24_row15_col18\" class=\"data row15 col18\" >5.1</td>\n",
       "      <td id=\"T_b9c24_row15_col19\" class=\"data row15 col19\" >31.6</td>\n",
       "      <td id=\"T_b9c24_row15_col20\" class=\"data row15 col20\" >21.5</td>\n",
       "      <td id=\"T_b9c24_row15_col21\" class=\"data row15 col21\" >8.7</td>\n",
       "      <td id=\"T_b9c24_row15_col22\" class=\"data row15 col22\" >22.8</td>\n",
       "      <td id=\"T_b9c24_row15_col23\" class=\"data row15 col23\" >-57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_b9c24_row16_col0\" class=\"data row16 col0\" >Alpagasus Rating ($\\uparrow$)</td>\n",
       "      <td id=\"T_b9c24_row16_col1\" class=\"data row16 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row16_col2\" class=\"data row16 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row16_col3\" class=\"data row16 col3\" >21.6</td>\n",
       "      <td id=\"T_b9c24_row16_col4\" class=\"data row16 col4\" >21.2</td>\n",
       "      <td id=\"T_b9c24_row16_col5\" class=\"data row16 col5\" >95.0</td>\n",
       "      <td id=\"T_b9c24_row16_col6\" class=\"data row16 col6\" >26.7</td>\n",
       "      <td id=\"T_b9c24_row16_col7\" class=\"data row16 col7\" >27.2</td>\n",
       "      <td id=\"T_b9c24_row16_col8\" class=\"data row16 col8\" >35.7</td>\n",
       "      <td id=\"T_b9c24_row16_col9\" class=\"data row16 col9\" >36.8</td>\n",
       "      <td id=\"T_b9c24_row16_col10\" class=\"data row16 col10\" >4.2</td>\n",
       "      <td id=\"T_b9c24_row16_col11\" class=\"data row16 col11\" >6.2</td>\n",
       "      <td id=\"T_b9c24_row16_col12\" class=\"data row16 col12\" >32.4</td>\n",
       "      <td id=\"T_b9c24_row16_col13\" class=\"data row16 col13\" >30.2</td>\n",
       "      <td id=\"T_b9c24_row16_col14\" class=\"data row16 col14\" >6.9</td>\n",
       "      <td id=\"T_b9c24_row16_col15\" class=\"data row16 col15\" >32.7</td>\n",
       "      <td id=\"T_b9c24_row16_col16\" class=\"data row16 col16\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row16_col17\" class=\"data row16 col17\" >36.2</td>\n",
       "      <td id=\"T_b9c24_row16_col18\" class=\"data row16 col18\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row16_col19\" class=\"data row16 col19\" >31.3</td>\n",
       "      <td id=\"T_b9c24_row16_col20\" class=\"data row16 col20\" >19.8</td>\n",
       "      <td id=\"T_b9c24_row16_col21\" class=\"data row16 col21\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row16_col22\" class=\"data row16 col22\" >22.4</td>\n",
       "      <td id=\"T_b9c24_row16_col23\" class=\"data row16 col23\" >-62.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_b9c24_row17_col0\" class=\"data row17 col0\" >Random</td>\n",
       "      <td id=\"T_b9c24_row17_col1\" class=\"data row17 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row17_col2\" class=\"data row17 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row17_col3\" class=\"data row17 col3\" >21.6</td>\n",
       "      <td id=\"T_b9c24_row17_col4\" class=\"data row17 col4\" >18.3</td>\n",
       "      <td id=\"T_b9c24_row17_col5\" class=\"data row17 col5\" >90.1</td>\n",
       "      <td id=\"T_b9c24_row17_col6\" class=\"data row17 col6\" >25.0</td>\n",
       "      <td id=\"T_b9c24_row17_col7\" class=\"data row17 col7\" >27.1</td>\n",
       "      <td id=\"T_b9c24_row17_col8\" class=\"data row17 col8\" >30.9</td>\n",
       "      <td id=\"T_b9c24_row17_col9\" class=\"data row17 col9\" >33.6</td>\n",
       "      <td id=\"T_b9c24_row17_col10\" class=\"data row17 col10\" >4.0</td>\n",
       "      <td id=\"T_b9c24_row17_col11\" class=\"data row17 col11\" >5.6</td>\n",
       "      <td id=\"T_b9c24_row17_col12\" class=\"data row17 col12\" >34.5</td>\n",
       "      <td id=\"T_b9c24_row17_col13\" class=\"data row17 col13\" >32.3</td>\n",
       "      <td id=\"T_b9c24_row17_col14\" class=\"data row17 col14\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row17_col15\" class=\"data row17 col15\" >36.8</td>\n",
       "      <td id=\"T_b9c24_row17_col16\" class=\"data row17 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row17_col17\" class=\"data row17 col17\" >32.3</td>\n",
       "      <td id=\"T_b9c24_row17_col18\" class=\"data row17 col18\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row17_col19\" class=\"data row17 col19\" >33.4</td>\n",
       "      <td id=\"T_b9c24_row17_col20\" class=\"data row17 col20\" >22.4</td>\n",
       "      <td id=\"T_b9c24_row17_col21\" class=\"data row17 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row17_col22\" class=\"data row17 col22\" >22.1</td>\n",
       "      <td id=\"T_b9c24_row17_col23\" class=\"data row17 col23\" >-60.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_b9c24_row18_col0\" class=\"data row18 col0\" >IFD ($\\uparrow$)</td>\n",
       "      <td id=\"T_b9c24_row18_col1\" class=\"data row18 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row18_col2\" class=\"data row18 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row18_col3\" class=\"data row18 col3\" >21.5</td>\n",
       "      <td id=\"T_b9c24_row18_col4\" class=\"data row18 col4\" >29.3</td>\n",
       "      <td id=\"T_b9c24_row18_col5\" class=\"data row18 col5\" >113.2</td>\n",
       "      <td id=\"T_b9c24_row18_col6\" class=\"data row18 col6\" >27.7</td>\n",
       "      <td id=\"T_b9c24_row18_col7\" class=\"data row18 col7\" >27.0</td>\n",
       "      <td id=\"T_b9c24_row18_col8\" class=\"data row18 col8\" >32.7</td>\n",
       "      <td id=\"T_b9c24_row18_col9\" class=\"data row18 col9\" >36.5</td>\n",
       "      <td id=\"T_b9c24_row18_col10\" class=\"data row18 col10\" >4.0</td>\n",
       "      <td id=\"T_b9c24_row18_col11\" class=\"data row18 col11\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row18_col12\" class=\"data row18 col12\" >31.1</td>\n",
       "      <td id=\"T_b9c24_row18_col13\" class=\"data row18 col13\" >30.6</td>\n",
       "      <td id=\"T_b9c24_row18_col14\" class=\"data row18 col14\" >8.8</td>\n",
       "      <td id=\"T_b9c24_row18_col15\" class=\"data row18 col15\" >38.2</td>\n",
       "      <td id=\"T_b9c24_row18_col16\" class=\"data row18 col16\" >6.7</td>\n",
       "      <td id=\"T_b9c24_row18_col17\" class=\"data row18 col17\" >34.6</td>\n",
       "      <td id=\"T_b9c24_row18_col18\" class=\"data row18 col18\" >4.4</td>\n",
       "      <td id=\"T_b9c24_row18_col19\" class=\"data row18 col19\" >30.9</td>\n",
       "      <td id=\"T_b9c24_row18_col20\" class=\"data row18 col20\" >23.5</td>\n",
       "      <td id=\"T_b9c24_row18_col21\" class=\"data row18 col21\" >6.7</td>\n",
       "      <td id=\"T_b9c24_row18_col22\" class=\"data row18 col22\" >23.1</td>\n",
       "      <td id=\"T_b9c24_row18_col23\" class=\"data row18 col23\" >-58.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_b9c24_row19_col0\" class=\"data row19 col0\" >DPP (Llama Emb)</td>\n",
       "      <td id=\"T_b9c24_row19_col1\" class=\"data row19 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row19_col2\" class=\"data row19 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row19_col3\" class=\"data row19 col3\" >22.5</td>\n",
       "      <td id=\"T_b9c24_row19_col4\" class=\"data row19 col4\" >20.9</td>\n",
       "      <td id=\"T_b9c24_row19_col5\" class=\"data row19 col5\" >91.7</td>\n",
       "      <td id=\"T_b9c24_row19_col6\" class=\"data row19 col6\" >25.5</td>\n",
       "      <td id=\"T_b9c24_row19_col7\" class=\"data row19 col7\" >26.6</td>\n",
       "      <td id=\"T_b9c24_row19_col8\" class=\"data row19 col8\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row19_col9\" class=\"data row19 col9\" >38.5</td>\n",
       "      <td id=\"T_b9c24_row19_col10\" class=\"data row19 col10\" >4.6</td>\n",
       "      <td id=\"T_b9c24_row19_col11\" class=\"data row19 col11\" >9.2</td>\n",
       "      <td id=\"T_b9c24_row19_col12\" class=\"data row19 col12\" >30.0</td>\n",
       "      <td id=\"T_b9c24_row19_col13\" class=\"data row19 col13\" >29.8</td>\n",
       "      <td id=\"T_b9c24_row19_col14\" class=\"data row19 col14\" >6.6</td>\n",
       "      <td id=\"T_b9c24_row19_col15\" class=\"data row19 col15\" >35.4</td>\n",
       "      <td id=\"T_b9c24_row19_col16\" class=\"data row19 col16\" >11.2</td>\n",
       "      <td id=\"T_b9c24_row19_col17\" class=\"data row19 col17\" >37.9</td>\n",
       "      <td id=\"T_b9c24_row19_col18\" class=\"data row19 col18\" >6.9</td>\n",
       "      <td id=\"T_b9c24_row19_col19\" class=\"data row19 col19\" >29.9</td>\n",
       "      <td id=\"T_b9c24_row19_col20\" class=\"data row19 col20\" >21.0</td>\n",
       "      <td id=\"T_b9c24_row19_col21\" class=\"data row19 col21\" >11.2</td>\n",
       "      <td id=\"T_b9c24_row19_col22\" class=\"data row19 col22\" >23.0</td>\n",
       "      <td id=\"T_b9c24_row19_col23\" class=\"data row19 col23\" >-55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_b9c24_row20_col0\" class=\"data row20 col0\" >llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3_wrong_inds</td>\n",
       "      <td id=\"T_b9c24_row20_col1\" class=\"data row20 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row20_col2\" class=\"data row20 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row20_col3\" class=\"data row20 col3\" >23.4</td>\n",
       "      <td id=\"T_b9c24_row20_col4\" class=\"data row20 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row20_col5\" class=\"data row20 col5\" >88.4</td>\n",
       "      <td id=\"T_b9c24_row20_col6\" class=\"data row20 col6\" >25.7</td>\n",
       "      <td id=\"T_b9c24_row20_col7\" class=\"data row20 col7\" >26.3</td>\n",
       "      <td id=\"T_b9c24_row20_col8\" class=\"data row20 col8\" >40.6</td>\n",
       "      <td id=\"T_b9c24_row20_col9\" class=\"data row20 col9\" >40.2</td>\n",
       "      <td id=\"T_b9c24_row20_col10\" class=\"data row20 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row20_col11\" class=\"data row20 col11\" >6.2</td>\n",
       "      <td id=\"T_b9c24_row20_col12\" class=\"data row20 col12\" >36.2</td>\n",
       "      <td id=\"T_b9c24_row20_col13\" class=\"data row20 col13\" >30.5</td>\n",
       "      <td id=\"T_b9c24_row20_col14\" class=\"data row20 col14\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row20_col15\" class=\"data row20 col15\" >35.5</td>\n",
       "      <td id=\"T_b9c24_row20_col16\" class=\"data row20 col16\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row20_col17\" class=\"data row20 col17\" >40.4</td>\n",
       "      <td id=\"T_b9c24_row20_col18\" class=\"data row20 col18\" >5.5</td>\n",
       "      <td id=\"T_b9c24_row20_col19\" class=\"data row20 col19\" >33.3</td>\n",
       "      <td id=\"T_b9c24_row20_col20\" class=\"data row20 col20\" >21.7</td>\n",
       "      <td id=\"T_b9c24_row20_col21\" class=\"data row20 col21\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row20_col22\" class=\"data row20 col22\" >23.9</td>\n",
       "      <td id=\"T_b9c24_row20_col23\" class=\"data row20 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_b9c24_row21_col0\" class=\"data row21 col0\" >llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row21_col1\" class=\"data row21 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row21_col2\" class=\"data row21 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row21_col3\" class=\"data row21 col3\" >nan</td>\n",
       "      <td id=\"T_b9c24_row21_col4\" class=\"data row21 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row21_col5\" class=\"data row21 col5\" >88.8</td>\n",
       "      <td id=\"T_b9c24_row21_col6\" class=\"data row21 col6\" >24.5</td>\n",
       "      <td id=\"T_b9c24_row21_col7\" class=\"data row21 col7\" >25.7</td>\n",
       "      <td id=\"T_b9c24_row21_col8\" class=\"data row21 col8\" >37.2</td>\n",
       "      <td id=\"T_b9c24_row21_col9\" class=\"data row21 col9\" >nan</td>\n",
       "      <td id=\"T_b9c24_row21_col10\" class=\"data row21 col10\" >3.8</td>\n",
       "      <td id=\"T_b9c24_row21_col11\" class=\"data row21 col11\" >8.4</td>\n",
       "      <td id=\"T_b9c24_row21_col12\" class=\"data row21 col12\" >32.7</td>\n",
       "      <td id=\"T_b9c24_row21_col13\" class=\"data row21 col13\" >30.7</td>\n",
       "      <td id=\"T_b9c24_row21_col14\" class=\"data row21 col14\" >7.0</td>\n",
       "      <td id=\"T_b9c24_row21_col15\" class=\"data row21 col15\" >35.5</td>\n",
       "      <td id=\"T_b9c24_row21_col16\" class=\"data row21 col16\" >7.3</td>\n",
       "      <td id=\"T_b9c24_row21_col17\" class=\"data row21 col17\" >nan</td>\n",
       "      <td id=\"T_b9c24_row21_col18\" class=\"data row21 col18\" >6.1</td>\n",
       "      <td id=\"T_b9c24_row21_col19\" class=\"data row21 col19\" >31.7</td>\n",
       "      <td id=\"T_b9c24_row21_col20\" class=\"data row21 col20\" >21.2</td>\n",
       "      <td id=\"T_b9c24_row21_col21\" class=\"data row21 col21\" >7.3</td>\n",
       "      <td id=\"T_b9c24_row21_col22\" class=\"data row21 col22\" >21.3</td>\n",
       "      <td id=\"T_b9c24_row21_col23\" class=\"data row21 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_b9c24_row22_col0\" class=\"data row22 col0\" >DPP (MpNet Emb)</td>\n",
       "      <td id=\"T_b9c24_row22_col1\" class=\"data row22 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row22_col2\" class=\"data row22 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row22_col3\" class=\"data row22 col3\" >22.9</td>\n",
       "      <td id=\"T_b9c24_row22_col4\" class=\"data row22 col4\" >19.4</td>\n",
       "      <td id=\"T_b9c24_row22_col5\" class=\"data row22 col5\" >87.9</td>\n",
       "      <td id=\"T_b9c24_row22_col6\" class=\"data row22 col6\" >24.7</td>\n",
       "      <td id=\"T_b9c24_row22_col7\" class=\"data row22 col7\" >25.2</td>\n",
       "      <td id=\"T_b9c24_row22_col8\" class=\"data row22 col8\" >40.1</td>\n",
       "      <td id=\"T_b9c24_row22_col9\" class=\"data row22 col9\" >38.1</td>\n",
       "      <td id=\"T_b9c24_row22_col10\" class=\"data row22 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row22_col11\" class=\"data row22 col11\" >6.8</td>\n",
       "      <td id=\"T_b9c24_row22_col12\" class=\"data row22 col12\" >35.6</td>\n",
       "      <td id=\"T_b9c24_row22_col13\" class=\"data row22 col13\" >31.8</td>\n",
       "      <td id=\"T_b9c24_row22_col14\" class=\"data row22 col14\" >6.8</td>\n",
       "      <td id=\"T_b9c24_row22_col15\" class=\"data row22 col15\" >33.2</td>\n",
       "      <td id=\"T_b9c24_row22_col16\" class=\"data row22 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row22_col17\" class=\"data row22 col17\" >39.1</td>\n",
       "      <td id=\"T_b9c24_row22_col18\" class=\"data row22 col18\" >5.8</td>\n",
       "      <td id=\"T_b9c24_row22_col19\" class=\"data row22 col19\" >33.7</td>\n",
       "      <td id=\"T_b9c24_row22_col20\" class=\"data row22 col20\" >20.0</td>\n",
       "      <td id=\"T_b9c24_row22_col21\" class=\"data row22 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row22_col22\" class=\"data row22 col22\" >22.9</td>\n",
       "      <td id=\"T_b9c24_row22_col23\" class=\"data row22 col23\" >-56.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_b9c24_row23_col0\" class=\"data row23 col0\" >llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3_wrong_inds</td>\n",
       "      <td id=\"T_b9c24_row23_col1\" class=\"data row23 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row23_col2\" class=\"data row23 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row23_col3\" class=\"data row23 col3\" >23.6</td>\n",
       "      <td id=\"T_b9c24_row23_col4\" class=\"data row23 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row23_col5\" class=\"data row23 col5\" >89.2</td>\n",
       "      <td id=\"T_b9c24_row23_col6\" class=\"data row23 col6\" >24.5</td>\n",
       "      <td id=\"T_b9c24_row23_col7\" class=\"data row23 col7\" >25.0</td>\n",
       "      <td id=\"T_b9c24_row23_col8\" class=\"data row23 col8\" >38.9</td>\n",
       "      <td id=\"T_b9c24_row23_col9\" class=\"data row23 col9\" >39.2</td>\n",
       "      <td id=\"T_b9c24_row23_col10\" class=\"data row23 col10\" >6.4</td>\n",
       "      <td id=\"T_b9c24_row23_col11\" class=\"data row23 col11\" >8.4</td>\n",
       "      <td id=\"T_b9c24_row23_col12\" class=\"data row23 col12\" >33.1</td>\n",
       "      <td id=\"T_b9c24_row23_col13\" class=\"data row23 col13\" >31.7</td>\n",
       "      <td id=\"T_b9c24_row23_col14\" class=\"data row23 col14\" >7.5</td>\n",
       "      <td id=\"T_b9c24_row23_col15\" class=\"data row23 col15\" >35.9</td>\n",
       "      <td id=\"T_b9c24_row23_col16\" class=\"data row23 col16\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row23_col17\" class=\"data row23 col17\" >39.1</td>\n",
       "      <td id=\"T_b9c24_row23_col18\" class=\"data row23 col18\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row23_col19\" class=\"data row23 col19\" >32.4</td>\n",
       "      <td id=\"T_b9c24_row23_col20\" class=\"data row23 col20\" >21.7</td>\n",
       "      <td id=\"T_b9c24_row23_col21\" class=\"data row23 col21\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row23_col22\" class=\"data row23 col22\" >23.8</td>\n",
       "      <td id=\"T_b9c24_row23_col23\" class=\"data row23 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_b9c24_row24_col0\" class=\"data row24 col0\" >llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=1:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row24_col1\" class=\"data row24 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row24_col2\" class=\"data row24 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row24_col3\" class=\"data row24 col3\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col4\" class=\"data row24 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col5\" class=\"data row24 col5\" >93.4</td>\n",
       "      <td id=\"T_b9c24_row24_col6\" class=\"data row24 col6\" >23.6</td>\n",
       "      <td id=\"T_b9c24_row24_col7\" class=\"data row24 col7\" >24.5</td>\n",
       "      <td id=\"T_b9c24_row24_col8\" class=\"data row24 col8\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col9\" class=\"data row24 col9\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col10\" class=\"data row24 col10\" >5.6</td>\n",
       "      <td id=\"T_b9c24_row24_col11\" class=\"data row24 col11\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col12\" class=\"data row24 col12\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col13\" class=\"data row24 col13\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col14\" class=\"data row24 col14\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col15\" class=\"data row24 col15\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col16\" class=\"data row24 col16\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col17\" class=\"data row24 col17\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col18\" class=\"data row24 col18\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col19\" class=\"data row24 col19\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col20\" class=\"data row24 col20\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col21\" class=\"data row24 col21\" >nan</td>\n",
       "      <td id=\"T_b9c24_row24_col22\" class=\"data row24 col22\" >17.9</td>\n",
       "      <td id=\"T_b9c24_row24_col23\" class=\"data row24 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_b9c24_row25_col0\" class=\"data row25 col0\" >llama-7b_stanford_alpaca50k_score=fl:k=vmf:gamma=10:kmd=mpnet:kemb=text+embedding_pace=prune:size=30000:ep=3_wrong_inds</td>\n",
       "      <td id=\"T_b9c24_row25_col1\" class=\"data row25 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row25_col2\" class=\"data row25 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row25_col3\" class=\"data row25 col3\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row25_col4\" class=\"data row25 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row25_col5\" class=\"data row25 col5\" >91.7</td>\n",
       "      <td id=\"T_b9c24_row25_col6\" class=\"data row25 col6\" >23.4</td>\n",
       "      <td id=\"T_b9c24_row25_col7\" class=\"data row25 col7\" >24.3</td>\n",
       "      <td id=\"T_b9c24_row25_col8\" class=\"data row25 col8\" >39.9</td>\n",
       "      <td id=\"T_b9c24_row25_col9\" class=\"data row25 col9\" >40.5</td>\n",
       "      <td id=\"T_b9c24_row25_col10\" class=\"data row25 col10\" >5.4</td>\n",
       "      <td id=\"T_b9c24_row25_col11\" class=\"data row25 col11\" >5.0</td>\n",
       "      <td id=\"T_b9c24_row25_col12\" class=\"data row25 col12\" >31.9</td>\n",
       "      <td id=\"T_b9c24_row25_col13\" class=\"data row25 col13\" >29.4</td>\n",
       "      <td id=\"T_b9c24_row25_col14\" class=\"data row25 col14\" >6.5</td>\n",
       "      <td id=\"T_b9c24_row25_col15\" class=\"data row25 col15\" >33.1</td>\n",
       "      <td id=\"T_b9c24_row25_col16\" class=\"data row25 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row25_col17\" class=\"data row25 col17\" >40.2</td>\n",
       "      <td id=\"T_b9c24_row25_col18\" class=\"data row25 col18\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row25_col19\" class=\"data row25 col19\" >30.6</td>\n",
       "      <td id=\"T_b9c24_row25_col20\" class=\"data row25 col20\" >19.8</td>\n",
       "      <td id=\"T_b9c24_row25_col21\" class=\"data row25 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row25_col22\" class=\"data row25 col22\" >22.5</td>\n",
       "      <td id=\"T_b9c24_row25_col23\" class=\"data row25 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_b9c24_row26_col0\" class=\"data row26 col0\" >Dedup(MpNet Emb)</td>\n",
       "      <td id=\"T_b9c24_row26_col1\" class=\"data row26 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row26_col2\" class=\"data row26 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row26_col3\" class=\"data row26 col3\" >22.8</td>\n",
       "      <td id=\"T_b9c24_row26_col4\" class=\"data row26 col4\" >18.5</td>\n",
       "      <td id=\"T_b9c24_row26_col5\" class=\"data row26 col5\" >84.9</td>\n",
       "      <td id=\"T_b9c24_row26_col6\" class=\"data row26 col6\" >23.2</td>\n",
       "      <td id=\"T_b9c24_row26_col7\" class=\"data row26 col7\" >24.2</td>\n",
       "      <td id=\"T_b9c24_row26_col8\" class=\"data row26 col8\" >37.8</td>\n",
       "      <td id=\"T_b9c24_row26_col9\" class=\"data row26 col9\" >38.3</td>\n",
       "      <td id=\"T_b9c24_row26_col10\" class=\"data row26 col10\" >4.6</td>\n",
       "      <td id=\"T_b9c24_row26_col11\" class=\"data row26 col11\" >7.0</td>\n",
       "      <td id=\"T_b9c24_row26_col12\" class=\"data row26 col12\" >34.4</td>\n",
       "      <td id=\"T_b9c24_row26_col13\" class=\"data row26 col13\" >29.8</td>\n",
       "      <td id=\"T_b9c24_row26_col14\" class=\"data row26 col14\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row26_col15\" class=\"data row26 col15\" >34.9</td>\n",
       "      <td id=\"T_b9c24_row26_col16\" class=\"data row26 col16\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row26_col17\" class=\"data row26 col17\" >38.0</td>\n",
       "      <td id=\"T_b9c24_row26_col18\" class=\"data row26 col18\" >5.8</td>\n",
       "      <td id=\"T_b9c24_row26_col19\" class=\"data row26 col19\" >32.1</td>\n",
       "      <td id=\"T_b9c24_row26_col20\" class=\"data row26 col20\" >21.4</td>\n",
       "      <td id=\"T_b9c24_row26_col21\" class=\"data row26 col21\" >11.0</td>\n",
       "      <td id=\"T_b9c24_row26_col22\" class=\"data row26 col22\" >22.6</td>\n",
       "      <td id=\"T_b9c24_row26_col23\" class=\"data row26 col23\" >-55.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_b9c24_row27_col0\" class=\"data row27 col0\" >EL2N ($\\downarrow$)</td>\n",
       "      <td id=\"T_b9c24_row27_col1\" class=\"data row27 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row27_col2\" class=\"data row27 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row27_col3\" class=\"data row27 col3\" >22.8</td>\n",
       "      <td id=\"T_b9c24_row27_col4\" class=\"data row27 col4\" >21.2</td>\n",
       "      <td id=\"T_b9c24_row27_col5\" class=\"data row27 col5\" >98.7</td>\n",
       "      <td id=\"T_b9c24_row27_col6\" class=\"data row27 col6\" >22.6</td>\n",
       "      <td id=\"T_b9c24_row27_col7\" class=\"data row27 col7\" >23.6</td>\n",
       "      <td id=\"T_b9c24_row27_col8\" class=\"data row27 col8\" >38.1</td>\n",
       "      <td id=\"T_b9c24_row27_col9\" class=\"data row27 col9\" >38.5</td>\n",
       "      <td id=\"T_b9c24_row27_col10\" class=\"data row27 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row27_col11\" class=\"data row27 col11\" >8.0</td>\n",
       "      <td id=\"T_b9c24_row27_col12\" class=\"data row27 col12\" >34.3</td>\n",
       "      <td id=\"T_b9c24_row27_col13\" class=\"data row27 col13\" >29.8</td>\n",
       "      <td id=\"T_b9c24_row27_col14\" class=\"data row27 col14\" >7.2</td>\n",
       "      <td id=\"T_b9c24_row27_col15\" class=\"data row27 col15\" >32.6</td>\n",
       "      <td id=\"T_b9c24_row27_col16\" class=\"data row27 col16\" >12.2</td>\n",
       "      <td id=\"T_b9c24_row27_col17\" class=\"data row27 col17\" >38.3</td>\n",
       "      <td id=\"T_b9c24_row27_col18\" class=\"data row27 col18\" >6.4</td>\n",
       "      <td id=\"T_b9c24_row27_col19\" class=\"data row27 col19\" >32.0</td>\n",
       "      <td id=\"T_b9c24_row27_col20\" class=\"data row27 col20\" >19.9</td>\n",
       "      <td id=\"T_b9c24_row27_col21\" class=\"data row27 col21\" >12.2</td>\n",
       "      <td id=\"T_b9c24_row27_col22\" class=\"data row27 col22\" >22.7</td>\n",
       "      <td id=\"T_b9c24_row27_col23\" class=\"data row27 col23\" >-53.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_b9c24_row28_col0\" class=\"data row28 col0\" >\\#Input Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_b9c24_row28_col1\" class=\"data row28 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row28_col2\" class=\"data row28 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row28_col3\" class=\"data row28 col3\" >25.1</td>\n",
       "      <td id=\"T_b9c24_row28_col4\" class=\"data row28 col4\" >18.8</td>\n",
       "      <td id=\"T_b9c24_row28_col5\" class=\"data row28 col5\" >87.9</td>\n",
       "      <td id=\"T_b9c24_row28_col6\" class=\"data row28 col6\" >22.5</td>\n",
       "      <td id=\"T_b9c24_row28_col7\" class=\"data row28 col7\" >22.8</td>\n",
       "      <td id=\"T_b9c24_row28_col8\" class=\"data row28 col8\" >42.8</td>\n",
       "      <td id=\"T_b9c24_row28_col9\" class=\"data row28 col9\" >44.8</td>\n",
       "      <td id=\"T_b9c24_row28_col10\" class=\"data row28 col10\" >4.2</td>\n",
       "      <td id=\"T_b9c24_row28_col11\" class=\"data row28 col11\" >6.4</td>\n",
       "      <td id=\"T_b9c24_row28_col12\" class=\"data row28 col12\" >34.4</td>\n",
       "      <td id=\"T_b9c24_row28_col13\" class=\"data row28 col13\" >33.7</td>\n",
       "      <td id=\"T_b9c24_row28_col14\" class=\"data row28 col14\" >8.9</td>\n",
       "      <td id=\"T_b9c24_row28_col15\" class=\"data row28 col15\" >40.7</td>\n",
       "      <td id=\"T_b9c24_row28_col16\" class=\"data row28 col16\" >10.4</td>\n",
       "      <td id=\"T_b9c24_row28_col17\" class=\"data row28 col17\" >43.8</td>\n",
       "      <td id=\"T_b9c24_row28_col18\" class=\"data row28 col18\" >5.3</td>\n",
       "      <td id=\"T_b9c24_row28_col19\" class=\"data row28 col19\" >34.0</td>\n",
       "      <td id=\"T_b9c24_row28_col20\" class=\"data row28 col20\" >24.8</td>\n",
       "      <td id=\"T_b9c24_row28_col21\" class=\"data row28 col21\" >10.4</td>\n",
       "      <td id=\"T_b9c24_row28_col22\" class=\"data row28 col22\" >24.2</td>\n",
       "      <td id=\"T_b9c24_row28_col23\" class=\"data row28 col23\" >-43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_b9c24_row29_col0\" class=\"data row29 col0\" >llama-7b_stanford_alpaca50k_score=dedup:dist=cd:md=llama7br512p4096:emb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row29_col1\" class=\"data row29 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row29_col2\" class=\"data row29 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row29_col3\" class=\"data row29 col3\" >22.7</td>\n",
       "      <td id=\"T_b9c24_row29_col4\" class=\"data row29 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row29_col5\" class=\"data row29 col5\" >81.9</td>\n",
       "      <td id=\"T_b9c24_row29_col6\" class=\"data row29 col6\" >21.6</td>\n",
       "      <td id=\"T_b9c24_row29_col7\" class=\"data row29 col7\" >22.5</td>\n",
       "      <td id=\"T_b9c24_row29_col8\" class=\"data row29 col8\" >37.6</td>\n",
       "      <td id=\"T_b9c24_row29_col9\" class=\"data row29 col9\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row29_col10\" class=\"data row29 col10\" >6.2</td>\n",
       "      <td id=\"T_b9c24_row29_col11\" class=\"data row29 col11\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row29_col12\" class=\"data row29 col12\" >33.0</td>\n",
       "      <td id=\"T_b9c24_row29_col13\" class=\"data row29 col13\" >28.4</td>\n",
       "      <td id=\"T_b9c24_row29_col14\" class=\"data row29 col14\" >8.1</td>\n",
       "      <td id=\"T_b9c24_row29_col15\" class=\"data row29 col15\" >38.2</td>\n",
       "      <td id=\"T_b9c24_row29_col16\" class=\"data row29 col16\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row29_col17\" class=\"data row29 col17\" >37.5</td>\n",
       "      <td id=\"T_b9c24_row29_col18\" class=\"data row29 col18\" >6.1</td>\n",
       "      <td id=\"T_b9c24_row29_col19\" class=\"data row29 col19\" >30.7</td>\n",
       "      <td id=\"T_b9c24_row29_col20\" class=\"data row29 col20\" >23.1</td>\n",
       "      <td id=\"T_b9c24_row29_col21\" class=\"data row29 col21\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row29_col22\" class=\"data row29 col22\" >22.6</td>\n",
       "      <td id=\"T_b9c24_row29_col23\" class=\"data row29 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_b9c24_row30_col0\" class=\"data row30 col0\" >$\\log(\\text{pmi})$ ($\\uparrow$)</td>\n",
       "      <td id=\"T_b9c24_row30_col1\" class=\"data row30 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row30_col2\" class=\"data row30 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row30_col3\" class=\"data row30 col3\" >23.7</td>\n",
       "      <td id=\"T_b9c24_row30_col4\" class=\"data row30 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row30_col5\" class=\"data row30 col5\" >79.4</td>\n",
       "      <td id=\"T_b9c24_row30_col6\" class=\"data row30 col6\" >18.3</td>\n",
       "      <td id=\"T_b9c24_row30_col7\" class=\"data row30 col7\" >18.9</td>\n",
       "      <td id=\"T_b9c24_row30_col8\" class=\"data row30 col8\" >42.7</td>\n",
       "      <td id=\"T_b9c24_row30_col9\" class=\"data row30 col9\" >42.8</td>\n",
       "      <td id=\"T_b9c24_row30_col10\" class=\"data row30 col10\" >5.2</td>\n",
       "      <td id=\"T_b9c24_row30_col11\" class=\"data row30 col11\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row30_col12\" class=\"data row30 col12\" >30.7</td>\n",
       "      <td id=\"T_b9c24_row30_col13\" class=\"data row30 col13\" >26.1</td>\n",
       "      <td id=\"T_b9c24_row30_col14\" class=\"data row30 col14\" >9.0</td>\n",
       "      <td id=\"T_b9c24_row30_col15\" class=\"data row30 col15\" >42.6</td>\n",
       "      <td id=\"T_b9c24_row30_col16\" class=\"data row30 col16\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row30_col17\" class=\"data row30 col17\" >42.7</td>\n",
       "      <td id=\"T_b9c24_row30_col18\" class=\"data row30 col18\" >5.6</td>\n",
       "      <td id=\"T_b9c24_row30_col19\" class=\"data row30 col19\" >28.4</td>\n",
       "      <td id=\"T_b9c24_row30_col20\" class=\"data row30 col20\" >25.8</td>\n",
       "      <td id=\"T_b9c24_row30_col21\" class=\"data row30 col21\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row30_col22\" class=\"data row30 col22\" >22.7</td>\n",
       "      <td id=\"T_b9c24_row30_col23\" class=\"data row30 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_b9c24_row31_col0\" class=\"data row31 col0\" >llama-7b_stanford_alpaca50k_score=dedup:dist=cd:md=llama7br512p4096:emb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row31_col1\" class=\"data row31 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row31_col2\" class=\"data row31 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row31_col3\" class=\"data row31 col3\" >22.4</td>\n",
       "      <td id=\"T_b9c24_row31_col4\" class=\"data row31 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row31_col5\" class=\"data row31 col5\" >47.4</td>\n",
       "      <td id=\"T_b9c24_row31_col6\" class=\"data row31 col6\" >14.5</td>\n",
       "      <td id=\"T_b9c24_row31_col7\" class=\"data row31 col7\" >18.1</td>\n",
       "      <td id=\"T_b9c24_row31_col8\" class=\"data row31 col8\" >41.9</td>\n",
       "      <td id=\"T_b9c24_row31_col9\" class=\"data row31 col9\" >42.4</td>\n",
       "      <td id=\"T_b9c24_row31_col10\" class=\"data row31 col10\" >3.8</td>\n",
       "      <td id=\"T_b9c24_row31_col11\" class=\"data row31 col11\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row31_col12\" class=\"data row31 col12\" >33.8</td>\n",
       "      <td id=\"T_b9c24_row31_col13\" class=\"data row31 col13\" >22.0</td>\n",
       "      <td id=\"T_b9c24_row31_col14\" class=\"data row31 col14\" >7.6</td>\n",
       "      <td id=\"T_b9c24_row31_col15\" class=\"data row31 col15\" >36.5</td>\n",
       "      <td id=\"T_b9c24_row31_col16\" class=\"data row31 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row31_col17\" class=\"data row31 col17\" >42.1</td>\n",
       "      <td id=\"T_b9c24_row31_col18\" class=\"data row31 col18\" >4.3</td>\n",
       "      <td id=\"T_b9c24_row31_col19\" class=\"data row31 col19\" >27.9</td>\n",
       "      <td id=\"T_b9c24_row31_col20\" class=\"data row31 col20\" >22.0</td>\n",
       "      <td id=\"T_b9c24_row31_col21\" class=\"data row31 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row31_col22\" class=\"data row31 col22\" >21.3</td>\n",
       "      <td id=\"T_b9c24_row31_col23\" class=\"data row31 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_b9c24_row32_col0\" class=\"data row32 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$) Not Norm.</td>\n",
       "      <td id=\"T_b9c24_row32_col1\" class=\"data row32 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row32_col2\" class=\"data row32 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row32_col3\" class=\"data row32 col3\" >22.7</td>\n",
       "      <td id=\"T_b9c24_row32_col4\" class=\"data row32 col4\" >7.6</td>\n",
       "      <td id=\"T_b9c24_row32_col5\" class=\"data row32 col5\" >33.9</td>\n",
       "      <td id=\"T_b9c24_row32_col6\" class=\"data row32 col6\" >10.6</td>\n",
       "      <td id=\"T_b9c24_row32_col7\" class=\"data row32 col7\" >15.4</td>\n",
       "      <td id=\"T_b9c24_row32_col8\" class=\"data row32 col8\" >41.2</td>\n",
       "      <td id=\"T_b9c24_row32_col9\" class=\"data row32 col9\" >41.4</td>\n",
       "      <td id=\"T_b9c24_row32_col10\" class=\"data row32 col10\" >4.8</td>\n",
       "      <td id=\"T_b9c24_row32_col11\" class=\"data row32 col11\" >5.8</td>\n",
       "      <td id=\"T_b9c24_row32_col12\" class=\"data row32 col12\" >32.8</td>\n",
       "      <td id=\"T_b9c24_row32_col13\" class=\"data row32 col13\" >19.9</td>\n",
       "      <td id=\"T_b9c24_row32_col14\" class=\"data row32 col14\" >8.7</td>\n",
       "      <td id=\"T_b9c24_row32_col15\" class=\"data row32 col15\" >41.3</td>\n",
       "      <td id=\"T_b9c24_row32_col16\" class=\"data row32 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row32_col17\" class=\"data row32 col17\" >41.3</td>\n",
       "      <td id=\"T_b9c24_row32_col18\" class=\"data row32 col18\" >5.3</td>\n",
       "      <td id=\"T_b9c24_row32_col19\" class=\"data row32 col19\" >26.3</td>\n",
       "      <td id=\"T_b9c24_row32_col20\" class=\"data row32 col20\" >25.0</td>\n",
       "      <td id=\"T_b9c24_row32_col21\" class=\"data row32 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row32_col22\" class=\"data row32 col22\" >19.8</td>\n",
       "      <td id=\"T_b9c24_row32_col23\" class=\"data row32 col23\" >-56.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_b9c24_row33_col0\" class=\"data row33 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:theta=0.3:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=alpagasus+rating:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row33_col1\" class=\"data row33 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row33_col2\" class=\"data row33 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row33_col3\" class=\"data row33 col3\" >23.2</td>\n",
       "      <td id=\"T_b9c24_row33_col4\" class=\"data row33 col4\" >26.6</td>\n",
       "      <td id=\"T_b9c24_row33_col5\" class=\"data row33 col5\" >nan</td>\n",
       "      <td id=\"T_b9c24_row33_col6\" class=\"data row33 col6\" >nan</td>\n",
       "      <td id=\"T_b9c24_row33_col7\" class=\"data row33 col7\" >nan</td>\n",
       "      <td id=\"T_b9c24_row33_col8\" class=\"data row33 col8\" >37.5</td>\n",
       "      <td id=\"T_b9c24_row33_col9\" class=\"data row33 col9\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row33_col10\" class=\"data row33 col10\" >5.4</td>\n",
       "      <td id=\"T_b9c24_row33_col11\" class=\"data row33 col11\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row33_col12\" class=\"data row33 col12\" >34.4</td>\n",
       "      <td id=\"T_b9c24_row33_col13\" class=\"data row33 col13\" >33.2</td>\n",
       "      <td id=\"T_b9c24_row33_col14\" class=\"data row33 col14\" >8.3</td>\n",
       "      <td id=\"T_b9c24_row33_col15\" class=\"data row33 col15\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row33_col16\" class=\"data row33 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row33_col17\" class=\"data row33 col17\" >37.4</td>\n",
       "      <td id=\"T_b9c24_row33_col18\" class=\"data row33 col18\" >6.4</td>\n",
       "      <td id=\"T_b9c24_row33_col19\" class=\"data row33 col19\" >33.8</td>\n",
       "      <td id=\"T_b9c24_row33_col20\" class=\"data row33 col20\" >22.3</td>\n",
       "      <td id=\"T_b9c24_row33_col21\" class=\"data row33 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row33_col22\" class=\"data row33 col22\" >23.5</td>\n",
       "      <td id=\"T_b9c24_row33_col23\" class=\"data row33 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_b9c24_row34_col0\" class=\"data row34 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:theta=0.6:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=alpagasus+rating:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row34_col1\" class=\"data row34 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row34_col2\" class=\"data row34 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row34_col3\" class=\"data row34 col3\" >22.1</td>\n",
       "      <td id=\"T_b9c24_row34_col4\" class=\"data row34 col4\" >24.9</td>\n",
       "      <td id=\"T_b9c24_row34_col5\" class=\"data row34 col5\" >nan</td>\n",
       "      <td id=\"T_b9c24_row34_col6\" class=\"data row34 col6\" >nan</td>\n",
       "      <td id=\"T_b9c24_row34_col7\" class=\"data row34 col7\" >nan</td>\n",
       "      <td id=\"T_b9c24_row34_col8\" class=\"data row34 col8\" >35.3</td>\n",
       "      <td id=\"T_b9c24_row34_col9\" class=\"data row34 col9\" >35.7</td>\n",
       "      <td id=\"T_b9c24_row34_col10\" class=\"data row34 col10\" >3.4</td>\n",
       "      <td id=\"T_b9c24_row34_col11\" class=\"data row34 col11\" >8.6</td>\n",
       "      <td id=\"T_b9c24_row34_col12\" class=\"data row34 col12\" >30.6</td>\n",
       "      <td id=\"T_b9c24_row34_col13\" class=\"data row34 col13\" >32.6</td>\n",
       "      <td id=\"T_b9c24_row34_col14\" class=\"data row34 col14\" >7.6</td>\n",
       "      <td id=\"T_b9c24_row34_col15\" class=\"data row34 col15\" >36.3</td>\n",
       "      <td id=\"T_b9c24_row34_col16\" class=\"data row34 col16\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row34_col17\" class=\"data row34 col17\" >35.5</td>\n",
       "      <td id=\"T_b9c24_row34_col18\" class=\"data row34 col18\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row34_col19\" class=\"data row34 col19\" >31.6</td>\n",
       "      <td id=\"T_b9c24_row34_col20\" class=\"data row34 col20\" >21.9</td>\n",
       "      <td id=\"T_b9c24_row34_col21\" class=\"data row34 col21\" >9.1</td>\n",
       "      <td id=\"T_b9c24_row34_col22\" class=\"data row34 col22\" >22.4</td>\n",
       "      <td id=\"T_b9c24_row34_col23\" class=\"data row34 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_b9c24_row35_col0\" class=\"data row35 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=alpagasus+rating:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row35_col1\" class=\"data row35 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row35_col2\" class=\"data row35 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row35_col3\" class=\"data row35 col3\" >21.9</td>\n",
       "      <td id=\"T_b9c24_row35_col4\" class=\"data row35 col4\" >25.8</td>\n",
       "      <td id=\"T_b9c24_row35_col5\" class=\"data row35 col5\" >nan</td>\n",
       "      <td id=\"T_b9c24_row35_col6\" class=\"data row35 col6\" >nan</td>\n",
       "      <td id=\"T_b9c24_row35_col7\" class=\"data row35 col7\" >nan</td>\n",
       "      <td id=\"T_b9c24_row35_col8\" class=\"data row35 col8\" >30.8</td>\n",
       "      <td id=\"T_b9c24_row35_col9\" class=\"data row35 col9\" >35.1</td>\n",
       "      <td id=\"T_b9c24_row35_col10\" class=\"data row35 col10\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row35_col11\" class=\"data row35 col11\" >8.0</td>\n",
       "      <td id=\"T_b9c24_row35_col12\" class=\"data row35 col12\" >32.3</td>\n",
       "      <td id=\"T_b9c24_row35_col13\" class=\"data row35 col13\" >32.3</td>\n",
       "      <td id=\"T_b9c24_row35_col14\" class=\"data row35 col14\" >7.3</td>\n",
       "      <td id=\"T_b9c24_row35_col15\" class=\"data row35 col15\" >35.3</td>\n",
       "      <td id=\"T_b9c24_row35_col16\" class=\"data row35 col16\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row35_col17\" class=\"data row35 col17\" >33.0</td>\n",
       "      <td id=\"T_b9c24_row35_col18\" class=\"data row35 col18\" >7.0</td>\n",
       "      <td id=\"T_b9c24_row35_col19\" class=\"data row35 col19\" >32.3</td>\n",
       "      <td id=\"T_b9c24_row35_col20\" class=\"data row35 col20\" >21.3</td>\n",
       "      <td id=\"T_b9c24_row35_col21\" class=\"data row35 col21\" >9.8</td>\n",
       "      <td id=\"T_b9c24_row35_col22\" class=\"data row35 col22\" >22.3</td>\n",
       "      <td id=\"T_b9c24_row35_col23\" class=\"data row35 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_b9c24_row36_col0\" class=\"data row36 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=acos0:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row36_col1\" class=\"data row36 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row36_col2\" class=\"data row36 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row36_col3\" class=\"data row36 col3\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row36_col4\" class=\"data row36 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row36_col5\" class=\"data row36 col5\" >nan</td>\n",
       "      <td id=\"T_b9c24_row36_col6\" class=\"data row36 col6\" >nan</td>\n",
       "      <td id=\"T_b9c24_row36_col7\" class=\"data row36 col7\" >nan</td>\n",
       "      <td id=\"T_b9c24_row36_col8\" class=\"data row36 col8\" >32.8</td>\n",
       "      <td id=\"T_b9c24_row36_col9\" class=\"data row36 col9\" >36.4</td>\n",
       "      <td id=\"T_b9c24_row36_col10\" class=\"data row36 col10\" >4.2</td>\n",
       "      <td id=\"T_b9c24_row36_col11\" class=\"data row36 col11\" >8.6</td>\n",
       "      <td id=\"T_b9c24_row36_col12\" class=\"data row36 col12\" >33.1</td>\n",
       "      <td id=\"T_b9c24_row36_col13\" class=\"data row36 col13\" >30.6</td>\n",
       "      <td id=\"T_b9c24_row36_col14\" class=\"data row36 col14\" >8.0</td>\n",
       "      <td id=\"T_b9c24_row36_col15\" class=\"data row36 col15\" >37.6</td>\n",
       "      <td id=\"T_b9c24_row36_col16\" class=\"data row36 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row36_col17\" class=\"data row36 col17\" >34.6</td>\n",
       "      <td id=\"T_b9c24_row36_col18\" class=\"data row36 col18\" >6.4</td>\n",
       "      <td id=\"T_b9c24_row36_col19\" class=\"data row36 col19\" >31.9</td>\n",
       "      <td id=\"T_b9c24_row36_col20\" class=\"data row36 col20\" >22.8</td>\n",
       "      <td id=\"T_b9c24_row36_col21\" class=\"data row36 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row36_col22\" class=\"data row36 col22\" >22.2</td>\n",
       "      <td id=\"T_b9c24_row36_col23\" class=\"data row36 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_b9c24_row37_col0\" class=\"data row37 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=acos0:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row37_col1\" class=\"data row37 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row37_col2\" class=\"data row37 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row37_col3\" class=\"data row37 col3\" >21.5</td>\n",
       "      <td id=\"T_b9c24_row37_col4\" class=\"data row37 col4\" >nan</td>\n",
       "      <td id=\"T_b9c24_row37_col5\" class=\"data row37 col5\" >nan</td>\n",
       "      <td id=\"T_b9c24_row37_col6\" class=\"data row37 col6\" >nan</td>\n",
       "      <td id=\"T_b9c24_row37_col7\" class=\"data row37 col7\" >nan</td>\n",
       "      <td id=\"T_b9c24_row37_col8\" class=\"data row37 col8\" >34.8</td>\n",
       "      <td id=\"T_b9c24_row37_col9\" class=\"data row37 col9\" >38.1</td>\n",
       "      <td id=\"T_b9c24_row37_col10\" class=\"data row37 col10\" >4.6</td>\n",
       "      <td id=\"T_b9c24_row37_col11\" class=\"data row37 col11\" >7.2</td>\n",
       "      <td id=\"T_b9c24_row37_col12\" class=\"data row37 col12\" >30.5</td>\n",
       "      <td id=\"T_b9c24_row37_col13\" class=\"data row37 col13\" >29.8</td>\n",
       "      <td id=\"T_b9c24_row37_col14\" class=\"data row37 col14\" >6.7</td>\n",
       "      <td id=\"T_b9c24_row37_col15\" class=\"data row37 col15\" >33.2</td>\n",
       "      <td id=\"T_b9c24_row37_col16\" class=\"data row37 col16\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row37_col17\" class=\"data row37 col17\" >36.5</td>\n",
       "      <td id=\"T_b9c24_row37_col18\" class=\"data row37 col18\" >5.9</td>\n",
       "      <td id=\"T_b9c24_row37_col19\" class=\"data row37 col19\" >30.1</td>\n",
       "      <td id=\"T_b9c24_row37_col20\" class=\"data row37 col20\" >19.9</td>\n",
       "      <td id=\"T_b9c24_row37_col21\" class=\"data row37 col21\" >8.5</td>\n",
       "      <td id=\"T_b9c24_row37_col22\" class=\"data row37 col22\" >21.5</td>\n",
       "      <td id=\"T_b9c24_row37_col23\" class=\"data row37 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c24_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_b9c24_row38_col0\" class=\"data row38 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=1:theta=0.9:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=alpagasus+rating:qmd=llama7br512p4096_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_b9c24_row38_col1\" class=\"data row38 col1\" >30000</td>\n",
       "      <td id=\"T_b9c24_row38_col2\" class=\"data row38 col2\" >10000</td>\n",
       "      <td id=\"T_b9c24_row38_col3\" class=\"data row38 col3\" >21.4</td>\n",
       "      <td id=\"T_b9c24_row38_col4\" class=\"data row38 col4\" >21.8</td>\n",
       "      <td id=\"T_b9c24_row38_col5\" class=\"data row38 col5\" >nan</td>\n",
       "      <td id=\"T_b9c24_row38_col6\" class=\"data row38 col6\" >nan</td>\n",
       "      <td id=\"T_b9c24_row38_col7\" class=\"data row38 col7\" >nan</td>\n",
       "      <td id=\"T_b9c24_row38_col8\" class=\"data row38 col8\" >29.9</td>\n",
       "      <td id=\"T_b9c24_row38_col9\" class=\"data row38 col9\" >36.9</td>\n",
       "      <td id=\"T_b9c24_row38_col10\" class=\"data row38 col10\" >4.6</td>\n",
       "      <td id=\"T_b9c24_row38_col11\" class=\"data row38 col11\" >7.4</td>\n",
       "      <td id=\"T_b9c24_row38_col12\" class=\"data row38 col12\" >30.6</td>\n",
       "      <td id=\"T_b9c24_row38_col13\" class=\"data row38 col13\" >32.8</td>\n",
       "      <td id=\"T_b9c24_row38_col14\" class=\"data row38 col14\" >7.5</td>\n",
       "      <td id=\"T_b9c24_row38_col15\" class=\"data row38 col15\" >35.3</td>\n",
       "      <td id=\"T_b9c24_row38_col16\" class=\"data row38 col16\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row38_col17\" class=\"data row38 col17\" >33.4</td>\n",
       "      <td id=\"T_b9c24_row38_col18\" class=\"data row38 col18\" >6.0</td>\n",
       "      <td id=\"T_b9c24_row38_col19\" class=\"data row38 col19\" >31.7</td>\n",
       "      <td id=\"T_b9c24_row38_col20\" class=\"data row38 col20\" >21.4</td>\n",
       "      <td id=\"T_b9c24_row38_col21\" class=\"data row38 col21\" >7.9</td>\n",
       "      <td id=\"T_b9c24_row38_col22\" class=\"data row38 col22\" >21.5</td>\n",
       "      <td id=\"T_b9c24_row38_col23\" class=\"data row38 col23\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca27f181f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_30c55 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_30c55_row0_col0, #T_30c55_row1_col0, #T_30c55_row2_col0, #T_30c55_row3_col0, #T_30c55_row4_col0, #T_30c55_row5_col0, #T_30c55_row6_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_30c55_row0_col1, #T_30c55_row0_col2, #T_30c55_row1_col1, #T_30c55_row1_col2, #T_30c55_row1_col3, #T_30c55_row1_col9, #T_30c55_row1_col15, #T_30c55_row1_col17, #T_30c55_row2_col1, #T_30c55_row2_col2, #T_30c55_row2_col8, #T_30c55_row3_col1, #T_30c55_row3_col2, #T_30c55_row3_col5, #T_30c55_row4_col1, #T_30c55_row4_col2, #T_30c55_row4_col11, #T_30c55_row4_col12, #T_30c55_row4_col13, #T_30c55_row4_col19, #T_30c55_row4_col22, #T_30c55_row5_col1, #T_30c55_row5_col2, #T_30c55_row5_col10, #T_30c55_row5_col14, #T_30c55_row5_col16, #T_30c55_row5_col18, #T_30c55_row5_col20, #T_30c55_row5_col21, #T_30c55_row6_col1, #T_30c55_row6_col2, #T_30c55_row6_col6, #T_30c55_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col4, #T_30c55_row0_col23, #T_30c55_row1_col4, #T_30c55_row1_col23, #T_30c55_row2_col4, #T_30c55_row2_col23, #T_30c55_row3_col4, #T_30c55_row3_col23, #T_30c55_row4_col4, #T_30c55_row4_col23, #T_30c55_row5_col4, #T_30c55_row5_col23, #T_30c55_row6_col4, #T_30c55_row6_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col6, #T_30c55_row0_col7, #T_30c55_row0_col16, #T_30c55_row0_col21, #T_30c55_row1_col10, #T_30c55_row1_col14, #T_30c55_row2_col3, #T_30c55_row2_col5, #T_30c55_row2_col11, #T_30c55_row2_col13, #T_30c55_row2_col16, #T_30c55_row2_col18, #T_30c55_row2_col19, #T_30c55_row2_col21, #T_30c55_row2_col22, #T_30c55_row5_col8, #T_30c55_row5_col9, #T_30c55_row5_col12, #T_30c55_row5_col17, #T_30c55_row6_col15, #T_30c55_row6_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col8, #T_30c55_row0_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col9, #T_30c55_row3_col16, #T_30c55_row3_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col10, #T_30c55_row4_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col13, #T_30c55_row3_col6, #T_30c55_row4_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col15, #T_30c55_row5_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row0_col19, #T_30c55_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row0_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row1_col8, #T_30c55_row2_col17, #T_30c55_row6_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #94b6ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row1_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #799cf8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row1_col13, #T_30c55_row1_col16, #T_30c55_row1_col21, #T_30c55_row3_col13, #T_30c55_row4_col16, #T_30c55_row4_col21, #T_30c55_row6_col16, #T_30c55_row6_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #e9d5cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row1_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row1_col19, #T_30c55_row3_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row1_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row1_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row2_col9, #T_30c55_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row2_col10, #T_30c55_row5_col19, #T_30c55_row6_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row2_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row2_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row3_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col15, #T_30c55_row6_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row3_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row3_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row3_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row3_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row4_col6, #T_30c55_row4_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e0dbd8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row4_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row4_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row4_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row4_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row5_col7, #T_30c55_row6_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row5_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row5_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #6384eb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row5_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_30c55_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d7dce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row6_col9, #T_30c55_row6_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row6_col11, #T_30c55_row6_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row6_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #d2dbe8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_30c55_row6_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_30c55\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_30c55_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_30c55_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_30c55_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_30c55_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_30c55_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_30c55_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_30c55_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_30c55_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_30c55_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_30c55_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_30c55_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_30c55_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_30c55_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_30c55_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_30c55_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_30c55_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_30c55_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_30c55_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_30c55_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_30c55_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_30c55_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_30c55_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_30c55_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_30c55_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_30c55_row0_col0\" class=\"data row0 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td id=\"T_30c55_row0_col1\" class=\"data row0 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row0_col2\" class=\"data row0 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row0_col3\" class=\"data row0 col3\" >22.8</td>\n",
       "      <td id=\"T_30c55_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row0_col5\" class=\"data row0 col5\" >106.8</td>\n",
       "      <td id=\"T_30c55_row0_col6\" class=\"data row0 col6\" >31.9</td>\n",
       "      <td id=\"T_30c55_row0_col7\" class=\"data row0 col7\" >32.6</td>\n",
       "      <td id=\"T_30c55_row0_col8\" class=\"data row0 col8\" >36.7</td>\n",
       "      <td id=\"T_30c55_row0_col9\" class=\"data row0 col9\" >37.8</td>\n",
       "      <td id=\"T_30c55_row0_col10\" class=\"data row0 col10\" >4.8</td>\n",
       "      <td id=\"T_30c55_row0_col11\" class=\"data row0 col11\" >8.6</td>\n",
       "      <td id=\"T_30c55_row0_col12\" class=\"data row0 col12\" >31.6</td>\n",
       "      <td id=\"T_30c55_row0_col13\" class=\"data row0 col13\" >32.5</td>\n",
       "      <td id=\"T_30c55_row0_col14\" class=\"data row0 col14\" >7.2</td>\n",
       "      <td id=\"T_30c55_row0_col15\" class=\"data row0 col15\" >34.7</td>\n",
       "      <td id=\"T_30c55_row0_col16\" class=\"data row0 col16\" >11.0</td>\n",
       "      <td id=\"T_30c55_row0_col17\" class=\"data row0 col17\" >37.2</td>\n",
       "      <td id=\"T_30c55_row0_col18\" class=\"data row0 col18\" >6.7</td>\n",
       "      <td id=\"T_30c55_row0_col19\" class=\"data row0 col19\" >32.0</td>\n",
       "      <td id=\"T_30c55_row0_col20\" class=\"data row0 col20\" >21.0</td>\n",
       "      <td id=\"T_30c55_row0_col21\" class=\"data row0 col21\" >11.0</td>\n",
       "      <td id=\"T_30c55_row0_col22\" class=\"data row0 col22\" >24.5</td>\n",
       "      <td id=\"T_30c55_row0_col23\" class=\"data row0 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_30c55_row1_col0\" class=\"data row1 col0\" >llama-7b_stanford_alpaca50k_score=dppmap:k=vmf:gamma=0.1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=60000:ep=3</td>\n",
       "      <td id=\"T_30c55_row1_col1\" class=\"data row1 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row1_col2\" class=\"data row1 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row1_col3\" class=\"data row1 col3\" >22.4</td>\n",
       "      <td id=\"T_30c55_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row1_col5\" class=\"data row1 col5\" >102.7</td>\n",
       "      <td id=\"T_30c55_row1_col6\" class=\"data row1 col6\" >31.1</td>\n",
       "      <td id=\"T_30c55_row1_col7\" class=\"data row1 col7\" >31.9</td>\n",
       "      <td id=\"T_30c55_row1_col8\" class=\"data row1 col8\" >35.8</td>\n",
       "      <td id=\"T_30c55_row1_col9\" class=\"data row1 col9\" >36.0</td>\n",
       "      <td id=\"T_30c55_row1_col10\" class=\"data row1 col10\" >5.2</td>\n",
       "      <td id=\"T_30c55_row1_col11\" class=\"data row1 col11\" >7.2</td>\n",
       "      <td id=\"T_30c55_row1_col12\" class=\"data row1 col12\" >31.8</td>\n",
       "      <td id=\"T_30c55_row1_col13\" class=\"data row1 col13\" >33.3</td>\n",
       "      <td id=\"T_30c55_row1_col14\" class=\"data row1 col14\" >8.1</td>\n",
       "      <td id=\"T_30c55_row1_col15\" class=\"data row1 col15\" >34.7</td>\n",
       "      <td id=\"T_30c55_row1_col16\" class=\"data row1 col16\" >9.1</td>\n",
       "      <td id=\"T_30c55_row1_col17\" class=\"data row1 col17\" >35.9</td>\n",
       "      <td id=\"T_30c55_row1_col18\" class=\"data row1 col18\" >6.2</td>\n",
       "      <td id=\"T_30c55_row1_col19\" class=\"data row1 col19\" >32.5</td>\n",
       "      <td id=\"T_30c55_row1_col20\" class=\"data row1 col20\" >21.4</td>\n",
       "      <td id=\"T_30c55_row1_col21\" class=\"data row1 col21\" >9.1</td>\n",
       "      <td id=\"T_30c55_row1_col22\" class=\"data row1 col22\" >24.0</td>\n",
       "      <td id=\"T_30c55_row1_col23\" class=\"data row1 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_30c55_row2_col0\" class=\"data row2 col0\" >\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_30c55_row2_col1\" class=\"data row2 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row2_col2\" class=\"data row2 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row2_col3\" class=\"data row2 col3\" >23.3</td>\n",
       "      <td id=\"T_30c55_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row2_col5\" class=\"data row2 col5\" >121.0</td>\n",
       "      <td id=\"T_30c55_row2_col6\" class=\"data row2 col6\" >31.5</td>\n",
       "      <td id=\"T_30c55_row2_col7\" class=\"data row2 col7\" >30.4</td>\n",
       "      <td id=\"T_30c55_row2_col8\" class=\"data row2 col8\" >35.2</td>\n",
       "      <td id=\"T_30c55_row2_col9\" class=\"data row2 col9\" >37.7</td>\n",
       "      <td id=\"T_30c55_row2_col10\" class=\"data row2 col10\" >4.4</td>\n",
       "      <td id=\"T_30c55_row2_col11\" class=\"data row2 col11\" >10.4</td>\n",
       "      <td id=\"T_30c55_row2_col12\" class=\"data row2 col12\" >33.4</td>\n",
       "      <td id=\"T_30c55_row2_col13\" class=\"data row2 col13\" >34.6</td>\n",
       "      <td id=\"T_30c55_row2_col14\" class=\"data row2 col14\" >7.8</td>\n",
       "      <td id=\"T_30c55_row2_col15\" class=\"data row2 col15\" >35.2</td>\n",
       "      <td id=\"T_30c55_row2_col16\" class=\"data row2 col16\" >11.0</td>\n",
       "      <td id=\"T_30c55_row2_col17\" class=\"data row2 col17\" >36.4</td>\n",
       "      <td id=\"T_30c55_row2_col18\" class=\"data row2 col18\" >7.4</td>\n",
       "      <td id=\"T_30c55_row2_col19\" class=\"data row2 col19\" >34.0</td>\n",
       "      <td id=\"T_30c55_row2_col20\" class=\"data row2 col20\" >21.5</td>\n",
       "      <td id=\"T_30c55_row2_col21\" class=\"data row2 col21\" >11.0</td>\n",
       "      <td id=\"T_30c55_row2_col22\" class=\"data row2 col22\" >24.7</td>\n",
       "      <td id=\"T_30c55_row2_col23\" class=\"data row2 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_30c55_row3_col0\" class=\"data row3 col0\" >DPP (Llama Emb Not Norm.)</td>\n",
       "      <td id=\"T_30c55_row3_col1\" class=\"data row3 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row3_col2\" class=\"data row3 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row3_col3\" class=\"data row3 col3\" >23.2</td>\n",
       "      <td id=\"T_30c55_row3_col4\" class=\"data row3 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row3_col5\" class=\"data row3 col5\" >90.5</td>\n",
       "      <td id=\"T_30c55_row3_col6\" class=\"data row3 col6\" >26.6</td>\n",
       "      <td id=\"T_30c55_row3_col7\" class=\"data row3 col7\" >28.2</td>\n",
       "      <td id=\"T_30c55_row3_col8\" class=\"data row3 col8\" >40.3</td>\n",
       "      <td id=\"T_30c55_row3_col9\" class=\"data row3 col9\" >41.3</td>\n",
       "      <td id=\"T_30c55_row3_col10\" class=\"data row3 col10\" >4.2</td>\n",
       "      <td id=\"T_30c55_row3_col11\" class=\"data row3 col11\" >6.2</td>\n",
       "      <td id=\"T_30c55_row3_col12\" class=\"data row3 col12\" >32.4</td>\n",
       "      <td id=\"T_30c55_row3_col13\" class=\"data row3 col13\" >33.3</td>\n",
       "      <td id=\"T_30c55_row3_col14\" class=\"data row3 col14\" >7.6</td>\n",
       "      <td id=\"T_30c55_row3_col15\" class=\"data row3 col15\" >35.3</td>\n",
       "      <td id=\"T_30c55_row3_col16\" class=\"data row3 col16\" >8.1</td>\n",
       "      <td id=\"T_30c55_row3_col17\" class=\"data row3 col17\" >40.8</td>\n",
       "      <td id=\"T_30c55_row3_col18\" class=\"data row3 col18\" >5.2</td>\n",
       "      <td id=\"T_30c55_row3_col19\" class=\"data row3 col19\" >32.9</td>\n",
       "      <td id=\"T_30c55_row3_col20\" class=\"data row3 col20\" >21.5</td>\n",
       "      <td id=\"T_30c55_row3_col21\" class=\"data row3 col21\" >8.1</td>\n",
       "      <td id=\"T_30c55_row3_col22\" class=\"data row3 col22\" >24.0</td>\n",
       "      <td id=\"T_30c55_row3_col23\" class=\"data row3 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_30c55_row4_col0\" class=\"data row4 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$)</td>\n",
       "      <td id=\"T_30c55_row4_col1\" class=\"data row4 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row4_col2\" class=\"data row4 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row4_col3\" class=\"data row4 col3\" >22.6</td>\n",
       "      <td id=\"T_30c55_row4_col4\" class=\"data row4 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row4_col5\" class=\"data row4 col5\" >94.7</td>\n",
       "      <td id=\"T_30c55_row4_col6\" class=\"data row4 col6\" >27.3</td>\n",
       "      <td id=\"T_30c55_row4_col7\" class=\"data row4 col7\" >27.8</td>\n",
       "      <td id=\"T_30c55_row4_col8\" class=\"data row4 col8\" >38.7</td>\n",
       "      <td id=\"T_30c55_row4_col9\" class=\"data row4 col9\" >38.1</td>\n",
       "      <td id=\"T_30c55_row4_col10\" class=\"data row4 col10\" >4.8</td>\n",
       "      <td id=\"T_30c55_row4_col11\" class=\"data row4 col11\" >6.0</td>\n",
       "      <td id=\"T_30c55_row4_col12\" class=\"data row4 col12\" >31.3</td>\n",
       "      <td id=\"T_30c55_row4_col13\" class=\"data row4 col13\" >31.8</td>\n",
       "      <td id=\"T_30c55_row4_col14\" class=\"data row4 col14\" >7.7</td>\n",
       "      <td id=\"T_30c55_row4_col15\" class=\"data row4 col15\" >35.6</td>\n",
       "      <td id=\"T_30c55_row4_col16\" class=\"data row4 col16\" >9.1</td>\n",
       "      <td id=\"T_30c55_row4_col17\" class=\"data row4 col17\" >38.4</td>\n",
       "      <td id=\"T_30c55_row4_col18\" class=\"data row4 col18\" >5.4</td>\n",
       "      <td id=\"T_30c55_row4_col19\" class=\"data row4 col19\" >31.5</td>\n",
       "      <td id=\"T_30c55_row4_col20\" class=\"data row4 col20\" >21.6</td>\n",
       "      <td id=\"T_30c55_row4_col21\" class=\"data row4 col21\" >9.1</td>\n",
       "      <td id=\"T_30c55_row4_col22\" class=\"data row4 col22\" >23.5</td>\n",
       "      <td id=\"T_30c55_row4_col23\" class=\"data row4 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_30c55_row5_col0\" class=\"data row5 col0\" >DPP (Llama Emb)</td>\n",
       "      <td id=\"T_30c55_row5_col1\" class=\"data row5 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row5_col2\" class=\"data row5 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row5_col3\" class=\"data row5 col3\" >23.1</td>\n",
       "      <td id=\"T_30c55_row5_col4\" class=\"data row5 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row5_col5\" class=\"data row5 col5\" >91.4</td>\n",
       "      <td id=\"T_30c55_row5_col6\" class=\"data row5 col6\" >25.2</td>\n",
       "      <td id=\"T_30c55_row5_col7\" class=\"data row5 col7\" >26.6</td>\n",
       "      <td id=\"T_30c55_row5_col8\" class=\"data row5 col8\" >41.9</td>\n",
       "      <td id=\"T_30c55_row5_col9\" class=\"data row5 col9\" >41.9</td>\n",
       "      <td id=\"T_30c55_row5_col10\" class=\"data row5 col10\" >3.4</td>\n",
       "      <td id=\"T_30c55_row5_col11\" class=\"data row5 col11\" >6.6</td>\n",
       "      <td id=\"T_30c55_row5_col12\" class=\"data row5 col12\" >33.7</td>\n",
       "      <td id=\"T_30c55_row5_col13\" class=\"data row5 col13\" >32.1</td>\n",
       "      <td id=\"T_30c55_row5_col14\" class=\"data row5 col14\" >6.8</td>\n",
       "      <td id=\"T_30c55_row5_col15\" class=\"data row5 col15\" >34.7</td>\n",
       "      <td id=\"T_30c55_row5_col16\" class=\"data row5 col16\" >6.9</td>\n",
       "      <td id=\"T_30c55_row5_col17\" class=\"data row5 col17\" >41.9</td>\n",
       "      <td id=\"T_30c55_row5_col18\" class=\"data row5 col18\" >5.0</td>\n",
       "      <td id=\"T_30c55_row5_col19\" class=\"data row5 col19\" >32.9</td>\n",
       "      <td id=\"T_30c55_row5_col20\" class=\"data row5 col20\" >20.7</td>\n",
       "      <td id=\"T_30c55_row5_col21\" class=\"data row5 col21\" >6.9</td>\n",
       "      <td id=\"T_30c55_row5_col22\" class=\"data row5 col22\" >23.6</td>\n",
       "      <td id=\"T_30c55_row5_col23\" class=\"data row5 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_30c55_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_30c55_row6_col0\" class=\"data row6 col0\" >Random</td>\n",
       "      <td id=\"T_30c55_row6_col1\" class=\"data row6 col1\" >60000</td>\n",
       "      <td id=\"T_30c55_row6_col2\" class=\"data row6 col2\" >20000</td>\n",
       "      <td id=\"T_30c55_row6_col3\" class=\"data row6 col3\" >23.2</td>\n",
       "      <td id=\"T_30c55_row6_col4\" class=\"data row6 col4\" >nan</td>\n",
       "      <td id=\"T_30c55_row6_col5\" class=\"data row6 col5\" >96.1</td>\n",
       "      <td id=\"T_30c55_row6_col6\" class=\"data row6 col6\" >24.8</td>\n",
       "      <td id=\"T_30c55_row6_col7\" class=\"data row6 col7\" >25.9</td>\n",
       "      <td id=\"T_30c55_row6_col8\" class=\"data row6 col8\" >38.4</td>\n",
       "      <td id=\"T_30c55_row6_col9\" class=\"data row6 col9\" >38.5</td>\n",
       "      <td id=\"T_30c55_row6_col10\" class=\"data row6 col10\" >4.0</td>\n",
       "      <td id=\"T_30c55_row6_col11\" class=\"data row6 col11\" >8.2</td>\n",
       "      <td id=\"T_30c55_row6_col12\" class=\"data row6 col12\" >32.5</td>\n",
       "      <td id=\"T_30c55_row6_col13\" class=\"data row6 col13\" >32.0</td>\n",
       "      <td id=\"T_30c55_row6_col14\" class=\"data row6 col14\" >7.5</td>\n",
       "      <td id=\"T_30c55_row6_col15\" class=\"data row6 col15\" >38.2</td>\n",
       "      <td id=\"T_30c55_row6_col16\" class=\"data row6 col16\" >9.1</td>\n",
       "      <td id=\"T_30c55_row6_col17\" class=\"data row6 col17\" >38.5</td>\n",
       "      <td id=\"T_30c55_row6_col18\" class=\"data row6 col18\" >6.1</td>\n",
       "      <td id=\"T_30c55_row6_col19\" class=\"data row6 col19\" >32.3</td>\n",
       "      <td id=\"T_30c55_row6_col20\" class=\"data row6 col20\" >22.9</td>\n",
       "      <td id=\"T_30c55_row6_col21\" class=\"data row6 col21\" >9.1</td>\n",
       "      <td id=\"T_30c55_row6_col22\" class=\"data row6 col22\" >23.6</td>\n",
       "      <td id=\"T_30c55_row6_col23\" class=\"data row6 col23\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca2c660ca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d171e td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_d171e_row0_col0, #T_d171e_row1_col0, #T_d171e_row2_col0, #T_d171e_row3_col0, #T_d171e_row4_col0, #T_d171e_row5_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d171e_row0_col1, #T_d171e_row0_col2, #T_d171e_row1_col1, #T_d171e_row1_col2, #T_d171e_row1_col8, #T_d171e_row1_col9, #T_d171e_row1_col10, #T_d171e_row1_col17, #T_d171e_row2_col1, #T_d171e_row2_col2, #T_d171e_row2_col10, #T_d171e_row3_col1, #T_d171e_row3_col2, #T_d171e_row3_col4, #T_d171e_row3_col12, #T_d171e_row3_col13, #T_d171e_row3_col15, #T_d171e_row3_col19, #T_d171e_row3_col20, #T_d171e_row4_col1, #T_d171e_row4_col2, #T_d171e_row4_col3, #T_d171e_row4_col14, #T_d171e_row4_col16, #T_d171e_row4_col21, #T_d171e_row4_col22, #T_d171e_row4_col23, #T_d171e_row5_col1, #T_d171e_row5_col2, #T_d171e_row5_col5, #T_d171e_row5_col6, #T_d171e_row5_col7, #T_d171e_row5_col11, #T_d171e_row5_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dbdcde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col4, #T_d171e_row0_col6, #T_d171e_row0_col7, #T_d171e_row0_col23, #T_d171e_row1_col5, #T_d171e_row1_col11, #T_d171e_row1_col13, #T_d171e_row1_col14, #T_d171e_row1_col18, #T_d171e_row2_col3, #T_d171e_row2_col15, #T_d171e_row2_col16, #T_d171e_row2_col19, #T_d171e_row2_col20, #T_d171e_row2_col21, #T_d171e_row2_col22, #T_d171e_row3_col8, #T_d171e_row3_col9, #T_d171e_row3_col17, #T_d171e_row4_col10, #T_d171e_row4_col12, #T_d171e_row5_col16, #T_d171e_row5_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row0_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b497;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col8, #T_d171e_row5_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col9, #T_d171e_row2_col9, #T_d171e_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col10, #T_d171e_row0_col16, #T_d171e_row0_col21, #T_d171e_row4_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col17, #T_d171e_row3_col3, #T_d171e_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col18, #T_d171e_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col19, #T_d171e_row5_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col20, #T_d171e_row1_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row0_col22, #T_d171e_row4_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row1_col4, #T_d171e_row1_col23, #T_d171e_row2_col4, #T_d171e_row2_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #b8122a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col16, #T_d171e_row1_col21 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row1_col19 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row1_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row2_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ed8366;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row2_col14, #T_d171e_row3_col10, #T_d171e_row3_col16, #T_d171e_row3_col21, #T_d171e_row5_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row2_col17, #T_d171e_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #d4dbe6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row2_col18, #T_d171e_row4_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row3_col18 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row3_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row3_col23, #T_d171e_row4_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row4_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row5_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row5_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d171e_row5_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row5_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row5_col17 {\n",
       "  text-align: left;\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row5_col20 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row5_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d171e_row5_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d171e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d171e_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_d171e_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_d171e_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_d171e_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_d171e_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_d171e_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_d171e_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_d171e_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_d171e_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_d171e_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_d171e_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_d171e_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_d171e_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_d171e_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_d171e_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_d171e_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_d171e_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_d171e_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_d171e_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_d171e_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_d171e_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_d171e_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_d171e_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_d171e_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d171e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d171e_row0_col0\" class=\"data row0 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$)</td>\n",
       "      <td id=\"T_d171e_row0_col1\" class=\"data row0 col1\" >90000</td>\n",
       "      <td id=\"T_d171e_row0_col2\" class=\"data row0 col2\" >30000</td>\n",
       "      <td id=\"T_d171e_row0_col3\" class=\"data row0 col3\" >23.1</td>\n",
       "      <td id=\"T_d171e_row0_col4\" class=\"data row0 col4\" >24.5</td>\n",
       "      <td id=\"T_d171e_row0_col5\" class=\"data row0 col5\" >94.3</td>\n",
       "      <td id=\"T_d171e_row0_col6\" class=\"data row0 col6\" >29.5</td>\n",
       "      <td id=\"T_d171e_row0_col7\" class=\"data row0 col7\" >30.5</td>\n",
       "      <td id=\"T_d171e_row0_col8\" class=\"data row0 col8\" >39.8</td>\n",
       "      <td id=\"T_d171e_row0_col9\" class=\"data row0 col9\" >40.1</td>\n",
       "      <td id=\"T_d171e_row0_col10\" class=\"data row0 col10\" >4.4</td>\n",
       "      <td id=\"T_d171e_row0_col11\" class=\"data row0 col11\" >6.4</td>\n",
       "      <td id=\"T_d171e_row0_col12\" class=\"data row0 col12\" >32.5</td>\n",
       "      <td id=\"T_d171e_row0_col13\" class=\"data row0 col13\" >33.6</td>\n",
       "      <td id=\"T_d171e_row0_col14\" class=\"data row0 col14\" >7.2</td>\n",
       "      <td id=\"T_d171e_row0_col15\" class=\"data row0 col15\" >34.7</td>\n",
       "      <td id=\"T_d171e_row0_col16\" class=\"data row0 col16\" >9.1</td>\n",
       "      <td id=\"T_d171e_row0_col17\" class=\"data row0 col17\" >40.0</td>\n",
       "      <td id=\"T_d171e_row0_col18\" class=\"data row0 col18\" >5.4</td>\n",
       "      <td id=\"T_d171e_row0_col19\" class=\"data row0 col19\" >33.1</td>\n",
       "      <td id=\"T_d171e_row0_col20\" class=\"data row0 col20\" >20.9</td>\n",
       "      <td id=\"T_d171e_row0_col21\" class=\"data row0 col21\" >9.1</td>\n",
       "      <td id=\"T_d171e_row0_col22\" class=\"data row0 col22\" >24.4</td>\n",
       "      <td id=\"T_d171e_row0_col23\" class=\"data row0 col23\" >-47.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d171e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d171e_row1_col0\" class=\"data row1 col0\" >\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_d171e_row1_col1\" class=\"data row1 col1\" >90000</td>\n",
       "      <td id=\"T_d171e_row1_col2\" class=\"data row1 col2\" >30000</td>\n",
       "      <td id=\"T_d171e_row1_col3\" class=\"data row1 col3\" >22.9</td>\n",
       "      <td id=\"T_d171e_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
       "      <td id=\"T_d171e_row1_col5\" class=\"data row1 col5\" >99.0</td>\n",
       "      <td id=\"T_d171e_row1_col6\" class=\"data row1 col6\" >29.4</td>\n",
       "      <td id=\"T_d171e_row1_col7\" class=\"data row1 col7\" >30.4</td>\n",
       "      <td id=\"T_d171e_row1_col8\" class=\"data row1 col8\" >37.6</td>\n",
       "      <td id=\"T_d171e_row1_col9\" class=\"data row1 col9\" >37.9</td>\n",
       "      <td id=\"T_d171e_row1_col10\" class=\"data row1 col10\" >4.2</td>\n",
       "      <td id=\"T_d171e_row1_col11\" class=\"data row1 col11\" >7.6</td>\n",
       "      <td id=\"T_d171e_row1_col12\" class=\"data row1 col12\" >32.0</td>\n",
       "      <td id=\"T_d171e_row1_col13\" class=\"data row1 col13\" >34.4</td>\n",
       "      <td id=\"T_d171e_row1_col14\" class=\"data row1 col14\" >7.6</td>\n",
       "      <td id=\"T_d171e_row1_col15\" class=\"data row1 col15\" >35.9</td>\n",
       "      <td id=\"T_d171e_row1_col16\" class=\"data row1 col16\" >8.5</td>\n",
       "      <td id=\"T_d171e_row1_col17\" class=\"data row1 col17\" >37.8</td>\n",
       "      <td id=\"T_d171e_row1_col18\" class=\"data row1 col18\" >5.9</td>\n",
       "      <td id=\"T_d171e_row1_col19\" class=\"data row1 col19\" >33.2</td>\n",
       "      <td id=\"T_d171e_row1_col20\" class=\"data row1 col20\" >21.8</td>\n",
       "      <td id=\"T_d171e_row1_col21\" class=\"data row1 col21\" >8.5</td>\n",
       "      <td id=\"T_d171e_row1_col22\" class=\"data row1 col22\" >24.1</td>\n",
       "      <td id=\"T_d171e_row1_col23\" class=\"data row1 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d171e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d171e_row2_col0\" class=\"data row2 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td id=\"T_d171e_row2_col1\" class=\"data row2 col1\" >90000</td>\n",
       "      <td id=\"T_d171e_row2_col2\" class=\"data row2 col2\" >30000</td>\n",
       "      <td id=\"T_d171e_row2_col3\" class=\"data row2 col3\" >23.7</td>\n",
       "      <td id=\"T_d171e_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
       "      <td id=\"T_d171e_row2_col5\" class=\"data row2 col5\" >98.3</td>\n",
       "      <td id=\"T_d171e_row2_col6\" class=\"data row2 col6\" >28.7</td>\n",
       "      <td id=\"T_d171e_row2_col7\" class=\"data row2 col7\" >29.6</td>\n",
       "      <td id=\"T_d171e_row2_col8\" class=\"data row2 col8\" >39.3</td>\n",
       "      <td id=\"T_d171e_row2_col9\" class=\"data row2 col9\" >40.1</td>\n",
       "      <td id=\"T_d171e_row2_col10\" class=\"data row2 col10\" >4.2</td>\n",
       "      <td id=\"T_d171e_row2_col11\" class=\"data row2 col11\" >6.2</td>\n",
       "      <td id=\"T_d171e_row2_col12\" class=\"data row2 col12\" >34.4</td>\n",
       "      <td id=\"T_d171e_row2_col13\" class=\"data row2 col13\" >34.1</td>\n",
       "      <td id=\"T_d171e_row2_col14\" class=\"data row2 col14\" >7.3</td>\n",
       "      <td id=\"T_d171e_row2_col15\" class=\"data row2 col15\" >36.3</td>\n",
       "      <td id=\"T_d171e_row2_col16\" class=\"data row2 col16\" >11.6</td>\n",
       "      <td id=\"T_d171e_row2_col17\" class=\"data row2 col17\" >39.7</td>\n",
       "      <td id=\"T_d171e_row2_col18\" class=\"data row2 col18\" >5.2</td>\n",
       "      <td id=\"T_d171e_row2_col19\" class=\"data row2 col19\" >34.3</td>\n",
       "      <td id=\"T_d171e_row2_col20\" class=\"data row2 col20\" >21.8</td>\n",
       "      <td id=\"T_d171e_row2_col21\" class=\"data row2 col21\" >11.6</td>\n",
       "      <td id=\"T_d171e_row2_col22\" class=\"data row2 col22\" >24.7</td>\n",
       "      <td id=\"T_d171e_row2_col23\" class=\"data row2 col23\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d171e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d171e_row3_col0\" class=\"data row3 col0\" >DPP (MpNet Emb)</td>\n",
       "      <td id=\"T_d171e_row3_col1\" class=\"data row3 col1\" >90000</td>\n",
       "      <td id=\"T_d171e_row3_col2\" class=\"data row3 col2\" >30000</td>\n",
       "      <td id=\"T_d171e_row3_col3\" class=\"data row3 col3\" >23.1</td>\n",
       "      <td id=\"T_d171e_row3_col4\" class=\"data row3 col4\" >20.4</td>\n",
       "      <td id=\"T_d171e_row3_col5\" class=\"data row3 col5\" >91.8</td>\n",
       "      <td id=\"T_d171e_row3_col6\" class=\"data row3 col6\" >25.7</td>\n",
       "      <td id=\"T_d171e_row3_col7\" class=\"data row3 col7\" >27.5</td>\n",
       "      <td id=\"T_d171e_row3_col8\" class=\"data row3 col8\" >41.6</td>\n",
       "      <td id=\"T_d171e_row3_col9\" class=\"data row3 col9\" >42.3</td>\n",
       "      <td id=\"T_d171e_row3_col10\" class=\"data row3 col10\" >4.6</td>\n",
       "      <td id=\"T_d171e_row3_col11\" class=\"data row3 col11\" >6.0</td>\n",
       "      <td id=\"T_d171e_row3_col12\" class=\"data row3 col12\" >31.9</td>\n",
       "      <td id=\"T_d171e_row3_col13\" class=\"data row3 col13\" >33.1</td>\n",
       "      <td id=\"T_d171e_row3_col14\" class=\"data row3 col14\" >6.7</td>\n",
       "      <td id=\"T_d171e_row3_col15\" class=\"data row3 col15\" >31.7</td>\n",
       "      <td id=\"T_d171e_row3_col16\" class=\"data row3 col16\" >10.4</td>\n",
       "      <td id=\"T_d171e_row3_col17\" class=\"data row3 col17\" >41.9</td>\n",
       "      <td id=\"T_d171e_row3_col18\" class=\"data row3 col18\" >5.3</td>\n",
       "      <td id=\"T_d171e_row3_col19\" class=\"data row3 col19\" >32.5</td>\n",
       "      <td id=\"T_d171e_row3_col20\" class=\"data row3 col20\" >19.2</td>\n",
       "      <td id=\"T_d171e_row3_col21\" class=\"data row3 col21\" >10.4</td>\n",
       "      <td id=\"T_d171e_row3_col22\" class=\"data row3 col22\" >23.5</td>\n",
       "      <td id=\"T_d171e_row3_col23\" class=\"data row3 col23\" >-53.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d171e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d171e_row4_col0\" class=\"data row4 col0\" >DPP (Llama Emb Not Norm.)</td>\n",
       "      <td id=\"T_d171e_row4_col1\" class=\"data row4 col1\" >90000</td>\n",
       "      <td id=\"T_d171e_row4_col2\" class=\"data row4 col2\" >30000</td>\n",
       "      <td id=\"T_d171e_row4_col3\" class=\"data row4 col3\" >22.5</td>\n",
       "      <td id=\"T_d171e_row4_col4\" class=\"data row4 col4\" >20.5</td>\n",
       "      <td id=\"T_d171e_row4_col5\" class=\"data row4 col5\" >91.0</td>\n",
       "      <td id=\"T_d171e_row4_col6\" class=\"data row4 col6\" >26.0</td>\n",
       "      <td id=\"T_d171e_row4_col7\" class=\"data row4 col7\" >27.4</td>\n",
       "      <td id=\"T_d171e_row4_col8\" class=\"data row4 col8\" >38.1</td>\n",
       "      <td id=\"T_d171e_row4_col9\" class=\"data row4 col9\" >39.1</td>\n",
       "      <td id=\"T_d171e_row4_col10\" class=\"data row4 col10\" >4.8</td>\n",
       "      <td id=\"T_d171e_row4_col11\" class=\"data row4 col11\" >5.6</td>\n",
       "      <td id=\"T_d171e_row4_col12\" class=\"data row4 col12\" >34.5</td>\n",
       "      <td id=\"T_d171e_row4_col13\" class=\"data row4 col13\" >33.2</td>\n",
       "      <td id=\"T_d171e_row4_col14\" class=\"data row4 col14\" >6.6</td>\n",
       "      <td id=\"T_d171e_row4_col15\" class=\"data row4 col15\" >32.3</td>\n",
       "      <td id=\"T_d171e_row4_col16\" class=\"data row4 col16\" >7.9</td>\n",
       "      <td id=\"T_d171e_row4_col17\" class=\"data row4 col17\" >38.6</td>\n",
       "      <td id=\"T_d171e_row4_col18\" class=\"data row4 col18\" >5.2</td>\n",
       "      <td id=\"T_d171e_row4_col19\" class=\"data row4 col19\" >33.9</td>\n",
       "      <td id=\"T_d171e_row4_col20\" class=\"data row4 col20\" >19.5</td>\n",
       "      <td id=\"T_d171e_row4_col21\" class=\"data row4 col21\" >7.9</td>\n",
       "      <td id=\"T_d171e_row4_col22\" class=\"data row4 col22\" >23.0</td>\n",
       "      <td id=\"T_d171e_row4_col23\" class=\"data row4 col23\" >-55.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d171e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d171e_row5_col0\" class=\"data row5 col0\" >Random</td>\n",
       "      <td id=\"T_d171e_row5_col1\" class=\"data row5 col1\" >90000</td>\n",
       "      <td id=\"T_d171e_row5_col2\" class=\"data row5 col2\" >30000</td>\n",
       "      <td id=\"T_d171e_row5_col3\" class=\"data row5 col3\" >23.1</td>\n",
       "      <td id=\"T_d171e_row5_col4\" class=\"data row5 col4\" >20.6</td>\n",
       "      <td id=\"T_d171e_row5_col5\" class=\"data row5 col5\" >84.7</td>\n",
       "      <td id=\"T_d171e_row5_col6\" class=\"data row5 col6\" >25.5</td>\n",
       "      <td id=\"T_d171e_row5_col7\" class=\"data row5 col7\" >27.1</td>\n",
       "      <td id=\"T_d171e_row5_col8\" class=\"data row5 col8\" >40.0</td>\n",
       "      <td id=\"T_d171e_row5_col9\" class=\"data row5 col9\" >39.7</td>\n",
       "      <td id=\"T_d171e_row5_col10\" class=\"data row5 col10\" >4.6</td>\n",
       "      <td id=\"T_d171e_row5_col11\" class=\"data row5 col11\" >4.6</td>\n",
       "      <td id=\"T_d171e_row5_col12\" class=\"data row5 col12\" >32.3</td>\n",
       "      <td id=\"T_d171e_row5_col13\" class=\"data row5 col13\" >33.8</td>\n",
       "      <td id=\"T_d171e_row5_col14\" class=\"data row5 col14\" >7.2</td>\n",
       "      <td id=\"T_d171e_row5_col15\" class=\"data row5 col15\" >34.4</td>\n",
       "      <td id=\"T_d171e_row5_col16\" class=\"data row5 col16\" >11.6</td>\n",
       "      <td id=\"T_d171e_row5_col17\" class=\"data row5 col17\" >39.9</td>\n",
       "      <td id=\"T_d171e_row5_col18\" class=\"data row5 col18\" >4.6</td>\n",
       "      <td id=\"T_d171e_row5_col19\" class=\"data row5 col19\" >33.1</td>\n",
       "      <td id=\"T_d171e_row5_col20\" class=\"data row5 col20\" >20.8</td>\n",
       "      <td id=\"T_d171e_row5_col21\" class=\"data row5 col21\" >11.6</td>\n",
       "      <td id=\"T_d171e_row5_col22\" class=\"data row5 col22\" >23.5</td>\n",
       "      <td id=\"T_d171e_row5_col23\" class=\"data row5 col23\" >-54.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca2cef7cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a8bb1 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_a8bb1_row0_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a8bb1_row0_col1, #T_a8bb1_row0_col2, #T_a8bb1_row0_col3, #T_a8bb1_row0_col8, #T_a8bb1_row0_col9, #T_a8bb1_row0_col10, #T_a8bb1_row0_col11, #T_a8bb1_row0_col12, #T_a8bb1_row0_col13, #T_a8bb1_row0_col14, #T_a8bb1_row0_col15, #T_a8bb1_row0_col16, #T_a8bb1_row0_col17, #T_a8bb1_row0_col18, #T_a8bb1_row0_col19, #T_a8bb1_row0_col20, #T_a8bb1_row0_col21, #T_a8bb1_row0_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a8bb1_row0_col4, #T_a8bb1_row0_col5, #T_a8bb1_row0_col6, #T_a8bb1_row0_col7, #T_a8bb1_row0_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a8bb1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a8bb1_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_a8bb1_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_a8bb1_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_a8bb1_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_a8bb1_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_a8bb1_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_a8bb1_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_a8bb1_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_a8bb1_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_a8bb1_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_a8bb1_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_a8bb1_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_a8bb1_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_a8bb1_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_a8bb1_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_a8bb1_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_a8bb1_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_a8bb1_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_a8bb1_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_a8bb1_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_a8bb1_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_a8bb1_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_a8bb1_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_a8bb1_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a8bb1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a8bb1_row0_col0\" class=\"data row0 col0\" >llama-7b_stanford_alpaca50k_ep=2</td>\n",
       "      <td id=\"T_a8bb1_row0_col1\" class=\"data row0 col1\" >100000</td>\n",
       "      <td id=\"T_a8bb1_row0_col2\" class=\"data row0 col2\" >50000</td>\n",
       "      <td id=\"T_a8bb1_row0_col3\" class=\"data row0 col3\" >23.5</td>\n",
       "      <td id=\"T_a8bb1_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_a8bb1_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_a8bb1_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "      <td id=\"T_a8bb1_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
       "      <td id=\"T_a8bb1_row0_col8\" class=\"data row0 col8\" >42.2</td>\n",
       "      <td id=\"T_a8bb1_row0_col9\" class=\"data row0 col9\" >42.6</td>\n",
       "      <td id=\"T_a8bb1_row0_col10\" class=\"data row0 col10\" >4.0</td>\n",
       "      <td id=\"T_a8bb1_row0_col11\" class=\"data row0 col11\" >5.0</td>\n",
       "      <td id=\"T_a8bb1_row0_col12\" class=\"data row0 col12\" >33.6</td>\n",
       "      <td id=\"T_a8bb1_row0_col13\" class=\"data row0 col13\" >31.7</td>\n",
       "      <td id=\"T_a8bb1_row0_col14\" class=\"data row0 col14\" >7.2</td>\n",
       "      <td id=\"T_a8bb1_row0_col15\" class=\"data row0 col15\" >33.9</td>\n",
       "      <td id=\"T_a8bb1_row0_col16\" class=\"data row0 col16\" >10.8</td>\n",
       "      <td id=\"T_a8bb1_row0_col17\" class=\"data row0 col17\" >42.4</td>\n",
       "      <td id=\"T_a8bb1_row0_col18\" class=\"data row0 col18\" >4.5</td>\n",
       "      <td id=\"T_a8bb1_row0_col19\" class=\"data row0 col19\" >32.6</td>\n",
       "      <td id=\"T_a8bb1_row0_col20\" class=\"data row0 col20\" >20.6</td>\n",
       "      <td id=\"T_a8bb1_row0_col21\" class=\"data row0 col21\" >10.8</td>\n",
       "      <td id=\"T_a8bb1_row0_col22\" class=\"data row0 col22\" >23.5</td>\n",
       "      <td id=\"T_a8bb1_row0_col23\" class=\"data row0 col23\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca2d61d9c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=llama, dataset=stanford_alpaca50k, N=150000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3204105/4142344388.py:324: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3819: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3820: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_02593 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_02593_row0_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_02593_row0_col1, #T_02593_row0_col2, #T_02593_row0_col3, #T_02593_row0_col4, #T_02593_row0_col8, #T_02593_row0_col9, #T_02593_row0_col10, #T_02593_row0_col11, #T_02593_row0_col12, #T_02593_row0_col13, #T_02593_row0_col14, #T_02593_row0_col15, #T_02593_row0_col16, #T_02593_row0_col17, #T_02593_row0_col18, #T_02593_row0_col19, #T_02593_row0_col20, #T_02593_row0_col21, #T_02593_row0_col22 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_02593_row0_col5, #T_02593_row0_col6, #T_02593_row0_col7, #T_02593_row0_col23 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_02593\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_02593_level0_col0\" class=\"col_heading level0 col0\" >sort_by_name</th>\n",
       "      <th id=\"T_02593_level0_col1\" class=\"col_heading level0 col1\" >compute</th>\n",
       "      <th id=\"T_02593_level0_col2\" class=\"col_heading level0 col2\" >subset_size</th>\n",
       "      <th id=\"T_02593_level0_col3\" class=\"col_heading level0 col3\" >academic_benchmark_avg</th>\n",
       "      <th id=\"T_02593_level0_col4\" class=\"col_heading level0 col4\" >AlpacaFarm(alpaca:eval:gpt4)/WR*</th>\n",
       "      <th id=\"T_02593_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th id=\"T_02593_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th id=\"T_02593_level0_col7\" class=\"col_heading level0 col7\" >AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "      <th id=\"T_02593_level0_col8\" class=\"col_heading level0 col8\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_02593_level0_col9\" class=\"col_heading level0 col9\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_02593_level0_col10\" class=\"col_heading level0 col10\" >GSM/Direct</th>\n",
       "      <th id=\"T_02593_level0_col11\" class=\"col_heading level0 col11\" >GSM/CoT</th>\n",
       "      <th id=\"T_02593_level0_col12\" class=\"col_heading level0 col12\" >BBH/Direct</th>\n",
       "      <th id=\"T_02593_level0_col13\" class=\"col_heading level0 col13\" >BBH/CoT</th>\n",
       "      <th id=\"T_02593_level0_col14\" class=\"col_heading level0 col14\" >TydiQA/CB</th>\n",
       "      <th id=\"T_02593_level0_col15\" class=\"col_heading level0 col15\" >TydiQA/GP</th>\n",
       "      <th id=\"T_02593_level0_col16\" class=\"col_heading level0 col16\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_02593_level0_col17\" class=\"col_heading level0 col17\" >MMLU</th>\n",
       "      <th id=\"T_02593_level0_col18\" class=\"col_heading level0 col18\" >GSM</th>\n",
       "      <th id=\"T_02593_level0_col19\" class=\"col_heading level0 col19\" >BBH</th>\n",
       "      <th id=\"T_02593_level0_col20\" class=\"col_heading level0 col20\" >TydiQA</th>\n",
       "      <th id=\"T_02593_level0_col21\" class=\"col_heading level0 col21\" >Codex-Eval</th>\n",
       "      <th id=\"T_02593_level0_col22\" class=\"col_heading level0 col22\" >Average</th>\n",
       "      <th id=\"T_02593_level0_col23\" class=\"col_heading level0 col23\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_02593_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_02593_row0_col0\" class=\"data row0 col0\" >100\\% Data</td>\n",
       "      <td id=\"T_02593_row0_col1\" class=\"data row0 col1\" >150000</td>\n",
       "      <td id=\"T_02593_row0_col2\" class=\"data row0 col2\" >50000</td>\n",
       "      <td id=\"T_02593_row0_col3\" class=\"data row0 col3\" >23.0</td>\n",
       "      <td id=\"T_02593_row0_col4\" class=\"data row0 col4\" >21.3</td>\n",
       "      <td id=\"T_02593_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_02593_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "      <td id=\"T_02593_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
       "      <td id=\"T_02593_row0_col8\" class=\"data row0 col8\" >41.8</td>\n",
       "      <td id=\"T_02593_row0_col9\" class=\"data row0 col9\" >41.6</td>\n",
       "      <td id=\"T_02593_row0_col10\" class=\"data row0 col10\" >4.0</td>\n",
       "      <td id=\"T_02593_row0_col11\" class=\"data row0 col11\" >5.0</td>\n",
       "      <td id=\"T_02593_row0_col12\" class=\"data row0 col12\" >33.1</td>\n",
       "      <td id=\"T_02593_row0_col13\" class=\"data row0 col13\" >31.4</td>\n",
       "      <td id=\"T_02593_row0_col14\" class=\"data row0 col14\" >6.5</td>\n",
       "      <td id=\"T_02593_row0_col15\" class=\"data row0 col15\" >33.8</td>\n",
       "      <td id=\"T_02593_row0_col16\" class=\"data row0 col16\" >9.8</td>\n",
       "      <td id=\"T_02593_row0_col17\" class=\"data row0 col17\" >41.7</td>\n",
       "      <td id=\"T_02593_row0_col18\" class=\"data row0 col18\" >4.5</td>\n",
       "      <td id=\"T_02593_row0_col19\" class=\"data row0 col19\" >32.2</td>\n",
       "      <td id=\"T_02593_row0_col20\" class=\"data row0 col20\" >20.2</td>\n",
       "      <td id=\"T_02593_row0_col21\" class=\"data row0 col21\" >9.8</td>\n",
       "      <td id=\"T_02593_row0_col22\" class=\"data row0 col22\" >22.8</td>\n",
       "      <td id=\"T_02593_row0_col23\" class=\"data row0 col23\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca27fe81c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "from llm.evaluate import get_eval_results_with_useful_cols\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# sort_rows = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b_12.13update_before', '../results/baselines/huggyllama/llama-7b_12.13update_before/'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "# #     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "# #     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "# #     # llama2\n",
    "#     ('llama2-7b_12.13update_before', '../results/baselines/NousResearch/Llama-2-7b-hf_12.13update_before/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b-chat', '../results/baselines/NousResearch/Llama-2-7b-chat-hf/'),\n",
    "# #     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "# #     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "# #     # mistral\n",
    "# #     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct-v0.1_12.13update_before', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1_12.13update_before'),\n",
    "#     ('mistral-7b-Instruct-v0.1', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     # zephyr\n",
    "#     ('zephyr-7b-beta_12.13update_before', '../results/baselines/HuggingFaceH4/zephyr-7b-beta_12.13update_before'),\n",
    "#     ('zephyr-7b-beta', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "\n",
    "# # baselines\n",
    "save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b+lima_ep=2', '../results/ft1_ep=2/llama-7b_lima/'),\n",
    "# #     ('mistral-7b+lima_ep=2', '../results/ft1_ep=2/mistral-7b_lima/'), \n",
    "# ]\n",
    "# exp_dir = '../results/oi2/'\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# # save_dirs = [\n",
    "# #     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "# #     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# # ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'tuluv1m' in x]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# # exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# # exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# ###### ultrachat\n",
    "# save_dirs = [\n",
    "#     # baselines \n",
    "#     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "#     ('mistral-7b_ultrachat200k_aftersplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k'),\n",
    "#     ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "    \n",
    "#     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "#     ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "#     ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-alpha'),\n",
    "#     ('mistral-7b-sft-beta+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "# # exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_ultrachat200k:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# exp_dir = '../results/oi5_ultrachat15:mistral-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "# dataset = 'stanford_alpaca'\n",
    "# dataset = 'open_orca_slim'\n",
    "# dataset = 'sharegptv2'\n",
    "# dataset = 'ultrachat200kv2'\n",
    "# dataset = 'wizardlm'\n",
    "# dataset = 'wizardlmv2'\n",
    "# dataset = 'tulu_v2'\n",
    "# dataset = 'flan_v2'\n",
    "# dataset = 'oasst1'\n",
    "# dataset = 'dolly'\n",
    "# dataset_list = [\n",
    "#     'dolly',\n",
    "#     'oasst1', \n",
    "#     'flan_v2', \n",
    "#     'stanford_alpaca', \n",
    "#     'wizardlmv2', \n",
    "#     'sharegptv2', \n",
    "#     'ultrachat200kv2',\n",
    "# ]; finetune_type = 'sft'\n",
    "# dataset_list = [\n",
    "#     'ultrafeedback',\n",
    "# ]; finetune_type = 'pref'\n",
    "# dataset_list = [\n",
    "#     # add to get entire list of ep=3 full finetunes\n",
    "#     'flan_v250k',\n",
    "#     'oasst2',\n",
    "#     'wizardlm50k', \n",
    "#     'lima',\n",
    "#     'gpt4_alpaca',\n",
    "#     #\n",
    "#     'dolly',\n",
    "#     'stanford_alpaca50k', \n",
    "#     'sharegpt50k',\n",
    "#     'ultrachat50k',\n",
    "# ]; finetune_type = 'sft'\n",
    "dataset_list = ['stanford_alpaca50k']; finetune_type = 'sft'\n",
    "# dataset_list = ['sharegpt50k']; finetune_type = 'sft'\n",
    "# dataset_list = ['ultrachat50k']; finetune_type = 'sft'\n",
    "# dataset_list = ['stanford_alpaca50k', 'ultrachat50k']; finetune_type = 'sft'\n",
    "\n",
    "\n",
    "# dataset_list = ['mix_all50k']; finetune_type = 'sft'\n",
    "\n",
    "model_name_display = ['llama']\n",
    "# model_name_display = ['mistral']\n",
    "\n",
    "\n",
    "## older\n",
    "# dataset = 'tulu_v1_mix'\n",
    "save_dirs += [\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b'),\n",
    "#     ('llama-7b_lima_ep=5', '../results/oi2/llama-7b_lima_ep=5/'),\n",
    "#     ('llama-7b_lima_ep=10', '../results/oi2/llama-7b_lima_ep=10/'),\n",
    "]\n",
    "for dataset in dataset_list:\n",
    "    if dataset == 'tulu_v2':\n",
    "        save_dirs += [('llama-7b_tulu_v2:100k_ep=2', '../results/oi2/llama-7b_tulu_v2:100k_ep=2'),]\n",
    "    elif dataset == 'open_orca_slim':\n",
    "        save_dirs += [('llama-7b_openorcaslim:100k_ep=2', '../results/oi2/llama-7b_openorcaslim:100k_ep=2'),]\n",
    "    elif dataset == 'sharegptv2':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_sharegptv2_ep=2', '../results/oi2/llama-7b_sharegptv2_ep=2'),\n",
    "            ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),]\n",
    "    elif dataset == 'tulu_v1_mix':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "            # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "            # ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "        ]\n",
    "    elif dataset == 'ultrafeedback':\n",
    "        exp_dir = f'../results/dpo1/'\n",
    "        if os.path.isdir(exp_dir):\n",
    "            save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "    else:\n",
    "        save_dirs += [(f'llama-7b_{dataset}_ep=2', f'../results/oi2/llama-7b_{dataset}_ep=2'),]\n",
    "        save_dirs += [(f'llama-7b_{dataset}_ep=3', f'../results/oi2/llama-7b_{dataset}_ep=3'),]\n",
    "        save_dirs += [(f'llama-7b_{dataset}_ep=10', f'../results/oi2/llama-7b_{dataset}_ep=10'),]\n",
    "    \n",
    "    if finetune_type == 'sft':\n",
    "        exp_dirs = [\n",
    "            f'../results/oi5_{dataset}:llama-7b',\n",
    "            f'../results/oi6_{dataset}:llama-7b',\n",
    "            f'../results/oi6_{dataset}:mistral-7b', # add mistral\n",
    "                   ]\n",
    "    elif finetune_type == 'pref':\n",
    "        exp_dirs = [f'../results/dpo2_{dataset}:llama-7b+sharegptv2ep2/']\n",
    "    for exp_dir in exp_dirs:\n",
    "        if os.path.isdir(exp_dir):\n",
    "            save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'dppmapbd' not in x and 'semdedup' not in x]\n",
    "    \n",
    "    if finetune_type == 'pref':\n",
    "        sft_model_dataset = 'sharegptv2'\n",
    "        save_dirs += [('llama-7b_sharegptv2_ep=2', f'../results/oi2/llama-7b_{sft_model_dataset}_ep=2')]\n",
    "# ## just compare dppmap grad vs. text\n",
    "# save_dirs = [x for x in save_dirs if 'prune:size=10000:ep=10' in x[1] and (\n",
    "#         'random' in x[1] or \n",
    "#         'dppmap' in x[1]\n",
    "#     )\n",
    "# ]\n",
    "save_dirs = [x for x in save_dirs if ('size=80000:ep=2' not in x[1]) and ('size=80000:ep=3' not in x[1])]\n",
    "save_dirs = [x for x in save_dirs if os.path.isdir(x[1])]\n",
    "#####\n",
    "# save_dirs = [x for x in save_dirs if ('size=20000:ep=4' in x[1])]\n",
    "\n",
    "\n",
    "###### \n",
    "\n",
    "from llm.evaluate import detect_oom_evals\n",
    "oom_eval_paths = detect_oom_evals([x for l in [glob.glob(os.path.join(x[1], 'eval/*/*.out')) for x in save_dirs] for x in l])\n",
    "if oom_eval_paths: print(oom_eval_paths)\n",
    "    \n",
    "\n",
    "# chat_fmt = False\n",
    "chat_fmt = True\n",
    "chat_fmt = 'both'\n",
    "# chat_fmt = 'auto' # base model no chatfmt, tuned model with chatfmt\n",
    "chat_fmt = 'mix'  # non-alpacaeval no chatfmt, alpacaeval chatfmt\n",
    "\n",
    "alpacafarm_judge = 'chatgpt'\n",
    "alpacafarm_judge = 'alpaca:eval:gpt4'\n",
    "# alpacafarm_judge = 'weighted:alpaca:eval:gpt4:turbo'\n",
    "# alpacafarm_judge = 'alpaca:eval:gpt4:turbo:fn'\n",
    "mtbench_judge = 'gpt:4:1106:preview'\n",
    "mtbench_judge = 'gpt:4'\n",
    "cols = []\n",
    "# cols += [f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/Len*', f'AlpacaFarm({alpacafarm_judge})/LenMed', f'AlpacaFarm({alpacafarm_judge})/Len', f'AlpacaFarm({alpacafarm_judge})/Rep2']\n",
    "# cols += [f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/WR**', f'AlpacaFarm({alpacafarm_judge})/LenMed', f'AlpacaFarm({alpacafarm_judge})/Rep2']\n",
    "# cols += [f'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR*', f'AlpacaFarm(alpaca:eval:gpt4)/WR*', f'AlpacaFarm(alpaca:eval:gpt4)/WR']\n",
    "# cols += [f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/WR**', f'AlpacaFarm({alpacafarm_judge})/LenMed', f'AlpacaFarm({alpacafarm_judge})/Len', f'AlpacaFarm({alpacafarm_judge})/Len*',]\n",
    "# cols += [f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/WR**', f'AlpacaFarm({alpacafarm_judge})/WR***', f'AlpacaFarm({alpacafarm_judge})/Len*',]\n",
    "cols += [f'AlpacaFarm({alpacafarm_judge})/WR*', ] # f'AlpacaFarm({alpacafarm_judge})/Len*'\n",
    "alpacafarm_judge = 'weighted:alpaca:eval:gpt4:turbo'\n",
    "cols += [f'AlpacaFarm({alpacafarm_judge})/Len*', f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/LCWR',]\n",
    "# cols += [f'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/LenMed',  f'AlpacaFarm(alpaca:eval:gpt4)/Len']\n",
    "# cols += [f'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/LenMed', f'AlpacaFarm(alpaca:eval:gpt4)/LenMed']\n",
    "# cols += [f'AlpacaFarm(alpaca:eval:gpt4)/WR*', f'AlpacaFarm(alpaca:eval:gpt4)/WR**', f'AlpacaFarm(alpaca:eval:gpt4)/LenMed']\n",
    "# cols += [f'MTBench({mtbench_judge})/Turn-1',  f'MTBench({mtbench_judge})/Turn-2', f'MTBench({mtbench_judge})/Rating']\n",
    "# cols += [f'MTBench({mtbench_judge})/Turn-1']\n",
    "# cols += [f'MTBench(gpt:4:1106:preview)/Rating', f'MTBench(gpt:4)/Rating']\n",
    "cols += ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] \n",
    "\n",
    "\n",
    "print(f'chat_fmt={chat_fmt}')\n",
    "df = get_eval_results_with_useful_cols(save_dirs, chat_fmt=chat_fmt, cols=cols)\n",
    "\n",
    "for model_name_contain in model_name_display:\n",
    "    dfc = df.copy()\n",
    "    dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "        lambda x: model_name_contain in x.lower())]\n",
    "    if not len(dfc): continue\n",
    "    from rosemary import pd_average_col_contains_substr\n",
    "    Ns = sorted(np.unique([int(x) for x in list(df['compute']) if not np.isnan(x)]).tolist())\n",
    "    datasets = sorted(np.unique([x for x in df['dataset'] if x is not None]).tolist())\n",
    "    for dataset in datasets:\n",
    "        for N in Ns+[None]:\n",
    "            dfc = df.copy()\n",
    "            dfc = dfc[dfc['model_name_or_path'].apply(lambda x: model_name_contain in x.lower())]\n",
    "            dfc = dfc[dfc['compute'].apply(lambda x: x == N if (not np.isnan(x) or x is not None) else True)]\n",
    "            dfc = dfc[dfc['dataset'].apply(lambda x: x == dataset if x else True)]\n",
    "            if not len(dfc): continue\n",
    "            print(f'model={model_name_contain}, dataset={dataset}, N={N}')\n",
    "            col_runname = 'run_name' if chat_fmt != 'both' else ('run_name', '')\n",
    "            substitute = True\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, '_random_', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=10000:ep=10', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=50000:ep=5', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=3', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=1', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=100000', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=200000', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=400000', substitute=substitute)\n",
    "            #     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "            col = ('Average', 'chatfmt') if chat_fmt == 'both' else 'Average'\n",
    "            col = ('AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR', 'chatfmt') if chat_fmt == 'both' else 'AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR'\n",
    "        #     col = 'AlpacaFarm/WR'\n",
    "        #     col = 'MMLU/0-shot'|\n",
    "        #     col = 'GSM/CoT'\n",
    "        #     col = 'BBH/Direct'\n",
    "        #     col = 'TydiQA/GP'\n",
    "            dfc = dfc.sort_values(by=[col], ascending=False)\n",
    "            cols_drop = ['model_name_or_path', 'dataset', 'run_name', 'prune_method_type', 'sort_by']\n",
    "#             cols_drop = ['dataset', 'run_name', 'prune_method_type', 'sort_by']\n",
    "            dfc = dfc.drop(columns=cols_drop, \n",
    "                           axis=1, level=0 if chat_fmt=='both' else None)\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            display(dfc\n",
    "                    .style\n",
    "                    .applymap(lambda x: f'max-width: 60ch;', subset=['sort_by_name'])\n",
    "                    .set_table_styles([{'selector': 'td', 'props': [('white-space', 'pre-wrap'), ('word-wrap', 'break-word')]}])\n",
    "                    .set_properties(**{'text-align': 'left'})\n",
    "                    .background_gradient(cmap ='coolwarm')\n",
    "#                     .applymap(lambda x: 'text-decoration: underline;' \\\n",
    "#                               if x in dfc[list(set(dfc.columns) & set([(x, '') for x in cols]))+[col] if chat_fmt=='both' else cols+[col]].values.flatten() and chat_fmt=='both' else '')\n",
    "                    .format(precision=1))\n",
    "\n",
    "# llama-7b_tulu_v1_mix(paper)\n",
    "# MMLU/0-shot, MMLU/5-shot, GSM/Direct, GSM/CoT, BBH/Direct, BBH/CoT, TydiQA/GP, TydiQA/CB, CodexEval/Pass@1, AlpacaEval(vs.Davinci-003)\n",
    "# 44.8       , 47.1       , 7.0       , 25.0   , 38.5      , 38.5   , 43.5,    , 8.0      , 18.6,           , 48.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7bc49cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sharegpt50k': 38.03065846890563, 'wizardlm50k': 35.382812494761055, 'ultrachat50k': 40.257504805638774, 'gpt4_alpaca50k': 37.57532385492707, 'oasst2': 30.51178875149117, 'stanford_alpaca50k': 28.319525447868127, 'dolly': 15.758281735905047, 'flan_v250k': 6.376308577321406, 'self_instruct50k': 9.295872798702451}\n"
     ]
    }
   ],
   "source": [
    "dfc = df[['dataset', 'AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*', 'AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR']]\n",
    "d = dict(zip(dfc['dataset'], dfc['AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR']))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c1d4",
   "metadata": {},
   "source": [
    "# Compare pruning methods on 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e094e838",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stanford_alpaca50k'] mistral-7b\n",
      "not sure what prune method to assign for: llama-13b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f5034 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_f5034_row0_col0, #T_f5034_row1_col0, #T_f5034_row2_col0, #T_f5034_row3_col0, #T_f5034_row4_col0, #T_f5034_row5_col0, #T_f5034_row6_col0, #T_f5034_row7_col0, #T_f5034_row8_col0, #T_f5034_row9_col0, #T_f5034_row10_col0, #T_f5034_row11_col0, #T_f5034_row12_col0, #T_f5034_row13_col0, #T_f5034_row14_col0, #T_f5034_row15_col0, #T_f5034_row16_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f5034_row0_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row0_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row0_col4, #T_f5034_row2_col1, #T_f5034_row3_col3, #T_f5034_row8_col5, #T_f5034_row11_col5, #T_f5034_row15_col2, #T_f5034_row15_col3, #T_f5034_row15_col8, #T_f5034_row16_col5, #T_f5034_row16_col6, #T_f5034_row16_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row0_col5, #T_f5034_row3_col5, #T_f5034_row4_col7, #T_f5034_row7_col5, #T_f5034_row9_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #8caffe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row0_col7, #T_f5034_row3_col1, #T_f5034_row3_col2, #T_f5034_row3_col6, #T_f5034_row5_col5, #T_f5034_row5_col8, #T_f5034_row10_col3, #T_f5034_row14_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row0_col8, #T_f5034_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row1_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row1_col2, #T_f5034_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row1_col4, #T_f5034_row9_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row1_col5, #T_f5034_row6_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row2_col2, #T_f5034_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row2_col4, #T_f5034_row9_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row2_col5, #T_f5034_row13_col5, #T_f5034_row14_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row2_col7, #T_f5034_row3_col7, #T_f5034_row13_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row4_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cd423b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row4_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #df634e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row4_col5, #T_f5034_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row5_col1, #T_f5034_row11_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row5_col3, #T_f5034_row13_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row5_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row6_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row7_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e0dbd8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row7_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row7_col7, #T_f5034_row11_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #b3cdfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row7_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row8_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row8_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row9_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row9_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row9_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row10_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row10_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row10_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row10_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row11_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row11_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row11_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row11_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row11_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row12_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row12_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row12_col3, #T_f5034_row16_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ed8366;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row12_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row12_col5, #T_f5034_row15_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row12_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row12_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row13_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row13_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row13_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row13_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row14_col1, #T_f5034_row15_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row14_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row14_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row14_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row14_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row14_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row15_col1, #T_f5034_row16_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row15_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f5034_row15_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row16_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row16_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f5034_row16_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f5034\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5034_level0_col0\" class=\"col_heading level0 col0\" >Methods</th>\n",
       "      <th id=\"T_f5034_level0_col1\" class=\"col_heading level0 col1\" >MMLU</th>\n",
       "      <th id=\"T_f5034_level0_col2\" class=\"col_heading level0 col2\" >GSM</th>\n",
       "      <th id=\"T_f5034_level0_col3\" class=\"col_heading level0 col3\" >BBH</th>\n",
       "      <th id=\"T_f5034_level0_col4\" class=\"col_heading level0 col4\" >TydiQA</th>\n",
       "      <th id=\"T_f5034_level0_col5\" class=\"col_heading level0 col5\" >CodexEval</th>\n",
       "      <th id=\"T_f5034_level0_col6\" class=\"col_heading level0 col6\" >Avg</th>\n",
       "      <th id=\"T_f5034_level0_col7\" class=\"col_heading level0 col7\" >AlpacaEval/Length Controlled Win Rate</th>\n",
       "      <th id=\"T_f5034_level0_col8\" class=\"col_heading level0 col8\" >AlpacaEval/Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row0\" class=\"row_heading level0 row0\" >10</th>\n",
       "      <td id=\"T_f5034_row0_col0\" class=\"data row0 col0\" >Alpagasus Rating ($\\uparrow$)</td>\n",
       "      <td id=\"T_f5034_row0_col1\" class=\"data row0 col1\" >59.8</td>\n",
       "      <td id=\"T_f5034_row0_col2\" class=\"data row0 col2\" >21.7</td>\n",
       "      <td id=\"T_f5034_row0_col3\" class=\"data row0 col3\" >49.3</td>\n",
       "      <td id=\"T_f5034_row0_col4\" class=\"data row0 col4\" >35.9</td>\n",
       "      <td id=\"T_f5034_row0_col5\" class=\"data row0 col5\" >29.3</td>\n",
       "      <td id=\"T_f5034_row0_col6\" class=\"data row0 col6\" >40.3</td>\n",
       "      <td id=\"T_f5034_row0_col7\" class=\"data row0 col7\" >36.9</td>\n",
       "      <td id=\"T_f5034_row0_col8\" class=\"data row0 col8\" >86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row1\" class=\"row_heading level0 row1\" >15</th>\n",
       "      <td id=\"T_f5034_row1_col0\" class=\"data row1 col0\" >DPP (Llama Emb Not Norm.)</td>\n",
       "      <td id=\"T_f5034_row1_col1\" class=\"data row1 col1\" >58.8</td>\n",
       "      <td id=\"T_f5034_row1_col2\" class=\"data row1 col2\" >21.1</td>\n",
       "      <td id=\"T_f5034_row1_col3\" class=\"data row1 col3\" >49.5</td>\n",
       "      <td id=\"T_f5034_row1_col4\" class=\"data row1 col4\" >29.6</td>\n",
       "      <td id=\"T_f5034_row1_col5\" class=\"data row1 col5\" >31.1</td>\n",
       "      <td id=\"T_f5034_row1_col6\" class=\"data row1 col6\" >38.8</td>\n",
       "      <td id=\"T_f5034_row1_col7\" class=\"data row1 col7\" >38.8</td>\n",
       "      <td id=\"T_f5034_row1_col8\" class=\"data row1 col8\" >88.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row2\" class=\"row_heading level0 row2\" >11</th>\n",
       "      <td id=\"T_f5034_row2_col0\" class=\"data row2 col0\" >Random</td>\n",
       "      <td id=\"T_f5034_row2_col1\" class=\"data row2 col1\" >61.1</td>\n",
       "      <td id=\"T_f5034_row2_col2\" class=\"data row2 col2\" >21.4</td>\n",
       "      <td id=\"T_f5034_row2_col3\" class=\"data row2 col3\" >49.4</td>\n",
       "      <td id=\"T_f5034_row2_col4\" class=\"data row2 col4\" >31.2</td>\n",
       "      <td id=\"T_f5034_row2_col5\" class=\"data row2 col5\" >30.5</td>\n",
       "      <td id=\"T_f5034_row2_col6\" class=\"data row2 col6\" >39.6</td>\n",
       "      <td id=\"T_f5034_row2_col7\" class=\"data row2 col7\" >39.1</td>\n",
       "      <td id=\"T_f5034_row2_col8\" class=\"data row2 col8\" >91.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row3\" class=\"row_heading level0 row3\" >17</th>\n",
       "      <td id=\"T_f5034_row3_col0\" class=\"data row3 col0\" >100\\% Data</td>\n",
       "      <td id=\"T_f5034_row3_col1\" class=\"data row3 col1\" >46.8</td>\n",
       "      <td id=\"T_f5034_row3_col2\" class=\"data row3 col2\" >18.1</td>\n",
       "      <td id=\"T_f5034_row3_col3\" class=\"data row3 col3\" >50.1</td>\n",
       "      <td id=\"T_f5034_row3_col4\" class=\"data row3 col4\" >31.5</td>\n",
       "      <td id=\"T_f5034_row3_col5\" class=\"data row3 col5\" >29.3</td>\n",
       "      <td id=\"T_f5034_row3_col6\" class=\"data row3 col6\" >35.8</td>\n",
       "      <td id=\"T_f5034_row3_col7\" class=\"data row3 col7\" >39.1</td>\n",
       "      <td id=\"T_f5034_row3_col8\" class=\"data row3 col8\" >86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row4\" class=\"row_heading level0 row4\" >13</th>\n",
       "      <td id=\"T_f5034_row4_col0\" class=\"data row4 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$) Not Norm.</td>\n",
       "      <td id=\"T_f5034_row4_col1\" class=\"data row4 col1\" >60.1</td>\n",
       "      <td id=\"T_f5034_row4_col2\" class=\"data row4 col2\" >19.6</td>\n",
       "      <td id=\"T_f5034_row4_col3\" class=\"data row4 col3\" >49.9</td>\n",
       "      <td id=\"T_f5034_row4_col4\" class=\"data row4 col4\" >30.0</td>\n",
       "      <td id=\"T_f5034_row4_col5\" class=\"data row4 col5\" >31.7</td>\n",
       "      <td id=\"T_f5034_row4_col6\" class=\"data row4 col6\" >39.0</td>\n",
       "      <td id=\"T_f5034_row4_col7\" class=\"data row4 col7\" >39.7</td>\n",
       "      <td id=\"T_f5034_row4_col8\" class=\"data row4 col8\" >88.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row5\" class=\"row_heading level0 row5\" >8</th>\n",
       "      <td id=\"T_f5034_row5_col0\" class=\"data row5 col0\" >Dedup(MpNet Emb)</td>\n",
       "      <td id=\"T_f5034_row5_col1\" class=\"data row5 col1\" >60.3</td>\n",
       "      <td id=\"T_f5034_row5_col2\" class=\"data row5 col2\" >20.4</td>\n",
       "      <td id=\"T_f5034_row5_col3\" class=\"data row5 col3\" >49.2</td>\n",
       "      <td id=\"T_f5034_row5_col4\" class=\"data row5 col4\" >31.8</td>\n",
       "      <td id=\"T_f5034_row5_col5\" class=\"data row5 col5\" >27.4</td>\n",
       "      <td id=\"T_f5034_row5_col6\" class=\"data row5 col6\" >39.0</td>\n",
       "      <td id=\"T_f5034_row5_col7\" class=\"data row5 col7\" >40.1</td>\n",
       "      <td id=\"T_f5034_row5_col8\" class=\"data row5 col8\" >84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row6\" class=\"row_heading level0 row6\" >5</th>\n",
       "      <td id=\"T_f5034_row6_col0\" class=\"data row6 col0\" >\\#Input Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_f5034_row6_col1\" class=\"data row6 col1\" >60.5</td>\n",
       "      <td id=\"T_f5034_row6_col2\" class=\"data row6 col2\" >21.1</td>\n",
       "      <td id=\"T_f5034_row6_col3\" class=\"data row6 col3\" >48.7</td>\n",
       "      <td id=\"T_f5034_row6_col4\" class=\"data row6 col4\" >34.2</td>\n",
       "      <td id=\"T_f5034_row6_col5\" class=\"data row6 col5\" >31.1</td>\n",
       "      <td id=\"T_f5034_row6_col6\" class=\"data row6 col6\" >40.0</td>\n",
       "      <td id=\"T_f5034_row6_col7\" class=\"data row6 col7\" >40.5</td>\n",
       "      <td id=\"T_f5034_row6_col8\" class=\"data row6 col8\" >87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row7\" class=\"row_heading level0 row7\" >16</th>\n",
       "      <td id=\"T_f5034_row7_col0\" class=\"data row7 col0\" >DPP (MpNet Emb)</td>\n",
       "      <td id=\"T_f5034_row7_col1\" class=\"data row7 col1\" >58.3</td>\n",
       "      <td id=\"T_f5034_row7_col2\" class=\"data row7 col2\" >20.8</td>\n",
       "      <td id=\"T_f5034_row7_col3\" class=\"data row7 col3\" >48.1</td>\n",
       "      <td id=\"T_f5034_row7_col4\" class=\"data row7 col4\" >29.1</td>\n",
       "      <td id=\"T_f5034_row7_col5\" class=\"data row7 col5\" >29.3</td>\n",
       "      <td id=\"T_f5034_row7_col6\" class=\"data row7 col6\" >38.0</td>\n",
       "      <td id=\"T_f5034_row7_col7\" class=\"data row7 col7\" >40.9</td>\n",
       "      <td id=\"T_f5034_row7_col8\" class=\"data row7 col8\" >89.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row8\" class=\"row_heading level0 row8\" >6</th>\n",
       "      <td id=\"T_f5034_row8_col0\" class=\"data row8 col0\" >Perplexity ($\\downarrow$)</td>\n",
       "      <td id=\"T_f5034_row8_col1\" class=\"data row8 col1\" >59.3</td>\n",
       "      <td id=\"T_f5034_row8_col2\" class=\"data row8 col2\" >21.4</td>\n",
       "      <td id=\"T_f5034_row8_col3\" class=\"data row8 col3\" >48.9</td>\n",
       "      <td id=\"T_f5034_row8_col4\" class=\"data row8 col4\" >27.5</td>\n",
       "      <td id=\"T_f5034_row8_col5\" class=\"data row8 col5\" >34.8</td>\n",
       "      <td id=\"T_f5034_row8_col6\" class=\"data row8 col6\" >38.8</td>\n",
       "      <td id=\"T_f5034_row8_col7\" class=\"data row8 col7\" >41.2</td>\n",
       "      <td id=\"T_f5034_row8_col8\" class=\"data row8 col8\" >98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row9\" class=\"row_heading level0 row9\" >12</th>\n",
       "      <td id=\"T_f5034_row9_col0\" class=\"data row9 col0\" >IFD ($\\uparrow$)</td>\n",
       "      <td id=\"T_f5034_row9_col1\" class=\"data row9 col1\" >56.3</td>\n",
       "      <td id=\"T_f5034_row9_col2\" class=\"data row9 col2\" >20.0</td>\n",
       "      <td id=\"T_f5034_row9_col3\" class=\"data row9 col3\" >48.5</td>\n",
       "      <td id=\"T_f5034_row9_col4\" class=\"data row9 col4\" >33.8</td>\n",
       "      <td id=\"T_f5034_row9_col5\" class=\"data row9 col5\" >29.3</td>\n",
       "      <td id=\"T_f5034_row9_col6\" class=\"data row9 col6\" >38.5</td>\n",
       "      <td id=\"T_f5034_row9_col7\" class=\"data row9 col7\" >41.3</td>\n",
       "      <td id=\"T_f5034_row9_col8\" class=\"data row9 col8\" >113.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row10\" class=\"row_heading level0 row10\" >14</th>\n",
       "      <td id=\"T_f5034_row10_col0\" class=\"data row10 col0\" >EL2N ($\\downarrow$)</td>\n",
       "      <td id=\"T_f5034_row10_col1\" class=\"data row10 col1\" >58.6</td>\n",
       "      <td id=\"T_f5034_row10_col2\" class=\"data row10 col2\" >21.6</td>\n",
       "      <td id=\"T_f5034_row10_col3\" class=\"data row10 col3\" >48.0</td>\n",
       "      <td id=\"T_f5034_row10_col4\" class=\"data row10 col4\" >29.4</td>\n",
       "      <td id=\"T_f5034_row10_col5\" class=\"data row10 col5\" >31.7</td>\n",
       "      <td id=\"T_f5034_row10_col6\" class=\"data row10 col6\" >38.5</td>\n",
       "      <td id=\"T_f5034_row10_col7\" class=\"data row10 col7\" >41.7</td>\n",
       "      <td id=\"T_f5034_row10_col8\" class=\"data row10 col8\" >96.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row11\" class=\"row_heading level0 row11\" >1</th>\n",
       "      <td id=\"T_f5034_row11_col0\" class=\"data row11 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$)</td>\n",
       "      <td id=\"T_f5034_row11_col1\" class=\"data row11 col1\" >60.3</td>\n",
       "      <td id=\"T_f5034_row11_col2\" class=\"data row11 col2\" >21.2</td>\n",
       "      <td id=\"T_f5034_row11_col3\" class=\"data row11 col3\" >49.1</td>\n",
       "      <td id=\"T_f5034_row11_col4\" class=\"data row11 col4\" >33.5</td>\n",
       "      <td id=\"T_f5034_row11_col5\" class=\"data row11 col5\" >34.8</td>\n",
       "      <td id=\"T_f5034_row11_col6\" class=\"data row11 col6\" >40.3</td>\n",
       "      <td id=\"T_f5034_row11_col7\" class=\"data row11 col7\" >44.0</td>\n",
       "      <td id=\"T_f5034_row11_col8\" class=\"data row11 col8\" >98.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row12\" class=\"row_heading level0 row12\" >7</th>\n",
       "      <td id=\"T_f5034_row12_col0\" class=\"data row12 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.3$)</td>\n",
       "      <td id=\"T_f5034_row12_col1\" class=\"data row12 col1\" >60.2</td>\n",
       "      <td id=\"T_f5034_row12_col2\" class=\"data row12 col2\" >26.0</td>\n",
       "      <td id=\"T_f5034_row12_col3\" class=\"data row12 col3\" >49.7</td>\n",
       "      <td id=\"T_f5034_row12_col4\" class=\"data row12 col4\" >24.6</td>\n",
       "      <td id=\"T_f5034_row12_col5\" class=\"data row12 col5\" >32.3</td>\n",
       "      <td id=\"T_f5034_row12_col6\" class=\"data row12 col6\" >39.2</td>\n",
       "      <td id=\"T_f5034_row12_col7\" class=\"data row12 col7\" >44.2</td>\n",
       "      <td id=\"T_f5034_row12_col8\" class=\"data row12 col8\" >122.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row13\" class=\"row_heading level0 row13\" >9</th>\n",
       "      <td id=\"T_f5034_row13_col0\" class=\"data row13 col0\" >DPP (Llama Emb)</td>\n",
       "      <td id=\"T_f5034_row13_col1\" class=\"data row13 col1\" >57.6</td>\n",
       "      <td id=\"T_f5034_row13_col2\" class=\"data row13 col2\" >19.8</td>\n",
       "      <td id=\"T_f5034_row13_col3\" class=\"data row13 col3\" >49.2</td>\n",
       "      <td id=\"T_f5034_row13_col4\" class=\"data row13 col4\" >28.3</td>\n",
       "      <td id=\"T_f5034_row13_col5\" class=\"data row13 col5\" >30.5</td>\n",
       "      <td id=\"T_f5034_row13_col6\" class=\"data row13 col6\" >37.8</td>\n",
       "      <td id=\"T_f5034_row13_col7\" class=\"data row13 col7\" >44.9</td>\n",
       "      <td id=\"T_f5034_row13_col8\" class=\"data row13 col8\" >92.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row14\" class=\"row_heading level0 row14\" >3</th>\n",
       "      <td id=\"T_f5034_row14_col0\" class=\"data row14 col0\" >$\\norm{\\nabla_{\\theta} \\ell}_2$ ($\\downarrow$)</td>\n",
       "      <td id=\"T_f5034_row14_col1\" class=\"data row14 col1\" >59.8</td>\n",
       "      <td id=\"T_f5034_row14_col2\" class=\"data row14 col2\" >26.3</td>\n",
       "      <td id=\"T_f5034_row14_col3\" class=\"data row14 col3\" >49.4</td>\n",
       "      <td id=\"T_f5034_row14_col4\" class=\"data row14 col4\" >21.9</td>\n",
       "      <td id=\"T_f5034_row14_col5\" class=\"data row14 col5\" >30.5</td>\n",
       "      <td id=\"T_f5034_row14_col6\" class=\"data row14 col6\" >38.4</td>\n",
       "      <td id=\"T_f5034_row14_col7\" class=\"data row14 col7\" >45.4</td>\n",
       "      <td id=\"T_f5034_row14_col8\" class=\"data row14 col8\" >119.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row15\" class=\"row_heading level0 row15\" >4</th>\n",
       "      <td id=\"T_f5034_row15_col0\" class=\"data row15 col0\" >\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td id=\"T_f5034_row15_col1\" class=\"data row15 col1\" >60.7</td>\n",
       "      <td id=\"T_f5034_row15_col2\" class=\"data row15 col2\" >26.7</td>\n",
       "      <td id=\"T_f5034_row15_col3\" class=\"data row15 col3\" >50.1</td>\n",
       "      <td id=\"T_f5034_row15_col4\" class=\"data row15 col4\" >22.4</td>\n",
       "      <td id=\"T_f5034_row15_col5\" class=\"data row15 col5\" >32.3</td>\n",
       "      <td id=\"T_f5034_row15_col6\" class=\"data row15 col6\" >39.1</td>\n",
       "      <td id=\"T_f5034_row15_col7\" class=\"data row15 col7\" >46.8</td>\n",
       "      <td id=\"T_f5034_row15_col8\" class=\"data row15 col8\" >137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5034_level0_row16\" class=\"row_heading level0 row16\" >0</th>\n",
       "      <td id=\"T_f5034_row16_col0\" class=\"data row16 col0\" >DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td id=\"T_f5034_row16_col1\" class=\"data row16 col1\" >60.6</td>\n",
       "      <td id=\"T_f5034_row16_col2\" class=\"data row16 col2\" >25.0</td>\n",
       "      <td id=\"T_f5034_row16_col3\" class=\"data row16 col3\" >49.5</td>\n",
       "      <td id=\"T_f5034_row16_col4\" class=\"data row16 col4\" >30.4</td>\n",
       "      <td id=\"T_f5034_row16_col5\" class=\"data row16 col5\" >34.8</td>\n",
       "      <td id=\"T_f5034_row16_col6\" class=\"data row16 col6\" >40.6</td>\n",
       "      <td id=\"T_f5034_row16_col7\" class=\"data row16 col7\" >47.9</td>\n",
       "      <td id=\"T_f5034_row16_col8\" class=\"data row16 col8\" >109.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14ca2ef03490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[ht]\n",
      "\\centering\\sc\\small\n",
      "\\caption{\\appendixalpacaperftablecaption}\n",
      "\\begin{tabular}{l|cccccc|rr}\n",
      "\\toprule\n",
      "Methods & \\multicolumn{6}{c}{Academic Benchmarks} & \\multicolumn{2}{c}{AlpacaEval} \\\\\n",
      " & MMLU & GSM & BBH & TydiQA & CodexEval & Avg & LC-\\% Win & Length \\\\\n",
      "\\midrule\n",
      "100\\% Data & 46.8 & 18.1 & \\textbf{50.1} & 31.5 & 29.3 & 35.8 & 39 & 86 \\\\\n",
      "Random & \\textbf{61.1} & 21.4 & 49.4 & 31.2 & 30.5 & 39.6 & 39 & 91 \\\\\n",
      "\\midrule\n",
      "DPP (Llama $\\nabla_{\\theta}\\ell$) & 60.3 & 21.2 & 49.1 & 33.5 & \\textbf{34.8} & 40.3 & 44 & 99 \\\\\n",
      "DPP (Llama $\\nabla_{\\theta}\\ell$) Not Norm. & 60.1 & 19.6 & 49.9 & 30.0 & 31.7 & 39.0 & 40 & 89 \\\\\n",
      "DPP (Llama Emb Not Norm.) & 58.8 & 21.1 & 49.5 & 29.6 & 31.1 & 38.8 & 39 & 88 \\\\\n",
      "DPP (Llama Emb) & 57.6 & 19.8 & 49.2 & 28.3 & 30.5 & 37.8 & 45 & 92 \\\\\n",
      "DPP (MpNet Emb) & 58.3 & 20.8 & 48.1 & 29.1 & 29.3 & 38.0 & 41 & 90 \\\\\n",
      "Dedup(MpNet Emb) & 60.3 & 20.4 & 49.2 & 31.8 & 27.4 & 39.0 & 40 & 85 \\\\\n",
      "\\midrule\n",
      "$\\norm{\\nabla_{\\theta} \\ell}_2$ ($\\downarrow$) & 59.8 & 26.3 & 49.4 & 21.9 & 30.5 & 38.4 & 45 & 120 \\\\\n",
      "Alpagasus Rating ($\\uparrow$) & 59.8 & 21.7 & 49.3 & \\textbf{35.9} & 29.3 & 40.3 & 37 & 86 \\\\\n",
      "EL2N ($\\downarrow$) & 58.6 & 21.6 & 48.0 & 29.4 & 31.7 & 38.5 & 42 & 97 \\\\\n",
      "IFD ($\\uparrow$) & 56.3 & 20.0 & 48.5 & 33.8 & 29.3 & 38.5 & 41 & 113 \\\\\n",
      "Perplexity ($\\downarrow$) & 59.3 & 21.4 & 48.9 & 27.5 & \\textbf{34.8} & 38.8 & 41 & 98 \\\\\n",
      "\\#Input Tokens ($\\uparrow$) & 60.5 & 21.1 & 48.7 & 34.2 & 31.1 & 40.0 & 41 & 87 \\\\\n",
      "\\midrule\n",
      "\\#Output Tokens ($\\uparrow$) & 60.7 & \\textbf{26.7} & \\textbf{50.1} & 22.4 & 32.3 & 39.1 & 47 & \\textbf{137} \\\\\n",
      "DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$) & 60.6 & 25.0 & 49.5 & 30.4 & \\textbf{34.8} & \\textbf{40.6} & \\textbf{48} & 109 \\\\\n",
      "DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.3$) & 60.2 & 26.0 & 49.7 & 24.6 & 32.3 & 39.2 & 44 & 123 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from functools import partial\n",
    "from llm.evaluate import get_eval_results_with_useful_cols, table_column_name_mapper, to_latex_bold_formatter, df_to_latex\n",
    "save_dirs = []\n",
    "def display_df(df):\n",
    "    from IPython.display import display\n",
    "    display(df.style\n",
    "#         .applymap(lambda x: f'max-width: 60ch;', subset=['run_name'])\n",
    "        .set_table_styles([{'selector': 'td', 'props': [('white-space', 'pre-wrap'), ('word-wrap', 'break-word')]}])\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "combine_table = False\n",
    "if combine_table:\n",
    "    datasets = ['stanford_alpaca50k', 'ultrachat50k']; caption = r'\\twodatasetsperftablecaption'\n",
    "    col_sortby = f'sort_by_name'\n",
    "    alpacaeval_colname = f'AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR'\n",
    "else:\n",
    "    datasets = ['stanford_alpaca50k']; caption = r'\\appendixalpacaperftablecaption'\n",
    "#     datasets = ['ultrachat50k']; caption = r'\\appendixultrachatperftablecaption'\n",
    "#     col_sortby = f'AlpacaFarm(alpaca:eval:gpt4)/WR*'\n",
    "    col_sortby = f'AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR'\n",
    "    col_sortby = f'sort_by_name'\n",
    "    alpacaeval_colname = f'AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR'\n",
    "\n",
    "model_name = 'llama'\n",
    "model_name = 'llama-7b'\n",
    "model_name = 'mistral-7b'\n",
    "\n",
    "print(datasets, model_name)\n",
    "\n",
    "def run_filter_fn(row):\n",
    "    if 'acos' in row['run_name']: return False\n",
    "    if model_name not in row['model_name_or_path'].lower(): return False\n",
    "    if ((row['sort_by_name'] not in ['100% Data', '100\\% Data']) and row['subset_size'] != 10000): return False\n",
    "    if any(x in row['run_name'] for x in ['dedup:dist=cd:md=llama7br512p4096:emb=grad+rp+loraB', 'dedup:dist=cd:md=llama7br512p4096:emb=text+embedding']): return False\n",
    "    if np.isnan(row[alpacaeval_colname]): return False\n",
    "    if row['dataset'] not in datasets: return False\n",
    "    if 'log+prob' in row['run_name'] or 'pmi' in row['run_name']: return False\n",
    "    if row['sort_by_name'] == row['run_name']: return False # not in sort by name list\n",
    "    return True\n",
    "\n",
    "exp_dirs = [\n",
    "    f'../results/baselines/huggyllama/',\n",
    "    f'../results/oi3',\n",
    "] + [\n",
    "    f'../results/oi5_{dataset}:{model_name}' for dataset in datasets\n",
    "] + [\n",
    "    f'../results/oi6_{dataset}:{model_name}' for dataset in datasets\n",
    "]\n",
    "for exp_dir in exp_dirs:\n",
    "    if os.path.isdir(exp_dir):\n",
    "        save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "chat_fmt = 'mix'\n",
    "cols = []\n",
    "alpacafarm_judge = 'alpaca:eval:gpt4'\n",
    "cols += [f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/Len*', f'AlpacaFarm({alpacafarm_judge})/LenMed', f'AlpacaFarm({alpacafarm_judge})/LCWR']\n",
    "alpacafarm_judge = 'weighted:alpaca:eval:gpt4:turbo'\n",
    "cols += [f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/Len*', f'AlpacaFarm({alpacafarm_judge})/LenMed', f'AlpacaFarm({alpacafarm_judge})/LCWR']\n",
    "cols += ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] \n",
    "\n",
    "\n",
    "df = get_eval_results_with_useful_cols(save_dirs, chat_fmt=chat_fmt, cols=cols)\n",
    "df = df[df.apply(run_filter_fn, axis=1)]\n",
    "if combine_table:\n",
    "    # submission\n",
    "    # cols_in_table = ['dataset', 'sort_by_name', 'academic_benchmark_avg', f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/Len*']\n",
    "    # rebuttal:\n",
    "    cols_in_table = ['dataset', 'sort_by_name', 'academic_benchmark_avg', alpacaeval_colname, f'AlpacaFarm({alpacafarm_judge})/Len*']\n",
    "#     cols_in_table += [f'AlpacaFarm(alpaca:eval:gpt4)/WR*']\n",
    "else:\n",
    "    # submission\n",
    "    # cols_in_table = ['sort_by_name', 'MMLU', 'GSM', 'BBH', 'TydiQA', 'Codex-Eval', 'academic_benchmark_avg', f'AlpacaFarm({alpacafarm_judge})/WR', f'AlpacaFarm({alpacafarm_judge})/Len*']\n",
    "    # rebuttal:\n",
    "    cols_in_table = ['sort_by_name', 'MMLU', 'GSM', 'BBH', 'TydiQA', 'Codex-Eval', 'academic_benchmark_avg', alpacaeval_colname, f'AlpacaFarm({alpacafarm_judge})/Len*']\n",
    "\n",
    "\n",
    "# cols_in_table = ['sort_by_name', 'MMLU', 'GSM', 'BBH', 'TydiQA', 'academic_benchmark_avg', f'AlpacaFarm(alpaca:eval:gpt4)/WR', f'AlpacaFarm(alpaca:eval:gpt4)/Len*']\n",
    "\n",
    "dfs = []\n",
    "for prune_method_type in ['Base', 'Diversity', 'Quality', 'Diversity \\& Quality']: # , 'Diversity \\& Quality'\n",
    "    dfc = df[df['prune_method_type']==prune_method_type]\n",
    "    dfc = dfc.sort_values(by=[col_sortby])\n",
    "    dfc = dfc[cols_in_table]\n",
    "    if combine_table:\n",
    "        dfc = pd.merge(\n",
    "            dfc[dfc['dataset']==datasets[0]].drop(columns=['dataset']),\n",
    "            dfc[dfc['dataset']==datasets[1]].drop(columns=['dataset']),\n",
    "            on='sort_by_name', how='outer', suffixes=('|'+datasets[0], '|'+datasets[1]))\n",
    "    dfs.append(dfc)\n",
    "dfc = pd.concat(dfs)\n",
    "\n",
    "\n",
    "if combine_table:\n",
    "    columns = pd.MultiIndex.from_tuples([('Datasets','Methods') if '|' not in x else (x.split('|')[-1], x.split('|')[0]) for x in dfc.columns])\n",
    "    dfc.columns = columns\n",
    "    dfc = dfc.rename(columns=table_column_name_mapper)     \n",
    "    dfc = dfc.rename(columns={'AlpacaEval/Length Controlled Win Rate': r'\\makecell{AlpacaEval \\\\ LC-\\% Win}',\n",
    "                              'AlpacaEval/Win Rate': r'\\makecell{AlpacaEval \\\\ \\% Win}',\n",
    "                              'Avg': r'\\makecell{Benchmark \\\\ Avg}',\n",
    "                              'AlpacaEval/Length': r'\\makecell{ \\\\ Len}',\n",
    "                              'stanford_alpaca50k': 'Alpaca',\n",
    "                              'ultrachat50k': 'UltraChat',}) \n",
    "else:\n",
    "    ## rename cols to look better\n",
    "    dfc = dfc.rename(columns=table_column_name_mapper)    \n",
    "#     display_df(dfc.sort_values(by=[f'AlpacaEval/Win Rate']))\n",
    "    display_df(dfc.sort_values(by=[f'AlpacaEval/Length Controlled Win Rate']))\n",
    "\n",
    "    ## create 2 levels of columns \n",
    "    singleindex_to_multiindex = {\n",
    "        'Methods': ('Methods', ''),\n",
    "        'MMLU': ('Academic Benchmarks', 'MMLU'),\n",
    "        'GSM': ('Academic Benchmarks', 'GSM'),\n",
    "        'BBH': ('Academic Benchmarks', 'BBH'),\n",
    "        'TydiQA': ('Academic Benchmarks', 'TydiQA'),\n",
    "        'CodexEval': ('Academic Benchmarks', 'CodexEval'),\n",
    "        'Average': ('Academic Benchmarks', 'Average'),\n",
    "        'Avg': ('Academic Benchmarks', 'Avg'),\n",
    "        'AlpacaEval/Length Controlled Win Rate': ('AlpacaEval', 'LC-\\% Win'),\n",
    "        'AlpacaEval/Win Rate': ('AlpacaEval', '\\% Win'),\n",
    "        'AlpacaEval/Median Length': ('AlpacaEval', 'Length'),\n",
    "        'AlpacaEval/Length': ('AlpacaEval', 'Length'),\n",
    "    }\n",
    "    columns = pd.MultiIndex.from_tuples([singleindex_to_multiindex[x] for x in dfc.columns])\n",
    "    dfc.columns = columns\n",
    "## cell value floats formatter. put after done with naming columns\n",
    "formatter = {}\n",
    "for col in dfc.columns[1:]:\n",
    "    num_decimals = 0 if any(x in col[1] for x in ['Length', 'Win', 'Len']) else 1\n",
    "#     num_decimals = 1\n",
    "    formatter[col] = partial(to_latex_bold_formatter, value=dfc[col].max(), num_decimals=num_decimals)\n",
    "\n",
    "if combine_table:\n",
    "    column_format = 'l|' + 'ccr' + '|' + 'ccr'\n",
    "    midrule_insert_inds = [8+i for i in [2, 8, ]]\n",
    "else:\n",
    "    column_format = 'l|' + 'c'*6 + '|' + 'r'*2\n",
    "    midrule_insert_inds = [8+i for i in [2, 8, 14]]\n",
    "\n",
    "s = df_to_latex(\n",
    "    dfc,\n",
    "    floating_precision=1,\n",
    "    column_format=column_format,\n",
    "    midrule_insert_inds=midrule_insert_inds,\n",
    "    two_cols=True,\n",
    "    formatter=formatter,\n",
    "    small_fonts=True,\n",
    "    caption=caption,\n",
    ") \n",
    "\n",
    "\n",
    "print()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a787b48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>GSM</th>\n",
       "      <th>BBH</th>\n",
       "      <th>TydiQA</th>\n",
       "      <th>CodexEval</th>\n",
       "      <th>Avg</th>\n",
       "      <th>AlpacaEval/Length Controlled Win Rate</th>\n",
       "      <th>AlpacaEval/Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100\\% Data</td>\n",
       "      <td>46.848739</td>\n",
       "      <td>18.1</td>\n",
       "      <td>50.138889</td>\n",
       "      <td>31.501617</td>\n",
       "      <td>29.268293</td>\n",
       "      <td>35.827420</td>\n",
       "      <td>39.095859</td>\n",
       "      <td>86.408060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random</td>\n",
       "      <td>61.127332</td>\n",
       "      <td>21.4</td>\n",
       "      <td>49.351852</td>\n",
       "      <td>31.160570</td>\n",
       "      <td>30.487805</td>\n",
       "      <td>39.618590</td>\n",
       "      <td>39.095370</td>\n",
       "      <td>91.416352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DPP (MpNet Emb)</td>\n",
       "      <td>58.282296</td>\n",
       "      <td>20.8</td>\n",
       "      <td>48.101852</td>\n",
       "      <td>29.112506</td>\n",
       "      <td>29.268293</td>\n",
       "      <td>37.984622</td>\n",
       "      <td>40.877270</td>\n",
       "      <td>89.835427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dedup(MpNet Emb)</td>\n",
       "      <td>60.251389</td>\n",
       "      <td>20.4</td>\n",
       "      <td>49.212963</td>\n",
       "      <td>31.775582</td>\n",
       "      <td>27.439024</td>\n",
       "      <td>38.968766</td>\n",
       "      <td>40.085949</td>\n",
       "      <td>84.911139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$\\norm{\\nabla_{\\theta} \\ell}_2$ ($\\downarrow$)</td>\n",
       "      <td>59.752884</td>\n",
       "      <td>26.3</td>\n",
       "      <td>49.444444</td>\n",
       "      <td>21.901856</td>\n",
       "      <td>30.487805</td>\n",
       "      <td>38.365131</td>\n",
       "      <td>45.448625</td>\n",
       "      <td>119.815558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EL2N ($\\downarrow$)</td>\n",
       "      <td>58.567156</td>\n",
       "      <td>21.6</td>\n",
       "      <td>48.009259</td>\n",
       "      <td>29.370263</td>\n",
       "      <td>31.707317</td>\n",
       "      <td>38.533408</td>\n",
       "      <td>41.664322</td>\n",
       "      <td>96.545568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IFD ($\\uparrow$)</td>\n",
       "      <td>56.291839</td>\n",
       "      <td>20.0</td>\n",
       "      <td>48.472222</td>\n",
       "      <td>33.831026</td>\n",
       "      <td>29.268293</td>\n",
       "      <td>38.495385</td>\n",
       "      <td>41.315496</td>\n",
       "      <td>113.361568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perplexity ($\\downarrow$)</td>\n",
       "      <td>59.300669</td>\n",
       "      <td>21.4</td>\n",
       "      <td>48.935185</td>\n",
       "      <td>27.468481</td>\n",
       "      <td>34.756098</td>\n",
       "      <td>38.773863</td>\n",
       "      <td>41.249052</td>\n",
       "      <td>98.007491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\#Input Tokens ($\\uparrow$)</td>\n",
       "      <td>60.457912</td>\n",
       "      <td>21.1</td>\n",
       "      <td>48.657407</td>\n",
       "      <td>34.235876</td>\n",
       "      <td>31.097561</td>\n",
       "      <td>39.999995</td>\n",
       "      <td>40.541150</td>\n",
       "      <td>86.974874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\#Output Tokens ($\\uparrow$)</td>\n",
       "      <td>60.678678</td>\n",
       "      <td>26.7</td>\n",
       "      <td>50.138889</td>\n",
       "      <td>22.368824</td>\n",
       "      <td>32.317073</td>\n",
       "      <td>39.121095</td>\n",
       "      <td>46.839676</td>\n",
       "      <td>137.045169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)</td>\n",
       "      <td>60.646632</td>\n",
       "      <td>25.0</td>\n",
       "      <td>49.490741</td>\n",
       "      <td>30.396783</td>\n",
       "      <td>34.756098</td>\n",
       "      <td>40.647156</td>\n",
       "      <td>47.907102</td>\n",
       "      <td>109.127364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.3$)</td>\n",
       "      <td>60.165931</td>\n",
       "      <td>26.0</td>\n",
       "      <td>49.722222</td>\n",
       "      <td>24.550315</td>\n",
       "      <td>32.317073</td>\n",
       "      <td>39.243779</td>\n",
       "      <td>44.157842</td>\n",
       "      <td>122.542138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Methods       MMLU  \\\n",
       "15                                                      100\\% Data  46.848739   \n",
       "9                                                           Random  61.127332   \n",
       "14                                                 DPP (MpNet Emb)  58.282296   \n",
       "7                                                 Dedup(MpNet Emb)  60.251389   \n",
       "2                   $\\norm{\\nabla_{\\theta} \\ell}_2$ ($\\downarrow$)  59.752884   \n",
       "12                                             EL2N ($\\downarrow$)  58.567156   \n",
       "10                                                IFD ($\\uparrow$)  56.291839   \n",
       "5                                        Perplexity ($\\downarrow$)  59.300669   \n",
       "4                                      \\#Input Tokens ($\\uparrow$)  60.457912   \n",
       "3                                     \\#Output Tokens ($\\uparrow$)  60.678678   \n",
       "0   DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.1$)  60.646632   \n",
       "6   DPP (Llama $\\nabla_{\\theta}\\ell$ + \\#Output Toks $\\theta=0.3$)  60.165931   \n",
       "\n",
       "     GSM        BBH     TydiQA  CodexEval        Avg  \\\n",
       "15  18.1  50.138889  31.501617  29.268293  35.827420   \n",
       "9   21.4  49.351852  31.160570  30.487805  39.618590   \n",
       "14  20.8  48.101852  29.112506  29.268293  37.984622   \n",
       "7   20.4  49.212963  31.775582  27.439024  38.968766   \n",
       "2   26.3  49.444444  21.901856  30.487805  38.365131   \n",
       "12  21.6  48.009259  29.370263  31.707317  38.533408   \n",
       "10  20.0  48.472222  33.831026  29.268293  38.495385   \n",
       "5   21.4  48.935185  27.468481  34.756098  38.773863   \n",
       "4   21.1  48.657407  34.235876  31.097561  39.999995   \n",
       "3   26.7  50.138889  22.368824  32.317073  39.121095   \n",
       "0   25.0  49.490741  30.396783  34.756098  40.647156   \n",
       "6   26.0  49.722222  24.550315  32.317073  39.243779   \n",
       "\n",
       "    AlpacaEval/Length Controlled Win Rate  AlpacaEval/Length  \n",
       "15                              39.095859          86.408060  \n",
       "9                               39.095370          91.416352  \n",
       "14                              40.877270          89.835427  \n",
       "7                               40.085949          84.911139  \n",
       "2                               45.448625         119.815558  \n",
       "12                              41.664322          96.545568  \n",
       "10                              41.315496         113.361568  \n",
       "5                               41.249052          98.007491  \n",
       "4                               40.541150          86.974874  \n",
       "3                               46.839676         137.045169  \n",
       "0                               47.907102         109.127364  \n",
       "6                               44.157842         122.542138  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "902fdced",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR</th>\n",
       "      <th>AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR*</th>\n",
       "      <th>AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*</th>\n",
       "      <th>AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LenMed</th>\n",
       "      <th>AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.090546</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>241.871665</td>\n",
       "      <td>217.0</td>\n",
       "      <td>40.884791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR  \\\n",
       "0                                       48.090546   \n",
       "\n",
       "   AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/WR*  \\\n",
       "0                                         5.263158   \n",
       "\n",
       "   AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/Len*  \\\n",
       "0                                        241.871665   \n",
       "\n",
       "   AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LenMed  \\\n",
       "0                                               217.0   \n",
       "\n",
       "   AlpacaFarm(weighted:alpaca:eval:gpt4:turbo)/LCWR  \n",
       "0                                         40.884791  "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "l = [\n",
    "    ('llama-7b_ultrachat50k_score=dppmap:k=vmf:gamma=1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3',\n",
    "  '../results/oi6_ultrachat50k:llama-7b/llama-7b_ultrachat50k_score=dppmap:k=vmf:gamma=1:theta=0.1:kmd=llama7br512p4096:kemb=grad+rp+loraB:q=numtoks+output:qmd=llama7br512p4096_pace=prune:size=30000:ep=3'),\n",
    "]\n",
    "df = get_eval_results_with_useful_cols(l, chat_fmt=chat_fmt, cols=cols)\n",
    "df[[x for x in df.columns if x.startswith('Alpaca')]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "47f8fd81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Datasets', 'Methods'): {0: 'Random', 1: 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$)', 2: 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$) Not Norm.', 3: 'DPP (Llama Emb Not Norm.)', 4: 'DPP (Llama Emb)', 5: 'DPP (MpNet Emb)', 6: 'Dedup(MpNet Emb)', 7: '$\\\\norm{\\\\nabla_{\\\\theta} \\\\ell}_2$ ($\\\\downarrow$)', 8: 'Alpagasus Rating ($\\\\uparrow$)', 9: 'EL2N ($\\\\downarrow$)', 10: 'IFD ($\\\\uparrow$)', 11: 'Perplexity ($\\\\downarrow$)', 12: '\\\\#Input Tokens ($\\\\uparrow$)', 13: '\\\\#Output Tokens ($\\\\uparrow$)', 14: '\\\\#Total Tokens ($\\\\uparrow$)', 15: 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$ + \\\\#Output Toks $\\\\theta=0.1$)', 16: 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$ + \\\\#Output Toks $\\\\theta=0.3$)', 17: 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$ + \\\\#Output Toks $\\\\theta=0.6$)', 18: 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$ + \\\\#Output Toks)'}, ('Alpaca', '\\\\makecell{Benchmark \\\\\\\\ Avg}'): {0: 21.586655617298028, 1: 21.70769492361744, 2: 22.726482591700417, 3: 22.0979453675226, 4: 22.515489588242797, 5: 22.853927816002674, 6: 22.845217769457744, 7: 22.673602078282105, 8: 21.58587457354841, 9: 22.82543649585082, 10: 21.492337930416962, 11: 22.688455906562773, 12: 25.132897226301083, 13: 23.367317078941642, 14: 20.389618799076796, 15: 23.075241860652095, 16: 23.6697567241169, 17: 23.585770438162967, 18: 23.470334872332288}, ('Alpaca', '\\\\makecell{AlpacaEval \\\\\\\\ LC-\\\\% Win}'): {0: 27.119138113316094, 1: 28.527044681317253, 2: 15.364794573288687, 3: 27.454508661958503, 4: 26.57200403269087, 5: 25.161296537030143, 6: 24.24367346935219, 7: 30.874688529370143, 8: 27.21130586572541, 9: 23.631640343471936, 10: 27.038329254698645, 11: 28.54330332388978, 12: 22.836626033115866, 13: 30.86699543795891, 14: 29.66932063632551, 15: 31.5652429277846, 16: 30.97004822873226, 17: 29.66531259576764, 18: 29.205498234582763}, ('Alpaca', '\\\\makecell{ \\\\\\\\ Len}'): {0: 90.05183312262959, 1: 103.78333333333333, 2: 33.88944723618091, 3: 92.58015267175573, 4: 91.74775928297055, 5: 87.8733850129199, 6: 84.87435897435897, 7: 136.14030612244898, 8: 95.00904392764858, 9: 98.68147208121827, 10: 113.19687092568448, 11: 105.8510101010101, 12: 87.9037711313394, 13: 151.74615384615385, 14: 149.1853281853282, 15: 122.08111533586819, 16: 130.89144316730523, 17: 139.18346253229973, 18: 150.23346303501947}, ('Alpaca', '\\\\makecell{AlpacaEval \\\\\\\\ \\\\% Win}'): {0: 18.26086956521739, 1: 26.27329192546584, 2: 7.639751552795031, 3: 21.67701863354037, 4: 20.93167701863354, 5: 19.440993788819878, 6: 18.509316770186334, 7: 36.95652173913043, 8: 21.242236024844722, 9: 21.180124223602487, 10: 29.316770186335404, 11: 25.341614906832298, 12: 18.757763975155278, 13: 39.37888198757764, 14: 37.391304347826086, 15: 35.3416149068323, 16: 39.006211180124225, 17: 40.12422360248447, 18: 40.43478260869565}, ('UltraChat', '\\\\makecell{Benchmark \\\\\\\\ Avg}'): {0: 22.62959298527265, 1: 23.301394416284534, 2: 22.679686641480327, 3: 23.24370437693765, 4: 23.30406426737403, 5: 23.250675018027263, 6: 22.99422764911573, 7: 23.266910258392393, 8: nan, 9: 22.700041750787054, 10: 22.997530195603925, 11: 22.828419292719143, 12: 22.861764183642265, 13: 22.670826821605473, 14: 22.654575683604314, 15: nan, 16: nan, 17: nan, 18: 22.635577741318045}, ('UltraChat', '\\\\makecell{AlpacaEval \\\\\\\\ LC-\\\\% Win}'): {0: 36.41580753054116, 1: 36.904490359366754, 2: 36.42554810533453, 3: 37.719599174650234, 4: 35.25878486393304, 5: 35.27188660137868, 6: 39.11837747944911, 7: 37.77209632213875, 8: nan, 9: 36.66017635333701, 10: 38.29593869408444, 11: 36.72625066760657, 12: 35.31066862393464, 13: 37.557138989133, 14: 37.876224153477104, 15: nan, 16: nan, 17: nan, 18: 38.637536163267555}, ('UltraChat', '\\\\makecell{ \\\\\\\\ Len}'): {0: 213.41603053435114, 1: 217.0306905370844, 2: 197.00509554140126, 3: 214.6854942233633, 4: 204.24675324675326, 5: 210.8869123252859, 6: 219.02709677419355, 7: 248.56482670089858, 8: nan, 9: 220.4407643312102, 10: 271.43790012804095, 11: 230.80357142857142, 12: 216.79419525065964, 13: 261.9548969072165, 14: 250.66753246753248, 15: nan, 16: nan, 17: nan, 18: 272.9935897435897}, ('UltraChat', '\\\\makecell{AlpacaEval \\\\\\\\ \\\\% Win}'): {0: 56.89440993788819, 1: 58.012422360248436, 2: 53.41614906832298, 3: 55.21739130434782, 4: 52.04968944099379, 5: 57.51552795031055, 6: 57.63975155279504, 7: 62.85714285714287, 8: nan, 9: 55.46583850931677, 10: 62.2360248447205, 11: 59.62732919254658, 12: 50.68322981366459, 13: 63.16770186335404, 14: 60.931677018633536, 15: nan, 16: nan, 17: nan, 18: 63.788819875776404}}\n"
     ]
    }
   ],
   "source": [
    "print(dfc.reset_index(drop=True).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "deeba0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$)', '100\\\\% Data', 'DPP (Llama Emb Not Norm.)', 'Random', 'DPP (Llama Emb)', 'DPP (MpNet Emb)', 'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$) Not Norm.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run_name',\n",
       " 'sort_by',\n",
       " 'prune_method_type',\n",
       " 'compute',\n",
       " 'academic_benchmark_avg',\n",
       " 'model_name_or_path',\n",
       " 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR',\n",
       " 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR*',\n",
       " 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR**',\n",
       " 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR***',\n",
       " 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/Len*',\n",
       " 'MMLU/0-shot',\n",
       " 'MMLU/5-shot',\n",
       " 'GSM/Direct',\n",
       " 'GSM/CoT',\n",
       " 'BBH/Direct',\n",
       " 'BBH/CoT',\n",
       " 'TydiQA/CB',\n",
       " 'TydiQA/GP',\n",
       " 'Codex-Eval/Pass@1',\n",
       " 'MMLU',\n",
       " 'GSM',\n",
       " 'BBH',\n",
       " 'TydiQA',\n",
       " 'Codex-Eval',\n",
       " 'Average',\n",
       " 'ranking']"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from rosemary import parse_kv_from_string\n",
    "from functools import partial\n",
    "\n",
    "def get_dataset_size(dataset):\n",
    "    if dataset == 'dolly':\n",
    "        return 14956\n",
    "    else:\n",
    "        return 50_000\n",
    "    \n",
    "non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', ]\n",
    "non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', ]\n",
    "\n",
    "N = 50_000\n",
    "full_sft_run_name_substr = ('_ep=3', '_ep=10')\n",
    "# full_sft_run_name_substr = '_ep=2'\n",
    "full_sft_short = '100% Data'\n",
    "\n",
    "    \n",
    "base_model_perf = df[df['run_name'] == 'llama-7b'].to_dict(orient='records')[0]\n",
    "# base_model_perf['nonchat'] = compute_nonchateval_average_performance(base_model_perf)\n",
    "\n",
    "def run_filter_fn(row):\n",
    "    if 'acos' in row['run_name']: return False\n",
    "    if 'theta=' in row['run_name']: return False\n",
    "#     if np.isnan(row[f'AlpacaFarm({alpacafarm_judge})/WR']): return False\n",
    "    if row['sort_by_name'] == 'Random': return True\n",
    "    if row['sort_by_name'] in ['100% Data', '100\\% Data']: return True\n",
    "    if row['sort_by'] and row['sort_by'][0].startswith('dpp'): return True\n",
    "    return False\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc = dfc[dfc.apply(run_filter_fn, axis=1)]\n",
    "\n",
    "print(list(dfc['sort_by_name'].unique()))\n",
    "\n",
    "\n",
    "sort_by_name_to_include = [\n",
    "    '100\\\\% Data', \n",
    "    'Random',\n",
    "#     'DPP (MpNet Emb)',\n",
    "    'DPP (Llama Emb)', \n",
    "    'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$)',\n",
    "#     'DPP (Llama Emb Not Norm.)', \n",
    "#     'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$ Not Norm.)',\n",
    "]\n",
    "\n",
    "dfc = dfc[dfc.apply(lambda x: any(y == x['sort_by_name'] for y in sort_by_name_to_include), axis=1)]\n",
    "# # dfc = dfc[dfc['subset_size'].apply(lambda x: x>1000)]\n",
    "\n",
    "\n",
    "D = dfc.set_index(['sort_by_name', 'dataset', 'subset_size']).to_dict()\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class DKey:\n",
    "    sort_by_type: str\n",
    "    dataset: str\n",
    "    subset_size: int\n",
    "def convert_key_to_dataclass(d):\n",
    "    return {DKey(*k): v for k, v in d.items()}\n",
    "D = {k: convert_key_to_dataclass(v) for k, v in D.items()}\n",
    "\n",
    "list(D.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "41b95aee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x480 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "font_path = '/dccstor/data-pruning/miniconda3/pkgs/mscorefonts-0.0.1-3/fonts/times.ttf'  # Your font path goes here\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = prop.get_name()\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_task_name_display(task_name):\n",
    "    if task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR': return 'AlpacaEval\\n(% Win)'\n",
    "    if task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR*': return 'AlpacaEval\\n(Win Rate)'\n",
    "    if task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR**': return 'AlpacaEval\\n(Win Rate **)'\n",
    "    if task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/WR***': return 'AlpacaEval\\n(Win Rate ***)'\n",
    "    elif task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/Len*': return 'AlpacaEval\\n(Tok Length*)'\n",
    "    elif task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/Len': return 'AlpacaEval\\n(Tok Length)'\n",
    "    elif task_name == 'AlpacaFarm(alpaca:eval:gpt4:turbo:fn)/LenMed': return 'AlpacaEval\\n(Medium #Tokens)'\n",
    "    elif task_name == 'MTBench(gpt:4)/Turn-1': return 'MT-Bench(GPT-4)/Turn-1'\n",
    "    elif task_name == 'MTBench(gpt:4)/Turn-2': return 'MT-Bench(GPT-4)/Turn-2'\n",
    "    elif task_name == 'MTBench(gpt:4)/Rating': return 'MT-Bench(GPT-4)/Rating'\n",
    "    elif task_name == 'nonchat': return 'NLP Benchmarks\\n(Average Score)'\n",
    "    elif task_name == 'academic_benchmark_avg': return 'Benchmark\\n(Avg Scores)'\n",
    "    else: return task_name\n",
    "        \n",
    "def get_dataset_display(dataset):\n",
    "    if 'dolly' in dataset: return 'Dolly'\n",
    "    elif 'stanford_alpaca' in dataset: return 'Alpaca'\n",
    "    elif 'ultrachat' in dataset: return 'UltraChat'\n",
    "    elif 'sharegpt' in dataset: return 'ShareGPT'\n",
    "    elif 'ultrafeedback' in dataset: return 'UltraFeedback'\n",
    "    elif 'flan_v2' in dataset: return \"FLAN\"\n",
    "    elif 'oasst2' in dataset: return \"OASST2\"\n",
    "    elif 'wizardlm' in dataset: return \"WizardLM\"\n",
    "    else: raise ValueError(f'{dataset} not defined display name')\n",
    "\n",
    "\n",
    "data_to_compute_dict = dfc.set_index(['subset_size']).to_dict()['compute']\n",
    "data_to_compute_pct = lambda x: data_to_compute_dict[x]/max(data_to_compute_dict.values())*100\n",
    "\n",
    "\n",
    "plt_base_model = True\n",
    "plt_full_finetune = True\n",
    "xaxis_type = 'data' # 'compute'\n",
    "yaxis_type = 'abs'\n",
    "yaxis_type = 'delta_fullfinetune'; \n",
    "assert(yaxis_type in ['abs', 'delta_fullfinetune'])\n",
    "\n",
    "if finetune_type == 'sft':\n",
    "    datasets = ['dolly', 'stanford_alpaca50k', 'ultrachat50k', 'sharegpt50k']\n",
    "    datasets = ['flan_v250k', 'dolly', 'stanford_alpaca50k', 'oasst2', 'ultrachat50k', 'wizardlm50k', 'sharegpt50k']\n",
    "    datasets = ['flan_v250k', 'dolly', 'stanford_alpaca50k', 'ultrachat50k', 'wizardlm50k', 'sharegpt50k']\n",
    "    datasets = ['dolly', 'stanford_alpaca50k', 'ultrachat50k', 'sharegpt50k']\n",
    "else:\n",
    "    datasets = ['ultrafeedback']\n",
    "# datasets = list(np.unique(dfc['dataset']))\n",
    "\n",
    "task_names = []\n",
    "task_names += ['academic_benchmark_avg']\n",
    "# task_names += ['nonchat']\n",
    "# task_names += ['nonchat', 'MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*',]\n",
    "# task_names += ['MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1',]\n",
    "# task_names += [f'MTBench({mtbench_judge})/Turn-1',  f'MTBench({mtbench_judge})/Turn-2', f'MTBench({mtbench_judge})/Rating']\n",
    "# task_names += [f'MTBench({mtbench_judge})/Rating']\n",
    "# task_names += [f'MTBench({mtbench_judge})/Turn-1']\n",
    "task_names += [f'AlpacaFarm({alpacafarm_judge})/WR'] \n",
    "# task_names += [f'AlpacaFarm({alpacafarm_judge})/WR**'] \n",
    "# task_names += [f'AlpacaFarm({alpacafarm_judge})/WR***'] \n",
    "# task_names += [f'AlpacaFarm({alpacafarm_judge})/WR***'] \n",
    "# task_names += [f'AlpacaFarm({alpacafarm_judge})/Len*']\n",
    "# task_names += [f'AlpacaFarm({alpacafarm_judge})/WR*', f'AlpacaFarm({alpacafarm_judge})/LenMed'] # , f'AlpacaFarm({alpacafarm_judge})/Rep2'\n",
    "\n",
    "label_baseline = 'Base Model'\n",
    "full_sft_short = '100\\\\% Data'\n",
    "\n",
    "cmap = plt.get_cmap('tab20c')\n",
    "\n",
    "plt_settings = {\n",
    "    'color': {\n",
    "        full_sft_short: '#333333', #'#FFA500', # orange\n",
    "#         'Random': 'gray',\n",
    "        label_baseline: 'gray',\n",
    "#         label_dpp_vmf_grad: '#3498db', # bright blue\n",
    "#         label_dpp_vmf_text: '#1f618d', # dark blue\n",
    "#         label_dpp_vmf_text,\n",
    "#         label_dpp_rbf_grad,\n",
    "#         label_dpp_rbf_text,\n",
    "        'Random': cmap(1),\n",
    "    #     'DPP (MpNet Emb Norm.)'\n",
    "        'DPP (Llama $\\\\nabla_{\\\\theta}\\\\ell$ Norm.)': cmap(1+4),\n",
    "        'DPP (Llama Emb)': cmap(1+8), \n",
    "        'DPP (Llama Emb Norm.)': cmap(1+12),\n",
    "    },\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 25,   # ax title\n",
    "    'axes.labelsize': 19,   # labels (x-axis and y-axis)\n",
    "    'xtick.labelsize': 14,  # xtick size\n",
    "    'ytick.labelsize': 15,  # ytick size\n",
    "    'figure.labelsize': 25, # \n",
    "    'legend.fontsize': 15,  # legend font size\n",
    "})\n",
    "\n",
    "markers_list = ['^', 'o', '*', 'x', 's', ]\n",
    "markers_list = ['o']*10\n",
    "lineplot_kwargs = {'marker': '.', \n",
    "#                    'markerfacecolor': 'none', \n",
    "                   'markersize': 8, \n",
    "                   'alpha': .8,\n",
    "#                    'markeredgewidth': 2,\n",
    "#                    'linewidth': 1.5,\n",
    "                  }\n",
    "\n",
    "w = 2.5; h = 2.5\n",
    "# w = 3; h = 3\n",
    "w = 2.7; h = 2.4\n",
    "ncols = len(datasets)\n",
    "nrows = len(task_names)\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(w*ncols, h*nrows), sharey='row', sharex='col')\n",
    "\n",
    "xticks_data = defaultdict(list)\n",
    "\n",
    "for axi, task_name in enumerate(task_names):\n",
    "    d = D[task_name]\n",
    "    for axj, dataset in enumerate(datasets):\n",
    "        ax = axs.reshape(nrows, ncols)[axi, axj]\n",
    "        dataset_size = get_dataset_size(dataset)\n",
    "        is_first_subfig = (axi==0 and axj==0)\n",
    "\n",
    "        if plt_base_model and task_name == 'academic_benchmark_avg':\n",
    "            k = DKey(full_sft_short, dataset, dataset_size)\n",
    "            if k in d:\n",
    "                y_fullfinetune = d[k]\n",
    "                y_base = base_model_perf[task_name]\n",
    "                y = y_base-y_fullfinetune if yaxis_type == 'delta_fullfinetune' else y_base\n",
    "                ax.axhline(y=y, linestyle='--', color=plt_settings['color'][label_baseline], label=label_baseline if is_first_subfig else None, linewidth=2)\n",
    "        if plt_full_finetune:\n",
    "            k = DKey(full_sft_short, dataset, dataset_size)\n",
    "            if k in d:\n",
    "                y = 0 if yaxis_type == 'delta_fullfinetune' else d[k]\n",
    "                ax.axhline(y=y, linestyle='--', color=plt_settings['color'][full_sft_short], label='100% Data' if is_first_subfig else None, linewidth=3)\n",
    "\n",
    "        sort_by_types = sorted(set(x.sort_by_type for x in d.keys()))[::-1]\n",
    "#         sort_by_types = list(set([x.sort_by_type for x in d.keys()]))\n",
    "        for i, sort_by_type in enumerate(sort_by_types):\n",
    "            xs = sorted([x.subset_size for x in d.keys()\n",
    "                         if x.dataset == dataset and x.sort_by_type==sort_by_type])\n",
    "            xticks_data[dataset] += list(set(xs) - set(xticks_data[dataset]))\n",
    "\n",
    "            ys = [d[DKey(sort_by_type, dataset, x)] for x in xs]\n",
    "            if yaxis_type == 'delta_random':\n",
    "                if not all(DKey('random', dataset, x) in d for x in xs): continue\n",
    "                ys = [y-d[DKey('random', dataset, x)] for x, y in zip(xs, ys)] \n",
    "            elif yaxis_type == 'delta_fullfinetune':\n",
    "                if DKey(full_sft_short, dataset, dataset_size) not in d: continue\n",
    "                ys = [y-d[DKey(full_sft_short, dataset, dataset_size)] for y in ys]\n",
    "            kwargs = {}\n",
    "            if sort_by_type in plt_settings['color']:\n",
    "                kwargs['color'] = plt_settings['color'][sort_by_type]\n",
    "            if full_sft_short not in sort_by_type and is_first_subfig:\n",
    "                kwargs['label'] = sort_by_type \n",
    "            kwargs.update(lineplot_kwargs)\n",
    "            kwargs.update({'marker': markers_list[i]})\n",
    "            xs = xs if xaxis_type == 'data' else [data_to_compute_pct(x) for x in xs]\n",
    "#             print(task_name, dataset, sort_by_type, xs, ys)\n",
    "            ax.plot(xs, ys, **kwargs)\n",
    "            ax.grid(visible=True, axis='y')\n",
    "            ax.tick_params(axis='both', which='both', length=6) \n",
    "            ax.locator_params(nbins=4, axis='both')\n",
    "\n",
    "\n",
    "## left most subfigure set ylabel & yticks\n",
    "for axi, task_name in enumerate(task_names):\n",
    "    task_name_shortened = get_task_name_display(task_name)\n",
    "    ax = axs.reshape(nrows, ncols)[axi, 0]\n",
    "    ax.set_ylabel(task_name_shortened.replace('(', '(▲ ') if yaxis_type.startswith('delta') else task_name_shortened, va='bottom')\n",
    "    if yaxis_type == 'delta_fullfinetune':\n",
    "        if task_name == 'academic_benchmark_avg': yticks = [-3,-2,-1,0,1]\n",
    "        elif 'AlpacaFarm' in task_name and 'WR' in task_name: yticks = [-12, -8, -4, 0, 4, 8]\n",
    "        else: yticks = None\n",
    "        if yticks is not None: ax.set_yticks(yticks, yticks)\n",
    "    \n",
    "\n",
    "for axj, dataset in enumerate(datasets):\n",
    "    dataset_size = get_dataset_size(dataset)\n",
    "    xticks = np.array(sorted(xticks_data[dataset]))\n",
    "    ax = axs.reshape(nrows, ncols)[nrows-1, axj]\n",
    "    c = .05; ax.set_xlim((-N*c, N+c*N))\n",
    "    xticklabels = [f'{x*100:.0f}%' for x in xticks/get_dataset_size(dataset)]\n",
    "    ax.set_xticks(xticks, xticklabels, ha='center', rotation=45)\n",
    "\n",
    "    ax = axs.reshape(nrows, ncols)[0, axj]\n",
    "    ax.set_title(get_dataset_display(dataset))\n",
    "\n",
    "    if yaxis_type != 'abs':\n",
    "        axs.reshape(nrows, ncols)[0, axj].set_ylim((-3.5, 1))\n",
    "        axs.reshape(nrows, ncols)[1, axj].set_ylim((-14, 10))\n",
    "        \n",
    "        \n",
    "## data xlabel at the side s\n",
    "axs.reshape(nrows, ncols)[nrows-1, 0].annotate('Data', xy=(-0.3, -.15), xycoords=\"axes fraction\", ha='left', va='center', weight='bold', fontsize=plt.rcParams['axes.labelsize'])\n",
    "## legend at bottom\n",
    "fig.legend(loc='lower center', bbox_to_anchor=(0.5, -.15), ncol=3, frameon=True)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "save_plt = True\n",
    "if save_plt:\n",
    "    save_path = os.path.join(assets_dir, 'fig_vmf_grad_vs_random_cross_datasets.pdf')\n",
    "    fig.savefig(save_path, bbox_inches='tight', dpi=100)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
