{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 13 11:57:16 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:8D:00.0 Off |                    0 |\r\n",
      "| N/A   25C    P0    56W / 400W |   8930MiB / 40960MiB |      0%   E. Process |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:C7:00.0 Off |                    0 |\r\n",
      "| N/A   25C    P0    58W / 400W |   8932MiB / 40960MiB |      0%   E. Process |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A   1408655      C   .../open-instruct/bin/python     8928MiB |\r\n",
      "|    1   N/A  N/A   1408655      C   .../open-instruct/bin/python     8930MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from llm.submit import multiline_to_singleline, submit_job_ccc, get_run_statistics\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b984427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps.\n",
      "\n",
      "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft'\n",
    "test_run = True\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 20\n",
    "num_gpus = 1\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "if test_run:\n",
    "    output_dir = 'jpt_' + output_dir\n",
    "\n",
    "use_deepspeed = False\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''}\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "\n",
    "# things to test to see its effects on (1) eval perf (2) runtime.\n",
    "#\n",
    "# - int8\n",
    "# - mixed_precision bf16 or no\n",
    "# - with/without LoRA\n",
    "# - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# - batch size\n",
    "# - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions'), PosixPath('/usr/local/share/zsh/site-functions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1832148.tmpdir/.1689173487.1832148.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/12/2023 12:59:09 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "07/12/2023 12:59:09 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 292.59it/s]\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:50<00:00, 25.41s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Assigning <s> to the bos_token key of the tokenizer\n",
      "Assigning </s> to the eos_token key of the tokenizer\n",
      "Assigning <unk> to the unk_token key of the tokenizer\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/12/2023 13:00:49 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 2097152 || all params: 6740520960 || trainable%: 0.03111261002591705\n",
      "GPU memory occupied: 837 MB.\n",
      "07/12/2023 13:01:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1038c98439ac66d6_*_of_00016.arrow\n",
      "07/12/2023 13:01:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-88ed2eb4a77829ae.arrow\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 134049 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29925,  1745,   895,\n",
      "        29901,   376,  1576,  7306,   347,   310,   385, 14890, 18881,  3815,\n",
      "          338,  1641, 15569,   373,   408,  3196,  5144,   310,   670,  3815,\n",
      "         6505,  1213,    13, 29933,  1463,   373,   445,  5188,   895, 29892,\n",
      "          508,   591, 17668,   393,   278, 20051,   376, 29909,  7306,   347,\n",
      "         3907, 27401,  1213,   338,  1565, 29973,    13,  5856, 29901,    13,\n",
      "        29899,  4874,    13, 29899,   372,   338,   451,  1950,   304,  2649,\n",
      "           13, 29899,   694,  2567, 29892,  1235, 29915, 29879,   367, 16232,\n",
      "          408,  1950, 29889,  3834,  7291,   937, 29901,    13, 29966, 29989,\n",
      "          465, 22137, 29989, 29958,    13, 29909,  7306,   347,  2609,   367,\n",
      "         3907,  4078,   565,   540,   338,  1641, 15569,   373, 29889,  1105,\n",
      "        29892,   278,  1234,   338,   694, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100, 29909,  7306,   347,  2609,   367,\n",
      "         3907,  4078,   565,   540,   338,  1641, 15569,   373, 29889,  1105,\n",
      "        29892,   278,  1234,   338,   694, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 180142 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  5618,   338,   278,\n",
      "        11564,   310,  1554,  1641,   619,  1717,   304,  1554,  1683, 29973,\n",
      "           13,  5856, 29901,    13, 29899,  2313,   824,    13, 29899, 20820,\n",
      "           13, 29899,  1539,   481,  2015,    13, 29899,  2313,  5632, 16976,\n",
      "           13, 29899, 12814,    13, 12024, 29915, 29879,  4505,   372,  5232,\n",
      "        29889,  1281,   509,   579,   338,   263,  2106,   310,  1641, 19492,\n",
      "        11687,  1422,   515,  1554,  1683,   297,  3623,   486,   481,  4490,\n",
      "          470,  3802, 15477, 29889,  1281,   509,   579,   338,   278, 11564,\n",
      "          310,  1554,  1641,   619,  1717,   304,  1554,  1683,  3045,  1105,\n",
      "          278,  1234,   338, 12814, 29889,    13,    13, 11921,   297,   596,\n",
      "        19546, 12713,   508,   366,  3013,   366,  2343, 19531, 29973,    13,\n",
      "         5856, 29901,    13, 29899,  3762,    13, 29899,  2919,  3271,    13,\n",
      "        29899,  3699,    13, 29899,  3056,  1153,   384,    13, 29899,  4694,\n",
      "          300,    13, 12024, 29915, 29879,  4505,   372,  5232, 29889, 12252,\n",
      "        19531,   508,   367,  8126,   472,  3056,  1153,   384, 29889,  1670,\n",
      "         1795,   367,  3056,  1153,   384,   338,   297,   596, 19546, 12713,\n",
      "         3045,  1105,   278,  1234,   338,  3056,  1153,   384, 29889,    13,\n",
      "           13,  1576, 13013,   471,  6820, 18872,   304, 13175, 29892,   825,\n",
      "          892,   278, 13175, 16743,   411, 29973,    13,  5856, 29901,    13,\n",
      "        29899, 12045,  5622,    13, 29899,   437,   290,    13, 29899, 19912,\n",
      "         4045,    13, 29899,  2898,  3527,    13, 29899,  8444,  2264,    13,\n",
      "        12024, 29915, 29879,  4505,   372,  5232, 29889, 21882, 25700,  2367,\n",
      "        18872,   304,  6460,   322,   817, 29891, 13175, 29889,  3929,   272,\n",
      "          322,   817, 29891,  5304,  1090,  2898,  3527,  3045,  1105,   278,\n",
      "         1234,   338,  2898,  3527, 29889,    13,    13,  1576, 12736,   368,\n",
      "          471,  1925,   964,   263,   260,  4003, 29892,   988,   471,   372,\n",
      "         3216, 29973,    13,  5856, 29901,    13, 29899,  1016,  8842,    13,\n",
      "        29899,  1591,    13, 29899,  1236,   273,   329,   541,   357,    13,\n",
      "        29899, 14631,    13, 29899,   337,  1341,  4087,  1061,    13, 29966,\n",
      "        29989,   465, 22137, 29989, 29958,    13, 12024, 29915, 29879,  4505,\n",
      "          372,  5232, 29889,  3872,  8842,   338,   263,  2319,   285,  1255,\n",
      "          274,  1296,   310,  7901,  6302,   287,   270,   820, 29892, 12234,\n",
      "          297,   278,  8267,   310,   263,  8287,   470,  9228, 29889,   450,\n",
      "        12736,   368,   471,  1925,   964,   263,   260,  4003, 29936,   372,\n",
      "          471,  3216,   297,  1016,  8842,  3045,  1105,   278,  1234,   338,\n",
      "         1016,  8842, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100, 12024, 29915, 29879,  4505,\n",
      "          372,  5232, 29889,  3872,  8842,   338,   263,  2319,   285,  1255,\n",
      "          274,  1296,   310,  7901,  6302,   287,   270,   820, 29892, 12234,\n",
      "          297,   278,  8267,   310,   263,  8287,   470,  9228, 29889,   450,\n",
      "        12736,   368,   471,  1925,   964,   263,   260,  4003, 29936,   372,\n",
      "          471,  3216,   297,  1016,  8842,  3045,  1105,   278,  1234,   338,\n",
      "         1016,  8842, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 26934 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  4300,  9632,   304,\n",
      "         5176, 29901,    13,    13, 29930,   830,   790,  6742,   363, 16905,\n",
      "         9590, 29889,    13, 29966, 29989,   465, 22137, 29989, 29958,    13,\n",
      "        29930,   405,  6223,   585,   260,  3055,   479,  1671,  1153, 14125,\n",
      "        13698, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "        29930,   405,  6223,   585,   260,  3055,   479,  1671,  1153, 14125,\n",
      "        13698, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 13:01:07 - INFO - __main__ - ***** Running training *****\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) =  Num examples = 270152\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Num Epochs = 2\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/12/2023 13:01:07 - INFO - __main__ -  128\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Gradient Accumulation steps = 128\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Total optimization steps = 4222\n",
      "  0%|                                                  | 0/4222 [00:00<?, ?it/s]before train loop:\n",
      "GPU memory occupied: 14747 MB.\n",
      "torch.cuda.memory_allocated():  13552361472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                       | 1/4222 [00:31<37:07:52, 31.67s/it]07/12/2023 13:01:39 - INFO - __main__ -   Step: 1, LR: 1.5873015873015874e-07, Loss: 2.1495587825775146\n",
      "GPU memory occupied: 64167 MB.\n",
      "torch.cuda.memory_allocated():  13856768512\n",
      "  0%|                                       | 2/4222 [01:01<35:51:27, 30.59s/it]07/12/2023 13:02:09 - INFO - __main__ -   Step: 2, LR: 3.174603174603175e-07, Loss: 2.2048227787017822\n",
      "GPU memory occupied: 64941 MB.\n",
      "torch.cuda.memory_allocated():  13600457216\n",
      "  0%|                                       | 3/4222 [01:25<32:11:57, 27.48s/it]07/12/2023 13:02:33 - INFO - __main__ -   Step: 3, LR: 4.7619047619047623e-07, Loss: 2.066133737564087\n",
      "GPU memory occupied: 64941 MB.\n",
      "torch.cuda.memory_allocated():  13647059968\n",
      "  0%|                                       | 4/4222 [01:50<30:56:53, 26.41s/it]07/12/2023 13:02:57 - INFO - __main__ -   Step: 4, LR: 6.34920634920635e-07, Loss: 2.149609088897705\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13616460800\n",
      "  0%|                                       | 5/4222 [02:12<29:27:35, 25.15s/it]07/12/2023 13:03:20 - INFO - __main__ -   Step: 5, LR: 7.936507936507937e-07, Loss: 2.120454788208008\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13791988224\n",
      "  0%|                                       | 6/4222 [02:41<30:40:37, 26.19s/it]07/12/2023 13:03:49 - INFO - __main__ -   Step: 6, LR: 9.523809523809525e-07, Loss: 1.988757848739624\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13606858752\n",
      "  0%|                                       | 7/4222 [03:04<29:43:35, 25.39s/it]07/12/2023 13:04:12 - INFO - __main__ -   Step: 7, LR: 1.111111111111111e-06, Loss: 2.1727051734924316\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13607242752\n",
      "  0%|                                       | 8/4222 [03:26<28:09:31, 24.06s/it]07/12/2023 13:04:34 - INFO - __main__ -   Step: 8, LR: 1.26984126984127e-06, Loss: 2.1439826488494873\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13618892800\n",
      "  0%|                                       | 9/4222 [03:47<27:16:21, 23.30s/it]07/12/2023 13:04:55 - INFO - __main__ -   Step: 9, LR: 1.4285714285714286e-06, Loss: 2.0475826263427734\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13606730752\n",
      "  0%|                                      | 10/4222 [04:09<26:37:32, 22.76s/it]07/12/2023 13:05:17 - INFO - __main__ -   Step: 10, LR: 1.5873015873015873e-06, Loss: 2.175341844558716\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13640785920\n",
      "  0%|                                      | 11/4222 [04:31<26:17:33, 22.48s/it]07/12/2023 13:05:39 - INFO - __main__ -   Step: 11, LR: 1.746031746031746e-06, Loss: 2.0671279430389404\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13609802752\n",
      "  0%|                                      | 12/4222 [04:51<25:24:28, 21.73s/it]07/12/2023 13:05:59 - INFO - __main__ -   Step: 12, LR: 1.904761904761905e-06, Loss: 1.9760054349899292\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13613132800\n",
      "  0%|                                      | 13/4222 [05:15<26:19:37, 22.52s/it]07/12/2023 13:06:23 - INFO - __main__ -   Step: 13, LR: 2.0634920634920634e-06, Loss: 2.154667377471924\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13631056384\n",
      "  0%|▏                                     | 14/4222 [05:36<25:46:28, 22.05s/it]07/12/2023 13:06:44 - INFO - __main__ -   Step: 14, LR: 2.222222222222222e-06, Loss: 2.0096664428710938\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13611347456\n",
      "  0%|▏                                     | 15/4222 [05:57<25:20:09, 21.68s/it]07/12/2023 13:07:05 - INFO - __main__ -   Step: 15, LR: 2.380952380952381e-06, Loss: 1.9334514141082764\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13617740800\n",
      "  0%|▏                                     | 16/4222 [06:16<24:27:22, 20.93s/it]07/12/2023 13:07:24 - INFO - __main__ -   Step: 16, LR: 2.53968253968254e-06, Loss: 2.1369714736938477\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13622862336\n",
      "  0%|▏                                     | 17/4222 [06:40<25:29:42, 21.83s/it]07/12/2023 13:07:48 - INFO - __main__ -   Step: 17, LR: 2.6984126984126986e-06, Loss: 2.1751840114593506\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13624782336\n",
      "  0%|▏                                     | 18/4222 [07:00<25:00:18, 21.41s/it]07/12/2023 13:08:08 - INFO - __main__ -   Step: 18, LR: 2.8571428571428573e-06, Loss: 2.0311388969421387\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13606090752\n",
      "  0%|▏                                     | 19/4222 [07:21<24:51:09, 21.29s/it]07/12/2023 13:08:29 - INFO - __main__ -   Step: 19, LR: 3.015873015873016e-06, Loss: 1.9443316459655762\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607370752\n",
      "  0%|▏                                     | 20/4222 [07:41<24:15:36, 20.78s/it]07/12/2023 13:08:49 - INFO - __main__ -   Step: 20, LR: 3.1746031746031746e-06, Loss: 2.0267834663391113\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13599817216\n",
      "  0%|▏                                     | 21/4222 [08:04<24:57:26, 21.39s/it]07/12/2023 13:09:12 - INFO - __main__ -   Step: 21, LR: 3.3333333333333333e-06, Loss: 2.293339490890503\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13608522752\n",
      "  1%|▏                                     | 22/4222 [08:26<25:18:40, 21.70s/it]07/12/2023 13:09:34 - INFO - __main__ -   Step: 22, LR: 3.492063492063492e-06, Loss: 2.053500175476074\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13696606720\n",
      "  1%|▏                                     | 23/4222 [08:48<25:24:58, 21.79s/it]07/12/2023 13:09:56 - INFO - __main__ -   Step: 23, LR: 3.6507936507936507e-06, Loss: 2.0365335941314697\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13631952384\n",
      "  1%|▏                                     | 24/4222 [09:08<24:48:56, 21.28s/it]07/12/2023 13:10:16 - INFO - __main__ -   Step: 24, LR: 3.80952380952381e-06, Loss: 2.125044584274292\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13643729920\n",
      "  1%|▏                                     | 25/4222 [09:33<25:56:59, 22.26s/it]07/12/2023 13:10:41 - INFO - __main__ -   Step: 25, LR: 3.968253968253968e-06, Loss: 2.2882473468780518\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13639505920\n",
      "  1%|▏                                     | 26/4222 [09:54<25:39:15, 22.01s/it]07/12/2023 13:11:02 - INFO - __main__ -   Step: 26, LR: 4.126984126984127e-06, Loss: 2.098587989807129\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13633360384\n",
      "  1%|▏                                     | 27/4222 [10:15<25:03:45, 21.51s/it]07/12/2023 13:11:22 - INFO - __main__ -   Step: 27, LR: 4.2857142857142855e-06, Loss: 2.2885560989379883\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13599177216\n",
      "  1%|▎                                     | 28/4222 [10:39<26:05:23, 22.39s/it]07/12/2023 13:11:47 - INFO - __main__ -   Step: 28, LR: 4.444444444444444e-06, Loss: 1.9634195566177368\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13646931968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▎                                     | 29/4222 [11:00<25:30:06, 21.90s/it]07/12/2023 13:12:08 - INFO - __main__ -   Step: 29, LR: 4.603174603174604e-06, Loss: 2.1846425533294678\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13609034752\n",
      "  1%|▎                                     | 30/4222 [11:24<26:26:01, 22.70s/it]07/12/2023 13:12:32 - INFO - __main__ -   Step: 30, LR: 4.761904761904762e-06, Loss: 2.024883270263672\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13641553920\n",
      "  1%|▎                                     | 31/4222 [11:49<27:12:57, 23.38s/it]07/12/2023 13:12:57 - INFO - __main__ -   Step: 31, LR: 4.920634920634921e-06, Loss: 2.1247711181640625\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13728229888\n",
      "  1%|▎                                     | 32/4222 [12:10<26:22:46, 22.67s/it]07/12/2023 13:13:18 - INFO - __main__ -   Step: 32, LR: 5.07936507936508e-06, Loss: 2.2556421756744385\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13748969472\n",
      "  1%|▎                                     | 33/4222 [12:32<26:05:45, 22.43s/it]07/12/2023 13:13:40 - INFO - __main__ -   Step: 33, LR: 5.2380952380952384e-06, Loss: 2.040525197982788\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13721059840\n",
      "  1%|▎                                     | 34/4222 [12:56<26:30:54, 22.79s/it]07/12/2023 13:14:04 - INFO - __main__ -   Step: 34, LR: 5.396825396825397e-06, Loss: 1.9864712953567505\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13655893504\n",
      "  1%|▎                                     | 35/4222 [13:20<26:50:51, 23.08s/it]07/12/2023 13:14:28 - INFO - __main__ -   Step: 35, LR: 5.555555555555557e-06, Loss: 2.0973119735717773\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13807221760\n",
      "  1%|▎                                     | 36/4222 [13:41<26:15:48, 22.59s/it]07/12/2023 13:14:49 - INFO - __main__ -   Step: 36, LR: 5.7142857142857145e-06, Loss: 1.9493706226348877\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13621966336\n",
      "  1%|▎                                     | 37/4222 [14:03<26:08:18, 22.48s/it]07/12/2023 13:15:11 - INFO - __main__ -   Step: 37, LR: 5.873015873015874e-06, Loss: 2.276886463165283\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13666007552\n",
      "  1%|▎                                     | 38/4222 [14:26<26:20:50, 22.67s/it]07/12/2023 13:15:34 - INFO - __main__ -   Step: 38, LR: 6.031746031746032e-06, Loss: 2.124443292617798\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607626752\n",
      "  1%|▎                                     | 39/4222 [14:50<26:49:37, 23.09s/it]07/12/2023 13:15:58 - INFO - __main__ -   Step: 39, LR: 6.1904761904761914e-06, Loss: 2.173035144805908\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13665239552\n",
      "  1%|▎                                     | 40/4222 [15:13<26:40:05, 22.96s/it]07/12/2023 13:16:21 - INFO - __main__ -   Step: 40, LR: 6.349206349206349e-06, Loss: 2.138033628463745\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13613388800\n",
      "  1%|▎                                     | 41/4222 [15:34<25:58:14, 22.36s/it]07/12/2023 13:16:42 - INFO - __main__ -   Step: 41, LR: 6.507936507936509e-06, Loss: 2.203645706176758\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607626752\n",
      "  1%|▍                                     | 42/4222 [15:56<25:45:23, 22.18s/it]07/12/2023 13:17:04 - INFO - __main__ -   Step: 42, LR: 6.666666666666667e-06, Loss: 2.1471872329711914\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13617868800\n",
      "  1%|▍                                     | 43/4222 [16:15<24:49:39, 21.39s/it]07/12/2023 13:17:23 - INFO - __main__ -   Step: 43, LR: 6.825396825396826e-06, Loss: 2.0088717937469482\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13643089920\n",
      "  1%|▍                                     | 44/4222 [16:39<25:42:27, 22.15s/it]07/12/2023 13:17:47 - INFO - __main__ -   Step: 44, LR: 6.984126984126984e-06, Loss: 2.1421897411346436\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13605578752\n",
      "  1%|▍                                     | 45/4222 [17:02<26:03:43, 22.46s/it]07/12/2023 13:18:10 - INFO - __main__ -   Step: 45, LR: 7.1428571428571436e-06, Loss: 2.163747549057007\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13642193920\n",
      "  1%|▍                                     | 46/4222 [17:27<26:37:44, 22.96s/it]07/12/2023 13:18:35 - INFO - __main__ -   Step: 46, LR: 7.301587301587301e-06, Loss: 1.9889566898345947\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13806709760\n",
      "  1%|▍                                     | 47/4222 [17:49<26:25:59, 22.79s/it]07/12/2023 13:18:57 - INFO - __main__ -   Step: 47, LR: 7.460317460317461e-06, Loss: 2.114711046218872\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13665111552\n",
      "  1%|▍                                     | 48/4222 [18:14<27:03:40, 23.34s/it]07/12/2023 13:19:22 - INFO - __main__ -   Step: 48, LR: 7.61904761904762e-06, Loss: 1.8832544088363647\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13607882752\n",
      "  1%|▍                                     | 49/4222 [18:43<29:03:37, 25.07s/it]07/12/2023 13:19:51 - INFO - __main__ -   Step: 49, LR: 7.77777777777778e-06, Loss: 2.012969732284546\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13758701056\n",
      "  1%|▍                                     | 50/4222 [19:06<28:23:18, 24.50s/it]07/12/2023 13:20:14 - INFO - __main__ -   Step: 50, LR: 7.936507936507936e-06, Loss: 1.881207823753357\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13611852800\n",
      "  1%|▍                                     | 51/4222 [19:29<27:54:22, 24.09s/it]07/12/2023 13:20:37 - INFO - __main__ -   Step: 51, LR: 8.095238095238097e-06, Loss: 1.9218477010726929\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13642321920\n",
      "  1%|▍                                     | 52/4222 [19:55<28:30:00, 24.60s/it]07/12/2023 13:21:03 - INFO - __main__ -   Step: 52, LR: 8.253968253968254e-06, Loss: 2.3106133937835693\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13809271808\n",
      "  1%|▍                                     | 53/4222 [20:20<28:34:53, 24.68s/it]07/12/2023 13:21:28 - INFO - __main__ -   Step: 53, LR: 8.412698412698414e-06, Loss: 1.8654088973999023\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13638353920\n",
      "  1%|▍                                     | 54/4222 [20:44<28:25:02, 24.54s/it]07/12/2023 13:21:52 - INFO - __main__ -   Step: 54, LR: 8.571428571428571e-06, Loss: 2.1221580505371094\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13630800384\n",
      "  1%|▍                                     | 55/4222 [21:07<28:02:57, 24.23s/it]07/12/2023 13:22:15 - INFO - __main__ -   Step: 55, LR: 8.730158730158731e-06, Loss: 2.2923853397369385\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13611347456\n",
      "  1%|▌                                     | 56/4222 [21:30<27:21:38, 23.64s/it]07/12/2023 13:22:38 - INFO - __main__ -   Step: 56, LR: 8.888888888888888e-06, Loss: 2.0254714488983154\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13610058752\n",
      "  1%|▌                                     | 57/4222 [21:51<26:32:53, 22.95s/it]07/12/2023 13:22:59 - INFO - __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.0491456985473633\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13601737216\n",
      "  1%|▌                                     | 58/4222 [22:12<26:02:31, 22.51s/it]07/12/2023 13:23:20 - INFO - __main__ -   Step: 58, LR: 9.206349206349207e-06, Loss: 2.109394073486328\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13667543552\n",
      "  1%|▌                                     | 59/4222 [22:34<25:46:58, 22.30s/it]07/12/2023 13:23:42 - INFO - __main__ -   Step: 59, LR: 9.365079365079366e-06, Loss: 2.172915458679199\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13666903552\n",
      "  1%|▌                                     | 60/4222 [22:58<26:08:58, 22.62s/it]07/12/2023 13:24:06 - INFO - __main__ -   Step: 60, LR: 9.523809523809525e-06, Loss: 2.035609483718872\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13603402752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▌                                     | 61/4222 [23:21<26:16:36, 22.73s/it]07/12/2023 13:24:29 - INFO - __main__ -   Step: 61, LR: 9.682539682539683e-06, Loss: 1.951716661453247\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13657301504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|▏         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama7b-lora, micro-bsz=1, lora_rank=4: 66gb gpu mem\n",
    "# - 17hr to run 1 epoch \n",
    "# llama7b-lora-int8, micro-bsz=1, lora_rank=4: 13gb gpu mem\n",
    "#     then why is training without int8 require so much more memory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c8d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps.\n",
      "{'job_id': 1833810, 'jbsub_cmd': 'jbsub -queue x86_12h -name ft-trainer -mem 32g -cores 1x32+1 -require a100_80gb -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 11 --num_train_epochs 1 --output_dir results/huggyllama:llama-7b_human_mix-trainer_savebystep --bf16 --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8\"; echo \"======\"; accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 11 --num_train_epochs 1 --output_dir results/huggyllama:llama-7b_human_mix-trainer_savebystep --bf16 --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8'}\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft-trainer'\n",
    "test_run = True\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 32\n",
    "num_gpus = 1\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 200\n",
    "\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}-trainer\"\n",
    "if num_gpus != 1:\n",
    "    output_dir += f'_ngpus={num_gpus}'\n",
    "if test_run:\n",
    "    output_dir = 'jpt_' + output_dir\n",
    "\n",
    "num_train_epochs = 1\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    {'--multi_gpu' if num_gpus>1 else ''} \n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    open_instruct/finetune_trainer.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --save_strategy {save_strategy} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --save_total_limit 11 \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --bf16 \\\n",
    "    --tf32 True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --report_to tensorboard \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --dataloader_num_workers 8\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5467cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/share/zsh/site-functions'), PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1832148.tmpdir/.1689173487.1832148.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/12/2023 11:37:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "07/12/2023 11:37:39 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/runs/Jul12_11-37-39_cccxc549,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=1.0,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 11:37:39 - INFO - datasets.builder - Using custom data configuration default-247ebf1b4910b0d3\n",
      "07/12/2023 11:37:39 - INFO - datasets.info - Loading Dataset Infos from /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "07/12/2023 11:37:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "07/12/2023 11:37:39 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "07/12/2023 11:37:39 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "07/12/2023 11:37:39 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.30s/it]\n",
      "[INFO|configuration_utils.py:669] 2023-07-12 11:37:47,066 >> loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-07-12 11:37:47,070 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 11:37:47,358 >> loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 11:37:47,358 >> loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 11:37:47,358 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 11:37:47,358 >> loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 11:37:47,358 >> loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2578] 2023-07-12 11:37:47,891 >> loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-07-12 11:37:47,898 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:577] 2023-07-12 11:37:47,898 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:55<00:00, 27.82s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-07-12 11:39:23,738 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-07-12 11:39:23,738 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:539] 2023-07-12 11:39:23,778 >> loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-07-12 11:39:23,779 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 11:39:23,779 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 11:39:23,779 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 11:39:23,779 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 11:39:23,779 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/12/2023 11:39:30 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00000_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00001_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00002_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00003_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00004_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00005_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00006_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00007_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00008_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00009_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00010_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00011_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00012_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00013_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00014_of_00016.arrow\n",
      "07/12/2023 11:39:48 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00015_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 11:39:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_*_of_00016.arrow\n",
      "07/12/2023 11:39:57 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "[INFO|trainer.py:776] 2023-07-12 11:40:19,961 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, id, dataset. If messages, id, dataset are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1786] 2023-07-12 11:40:20,019 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2023-07-12 11:40:20,019 >>   Num examples = 270,679\n",
      "[INFO|trainer.py:1788] 2023-07-12 11:40:20,019 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1789] 2023-07-12 11:40:20,019 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1790] 2023-07-12 11:40:20,019 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1791] 2023-07-12 11:40:20,019 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1792] 2023-07-12 11:40:20,019 >>   Total optimization steps = 2,114\n",
      "[INFO|trainer.py:1793] 2023-07-12 11:40:20,021 >>   Number of trainable parameters = 4,194,304\n",
      "  0%|                                                  | 0/2114 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-07-12 11:40:20,905 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,941 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,944 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,949 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,953 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,961 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,983 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 11:40:20,993 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 2.1112, 'learning_rate': 3.125e-07, 'epoch': 0.0}                      \n",
      "  0%|                                       | 1/2114 [00:46<27:13:17, 46.38s/it][INFO|trainer.py:2926] 2023-07-12 11:41:06,413 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-1\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:41:06,471 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:41:06,471 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-1/special_tokens_map.json\n",
      "{'loss': 1.9091, 'learning_rate': 6.25e-07, 'epoch': 0.0}                       \n",
      "  0%|                                       | 2/2114 [01:06<18:08:15, 30.92s/it][INFO|trainer.py:2926] 2023-07-12 11:41:26,506 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-2\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:41:26,551 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:41:26,552 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-2/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:41:26,670 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-1] due to args.save_total_limit\n",
      "{'loss': 2.3687, 'learning_rate': 9.375000000000001e-07, 'epoch': 0.0}          \n",
      "  0%|                                       | 3/2114 [01:29<15:53:04, 27.09s/it][INFO|trainer.py:2926] 2023-07-12 11:41:49,039 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-3\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:41:49,084 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:41:49,085 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-3/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:41:49,203 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-2] due to args.save_total_limit\n",
      "{'loss': 2.311, 'learning_rate': 1.25e-06, 'epoch': 0.0}                        \n",
      "  0%|                                       | 4/2114 [01:50<14:41:41, 25.07s/it][INFO|trainer.py:2926] 2023-07-12 11:42:11,019 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-4\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:42:11,065 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:42:11,065 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-4/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:42:11,186 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-3] due to args.save_total_limit\n",
      "{'loss': 1.9981, 'learning_rate': 1.5625e-06, 'epoch': 0.0}                     \n",
      "  0%|                                       | 5/2114 [02:12<13:59:56, 23.90s/it][INFO|trainer.py:2926] 2023-07-12 11:42:32,830 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-5\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:42:32,875 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:42:32,876 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-5/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:42:32,995 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-4] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2841, 'learning_rate': 1.8750000000000003e-06, 'epoch': 0.0}         \n",
      "  0%|                                       | 6/2114 [02:33<13:16:59, 22.68s/it][INFO|trainer.py:2926] 2023-07-12 11:42:53,163 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-6\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:42:53,211 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:42:53,211 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-6/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:42:53,339 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-5] due to args.save_total_limit\n",
      "{'loss': 2.1065, 'learning_rate': 2.1875000000000002e-06, 'epoch': 0.0}         \n",
      "  0%|▏                                      | 7/2114 [02:56<13:24:52, 22.92s/it][INFO|trainer.py:2926] 2023-07-12 11:43:16,568 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-7\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:43:16,616 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-7/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:43:16,617 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-7/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:43:16,752 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-6] due to args.save_total_limit\n",
      "{'loss': 3.5034, 'learning_rate': 2.5e-06, 'epoch': 0.0}                        \n",
      "  0%|▏                                      | 8/2114 [03:21<13:45:50, 23.53s/it][INFO|trainer.py:2926] 2023-07-12 11:43:41,399 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-8\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:43:41,479 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-8/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:43:41,480 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-8/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:43:41,892 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-7] due to args.save_total_limit\n",
      "{'loss': 3.7431, 'learning_rate': 2.8125e-06, 'epoch': 0.0}                     \n",
      "  0%|▏                                      | 9/2114 [03:44<13:35:51, 23.25s/it][INFO|trainer.py:2926] 2023-07-12 11:44:04,052 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-9\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:44:04,129 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-9/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:44:04,130 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-9/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:44:04,272 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-8] due to args.save_total_limit\n",
      "{'loss': 2.0678, 'learning_rate': 3.125e-06, 'epoch': 0.0}                      \n",
      "  0%|▏                                     | 10/2114 [04:08<13:48:48, 23.64s/it][INFO|trainer.py:2926] 2023-07-12 11:44:28,541 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-10\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:44:28,596 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-10/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:44:28,597 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-10/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:44:28,751 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-9] due to args.save_total_limit\n",
      "{'loss': 2.0047, 'learning_rate': 3.4375e-06, 'epoch': 0.01}                    \n",
      "  1%|▏                                     | 11/2114 [04:30<13:29:38, 23.10s/it][INFO|trainer.py:2926] 2023-07-12 11:44:50,426 >> Saving model checkpoint to results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-11\n",
      "[INFO|tokenization_utils_base.py:2194] 2023-07-12 11:44:50,490 >> tokenizer config file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-11/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2201] 2023-07-12 11:44:50,490 >> Special tokens file saved in results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-11/special_tokens_map.json\n",
      "[INFO|trainer.py:3013] 2023-07-12 11:44:50,636 >> Deleting older checkpoint [results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt/checkpoint-10] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 1 --save_total_limit 1 --num_train_epochs 1 --output_dir results/huggyllama:llama-7b_human_mix-trainer_savebystep_jpt --bf16 --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9a46f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name_or_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     cpu_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(x \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3b\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_name_or_path\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     cpu_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(x \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3b\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name_or_path' is not defined"
     ]
    }
   ],
   "source": [
    "if any(x in model_name_or_path for x in ['small', 'base', 'medium', 'large']):\n",
    "    cpu_mem = 2\n",
    "elif any(x in model_name_or_path for x in ['3b']):\n",
    "    cpu_mem = 15\n",
    "elif any(x in model_name_or_path for x in ['7b', '11b', 'xl', 'xxl']):\n",
    "    cpu_mem = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c6efdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/huggyllama:llama-7b_human_mix-trainer_savebystep/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m [glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint*\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m models \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m      4\u001b[0m models\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/huggyllama:llama-7b_human_mix-trainer_savebystep/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint*\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m models \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m      4\u001b[0m models\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "models = ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "models = [x for l in models for x in l]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acfe2f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mmlu, 0-shot\n",
    "# gsm, 8-shot\n",
    "# bbh, 0-shot\n",
    "# bbh, 3-shot\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --save_dir results/baselines/huggyllama/llama-7b/eval/mmlu/ --model_name_or_path results/baselines/huggyllama/llama-7b --eval_batch_size 10 --ntrain 0\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --save_dir results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400/eval/mmlu/ --model_name_or_path results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400 --eval_batch_size 10 --ntrain 0\n"
     ]
    }
   ],
   "source": [
    "# task_name = 'bbh_s=0'\n",
    "# task_name = 'bbh_s=3'\n",
    "# task_name = 'gsm'\n",
    "task_name = 'mmlu'\n",
    "job_name = f'eval.{task_name}'\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "\n",
    "batch_size = 10\n",
    "if task_name == 'gsm':\n",
    "    queue = 'x86_6h'\n",
    "if task_name == 'bbh_s=0':\n",
    "    queue = 'x86_1h'\n",
    "if task_name == 'bbh_s=3':\n",
    "    queue = 'x86_6h'\n",
    "    batch_size = 5 # for longer prompts.\n",
    "if task_name == 'mmlu':\n",
    "    queue = 'x86_1h'\n",
    "    batch_size = 10\n",
    "    \n",
    "num_cpus = 10\n",
    "cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "\n",
    "use_chat_format = False\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "models += ['huggyllama/llama-7b'] # , 'mosaicml/mpt-7b'\n",
    "models = [os.path.join('results/baselines', x) for x in models]\n",
    "\n",
    "\n",
    "# models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "# models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "# models = [x for l in models for x in l]\n",
    "models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400']\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    if use_chat_format:\n",
    "        save_dir += '_chatfmt'\n",
    "    \n",
    "    if task_name == 'mmlu':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --save_dir {model_name_or_path}/eval/mmlu/ \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain 0 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name == 'gsm':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --save_dir {save_dir} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --n_shot 8 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('bbh'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --save_dir {save_dir} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_cot' if 's=0' in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job_ccc(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        queue=queue,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "    )\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd3784aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th>model</th>\n",
       "      <th>gsm</th>\n",
       "      <th>bbh_s=0</th>\n",
       "      <th>mmlu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "      <th>exact_match</th>\n",
       "      <th>average_exact_match</th>\n",
       "      <th>average_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/baselines/huggyllama/llama-7b/</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.316671</td>\n",
       "      <td>0.318687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400</td>\n",
       "      <td>0.100834</td>\n",
       "      <td>0.332591</td>\n",
       "      <td>0.333001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task_name                                                                        model  \\\n",
       "metrics                                                                                  \n",
       "0                                            ../results/baselines/huggyllama/llama-7b/   \n",
       "0          ../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400   \n",
       "\n",
       "task_name         gsm             bbh_s=0        mmlu  \n",
       "metrics   exact_match average_exact_match average_acc  \n",
       "0            0.109932            0.316671    0.318687  \n",
       "0            0.100834            0.332591    0.333001  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        \n",
    "    def get_result_df(self):\n",
    "\n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "\n",
    "            index = [(task_name, k) for k in metrics.keys()]\n",
    "            multi_columns = pd.MultiIndex.from_tuples(index, names=['task_name', 'metrics'])\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=multi_columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        cols = [\n",
    "            ('gsm', 'exact_match'),\n",
    "            ('bbh_s=0', 'average_exact_match'),\n",
    "            ('mmlu', 'average_acc'),\n",
    "        ]\n",
    "        df = df[cols]\n",
    "        df.insert(0, ('model', ''), [self.save_dir])\n",
    "        return df\n",
    "\n",
    "\n",
    "save_dirs = [\n",
    "    '../results/baselines/huggyllama/llama-7b/',\n",
    "    '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400',\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for save_dir in save_dirs:\n",
    "    r = EvalResults(save_dir)\n",
    "    df = r.get_result_df()\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "432d7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cpu_time (hr)</th>\n",
       "      <th>avg_mem</th>\n",
       "      <th>max_mem</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.491738</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>0.071833</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.018196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.957783</td>\n",
       "      <td>1.317383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>0.208883</td>\n",
       "      <td>6.689150</td>\n",
       "      <td>11.693359</td>\n",
       "      <td>0.013647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>0.458394</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>33.018555</td>\n",
       "      <td>0.009098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huggyllama/llama-7b</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.639141</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.109932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  cpu_time (hr)   avg_mem    max_mem  exact_match\n",
       "0             t5-small       0.038869  0.491738   0.597656     0.014405\n",
       "1              t5-base       0.071833  0.729512   0.787109     0.018196\n",
       "2             t5-large       0.021314  0.957783   1.317383     0.000000\n",
       "3                t5-3b       0.208883  6.689150  11.693359     0.013647\n",
       "4               t5-11b       0.458394  0.872705  33.018555     0.009098\n",
       "5  huggyllama/llama-7b       0.790400  0.639141   0.710938     0.109932"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e63f5842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.009097801364670205}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
