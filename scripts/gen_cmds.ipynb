{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'ppc64le', 'cluster': 'dcs'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info\n",
    "\n",
    "info = get_host_info()\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##\n",
    "##\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     conda_env='open-instruct',\n",
    "#     cwd=os.path.dirname(os.getcwd()),\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir\n",
    "# )\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '00:33:12'\n",
    "n = 15\n",
    "# total = 1515; nnodes = 1\n",
    "# total = 2083; nnodes = 1\n",
    "total = 1587; nnodes = 1\n",
    "# total = 1041; nnodes = 1\n",
    "# total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_data_points = 200000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up checkpoints `optimizer.bin` to save disk space. \n",
    "# (e.g., 7b model, ~8*7=56GB for storing gradient/momentum in `optimizer.bin`)\n",
    "\n",
    "import glob, os\n",
    "\n",
    "def cleanup_checkpoints(save_dir, test_run=False):\n",
    "\n",
    "    checkpoints = glob.glob(os.path.join(save_dir, 'checkpoint-*'))\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "    checkpoints = checkpoints[:-1]\n",
    "    \n",
    "    if not checkpoints: return\n",
    "\n",
    "    for ckpt_path in checkpoints:\n",
    "        optimizer_bin_path = os.path.join(ckpt_path, 'optimizer.bin')\n",
    "        if os.path.isfile(optimizer_bin_path):\n",
    "            print(optimizer_bin_path)\n",
    "            if not test_run:\n",
    "                os.remove(optimizer_bin_path)\n",
    "        \n",
    "        \n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "exp_dirs = [\n",
    "    '../results/ft1',\n",
    "    '../results/ft2',\n",
    "    '../results/oi3',\n",
    "    '../results/oi4',\n",
    "    '../results/oi4_perf_cross_time',\n",
    "    '../results/oi4_tulu_v1_human_mix',\n",
    "    '../results/oi4_flanv2_prune_with_hmv1_model',\n",
    "    '../results/oi4_flan_v2_vary_subsetsize',\n",
    "]\n",
    "\n",
    "print('Remove extra files (e.g., optimizer.bin) for non-latest checkpoints:')\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    for run_name in os.listdir(exp_dir):\n",
    "        save_dir = os.path.join(exp_dir, run_name)\n",
    "        if os.path.islink(save_dir): continue\n",
    "        cleanup_checkpoints(save_dir, test_run=test_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/mistral-7b+lora:r=256:a=256/ultrachat/numtoks_in_neg/inds_prune_size=50000_ep=3.pkl does not exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 409\u001b[0m\n\u001b[1;32m    407\u001b[0m     p \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_inds_dir, scoring_fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minds_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpacing_fn\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(p):\n\u001b[0;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exists.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    410\u001b[0m     subsample_inds_file_list\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    414\u001b[0m test_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: path=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/mistral-7b+lora:r=256:a=256/ultrachat/numtoks_in_neg/inds_prune_size=50000_ep=3.pkl does not exists."
     ]
    }
   ],
   "source": [
    "def compute_mixture_num_samples(mixture, max_train_samples):\n",
    "    s = sum(mixture.values())\n",
    "    mixture = {k: int(max_train_samples*v/s) for k, v in mixture.items()}\n",
    "    return mixture\n",
    "\n",
    "add_hardwarespec_to_dirname = False\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  512 if arch == 'ppc64le' else 64\n",
    "\n",
    "\n",
    "evaluation_strategy = 'no'\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100; save_total_limit = 1\n",
    "\n",
    "\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "# model_name_or_path = 'results/baselines/gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-1.4b'; abbr_model_name = 'pythia-1.4b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-2.8b'; abbr_model_name = 'pythia-2.8b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-6.9b'; abbr_model_name = 'pythia-6.9b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "\n",
    "subsample_mixture_list = []\n",
    "# subsample_mixture_list += [\n",
    "#     {k: max_train_samples} for k in datasets\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     ('humanmix', dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items()))\n",
    "# ] # humanmix mixture.\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot':  0.13568177819252014, 'flan_v2': 0.3957784175872803, \n",
    "#      'dolly': 0.05964866653084755, 'oasst1': 0.4088916480541229}.items())\n",
    "# ] # gpt2-medium_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "# ] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "#         )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_normalized_list = []\n",
    "# subsample_mixture_normalized_list += [('uniform:1200k_doremiv2', # llama-7b_humanmix_uniform:1200k_doremiv2.json\n",
    "#                                        {\"cot\": 0.11419677734375, \"dolly\": 0.1024169921875, \"flan_v2\": 0.204833984375, \"oasst1\": 0.204833984375})]\n",
    "## 10 for trying out datamodels\n",
    "# mixes = [{'cot': 0.37664033529374275,\n",
    "#   'dolly': 0.0874640765523398,\n",
    "#   'flan_v2': 0.39740799933549775,\n",
    "#   'oasst1': 0.1384875888184196},\n",
    "#  {'cot': 0.23064419241874784,\n",
    "#   'dolly': 0.04693354147889885,\n",
    "#   'flan_v2': 0.72121745986295,\n",
    "#   'oasst1': 0.0012048062394032465},\n",
    "#  {'cot': 0.11244721555034376,\n",
    "#   'dolly': 0.21997027355988638,\n",
    "#   'flan_v2': 0.5826671754210359,\n",
    "#   'oasst1': 0.08491533546873392},\n",
    "#  {'cot': 0.27704626812045546,\n",
    "#   'dolly': 0.5712282144637615,\n",
    "#   'flan_v2': 0.024940119654536592,\n",
    "#   'oasst1': 0.12678539776124645},\n",
    "#  {'cot': 0.0024519793352964607,\n",
    "#   'dolly': 0.13274603201304974,\n",
    "#   'flan_v2': 0.012268378167304219,\n",
    "#   'oasst1': 0.8525336104843496},\n",
    "#  {'cot': 0.08065633865016615,\n",
    "#   'dolly': 0.41886215168938545,\n",
    "#   'flan_v2': 0.21723932820070485,\n",
    "#   'oasst1': 0.2832421814597436},\n",
    "#  {'cot': 0.13878643021160036,\n",
    "#   'dolly': 0.05686171157146557,\n",
    "#   'flan_v2': 0.6701353469446995,\n",
    "#   'oasst1': 0.13421651127223455},\n",
    "#  {'cot': 0.2461125374866837,\n",
    "#   'dolly': 0.09774240280444893,\n",
    "#   'flan_v2': 0.13974091986040005,\n",
    "#   'oasst1': 0.5164041398484672},\n",
    "#  {'cot': 0.4069781049152398,\n",
    "#   'dolly': 0.06318759506033228,\n",
    "#   'flan_v2': 0.09504719644992135,\n",
    "#   'oasst1': 0.4347871035745066},\n",
    "#  {'cot': 0.22379693013848484,\n",
    "#   'dolly': 0.30565901275011814,\n",
    "#   'flan_v2': 0.15457716965000887,\n",
    "#   'oasst1': 0.31596688746138824}]\n",
    "\n",
    "# mixes = [\n",
    "#     {'cot': 0.46638974, 'dolly': 0.01456044, 'flan_v2': 0.50886009, 'oasst1': 0.01018973},\n",
    "#     {'cot': 0.39744481, 'dolly': 0.00472114, 'flan_v2': 0.59104177, 'oasst1': 0.00679229},\n",
    "# ]\n",
    "\n",
    "# subsample_mixture_normalized_list += [('', d) for d in mixes]\n",
    "# subsample_mixture_normalized_list += [('humanmix', # humanmix\n",
    "#                                        {'cot': 0.48785105, 'dolly': 0.00732313, 'flan_v2': 0.48785105, 'oasst1': 0.01697478})]\n",
    "# subsample_mixture_normalized_list = [(x[0],  compute_mixture_num_samples(x[1], max_train_samples)) \n",
    "#                                      for x in subsample_mixture_normalized_list]\n",
    "# subsample_mixture_list += subsample_mixture_normalized_list\n",
    "\n",
    "\n",
    "subsample_mixture_list = [('',None)]\n",
    "subsample_inds_file_list = [None]\n",
    "\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "\n",
    "\n",
    "\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subsample_inds_file_abbr_fn(x):\n",
    "    s = os.path.basename(x).split('.pkl')[0]\n",
    "    if s.startswith('inds_'):\n",
    "        scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "        pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "        return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "# # ft1: reproduce open-instruct table with llama7b\n",
    "# # job_name = 'ft1'; num_train_epochs_list = [2]\n",
    "# # job_name = 'ft1_ep=1'; num_train_epochs_list = [1] # train for 1 epoch (baseline for comparison.)\n",
    "# job_name = 'ft1_ep=2'; num_train_epochs_list = [2]\n",
    "\n",
    "# # # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# # # train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# # # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# # # # # train_file = 'data/processed/wpq/cot_flanv2_data.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "# # # # # train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'hmv1'\n",
    "# # # train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# # train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# # # max_train_samples_list = [120]; save_steps = 1; save_total_limit = 100\n",
    "\n",
    "\n",
    "\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "# # train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# # train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# # train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "# # train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# # train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# # train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "# # ft2: test mixture weights\n",
    "# # vary mixture weights\n",
    "# job_name = 'ft2'\n",
    "\n",
    "# # oi3: instruction tuning performance w.r.t. steps.\n",
    "# job_name = 'oi3'\n",
    "\n",
    "# # oi4: data pruning \n",
    "# job_name = 'oi4_flan_v2_vary_subsetsize'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.1, .3, .5]]; num_train_epochs_list = [2]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # oi4_perf_cross_time: perf cross time on flan_v2\n",
    "# job_name = 'oi4_perf_cross_time'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# # ## oi4_flanv2_prune_with_hmv1_model\n",
    "# job_name = 'oi4_flanv2_prune_with_hmv1_model'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b_ft=hmv1/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# ## tulu mix v1.\n",
    "# dataset = 'tulu_v1_human_mix'; train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'tuluv1hm'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [30000]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/{dataset}/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'flan2022_1m'; train_file = 'data/processed/flan2022/flan2022_1m_data.jsonl'; abbr_train_file = 'flan2022_1m'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# ## full data\n",
    "# # max_train_samples_list = [1000000]; num_train_epochs_list = [1]\n",
    "# # subsample_inds_file_list = ['']\n",
    "# # subset\n",
    "# max_train_samples_list = [200000]; num_train_epochs_list = [1]\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "#     # not that helpful\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     ## gradnorm outputs\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "#     ## random baselines.\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "#     ## kmeans on grads\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# # job_name = f'oi4_{dataset}_ep=3'\n",
    "# job_name = f'oi5_tulu_v1_mix:llama-7b' # re-run to see if transformers upgrade altered eval performance.\n",
    "# # save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# max_train_samples_list = [50000]; num_train_epochs_list = [3]\n",
    "# subsample_inds_file_list = [\n",
    "#     # random baselines\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "# #     # correlated statistics\n",
    "# #     os.path.join(data_inds_dir, 'log_prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'log_prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_decr.pkl'),\n",
    "# #     # grad norm\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "# #     # kmeans\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_decr.pkl'),\n",
    "# # #     # kcos only 50k data\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos1np.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "## oi5: try curriculum learning\n",
    "# \n",
    "scoring_fn_and_pacing_fn = []\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# M = 150_000; dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "\n",
    "# scoring_fn_list = ['log_prob_neg']\n",
    "# pacing_fn_list = [\n",
    "# #     f'prune_size={M}_ep=3',\n",
    "#     f'singlestep_size={M}_startingfrac=0.05',\n",
    "# #     f'singlestep_size={M}_startingfrac=0.1',\n",
    "# #     f'singlestep_size={M}_startingfrac=0.2',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "# ]\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "M =  50_000; dataset = 'ultrachat'; train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "evaluation_strategy = 'steps'; eval_steps = 100; # 50k train, 1k eval, apprx. 800 steps, added compute ( 1*(800/100) ) / 50 = 0.16 more time.\n",
    "\n",
    "# scoring_fn_list = ['log_prob_neg', 'el2n_agg=mean', 'logit_margin_neg', 'grad_loraB_l2n',] #  'kmeansl2_emb=text+embedding_nc=3000_incr'\n",
    "# scoring_fn_list = ['rhov1_log_prob', 'rhov1_log_prob_neg']\n",
    "scoring_fn_list = ['numtoks_in_neg', 'numtoks_out_neg', 'numtoks_total_neg']\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep=3',\n",
    "]\n",
    "scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/{abbr_model_name}+lora:r=256:a=256/{dataset}/'\n",
    "num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "dataloader_sampler = 'SequentialSampler'\n",
    "\n",
    "# random baselines\n",
    "# scoring_fn_list = ['random_s=0']; pacing_fn_list = [f'prune_size={M}_ep=1'] # gives advantage to random baselines. # 'random_s=1', 'random_s=2'\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "subsample_inds_file_list = []\n",
    "for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "    data_inds_dir = (f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/'\n",
    "                     f'{abbr_model_name}+lora:r=256:a=256/{dataset}/')\n",
    "    p = os.path.join(data_inds_dir, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "    if not os.path.isfile(p):\n",
    "        raise ValueError(f'path={p} does not exists.')\n",
    "    subsample_inds_file_list.append(p)\n",
    "\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12 # llama-7b on 100k. data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b on 100k. data\n",
    "# nodes = 10; num_gpus = 6; gpu_type = 'v100'; job_duration = 36 # llama-7b on 100k. data\n",
    "\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 18 # llama-7b on 400k data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 30 # llama-7b on 600k data\n",
    "\n",
    "# nodes = 1; num_gpus = 1; gpu_type = 'v100'; job_duration = 6  # gpt2\n",
    "# nodes = 2; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-1.4b\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-2.8b|6.9b\n",
    "\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "\n",
    "per_device_train_batch_size = 2; total_batch_size = 128 # 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = True\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_mixture_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     mix_name_and_subsample_mixture,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "    mix_name, subsample_mixture = mix_name_and_subsample_mixture\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "        \n",
    "    if job_name == 'ft2':\n",
    "        if subsample_mixture is not None:\n",
    "            assert(abbr_train_file=='all')\n",
    "            output_dirname += \\\n",
    "                '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "            \n",
    "    if job_name == 'oi3':\n",
    "        output_dirname += '_'+mix_name\n",
    "        \n",
    "#     if job_name.startswith('oi4'):\n",
    "    if subsample_inds_file:\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            '_dtype='+torch_dtype+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    \n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers=16 \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type=linear \\\n",
    "        --warmup_ratio=0.03 \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy={evaluation_strategy} \\\n",
    "        {'--eval_steps'+str(eval_steps) if eval_steps else ''} \\\n",
    "        --report_to tensorboard wandb \\\n",
    "        --logging_strategy=steps \\\n",
    "        --logging_first_step \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ba263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e614b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/baselines/huggyllama/llama-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "['</s>']\n",
      "[18637, 829, 29879, 29958, 29879, 381]\n",
      "['Hey', '</', 's', '>', 's', 'ir']\n",
      "results/baselines/NousResearch/Llama-2-7b-hf\n",
      "[2]\n",
      "['</s>']\n",
      "[18637, 829, 29879, 29958, 29879, 381]\n",
      "['Hey', '</', 's', '>', 's', 'ir']\n",
      "results/baselines/mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "['</s>']\n",
      "[17162, 2, 10116]\n",
      "['Hey', '</s>', 'sir']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok_path = 'huggyllama/llama-7b'\n",
    "# tok_path = \"NousResearch/Llama-2-7b-hf\"\n",
    "# tok_path = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "for tok_path in [\n",
    "    'results/baselines/huggyllama/llama-7b',\n",
    "    \"results/baselines/NousResearch/Llama-2-7b-hf\",\n",
    "    'results/baselines/mistralai/Mistral-7B-v0.1',\n",
    "]:\n",
    "    print(tok_path)\n",
    "    tok = AutoTokenizer.from_pretrained(tok_path)\n",
    "#     print(tok)\n",
    "\n",
    "    print(tok.encode(\"</s>\", add_special_tokens = False)) # [2]\n",
    "    print(tok.tokenize(\"</s>\", add_special_tokens = False)) # ['</s>']\n",
    "    print(tok.encode(\"Hey</s>sir\", add_special_tokens = False)) # [18637, 829, 29879, 29958, 29879, 381]\n",
    "    print(tok.tokenize(\"Hey</s>sir\", add_special_tokens = False)) # ['Hey', '</', 's', '>', 's', 'ir']\n",
    "\n",
    "\n",
    "# s = \"Hll hw are ?\"\n",
    "# tok.backend_tokenizer.normalizer.normalize_str(s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_all_symlinks(directory, verbose=False):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files + dirs:\n",
    "            path = os.path.join(root, name)\n",
    "            if os.path.islink(path):\n",
    "                os.unlink(path)\n",
    "                if verbose:\n",
    "                    print(f\"Removed symlink: {path}\")\n",
    "                \n",
    "import uuid\n",
    "\n",
    "def create_unique_symlinks(file_paths, verbose=False):\n",
    "    \"\"\"Create symlinks for each `file` in `files` in the same directory, with a unique name. \"\"\"\n",
    "    dirs = [os.path.dirname(x) for x in file_paths]\n",
    "\n",
    "    symlink_path_dict = {}\n",
    "    for directory, path in zip(dirs, file_paths):\n",
    "        if os.path.isdir(path):\n",
    "            symlink_name = f\"symlink_{str(uuid.uuid4())[:8]}\"  # Generate a unique symlink name\n",
    "            symlink_path = os.path.join(directory, symlink_name)\n",
    "            try:\n",
    "                os.symlink(os.path.abspath(path), symlink_path)\n",
    "                if verbose:\n",
    "                    print(f\"Created symlink: {symlink_path} -> {path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Failed to create symlink: {path}. Error: {e}\")\n",
    "            symlink_path_dict.update({path: symlink_path})\n",
    "    return symlink_path_dict\n",
    "\n",
    "\n",
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    model_name_or_path = model_name_or_path.lower()\n",
    "    if any(x in model_name_or_path for x in ['gpt2-medium', 'pythia-160m']):\n",
    "        return 50, 1\n",
    "    if any(x in model_name_or_path for x in ['gpt-xl']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp']):\n",
    "            return 16, 1\n",
    "        else:\n",
    "            return 32, 1\n",
    "    if any(x in model_name_or_path for x in ['llama', 'mistral', 'zephyr', 'pythia-1.4b', 'pythia-2.8b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 5, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    if any(x in model_name_or_path for x in ['pythia-6.9b', 'dolly-v2-7b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'mmlu_s=0', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 4, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    return 10, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b68375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alpacafarm_ann=chatgpt_chatfmt', 'results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3')\n",
      "('alpacafarm_ann=chatgpt_chatfmt', 'results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3')\n",
      "#cmds:  2 \n",
      "\n",
      "python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3\" --save_dir \"results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3/eval/alpacafarm_ann=chatgpt_chatfmt\" --eval_batch_size 5 --annotators_config chatgpt --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.alpacafarm_ann=chatgpt_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3\" --save_dir \"results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3/eval/alpacafarm_ann=chatgpt_chatfmt\" --eval_batch_size 5 --annotators_config chatgpt --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.alpacafarm_ann=chatgpt_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exp_dir = ''\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "\n",
    "\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "use_slow_tokenizer = False\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0', # \n",
    "    'mmlu_s=5', # ~1hr\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "# task_names = ['alpacafarm_ann=chatgpt']\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "# #     'gpt2',\n",
    "# #     'gpt2-medium',\n",
    "# #     'huggyllama/llama-7b', \n",
    "# #     'mistralai/Mistral-7B-v0.1',\n",
    "# #     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "# #     'NousResearch/Llama-2-7b-hf',\n",
    "#     'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "#     'HuggingFaceH4/zephyr-7b-alpha',\n",
    "# #     'HuggingFaceH4/zephyr-7b-beta',\n",
    "# #     'EleutherAI/pythia-1.4b',\n",
    "# #     'EleutherAI/pythia-2.8b',\n",
    "# #     'EleutherAI/pythia-6.9b',\n",
    "# #     'databricks/dolly-v2-7b',\n",
    "# ]]\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # ## baseline re-eval after merge upstream/main\n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b',\n",
    "# ]]\n",
    "# subdir_path_list += ['results/ft1/llama-7b_humanmix']\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1\n",
    "# exp_dir = 'results/ft1'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1_ep=1\n",
    "# exp_dir = 'results/ft1_ep=1'\n",
    "# exp_dir = 'results/ft1_ep=2'\n",
    "# subdir_filter_fn = lambda x: 'ultrachat200k_before' in x\n",
    "# # task_names = task_names_chatfmt\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "# task_names = ['alpacafarm_ann=chatgpt']; task_names = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "# ## ft2\n",
    "# exp_dir = 'results/ft2/'\n",
    "# create_symlinks = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['llama-7b'])\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## llama-7b time-series 400k, 600k\n",
    "# exp_dir = 'results/oi3/'\n",
    "# include_checkpoints = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['400k', '600k']) # , '600k'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # oi4 include checkpoints!\n",
    "# # exp_dir = 'results/oi4_perf_cross_time/'\n",
    "# # exp_dir = 'results/oi4_tulu_v1_human_mix/'\n",
    "# # exp_dir = 'results/oi4_flanv2_prune_with_hmv1_model/'\n",
    "# # exp_dir = 'results/oi4_flan2022_1m/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix_ep=3/'\n",
    "# include_checkpoints = True\n",
    "# include_checkpoints = False\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['log_prob_decr', 'el2n_agg=mean_incr', 'logit_margin_decr', 'grad_loraB'])\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "# task_names = task_names_chatfmt # eval alpacafarm only\n",
    "\n",
    "# # oi4 without checkpoint \n",
    "# # exp_dir = 'results/oi4/'\n",
    "# exp_dir = 'results/oi4_flan_v2_vary_subsetsize/'\n",
    "# task_names = task_names_chatfmt\n",
    "\n",
    "# oi5\n",
    "# exp_dir = 'results/oi5_tulu_v1_mix:llama-7b/'\n",
    "exp_dir = 'results/oi5_ultrachat:mistral-7b'\n",
    "task_names = ['alpacafarm_ann=chatgpt']; task_names = [x+'_chatfmt' for x in task_names]\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    if 'gpt2' in ft_args_model_name_or_path:\n",
    "        tydiqa_max_context_length = 400 # max ctx len without exceeding max_seq_len\n",
    "    else:\n",
    "        tydiqa_max_context_length = 512\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length {tydiqa_max_context_length} \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c86e5_row0_col0, #T_c86e5_row1_col0, #T_c86e5_row2_col0, #T_c86e5_row3_col0, #T_c86e5_row4_col0, #T_c86e5_row5_col0, #T_c86e5_row6_col0, #T_c86e5_row7_col0, #T_c86e5_row8_col0, #T_c86e5_row9_col0, #T_c86e5_row10_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_c86e5_row0_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row0_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row0_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row0_col5, #T_c86e5_row0_col6, #T_c86e5_row0_col7, #T_c86e5_row1_col2, #T_c86e5_row3_col3, #T_c86e5_row5_col4, #T_c86e5_row7_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row1_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row1_col4, #T_c86e5_row2_col4, #T_c86e5_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row2_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row2_col2, #T_c86e5_row4_col2, #T_c86e5_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e7d7ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row2_col5, #T_c86e5_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row2_col6, #T_c86e5_row2_col7, #T_c86e5_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row3_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row3_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c5ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row3_col6, #T_c86e5_row10_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row4_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row5_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row6_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row6_col5, #T_c86e5_row7_col2, #T_c86e5_row8_col1, #T_c86e5_row9_col3, #T_c86e5_row9_col7, #T_c86e5_row10_col4, #T_c86e5_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row7_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row7_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row7_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row8_col3, #T_c86e5_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row9_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row9_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row9_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row9_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row10_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c86e5_row10_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c86e5_row10_col5, #T_c86e5_row10_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c86e5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c86e5_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_c86e5_level0_col1\" class=\"col_heading level0 col1\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_c86e5_level0_col2\" class=\"col_heading level0 col2\" >GSM/CoT</th>\n",
       "      <th id=\"T_c86e5_level0_col3\" class=\"col_heading level0 col3\" >BBH/CoT</th>\n",
       "      <th id=\"T_c86e5_level0_col4\" class=\"col_heading level0 col4\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_c86e5_level0_col5\" class=\"col_heading level0 col5\" >AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th id=\"T_c86e5_level0_col6\" class=\"col_heading level0 col6\" >Average</th>\n",
       "      <th id=\"T_c86e5_level0_col7\" class=\"col_heading level0 col7\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c86e5_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c86e5_row0_col1\" class=\"data row0 col1\" >59.02</td>\n",
       "      <td id=\"T_c86e5_row0_col2\" class=\"data row0 col2\" >36.20</td>\n",
       "      <td id=\"T_c86e5_row0_col3\" class=\"data row0 col3\" >45.37</td>\n",
       "      <td id=\"T_c86e5_row0_col4\" class=\"data row0 col4\" >33.54</td>\n",
       "      <td id=\"T_c86e5_row0_col5\" class=\"data row0 col5\" >61.66</td>\n",
       "      <td id=\"T_c86e5_row0_col6\" class=\"data row0 col6\" >47.16</td>\n",
       "      <td id=\"T_c86e5_row0_col7\" class=\"data row0 col7\" >-2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c86e5_row1_col0\" class=\"data row1 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_c86e5_row1_col1\" class=\"data row1 col1\" >58.13</td>\n",
       "      <td id=\"T_c86e5_row1_col2\" class=\"data row1 col2\" >41.80</td>\n",
       "      <td id=\"T_c86e5_row1_col3\" class=\"data row1 col3\" >46.02</td>\n",
       "      <td id=\"T_c86e5_row1_col4\" class=\"data row1 col4\" >31.10</td>\n",
       "      <td id=\"T_c86e5_row1_col5\" class=\"data row1 col5\" >58.00</td>\n",
       "      <td id=\"T_c86e5_row1_col6\" class=\"data row1 col6\" >47.01</td>\n",
       "      <td id=\"T_c86e5_row1_col7\" class=\"data row1 col7\" >-4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c86e5_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat200k_score=el2n:agg=mean_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c86e5_row2_col1\" class=\"data row2 col1\" >59.09</td>\n",
       "      <td id=\"T_c86e5_row2_col2\" class=\"data row2 col2\" >34.00</td>\n",
       "      <td id=\"T_c86e5_row2_col3\" class=\"data row2 col3\" >46.11</td>\n",
       "      <td id=\"T_c86e5_row2_col4\" class=\"data row2 col4\" >31.10</td>\n",
       "      <td id=\"T_c86e5_row2_col5\" class=\"data row2 col5\" >59.55</td>\n",
       "      <td id=\"T_c86e5_row2_col6\" class=\"data row2 col6\" >45.97</td>\n",
       "      <td id=\"T_c86e5_row2_col7\" class=\"data row2 col7\" >-3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c86e5_row3_col0\" class=\"data row3 col0\" >mistral-7b_ultrachat200k_score=grad:loraB:l2n_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c86e5_row3_col1\" class=\"data row3 col1\" >58.75</td>\n",
       "      <td id=\"T_c86e5_row3_col2\" class=\"data row3 col2\" >34.60</td>\n",
       "      <td id=\"T_c86e5_row3_col3\" class=\"data row3 col3\" >46.76</td>\n",
       "      <td id=\"T_c86e5_row3_col4\" class=\"data row3 col4\" >28.66</td>\n",
       "      <td id=\"T_c86e5_row3_col5\" class=\"data row3 col5\" >58.08</td>\n",
       "      <td id=\"T_c86e5_row3_col6\" class=\"data row3 col6\" >45.37</td>\n",
       "      <td id=\"T_c86e5_row3_col7\" class=\"data row3 col7\" >-4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c86e5_row4_col0\" class=\"data row4 col0\" >mistral-7b_ultrachat200k_score=logit:margin:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c86e5_row4_col1\" class=\"data row4 col1\" >58.66</td>\n",
       "      <td id=\"T_c86e5_row4_col2\" class=\"data row4 col2\" >34.00</td>\n",
       "      <td id=\"T_c86e5_row4_col3\" class=\"data row4 col3\" >45.93</td>\n",
       "      <td id=\"T_c86e5_row4_col4\" class=\"data row4 col4\" >28.05</td>\n",
       "      <td id=\"T_c86e5_row4_col5\" class=\"data row4 col5\" >59.57</td>\n",
       "      <td id=\"T_c86e5_row4_col6\" class=\"data row4 col6\" >45.24</td>\n",
       "      <td id=\"T_c86e5_row4_col7\" class=\"data row4 col7\" >-5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c86e5_row5_col0\" class=\"data row5 col0\" >mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c86e5_row5_col1\" class=\"data row5 col1\" >58.30</td>\n",
       "      <td id=\"T_c86e5_row5_col2\" class=\"data row5 col2\" >27.60</td>\n",
       "      <td id=\"T_c86e5_row5_col3\" class=\"data row5 col3\" >42.50</td>\n",
       "      <td id=\"T_c86e5_row5_col4\" class=\"data row5 col4\" >34.15</td>\n",
       "      <td id=\"T_c86e5_row5_col5\" class=\"data row5 col5\" >57.51</td>\n",
       "      <td id=\"T_c86e5_row5_col6\" class=\"data row5 col6\" >44.01</td>\n",
       "      <td id=\"T_c86e5_row5_col7\" class=\"data row5 col7\" >-7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c86e5_row6_col0\" class=\"data row6 col0\" >mistral-7b_ultrachat200k_score=random:s=0_pace=prune:size=50000:ep=1</td>\n",
       "      <td id=\"T_c86e5_row6_col1\" class=\"data row6 col1\" >58.63</td>\n",
       "      <td id=\"T_c86e5_row6_col2\" class=\"data row6 col2\" >36.00</td>\n",
       "      <td id=\"T_c86e5_row6_col3\" class=\"data row6 col3\" >46.39</td>\n",
       "      <td id=\"T_c86e5_row6_col4\" class=\"data row6 col4\" >23.17</td>\n",
       "      <td id=\"T_c86e5_row6_col5\" class=\"data row6 col5\" >54.34</td>\n",
       "      <td id=\"T_c86e5_row6_col6\" class=\"data row6 col6\" >43.71</td>\n",
       "      <td id=\"T_c86e5_row6_col7\" class=\"data row6 col7\" >-6.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c86e5_row7_col0\" class=\"data row7 col0\" >mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c86e5_row7_col1\" class=\"data row7 col1\" >59.36</td>\n",
       "      <td id=\"T_c86e5_row7_col2\" class=\"data row7 col2\" >24.80</td>\n",
       "      <td id=\"T_c86e5_row7_col3\" class=\"data row7 col3\" >45.19</td>\n",
       "      <td id=\"T_c86e5_row7_col4\" class=\"data row7 col4\" >27.44</td>\n",
       "      <td id=\"T_c86e5_row7_col5\" class=\"data row7 col5\" >60.24</td>\n",
       "      <td id=\"T_c86e5_row7_col6\" class=\"data row7 col6\" >43.40</td>\n",
       "      <td id=\"T_c86e5_row7_col7\" class=\"data row7 col7\" >-6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c86e5_row8_col0\" class=\"data row8 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_c86e5_row8_col1\" class=\"data row8 col1\" >52.42</td>\n",
       "      <td id=\"T_c86e5_row8_col2\" class=\"data row8 col2\" >30.60</td>\n",
       "      <td id=\"T_c86e5_row8_col3\" class=\"data row8 col3\" >38.06</td>\n",
       "      <td id=\"T_c86e5_row8_col4\" class=\"data row8 col4\" >31.10</td>\n",
       "      <td id=\"T_c86e5_row8_col5\" class=\"data row8 col5\" >59.06</td>\n",
       "      <td id=\"T_c86e5_row8_col6\" class=\"data row8 col6\" >42.25</td>\n",
       "      <td id=\"T_c86e5_row8_col7\" class=\"data row8 col7\" >-7.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c86e5_row9_col0\" class=\"data row9 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_c86e5_row9_col1\" class=\"data row9 col1\" >57.26</td>\n",
       "      <td id=\"T_c86e5_row9_col2\" class=\"data row9 col2\" >29.60</td>\n",
       "      <td id=\"T_c86e5_row9_col3\" class=\"data row9 col3\" >35.56</td>\n",
       "      <td id=\"T_c86e5_row9_col4\" class=\"data row9 col4\" >20.73</td>\n",
       "      <td id=\"T_c86e5_row9_col5\" class=\"data row9 col5\" >58.64</td>\n",
       "      <td id=\"T_c86e5_row9_col6\" class=\"data row9 col6\" >40.36</td>\n",
       "      <td id=\"T_c86e5_row9_col7\" class=\"data row9 col7\" >-9.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c86e5_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c86e5_row10_col0\" class=\"data row10 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_c86e5_row10_col1\" class=\"data row10 col1\" >57.89</td>\n",
       "      <td id=\"T_c86e5_row10_col2\" class=\"data row10 col2\" >31.60</td>\n",
       "      <td id=\"T_c86e5_row10_col3\" class=\"data row10 col3\" >45.74</td>\n",
       "      <td id=\"T_c86e5_row10_col4\" class=\"data row10 col4\" >19.51</td>\n",
       "      <td id=\"T_c86e5_row10_col5\" class=\"data row10 col5\" >nan</td>\n",
       "      <td id=\"T_c86e5_row10_col6\" class=\"data row10 col6\" >38.69</td>\n",
       "      <td id=\"T_c86e5_row10_col7\" class=\"data row10 col7\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffca20dbc40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "#     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "#     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "#     # llama2\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "#     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "#     # mistral\n",
    "#     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct_10.16update', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "# ]\n",
    "\n",
    "# baselines\n",
    "save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "# #     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1')\n",
    "# ]\n",
    "# use_normalized_preferred_metric = False\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "#     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# # exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# use_normalized_preferred_metric = True\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# # exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# exp_dir = '../results/oi5_tulu_v1_mix:llama-7b'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [\n",
    "#     # baselines \n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "# #     ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),\n",
    "#     # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "# #     ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "# ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "use_normalized_preferred_metric = False\n",
    "save_dirs = [\n",
    "    # baselines \n",
    "    ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "    ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "    ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "#     ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "    ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "#     ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-alpha'),\n",
    "#     ('mistral-7b-sft-beta+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "]\n",
    "save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['', 'ft2', 'oi4']):\n",
    "    chat_fmt = False\n",
    "    chat_fmt = True\n",
    "#     chat_fmt = 'both'\n",
    "    ft_args_fields = [\n",
    "        'run_name',\n",
    "        'model_args.model_name_or_path',\n",
    "        'data_args.subsample_mixture',\n",
    "        'data_args.max_train_samples',\n",
    "    ]\n",
    "    print(f'chat_fmt={chat_fmt}')\n",
    "    df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "#     df = get_eval_results(save_dirs, chat_fmt='both', ft_args_fields=ft_args_fields)\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] #  'ToxiGen/Acc'\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR']\n",
    "    cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR'] #  'ToxiGen/Acc'\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR'] #  entire, without tydiqa, which has high variance\n",
    "    \n",
    "    df = df[ft_args_fields + cols]\n",
    "    df['Average'] = df[cols].mean(axis=1)\n",
    "    if sort_rows:\n",
    "        df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "        sort_value_col, sort_value_col_ascending = 'Average', False\n",
    "    #     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "        df = df.sort_values(sort_value_col, ascending=sort_value_col_ascending)\n",
    "    df = df.reset_index(drop=True)\n",
    "else:\n",
    "    ft_args_fields = [\n",
    "        'run_name',\n",
    "        'data_args.subsample_mixture',\n",
    "        'data_args.max_train_samples',\n",
    "    ]\n",
    "    df = get_eval_results(save_dirs, chat_fmt='both', ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "    df = df[ft_args_fields+cols]\n",
    "    \n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "elif any(exp_dir.endswith(x) for x in ['oi4']):\n",
    "    dfc = df.copy()\n",
    "    dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "    dfc = dfc.reset_index(drop=True)\n",
    "    if len(dfc):\n",
    "        display(dfc\n",
    "                .style\n",
    "                .set_properties(**{'text-align': 'left'})\n",
    "                .background_gradient(cmap ='coolwarm')\n",
    "                .format(precision=1))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x)]\n",
    "        dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "        dfc = dfc.reset_index(drop=True)\n",
    "        if len(dfc):\n",
    "            display(dfc\n",
    "                    .style\n",
    "                    .set_properties(**{'text-align': 'left'})\n",
    "                    .background_gradient(cmap ='coolwarm')\n",
    "                    .format(precision=2))\n",
    "\n",
    "# llama-7b_tulu_v1_mix(paper)\n",
    "# MMLU/0-shot, MMLU/5-shot, GSM/Direct, GSM/CoT, BBH/Direct, BBH/CoT, TydiQA/GP, TydiQA/CB, CodexEval/Pass@1, AlpacaEval(vs.Davinci-003)\n",
    "# 44.8       , 47.1       , 7.0       , 25.0   , 38.5      , 38.5   , 43.5,    , 8.0      , 18.6,           , 48.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e526bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>model_args.model_name_or_path</th>\n",
       "      <th>data_args.subsample_mixture</th>\n",
       "      <th>data_args.max_train_samples</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>MMLU/5-shot</th>\n",
       "      <th>GSM/Direct</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>BBH/CoT</th>\n",
       "      <th>TydiQA/CB</th>\n",
       "      <th>TydiQA/GP</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "      <th>AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th>Average</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58.132745</td>\n",
       "      <td>59.186726</td>\n",
       "      <td>9.8</td>\n",
       "      <td>41.8</td>\n",
       "      <td>40.185185</td>\n",
       "      <td>46.018519</td>\n",
       "      <td>11.364015</td>\n",
       "      <td>37.366029</td>\n",
       "      <td>31.097561</td>\n",
       "      <td>58.002481</td>\n",
       "      <td>39.295326</td>\n",
       "      <td>-5.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=el2n:agg=mean_pace=prune:size=50000:ep=3</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.087025</td>\n",
       "      <td>59.955847</td>\n",
       "      <td>9.4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.111111</td>\n",
       "      <td>46.111111</td>\n",
       "      <td>13.430068</td>\n",
       "      <td>37.954701</td>\n",
       "      <td>31.097561</td>\n",
       "      <td>59.553350</td>\n",
       "      <td>38.670077</td>\n",
       "      <td>-4.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=random:s=0_pace=prune:size=50000:ep=1</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58.631249</td>\n",
       "      <td>59.699473</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.111111</td>\n",
       "      <td>46.388889</td>\n",
       "      <td>12.606798</td>\n",
       "      <td>41.304310</td>\n",
       "      <td>23.170732</td>\n",
       "      <td>54.342432</td>\n",
       "      <td>38.225499</td>\n",
       "      <td>-5.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.022931</td>\n",
       "      <td>56.459194</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.2</td>\n",
       "      <td>28.518519</td>\n",
       "      <td>45.370370</td>\n",
       "      <td>9.833249</td>\n",
       "      <td>35.115738</td>\n",
       "      <td>33.536585</td>\n",
       "      <td>61.662531</td>\n",
       "      <td>37.871912</td>\n",
       "      <td>-5.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=logit:margin:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58.659735</td>\n",
       "      <td>57.506053</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>31.203704</td>\n",
       "      <td>45.925926</td>\n",
       "      <td>10.618796</td>\n",
       "      <td>38.305675</td>\n",
       "      <td>28.048780</td>\n",
       "      <td>59.565217</td>\n",
       "      <td>37.783389</td>\n",
       "      <td>-5.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=grad:loraB:l2n_pace=prune:size=50000:ep=3</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58.745193</td>\n",
       "      <td>54.166073</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>26.203704</td>\n",
       "      <td>46.759259</td>\n",
       "      <td>12.951809</td>\n",
       "      <td>47.341415</td>\n",
       "      <td>28.658537</td>\n",
       "      <td>58.084577</td>\n",
       "      <td>37.751057</td>\n",
       "      <td>-5.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58.303660</td>\n",
       "      <td>59.606894</td>\n",
       "      <td>10.6</td>\n",
       "      <td>27.6</td>\n",
       "      <td>35.185185</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>10.232907</td>\n",
       "      <td>36.617649</td>\n",
       "      <td>34.146341</td>\n",
       "      <td>57.506203</td>\n",
       "      <td>37.229884</td>\n",
       "      <td>-6.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td>results/baselines/mistralai/Mistral-7B-v0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59.357641</td>\n",
       "      <td>47.108674</td>\n",
       "      <td>9.8</td>\n",
       "      <td>24.8</td>\n",
       "      <td>34.259259</td>\n",
       "      <td>45.185185</td>\n",
       "      <td>13.233443</td>\n",
       "      <td>48.622536</td>\n",
       "      <td>27.439024</td>\n",
       "      <td>60.235732</td>\n",
       "      <td>37.004149</td>\n",
       "      <td>-6.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral-7b-sft-beta</td>\n",
       "      <td>mistral-7b-sft-beta</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>57.256801</td>\n",
       "      <td>56.523287</td>\n",
       "      <td>9.2</td>\n",
       "      <td>29.6</td>\n",
       "      <td>35.185185</td>\n",
       "      <td>35.555556</td>\n",
       "      <td>12.768927</td>\n",
       "      <td>38.986242</td>\n",
       "      <td>20.731707</td>\n",
       "      <td>58.644279</td>\n",
       "      <td>35.445198</td>\n",
       "      <td>-7.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mistral-7b-Instruct</td>\n",
       "      <td>mistral-7b-Instruct</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>52.421308</td>\n",
       "      <td>51.125196</td>\n",
       "      <td>11.2</td>\n",
       "      <td>30.6</td>\n",
       "      <td>37.592593</td>\n",
       "      <td>38.055556</td>\n",
       "      <td>6.093471</td>\n",
       "      <td>36.519455</td>\n",
       "      <td>31.097561</td>\n",
       "      <td>59.057072</td>\n",
       "      <td>35.376221</td>\n",
       "      <td>-7.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>57.890614</td>\n",
       "      <td>45.841048</td>\n",
       "      <td>11.2</td>\n",
       "      <td>31.6</td>\n",
       "      <td>40.277778</td>\n",
       "      <td>45.740741</td>\n",
       "      <td>13.675519</td>\n",
       "      <td>49.004697</td>\n",
       "      <td>19.512195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.971399</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        run_name  \\\n",
       "0                              mistral-7b_ultrachat200k_beforesplitlongconv_ep=2   \n",
       "1        mistral-7b_ultrachat200k_score=el2n:agg=mean_pace=prune:size=50000:ep=3   \n",
       "2           mistral-7b_ultrachat200k_score=random:s=0_pace=prune:size=50000:ep=1   \n",
       "3         mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3   \n",
       "4     mistral-7b_ultrachat200k_score=logit:margin:neg_pace=prune:size=50000:ep=3   \n",
       "5       mistral-7b_ultrachat200k_score=grad:loraB:l2n_pace=prune:size=50000:ep=3   \n",
       "6       mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3   \n",
       "7   mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3   \n",
       "8                                                            mistral-7b-sft-beta   \n",
       "9                                                            mistral-7b-Instruct   \n",
       "10                                                                    mistral-7b   \n",
       "\n",
       "                  model_args.model_name_or_path data_args.subsample_mixture  \\\n",
       "0   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "1   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "2   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "3   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "4   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "5   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "6   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "7   results/baselines/mistralai/Mistral-7B-v0.1                        None   \n",
       "8                           mistral-7b-sft-beta                        None   \n",
       "9                           mistral-7b-Instruct                        None   \n",
       "10                                   mistral-7b                        None   \n",
       "\n",
       "   data_args.max_train_samples  MMLU/0-shot  MMLU/5-shot  GSM/Direct  GSM/CoT  \\\n",
       "0                         None    58.132745    59.186726         9.8     41.8   \n",
       "1                         None    59.087025    59.955847         9.4     34.0   \n",
       "2                         None    58.631249    59.699473         9.0     36.0   \n",
       "3                         None    59.022931    56.459194        13.0     36.2   \n",
       "4                         None    58.659735    57.506053        14.0     34.0   \n",
       "5                         None    58.745193    54.166073        10.0     34.6   \n",
       "6                         None    58.303660    59.606894        10.6     27.6   \n",
       "7                         None    59.357641    47.108674         9.8     24.8   \n",
       "8                         None    57.256801    56.523287         9.2     29.6   \n",
       "9                         None    52.421308    51.125196        11.2     30.6   \n",
       "10                        None    57.890614    45.841048        11.2     31.6   \n",
       "\n",
       "    BBH/Direct    BBH/CoT  TydiQA/CB  TydiQA/GP  Codex-Eval/Pass@1  \\\n",
       "0    40.185185  46.018519  11.364015  37.366029          31.097561   \n",
       "1    36.111111  46.111111  13.430068  37.954701          31.097561   \n",
       "2    41.111111  46.388889  12.606798  41.304310          23.170732   \n",
       "3    28.518519  45.370370   9.833249  35.115738          33.536585   \n",
       "4    31.203704  45.925926  10.618796  38.305675          28.048780   \n",
       "5    26.203704  46.759259  12.951809  47.341415          28.658537   \n",
       "6    35.185185  42.500000  10.232907  36.617649          34.146341   \n",
       "7    34.259259  45.185185  13.233443  48.622536          27.439024   \n",
       "8    35.185185  35.555556  12.768927  38.986242          20.731707   \n",
       "9    37.592593  38.055556   6.093471  36.519455          31.097561   \n",
       "10   40.277778  45.740741  13.675519  49.004697          19.512195   \n",
       "\n",
       "    AlpacaFarm(v.Davinci-003)/WR    Average   ranking  \n",
       "0                      58.002481  39.295326 -5.045455  \n",
       "1                      59.553350  38.670077 -4.045455  \n",
       "2                      54.342432  38.225499 -5.181818  \n",
       "3                      61.662531  37.871912 -5.363636  \n",
       "4                      59.565217  37.783389 -5.409091  \n",
       "5                      58.084577  37.751057 -5.454545  \n",
       "6                      57.506203  37.229884 -6.863636  \n",
       "7                      60.235732  37.004149 -6.227273  \n",
       "8                      58.644279  35.445198 -7.954545  \n",
       "9                      59.057072  35.376221 -7.772727  \n",
       "10                           NaN  34.971399       NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16806ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c643a_row0_col0, #T_c643a_row1_col0, #T_c643a_row2_col0, #T_c643a_row3_col0, #T_c643a_row4_col0, #T_c643a_row5_col0, #T_c643a_row6_col0, #T_c643a_row7_col0, #T_c643a_row8_col0, #T_c643a_row9_col0, #T_c643a_row10_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_c643a_row0_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col3, #T_c643a_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col4, #T_c643a_row0_col11, #T_c643a_row1_col2, #T_c643a_row1_col12, #T_c643a_row2_col5, #T_c643a_row3_col10, #T_c643a_row4_col3, #T_c643a_row5_col6, #T_c643a_row6_col9, #T_c643a_row7_col1, #T_c643a_row10_col7, #T_c643a_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col9, #T_c643a_row1_col9, #T_c643a_row9_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row0_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row0_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row1_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row1_col4, #T_c643a_row4_col4, #T_c643a_row7_col5, #T_c643a_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #e7d7ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row1_col6, #T_c643a_row7_col7, #T_c643a_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row1_col7, #T_c643a_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row1_col10, #T_c643a_row4_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row2_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row2_col3, #T_c643a_row2_col10, #T_c643a_row3_col8, #T_c643a_row5_col5, #T_c643a_row7_col4, #T_c643a_row8_col6, #T_c643a_row8_col12, #T_c643a_row9_col1, #T_c643a_row9_col7, #T_c643a_row10_col2, #T_c643a_row10_col9, #T_c643a_row10_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row2_col11, #T_c643a_row3_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row3_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row3_col4, #T_c643a_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6b8df0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dbdcde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row3_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row4_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row4_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row4_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row4_col11, #T_c643a_row4_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row5_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row5_col2, #T_c643a_row8_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row5_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row5_col8, #T_c643a_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row5_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c5ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row5_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row5_col11, #T_c643a_row9_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row6_col5, #T_c643a_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row6_col12, #T_c643a_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row7_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row7_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row7_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #d5dbe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row7_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row8_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row8_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row8_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row8_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #5d7ce6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row9_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row9_col3, #T_c643a_row10_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row9_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row9_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row9_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row9_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row10_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row10_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c643a_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c643a_row10_col10, #T_c643a_row10_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c643a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c643a_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_c643a_level0_col1\" class=\"col_heading level0 col1\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_c643a_level0_col2\" class=\"col_heading level0 col2\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_c643a_level0_col3\" class=\"col_heading level0 col3\" >GSM/Direct</th>\n",
       "      <th id=\"T_c643a_level0_col4\" class=\"col_heading level0 col4\" >GSM/CoT</th>\n",
       "      <th id=\"T_c643a_level0_col5\" class=\"col_heading level0 col5\" >BBH/Direct</th>\n",
       "      <th id=\"T_c643a_level0_col6\" class=\"col_heading level0 col6\" >BBH/CoT</th>\n",
       "      <th id=\"T_c643a_level0_col7\" class=\"col_heading level0 col7\" >TydiQA/CB</th>\n",
       "      <th id=\"T_c643a_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/GP</th>\n",
       "      <th id=\"T_c643a_level0_col9\" class=\"col_heading level0 col9\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_c643a_level0_col10\" class=\"col_heading level0 col10\" >AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th id=\"T_c643a_level0_col11\" class=\"col_heading level0 col11\" >Average</th>\n",
       "      <th id=\"T_c643a_level0_col12\" class=\"col_heading level0 col12\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c643a_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_c643a_row0_col1\" class=\"data row0 col1\" >58.1</td>\n",
       "      <td id=\"T_c643a_row0_col2\" class=\"data row0 col2\" >59.2</td>\n",
       "      <td id=\"T_c643a_row0_col3\" class=\"data row0 col3\" >9.8</td>\n",
       "      <td id=\"T_c643a_row0_col4\" class=\"data row0 col4\" >41.8</td>\n",
       "      <td id=\"T_c643a_row0_col5\" class=\"data row0 col5\" >40.2</td>\n",
       "      <td id=\"T_c643a_row0_col6\" class=\"data row0 col6\" >46.0</td>\n",
       "      <td id=\"T_c643a_row0_col7\" class=\"data row0 col7\" >11.4</td>\n",
       "      <td id=\"T_c643a_row0_col8\" class=\"data row0 col8\" >37.4</td>\n",
       "      <td id=\"T_c643a_row0_col9\" class=\"data row0 col9\" >31.1</td>\n",
       "      <td id=\"T_c643a_row0_col10\" class=\"data row0 col10\" >58.0</td>\n",
       "      <td id=\"T_c643a_row0_col11\" class=\"data row0 col11\" >39.3</td>\n",
       "      <td id=\"T_c643a_row0_col12\" class=\"data row0 col12\" >-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c643a_row1_col0\" class=\"data row1 col0\" >mistral-7b_ultrachat200k_score=el2n:agg=mean_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c643a_row1_col1\" class=\"data row1 col1\" >59.1</td>\n",
       "      <td id=\"T_c643a_row1_col2\" class=\"data row1 col2\" >60.0</td>\n",
       "      <td id=\"T_c643a_row1_col3\" class=\"data row1 col3\" >9.4</td>\n",
       "      <td id=\"T_c643a_row1_col4\" class=\"data row1 col4\" >34.0</td>\n",
       "      <td id=\"T_c643a_row1_col5\" class=\"data row1 col5\" >36.1</td>\n",
       "      <td id=\"T_c643a_row1_col6\" class=\"data row1 col6\" >46.1</td>\n",
       "      <td id=\"T_c643a_row1_col7\" class=\"data row1 col7\" >13.4</td>\n",
       "      <td id=\"T_c643a_row1_col8\" class=\"data row1 col8\" >38.0</td>\n",
       "      <td id=\"T_c643a_row1_col9\" class=\"data row1 col9\" >31.1</td>\n",
       "      <td id=\"T_c643a_row1_col10\" class=\"data row1 col10\" >59.6</td>\n",
       "      <td id=\"T_c643a_row1_col11\" class=\"data row1 col11\" >38.7</td>\n",
       "      <td id=\"T_c643a_row1_col12\" class=\"data row1 col12\" >-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c643a_row2_col0\" class=\"data row2 col0\" >score=random:_avg (N=1)</td>\n",
       "      <td id=\"T_c643a_row2_col1\" class=\"data row2 col1\" >58.6</td>\n",
       "      <td id=\"T_c643a_row2_col2\" class=\"data row2 col2\" >59.7</td>\n",
       "      <td id=\"T_c643a_row2_col3\" class=\"data row2 col3\" >9.0</td>\n",
       "      <td id=\"T_c643a_row2_col4\" class=\"data row2 col4\" >36.0</td>\n",
       "      <td id=\"T_c643a_row2_col5\" class=\"data row2 col5\" >41.1</td>\n",
       "      <td id=\"T_c643a_row2_col6\" class=\"data row2 col6\" >46.4</td>\n",
       "      <td id=\"T_c643a_row2_col7\" class=\"data row2 col7\" >12.6</td>\n",
       "      <td id=\"T_c643a_row2_col8\" class=\"data row2 col8\" >41.3</td>\n",
       "      <td id=\"T_c643a_row2_col9\" class=\"data row2 col9\" >23.2</td>\n",
       "      <td id=\"T_c643a_row2_col10\" class=\"data row2 col10\" >54.3</td>\n",
       "      <td id=\"T_c643a_row2_col11\" class=\"data row2 col11\" >38.2</td>\n",
       "      <td id=\"T_c643a_row2_col12\" class=\"data row2 col12\" >-5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c643a_row3_col0\" class=\"data row3 col0\" >mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c643a_row3_col1\" class=\"data row3 col1\" >59.0</td>\n",
       "      <td id=\"T_c643a_row3_col2\" class=\"data row3 col2\" >56.5</td>\n",
       "      <td id=\"T_c643a_row3_col3\" class=\"data row3 col3\" >13.0</td>\n",
       "      <td id=\"T_c643a_row3_col4\" class=\"data row3 col4\" >36.2</td>\n",
       "      <td id=\"T_c643a_row3_col5\" class=\"data row3 col5\" >28.5</td>\n",
       "      <td id=\"T_c643a_row3_col6\" class=\"data row3 col6\" >45.4</td>\n",
       "      <td id=\"T_c643a_row3_col7\" class=\"data row3 col7\" >9.8</td>\n",
       "      <td id=\"T_c643a_row3_col8\" class=\"data row3 col8\" >35.1</td>\n",
       "      <td id=\"T_c643a_row3_col9\" class=\"data row3 col9\" >33.5</td>\n",
       "      <td id=\"T_c643a_row3_col10\" class=\"data row3 col10\" >61.7</td>\n",
       "      <td id=\"T_c643a_row3_col11\" class=\"data row3 col11\" >37.9</td>\n",
       "      <td id=\"T_c643a_row3_col12\" class=\"data row3 col12\" >-5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c643a_row4_col0\" class=\"data row4 col0\" >mistral-7b_ultrachat200k_score=logit:margin:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c643a_row4_col1\" class=\"data row4 col1\" >58.7</td>\n",
       "      <td id=\"T_c643a_row4_col2\" class=\"data row4 col2\" >57.5</td>\n",
       "      <td id=\"T_c643a_row4_col3\" class=\"data row4 col3\" >14.0</td>\n",
       "      <td id=\"T_c643a_row4_col4\" class=\"data row4 col4\" >34.0</td>\n",
       "      <td id=\"T_c643a_row4_col5\" class=\"data row4 col5\" >31.2</td>\n",
       "      <td id=\"T_c643a_row4_col6\" class=\"data row4 col6\" >45.9</td>\n",
       "      <td id=\"T_c643a_row4_col7\" class=\"data row4 col7\" >10.6</td>\n",
       "      <td id=\"T_c643a_row4_col8\" class=\"data row4 col8\" >38.3</td>\n",
       "      <td id=\"T_c643a_row4_col9\" class=\"data row4 col9\" >28.0</td>\n",
       "      <td id=\"T_c643a_row4_col10\" class=\"data row4 col10\" >59.6</td>\n",
       "      <td id=\"T_c643a_row4_col11\" class=\"data row4 col11\" >37.8</td>\n",
       "      <td id=\"T_c643a_row4_col12\" class=\"data row4 col12\" >-5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c643a_row5_col0\" class=\"data row5 col0\" >mistral-7b_ultrachat200k_score=grad:loraB:l2n_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c643a_row5_col1\" class=\"data row5 col1\" >58.7</td>\n",
       "      <td id=\"T_c643a_row5_col2\" class=\"data row5 col2\" >54.2</td>\n",
       "      <td id=\"T_c643a_row5_col3\" class=\"data row5 col3\" >10.0</td>\n",
       "      <td id=\"T_c643a_row5_col4\" class=\"data row5 col4\" >34.6</td>\n",
       "      <td id=\"T_c643a_row5_col5\" class=\"data row5 col5\" >26.2</td>\n",
       "      <td id=\"T_c643a_row5_col6\" class=\"data row5 col6\" >46.8</td>\n",
       "      <td id=\"T_c643a_row5_col7\" class=\"data row5 col7\" >13.0</td>\n",
       "      <td id=\"T_c643a_row5_col8\" class=\"data row5 col8\" >47.3</td>\n",
       "      <td id=\"T_c643a_row5_col9\" class=\"data row5 col9\" >28.7</td>\n",
       "      <td id=\"T_c643a_row5_col10\" class=\"data row5 col10\" >58.1</td>\n",
       "      <td id=\"T_c643a_row5_col11\" class=\"data row5 col11\" >37.8</td>\n",
       "      <td id=\"T_c643a_row5_col12\" class=\"data row5 col12\" >-5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c643a_row6_col0\" class=\"data row6 col0\" >mistral-7b_ultrachat200k_score=rhov1:log:prob_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c643a_row6_col1\" class=\"data row6 col1\" >58.3</td>\n",
       "      <td id=\"T_c643a_row6_col2\" class=\"data row6 col2\" >59.6</td>\n",
       "      <td id=\"T_c643a_row6_col3\" class=\"data row6 col3\" >10.6</td>\n",
       "      <td id=\"T_c643a_row6_col4\" class=\"data row6 col4\" >27.6</td>\n",
       "      <td id=\"T_c643a_row6_col5\" class=\"data row6 col5\" >35.2</td>\n",
       "      <td id=\"T_c643a_row6_col6\" class=\"data row6 col6\" >42.5</td>\n",
       "      <td id=\"T_c643a_row6_col7\" class=\"data row6 col7\" >10.2</td>\n",
       "      <td id=\"T_c643a_row6_col8\" class=\"data row6 col8\" >36.6</td>\n",
       "      <td id=\"T_c643a_row6_col9\" class=\"data row6 col9\" >34.1</td>\n",
       "      <td id=\"T_c643a_row6_col10\" class=\"data row6 col10\" >57.5</td>\n",
       "      <td id=\"T_c643a_row6_col11\" class=\"data row6 col11\" >37.2</td>\n",
       "      <td id=\"T_c643a_row6_col12\" class=\"data row6 col12\" >-6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c643a_row7_col0\" class=\"data row7 col0\" >mistral-7b_ultrachat200k_score=rhov1:log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_c643a_row7_col1\" class=\"data row7 col1\" >59.4</td>\n",
       "      <td id=\"T_c643a_row7_col2\" class=\"data row7 col2\" >47.1</td>\n",
       "      <td id=\"T_c643a_row7_col3\" class=\"data row7 col3\" >9.8</td>\n",
       "      <td id=\"T_c643a_row7_col4\" class=\"data row7 col4\" >24.8</td>\n",
       "      <td id=\"T_c643a_row7_col5\" class=\"data row7 col5\" >34.3</td>\n",
       "      <td id=\"T_c643a_row7_col6\" class=\"data row7 col6\" >45.2</td>\n",
       "      <td id=\"T_c643a_row7_col7\" class=\"data row7 col7\" >13.2</td>\n",
       "      <td id=\"T_c643a_row7_col8\" class=\"data row7 col8\" >48.6</td>\n",
       "      <td id=\"T_c643a_row7_col9\" class=\"data row7 col9\" >27.4</td>\n",
       "      <td id=\"T_c643a_row7_col10\" class=\"data row7 col10\" >60.2</td>\n",
       "      <td id=\"T_c643a_row7_col11\" class=\"data row7 col11\" >37.0</td>\n",
       "      <td id=\"T_c643a_row7_col12\" class=\"data row7 col12\" >-6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c643a_row8_col0\" class=\"data row8 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_c643a_row8_col1\" class=\"data row8 col1\" >57.3</td>\n",
       "      <td id=\"T_c643a_row8_col2\" class=\"data row8 col2\" >56.5</td>\n",
       "      <td id=\"T_c643a_row8_col3\" class=\"data row8 col3\" >9.2</td>\n",
       "      <td id=\"T_c643a_row8_col4\" class=\"data row8 col4\" >29.6</td>\n",
       "      <td id=\"T_c643a_row8_col5\" class=\"data row8 col5\" >35.2</td>\n",
       "      <td id=\"T_c643a_row8_col6\" class=\"data row8 col6\" >35.6</td>\n",
       "      <td id=\"T_c643a_row8_col7\" class=\"data row8 col7\" >12.8</td>\n",
       "      <td id=\"T_c643a_row8_col8\" class=\"data row8 col8\" >39.0</td>\n",
       "      <td id=\"T_c643a_row8_col9\" class=\"data row8 col9\" >20.7</td>\n",
       "      <td id=\"T_c643a_row8_col10\" class=\"data row8 col10\" >58.6</td>\n",
       "      <td id=\"T_c643a_row8_col11\" class=\"data row8 col11\" >35.4</td>\n",
       "      <td id=\"T_c643a_row8_col12\" class=\"data row8 col12\" >-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c643a_row9_col0\" class=\"data row9 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_c643a_row9_col1\" class=\"data row9 col1\" >52.4</td>\n",
       "      <td id=\"T_c643a_row9_col2\" class=\"data row9 col2\" >51.1</td>\n",
       "      <td id=\"T_c643a_row9_col3\" class=\"data row9 col3\" >11.2</td>\n",
       "      <td id=\"T_c643a_row9_col4\" class=\"data row9 col4\" >30.6</td>\n",
       "      <td id=\"T_c643a_row9_col5\" class=\"data row9 col5\" >37.6</td>\n",
       "      <td id=\"T_c643a_row9_col6\" class=\"data row9 col6\" >38.1</td>\n",
       "      <td id=\"T_c643a_row9_col7\" class=\"data row9 col7\" >6.1</td>\n",
       "      <td id=\"T_c643a_row9_col8\" class=\"data row9 col8\" >36.5</td>\n",
       "      <td id=\"T_c643a_row9_col9\" class=\"data row9 col9\" >31.1</td>\n",
       "      <td id=\"T_c643a_row9_col10\" class=\"data row9 col10\" >59.1</td>\n",
       "      <td id=\"T_c643a_row9_col11\" class=\"data row9 col11\" >35.4</td>\n",
       "      <td id=\"T_c643a_row9_col12\" class=\"data row9 col12\" >-7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c643a_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c643a_row10_col0\" class=\"data row10 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_c643a_row10_col1\" class=\"data row10 col1\" >57.9</td>\n",
       "      <td id=\"T_c643a_row10_col2\" class=\"data row10 col2\" >45.8</td>\n",
       "      <td id=\"T_c643a_row10_col3\" class=\"data row10 col3\" >11.2</td>\n",
       "      <td id=\"T_c643a_row10_col4\" class=\"data row10 col4\" >31.6</td>\n",
       "      <td id=\"T_c643a_row10_col5\" class=\"data row10 col5\" >40.3</td>\n",
       "      <td id=\"T_c643a_row10_col6\" class=\"data row10 col6\" >45.7</td>\n",
       "      <td id=\"T_c643a_row10_col7\" class=\"data row10 col7\" >13.7</td>\n",
       "      <td id=\"T_c643a_row10_col8\" class=\"data row10 col8\" >49.0</td>\n",
       "      <td id=\"T_c643a_row10_col9\" class=\"data row10 col9\" >19.5</td>\n",
       "      <td id=\"T_c643a_row10_col10\" class=\"data row10 col10\" >nan</td>\n",
       "      <td id=\"T_c643a_row10_col11\" class=\"data row10 col11\" >35.0</td>\n",
       "      <td id=\"T_c643a_row10_col12\" class=\"data row10 col12\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffca283bb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_average_col_contains_substr\n",
    "\n",
    "Ns = np.unique([x for x in df['data_args.max_train_samples'].to_numpy() if x]).tolist()\n",
    "for N in Ns+[None]:\n",
    "    dfc = df.copy()\n",
    "    dfc = dfc[dfc['data_args.max_train_samples'].apply(lambda x: x == N if x else True)]\n",
    "    dfc = pd_average_col_contains_substr(dfc, 'run_name', 'random_', substitute=True)\n",
    "    dfc = pd_average_col_contains_substr(dfc, 'run_name', 'score=random:', substitute=True)\n",
    "    #     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "    dfc = dfc.sort_values(['Average'], ascending=False)\n",
    "    dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "    dfc = dfc.reset_index(drop=True)\n",
    "    if len(dfc):\n",
    "        display(dfc\n",
    "                .style\n",
    "                .set_properties(**{'text-align': 'left'})\n",
    "                .background_gradient(cmap ='coolwarm')\n",
    "                .format(precision=1))\n",
    "\n",
    "# random\n",
    "# log_prob_decr\n",
    "# el2n agg_mean incr\n",
    "# logit_margin_decr\n",
    "# grad l2n incr\n",
    "# kmeansl2_emb=text+embedding_nc=3000_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fd168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runif_in_simplex(n):\n",
    "    ''' Return uniformly random vector in the n-simplex '''\n",
    "\n",
    "    k = np.random.exponential(scale=1.0, size=n)\n",
    "    return k / sum(k)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "L = []\n",
    "dataset_name = ['cot', 'dolly', 'flan_v2', 'oasst1']\n",
    "xs = np.arange(4)\n",
    "for _ in range(10):\n",
    "    ys = runif_in_simplex(4)\n",
    "    d = {k: v for k, v in zip(dataset_name, ys)}\n",
    "    L.append(d)\n",
    "    ax.plot(xs, ys)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dirs = []\n",
    "run_dirs = [\n",
    "#     'pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-2.8b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-6.9b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "    ''\n",
    "]\n",
    "for run_dir in run_dirs:\n",
    "    save_dirs += [(os.path.basename(x), x) \n",
    "                  for x in glob.glob(os.path.join('../results/ft2', run_dir, 'checkpoint-*'))]\n",
    "    break\n",
    "\n",
    "    \n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "# df['model'] = ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666b028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from llm.evaluate import get_eval_results_cross_time\n",
    "\n",
    "\n",
    "\n",
    "eval_name_list = [\n",
    "    'MMLU/0-shot',\n",
    "    'MMLU/5-shot',\n",
    "    'GSM/Direct',\n",
    "    'GSM/CoT',\n",
    "    'BBH/Direct',\n",
    "    'BBH/CoT',\n",
    "    'TydiQA/CB',\n",
    "    'TydiQA/GP',\n",
    "    'Codex-Eval/Pass@1',\n",
    "]\n",
    "eval_name_list += [eval_name_list.copy()]\n",
    "\n",
    "\n",
    "N = len(eval_name_list)\n",
    "w = 5\n",
    "fig, axs = plt.subplots(2, 5, figsize=(5*w, 2*w), sharey='row', sharex='col')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = 'tuluv1hm'; exp_dir = '../results/oi4_tulu_v1_human_mix'\n",
    "dataset = 'flan_v2'; exp_dir = '../results/oi4_perf_cross_time'; keep_size = '30k'\n",
    "dataset = 'flan_v2'; exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'; keep_size = '30k'\n",
    "dataset = 'flan2022_1m'; exp_dir = '../results/oi4_flan2022_1m'; keep_size = '100k'\n",
    "filter_fn = lambda x: '100k' in x\n",
    "\n",
    "\n",
    "runs = [x for x in os.listdir(exp_dir) if os.path.isdir(os.path.join(exp_dir, x))]\n",
    "runs = list(filter(filter_fn, runs))\n",
    "def get_name_and_path(x):\n",
    "    name = x.split(':')[-1]\n",
    "    return name, x\n",
    "runs = list(map(get_name_and_path, runs))\n",
    "\n",
    "# runs = [\n",
    "#     ('random', f'llama-7b_{dataset}:{keep_size}_random'),\n",
    "# #     ('dppmap_k=Kcos', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcos'),\n",
    "# #     ('dppmap_k=Kcos1np', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcos1np'),\n",
    "# #     ('dppmap_k=Kcosp', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcosp'),\n",
    "#     ('kmeansl2_nc=3000_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=3000_incr'),\n",
    "#     ('kmeansl2_nc=3000_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=3000_decr'),\n",
    "# #     ('kmeansl2_nc=1000_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=1000_incr'),\n",
    "# #     ('kmeansl2_nc=1000_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=1000_decr'),\n",
    "# #     ('kmeansl2_nc=300_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=300_incr'),\n",
    "# #     ('kmeansl2_nc=300_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=300_decr'),\n",
    "#     ('prob_incr', f'llama-7b_{dataset}:{keep_size}_prob_incr'),\n",
    "#     ('prob_decr', f'llama-7b_{dataset}:{keep_size}_prob_decr'),\n",
    "#     ('el2n_incr', f'llama-7b_{dataset}:{keep_size}_el2n_incr'),\n",
    "#     ('el2n_decr', f'llama-7b_{dataset}:{keep_size}_el2n_decr'),\n",
    "# ]\n",
    "\n",
    "\n",
    "runs = [(x, os.path.join(exp_dir, y)) for x, y in runs]\n",
    "\n",
    "\n",
    "for run_name, save_dir in runs:\n",
    "    df = get_eval_results_cross_time(save_dir, chat_fmt=True)\n",
    "\n",
    "    for axi, eval_name in enumerate(eval_name_list):\n",
    "        ax = axs.flatten()[axi]\n",
    "\n",
    "        xs = df['steps'].to_numpy()\n",
    "        ys = df[eval_name].to_numpy()\n",
    "        if ys.ndim == 2:\n",
    "            ys = ys.mean(-1)\n",
    "\n",
    "        ax.plot(xs, ys, label=run_name)\n",
    "\n",
    "\n",
    "        ax.grid()\n",
    "        ax.set_ylim(0, 55)\n",
    "        ax.set_xlabel('Steps', fontsize=15)\n",
    "        title = eval_name if isinstance(eval_name, str) else 'Avg'\n",
    "        ax.set_title(title, fontsize=20)\n",
    "        \n",
    "        ax.legend()\n",
    "        \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb043d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ylabel = 'llama-7b:600k'\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "ylabel = 'llama-7b_flan_v2:30k'\n",
    "exp_dir = '../results/oi4_perf_cross_time'\n",
    "# exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_decr', 'checkpoint-*'))]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "# add base model performance\n",
    "dfc.loc[dfc['model_args.model_name_or_path']=='huggyllama/llama-7b', 'model_args.model_name_or_path'] = 'checkpoint-0'\n",
    "# get steps \n",
    "dfc.insert(0, 'steps', dfc['model_args.model_name_or_path'].apply(lambda x: int(x.split('-')[-1])))\n",
    "dfc = dfc.sort_values('steps')\n",
    "\n",
    "\n",
    "y_labels_list = [\n",
    "    ['MMLU/0-shot',\n",
    "     'MMLU/0-shot_chatfmt',\n",
    "     'MMLU/5-shot',\n",
    "     'MMLU/5-shot_chatfmt',\n",
    "    ],\n",
    "    ['GSM/Direct',\n",
    "     'GSM/Direct_chatfmt',\n",
    "     'GSM/CoT', \n",
    "     'GSM/CoT_chatfmt', \n",
    "    ],\n",
    "    ['BBH/Direct',\n",
    "     'BBH/Direct_chatfmt',\n",
    "     'BBH/CoT',\n",
    "     'BBH/CoT_chatfmt',\n",
    "    ],\n",
    "    ['TydiQA/CB',\n",
    "     'TydiQA/CB_chatfmt',\n",
    "     'TydiQA/GP',\n",
    "     'TydiQA/GP_chatfmt',\n",
    "    ],\n",
    "    ['Codex-Eval/Pass@1',\n",
    "     'Codex-Eval/Pass@1_chatfmt'],\n",
    "    ['MMLU/0-shot',\n",
    "     'GSM/CoT',\n",
    "     'BBH/CoT',],\n",
    "]\n",
    "\n",
    "N = len(y_labels_list)\n",
    "\n",
    "fig, axs = plt.subplots(1,N,figsize=(5*N,5))\n",
    "\n",
    "axs[0].set_ylabel(ylabel, fontsize=20)\n",
    "\n",
    "for axi, y_labels in enumerate(y_labels_list):\n",
    "    ax = axs[axi]\n",
    "\n",
    "    x = dfc['steps']\n",
    "    y_list = []\n",
    "    for y_label in y_labels:\n",
    "        if y_label not in dfc.columns: continue\n",
    "        y = dfc[y_label].to_numpy()\n",
    "        y_list.append(y)\n",
    "        ax.plot(x, y, label=y_label)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 55)\n",
    "    \n",
    "    \n",
    "# for y_label in ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1']:\n",
    "    \n",
    "#     for chat_fmt in ['', 'chatfmt']:\n",
    "#         col = '_'.join([y_label, chat_fmt]) if chat_fmt else y_label\n",
    "#         y = dfc[col].to_numpy()\n",
    "#         print(f'{col}\\t{y.mean():.2f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_labels = [\n",
    "    'Answer:\\n<|assistant|>\\nThe answer is:',\n",
    "    'Answer:\\n<|assistant|>\\n',\n",
    "    '<|assistant|>\\nAnswer:',\n",
    "    '<|assistant|>\\nThe answer is:',\n",
    "]\n",
    "x_labels = [f'v{i+1}:\\n{x}' for i,x in enumerate(x_labels)]\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc = df.filter(regex='_v|run')\n",
    "\n",
    "runs = dfc['run_name'].to_list()[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for axi, task in enumerate(['MMLU/0-shot', 'MMLU/5-shot']):\n",
    "\n",
    "    ax = axs[axi]\n",
    "    cols = [f'{task}_v{x}' for x in [1, 2, 3, 4]]\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    width = .25\n",
    "    multiplier = 0\n",
    "\n",
    "    for run in runs:\n",
    "        offset = width*multiplier\n",
    "        y = dfc[dfc['run_name']==run][cols].to_numpy().squeeze()\n",
    "        rects = ax.bar(x+offset, y, width, label=run)\n",
    "        ax.bar_label(rects, padding=3, fmt='{:.2f}')\n",
    "        multiplier += 1\n",
    "\n",
    "    ax.set_title(task)\n",
    "    ax.set_xticks(x+width)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(0, 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6ba4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_data_points = 200000 # 10000, 50000, 100000, 200000\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "        )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6323+40966+81933+81933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'results/ft1'\n",
    "\n",
    "# d = {\n",
    "#     'bbh_s=0': 'bbh_s=3',\n",
    "#     'gsm': 'gsm_s=8_cot',\n",
    "#     'mmlu': 'mmlu_s=0',\n",
    "#     'tydiqa_cb': 'tydiqa_s=1_cb',\n",
    "#     'tydiqa_gp': 'tydiqa_s=1_gp',\n",
    "# }\n",
    "\n",
    "# d.update({k+'_chatfmt': v+'_chatfmt' for k,v in d.items()})\n",
    "\n",
    "# for subdir in os.listdir(exp_dir):    \n",
    "#     for task_name_src, task_name_tgt in d.items():\n",
    "#         path_src = os.path.join(exp_dir, subdir, 'eval', task_name_src)\n",
    "#         path_tgt = os.path.join(exp_dir, subdir, 'eval', task_name_tgt)\n",
    "#         if os.path.isdir(path_src):\n",
    "# #             os.rename(path_src, path_tgt)\n",
    "#             print(path_src)\n",
    "#             print(path_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27138820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc = df.copy()\n",
    "dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "    lambda d: sum(list(d.values())) if d else 200000))\n",
    "# dfc[dfc['total_train_samples'].apply(\n",
    "#     lambda x: total_train_samples-500<x<total_train_samples+500)]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat n istoria Uniunii Europene, la drepturile persoanelor care aparin acestor minoriti i la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
