{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'ppc64le', 'cluster': 'dcs'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info\n",
    "\n",
    "info = get_host_info()\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     conda_env='open-instruct',\n",
    "#     cwd=os.path.dirname(os.getcwd()),\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir\n",
    "# )\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '00:33:12'\n",
    "n = 15\n",
    "# total = 1515; nnodes = 1\n",
    "# total = 2083; nnodes = 1\n",
    "total = 1587; nnodes = 1\n",
    "# total = 1041; nnodes = 1\n",
    "# total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_data_points = 200000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up checkpoints `optimizer.bin` to save disk space. \n",
    "# (e.g., 7b model, ~8*7=56GB for storing gradient/momentum in `optimizer.bin`)\n",
    "\n",
    "import glob, os\n",
    "\n",
    "def cleanup_checkpoints(save_dir, test_run=False):\n",
    "\n",
    "    checkpoints = glob.glob(os.path.join(save_dir, 'checkpoint-*'))\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "    checkpoints = checkpoints[:-1]\n",
    "    \n",
    "    if not checkpoints: return\n",
    "\n",
    "    for ckpt_path in checkpoints:\n",
    "        optimizer_bin_path = os.path.join(ckpt_path, 'optimizer.bin')\n",
    "        if os.path.isfile(optimizer_bin_path):\n",
    "            print(optimizer_bin_path)\n",
    "            if not test_run:\n",
    "                os.remove(optimizer_bin_path)\n",
    "        \n",
    "        \n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "exp_dirs = [\n",
    "    '../results/ft1',\n",
    "    '../results/ft2',\n",
    "    '../results/oi3',\n",
    "    '../results/oi4',\n",
    "    '../results/oi4_perf_cross_time',\n",
    "    '../results/oi4_tulu_v1_human_mix',\n",
    "    '../results/oi4_flanv2_prune_with_hmv1_model',\n",
    "    '../results/oi4_flan_v2_vary_subsetsize',\n",
    "]\n",
    "\n",
    "print('Remove extra files (e.g., optimizer.bin) for non-latest checkpoints:')\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    for run_name in os.listdir(exp_dir):\n",
    "        save_dir = os.path.join(exp_dir, run_name)\n",
    "        if os.path.islink(save_dir): continue\n",
    "        cleanup_checkpoints(save_dir, test_run=test_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results/baselines/huggyllama/llama-7b using 6 GPUs, 2 batch size per GPU, 2 gradient accumulation steps, Effective batch size 120\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp5nfmny1g', 'job_id': 1133922}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133922 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2wumgw9b', 'job_id': 1133923}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpwa352b88', 'job_id': 1133924}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133924 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp0kjgfpdz', 'job_id': 1133925}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpvhgcoz91', 'job_id': 1133926}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133926 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpc0jdpnot', 'job_id': 1133927}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpl80vryh1', 'job_id': 1133928}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133928 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp8ybb6q5z', 'job_id': 1133929}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp9l0tr12r', 'job_id': 1133930}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133930 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpq16dtv9y', 'job_id': 1133931}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmph0fd_2bs', 'job_id': 1133932}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133932 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp6_ybw9kf', 'job_id': 1133933}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpvtdruien', 'job_id': 1133934}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133934 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpopxz94ir', 'job_id': 1133935}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpi2h2j8rs', 'job_id': 1133936}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133936 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp605nlsrh', 'job_id': 1133937}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_tulu_v1_mix:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 2\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:2.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp5mn6rrpc', 'job_id': 1133938}, {'args': 'sbatch --job-name=oi5_tulu_v1_mix:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:2.out --time=6:00:00 --dependency=afterany:1133938 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpb65n3adk', 'job_id': 1133939}]\n"
     ]
    }
   ],
   "source": [
    "def compute_mixture_num_samples(mixture, max_train_samples):\n",
    "    s = sum(mixture.values())\n",
    "    mixture = {k: int(max_train_samples*v/s) for k, v in mixture.items()}\n",
    "    return mixture\n",
    "\n",
    "add_hardwarespec_to_dirname = False\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  512 if arch == 'ppc64le' else 64\n",
    "\n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100; save_total_limit = 1\n",
    "\n",
    "\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "# model_name_or_path = 'results/baselines/gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-1.4b'; abbr_model_name = 'pythia-1.4b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-2.8b'; abbr_model_name = 'pythia-2.8b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-6.9b'; abbr_model_name = 'pythia-6.9b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "\n",
    "subsample_mixture_list = []\n",
    "# subsample_mixture_list += [\n",
    "#     {k: max_train_samples} for k in datasets\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     ('humanmix', dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items()))\n",
    "# ] # humanmix mixture.\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot':  0.13568177819252014, 'flan_v2': 0.3957784175872803, \n",
    "#      'dolly': 0.05964866653084755, 'oasst1': 0.4088916480541229}.items())\n",
    "# ] # gpt2-medium_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "# ] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "#         )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_normalized_list = []\n",
    "# subsample_mixture_normalized_list += [('uniform:1200k_doremiv2', # llama-7b_humanmix_uniform:1200k_doremiv2.json\n",
    "#                                        {\"cot\": 0.11419677734375, \"dolly\": 0.1024169921875, \"flan_v2\": 0.204833984375, \"oasst1\": 0.204833984375})]\n",
    "## 10 for trying out datamodels\n",
    "# mixes = [{'cot': 0.37664033529374275,\n",
    "#   'dolly': 0.0874640765523398,\n",
    "#   'flan_v2': 0.39740799933549775,\n",
    "#   'oasst1': 0.1384875888184196},\n",
    "#  {'cot': 0.23064419241874784,\n",
    "#   'dolly': 0.04693354147889885,\n",
    "#   'flan_v2': 0.72121745986295,\n",
    "#   'oasst1': 0.0012048062394032465},\n",
    "#  {'cot': 0.11244721555034376,\n",
    "#   'dolly': 0.21997027355988638,\n",
    "#   'flan_v2': 0.5826671754210359,\n",
    "#   'oasst1': 0.08491533546873392},\n",
    "#  {'cot': 0.27704626812045546,\n",
    "#   'dolly': 0.5712282144637615,\n",
    "#   'flan_v2': 0.024940119654536592,\n",
    "#   'oasst1': 0.12678539776124645},\n",
    "#  {'cot': 0.0024519793352964607,\n",
    "#   'dolly': 0.13274603201304974,\n",
    "#   'flan_v2': 0.012268378167304219,\n",
    "#   'oasst1': 0.8525336104843496},\n",
    "#  {'cot': 0.08065633865016615,\n",
    "#   'dolly': 0.41886215168938545,\n",
    "#   'flan_v2': 0.21723932820070485,\n",
    "#   'oasst1': 0.2832421814597436},\n",
    "#  {'cot': 0.13878643021160036,\n",
    "#   'dolly': 0.05686171157146557,\n",
    "#   'flan_v2': 0.6701353469446995,\n",
    "#   'oasst1': 0.13421651127223455},\n",
    "#  {'cot': 0.2461125374866837,\n",
    "#   'dolly': 0.09774240280444893,\n",
    "#   'flan_v2': 0.13974091986040005,\n",
    "#   'oasst1': 0.5164041398484672},\n",
    "#  {'cot': 0.4069781049152398,\n",
    "#   'dolly': 0.06318759506033228,\n",
    "#   'flan_v2': 0.09504719644992135,\n",
    "#   'oasst1': 0.4347871035745066},\n",
    "#  {'cot': 0.22379693013848484,\n",
    "#   'dolly': 0.30565901275011814,\n",
    "#   'flan_v2': 0.15457716965000887,\n",
    "#   'oasst1': 0.31596688746138824}]\n",
    "\n",
    "# mixes = [\n",
    "#     {'cot': 0.46638974, 'dolly': 0.01456044, 'flan_v2': 0.50886009, 'oasst1': 0.01018973},\n",
    "#     {'cot': 0.39744481, 'dolly': 0.00472114, 'flan_v2': 0.59104177, 'oasst1': 0.00679229},\n",
    "# ]\n",
    "\n",
    "# subsample_mixture_normalized_list += [('', d) for d in mixes]\n",
    "# subsample_mixture_normalized_list += [('humanmix', # humanmix\n",
    "#                                        {'cot': 0.48785105, 'dolly': 0.00732313, 'flan_v2': 0.48785105, 'oasst1': 0.01697478})]\n",
    "# subsample_mixture_normalized_list = [(x[0],  compute_mixture_num_samples(x[1], max_train_samples)) \n",
    "#                                      for x in subsample_mixture_normalized_list]\n",
    "# subsample_mixture_list += subsample_mixture_normalized_list\n",
    "\n",
    "\n",
    "subsample_mixture_list = [('',None)]\n",
    "subsample_inds_file_list = [None]\n",
    "\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "\n",
    "\n",
    "def subsample_inds_file_abbr_fn(x):\n",
    "    s = os.path.basename(x).split('.')[0]\n",
    "    if s.startswith('inds_'):\n",
    "        scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "        pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "        return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "\n",
    "\n",
    "\n",
    "# # ft1: reproduce open-instruct table with llama7b\n",
    "# # job_name = 'ft1'; num_train_epochs_list = [2]\n",
    "# # job_name = 'ft1_ep=1'; num_train_epochs_list = [1] # train for 1 epoch (baseline for comparison.)\n",
    "# job_name = 'ft1_ep=2'; num_train_epochs_list = [2]\n",
    "\n",
    "# # # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# # # train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# # # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# # # # # train_file = 'data/processed/wpq/cot_flanv2_data.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "# # # # # train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'hmv1'\n",
    "# # # train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# train_file = 'data/processed/ultrachat/ultrachat_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# # # max_train_samples_list = [120]; save_steps = 1; save_total_limit = 100\n",
    "\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "# train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "# train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "# train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "# # ft2: test mixture weights\n",
    "# # vary mixture weights\n",
    "# job_name = 'ft2'\n",
    "\n",
    "# # oi3: instruction tuning performance w.r.t. steps.\n",
    "# job_name = 'oi3'\n",
    "\n",
    "# # oi4: data pruning \n",
    "# job_name = 'oi4_flan_v2_vary_subsetsize'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.1, .3, .5]]; num_train_epochs_list = [2]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # oi4_perf_cross_time: perf cross time on flan_v2\n",
    "# job_name = 'oi4_perf_cross_time'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# # ## oi4_flanv2_prune_with_hmv1_model\n",
    "# job_name = 'oi4_flanv2_prune_with_hmv1_model'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b_ft=hmv1/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# ## tulu mix v1.\n",
    "# dataset = 'tulu_v1_human_mix'; train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'tuluv1hm'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [30000]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/{dataset}/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'flan2022_1m'; train_file = 'data/processed/flan2022/flan2022_1m_data.jsonl'; abbr_train_file = 'flan2022_1m'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# ## full data\n",
    "# # max_train_samples_list = [1000000]; num_train_epochs_list = [1]\n",
    "# # subsample_inds_file_list = ['']\n",
    "# # subset\n",
    "# max_train_samples_list = [200000]; num_train_epochs_list = [1]\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "#     # not that helpful\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     ## gradnorm outputs\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "#     ## random baselines.\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "#     ## kmeans on grads\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# job_name = f'oi4_{dataset}_ep=3'\n",
    "# # save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# max_train_samples_list = [50000]; num_train_epochs_list = [3]\n",
    "# subsample_inds_file_list = [\n",
    "#     # random baselines\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "# #     # correlated statistics\n",
    "# #     os.path.join(data_inds_dir, 'log_prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'log_prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_decr.pkl'),\n",
    "# #     # grad norm\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "# #     # kmeans\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_decr.pkl'),\n",
    "# # #     # kcos only 50k data\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos1np.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "## oi5: try curriculum learning\n",
    "# \n",
    "\n",
    "scoring_fn_and_pacing_fn = []\n",
    "\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "M = 150_000; dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "scoring_fn_list = ['log_prob_neg']\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep=3',\n",
    "    f'singlestep_size={M}_startingfrac=0.2',\n",
    "    f'singlestep_size={M}_startingfrac=0.1',\n",
    "]\n",
    "scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# M =  50_000; dataset = 'ultrachat'; train_file = 'data/processed/ultrachat/ultrachat_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# scoring_fn_list = ['log_prob_neg']\n",
    "# pacing_fn_list = [\n",
    "#     f'prune_size={M}_ep=3',\n",
    "#     f'singlestep_size={M}_startingfrac=0.2',\n",
    "#     f'singlestep_size={M}_startingfrac=0.1',\n",
    "# ]\n",
    "scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/{abbr_model_name}+lora:r=256:a=256/{dataset}/'\n",
    "num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "dataloader_sampler = 'SequentialSampler'\n",
    "\n",
    "\n",
    "scoring_fn_list = ['random_s=0', 'random_s=1', 'random_s=2']; pacing_fn_list = [f'prune_size={M}_ep=1'] # gives advantage to random baselines.\n",
    "scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "subsample_inds_file_list = []\n",
    "for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "    p = os.path.join(data_inds_dir, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "    if not os.path.isfile(p):\n",
    "        raise ValueError(f'path={p} does not exists.')\n",
    "    subsample_inds_file_list.append(p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12 # llama-7b on 100k. data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b on 100k. data\n",
    "# nodes = 10; num_gpus = 6; gpu_type = 'v100'; job_duration = 36 # llama-7b on 100k. data\n",
    "\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 18 # llama-7b on 400k data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 30 # llama-7b on 600k data\n",
    "\n",
    "# nodes = 1; num_gpus = 1; gpu_type = 'v100'; job_duration = 6  # gpt2\n",
    "# nodes = 2; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-1.4b\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-2.8b|6.9b\n",
    "\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "\n",
    "per_device_train_batch_size = 2; total_batch_size = 128 # 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = True\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_mixture_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     mix_name_and_subsample_mixture,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "    mix_name, subsample_mixture = mix_name_and_subsample_mixture\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "        \n",
    "    if job_name == 'ft2':\n",
    "        if subsample_mixture is not None:\n",
    "            assert(abbr_train_file=='all')\n",
    "            output_dirname += \\\n",
    "                '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "            \n",
    "    if job_name == 'oi3':\n",
    "        output_dirname += '_'+mix_name\n",
    "        \n",
    "#     if job_name.startswith('oi4'):\n",
    "    if subsample_inds_file:\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            '_dtype='+torch_dtype+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers=16 \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type=linear \\\n",
    "        --warmup_ratio=0.03 \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy=\"no\" \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --report_to=tensorboard \\\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba1bdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/log_prob_neg/inds_prune_size=150000_ep=3.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/log_prob_neg/inds_singlestep_size=150000_startingfrac=0.2.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/log_prob_neg/inds_singlestep_size=150000_startingfrac=0.1.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/el2n_agg=mean/inds_prune_size=150000_ep=3.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/el2n_agg=mean/inds_singlestep_size=150000_startingfrac=0.2.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/el2n_agg=mean/inds_singlestep_size=150000_startingfrac=0.1.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/random_s=0/inds_prune_size=150000_ep=1.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/random_s=1/inds_prune_size=150000_ep=1.pkl',\n",
       " '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/random_s=2/inds_prune_size=150000_ep=1.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e614b639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'score=random:s=0_pace=prune:size=150000:ep=1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x =  '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=256:a=256/tulu_v1_mix/random_s=0/inds_prune_size=150000_ep=1.pkl'\n",
    "\n",
    "subsample_inds_file_abbr_fn(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_all_symlinks(directory, verbose=False):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files + dirs:\n",
    "            path = os.path.join(root, name)\n",
    "            if os.path.islink(path):\n",
    "                os.unlink(path)\n",
    "                if verbose:\n",
    "                    print(f\"Removed symlink: {path}\")\n",
    "                \n",
    "import uuid\n",
    "\n",
    "def create_unique_symlinks(file_paths, verbose=False):\n",
    "    \"\"\"Create symlinks for each `file` in `files` in the same directory, with a unique name. \"\"\"\n",
    "    dirs = [os.path.dirname(x) for x in file_paths]\n",
    "\n",
    "    symlink_path_dict = {}\n",
    "    for directory, path in zip(dirs, file_paths):\n",
    "        if os.path.isdir(path):\n",
    "            symlink_name = f\"symlink_{str(uuid.uuid4())[:8]}\"  # Generate a unique symlink name\n",
    "            symlink_path = os.path.join(directory, symlink_name)\n",
    "            try:\n",
    "                os.symlink(os.path.abspath(path), symlink_path)\n",
    "                if verbose:\n",
    "                    print(f\"Created symlink: {symlink_path} -> {path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Failed to create symlink: {path}. Error: {e}\")\n",
    "            symlink_path_dict.update({path: symlink_path})\n",
    "    return symlink_path_dict\n",
    "\n",
    "\n",
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    model_name_or_path = model_name_or_path.lower()\n",
    "    if any(x in model_name_or_path for x in ['gpt2-medium', 'pythia-160m']):\n",
    "        return 50, 1\n",
    "    if any(x in model_name_or_path for x in ['gpt-xl']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp']):\n",
    "            return 16, 1\n",
    "        else:\n",
    "            return 32, 1\n",
    "    if any(x in model_name_or_path for x in ['llama', 'mistral', 'pythia-1.4b', 'pythia-2.8b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 5, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    if any(x in model_name_or_path for x in ['pythia-6.9b', 'dolly-v2-7b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'mmlu_s=0', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 4, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    return 10, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b68375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#cmds:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_dir = ''\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0', # \n",
    "    'mmlu_s=5', # ~1hr\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "task_names = ['mmlu_s=0']\n",
    "# task_names = ['alpacafarm_ann=chatgpt']\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "\n",
    "# # ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "# #     'gpt2',\n",
    "# #     'gpt2-medium',\n",
    "#     'huggyllama/llama-7b', \n",
    "#     'mistralai/Mistral-7B-v0.1',\n",
    "#     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "# #     'NousResearch/Llama-2-7b-hf',\n",
    "# #     'EleutherAI/pythia-1.4b',\n",
    "# #     'EleutherAI/pythia-2.8b',\n",
    "# #     'EleutherAI/pythia-6.9b',\n",
    "# #     'databricks/dolly-v2-7b',\n",
    "# ]]\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # ## baseline re-eval after merge upstream/main\n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b',\n",
    "# ]]\n",
    "# subdir_path_list += ['results/ft1/llama-7b_humanmix']\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1\n",
    "# exp_dir = 'results/ft1'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "## ft1_ep=1\n",
    "exp_dir = 'results/ft1_ep=1'\n",
    "exp_dir = 'results/ft1_ep=2'\n",
    "subdir_filter_fn = lambda x: 'ultrachat' in x\n",
    "# task_names = task_names_chatfmt\n",
    "task_names = task_names+task_names_chatfmt\n",
    "\n",
    "\n",
    "# ## ft2\n",
    "# exp_dir = 'results/ft2/'\n",
    "# create_symlinks = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['llama-7b'])\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## llama-7b time-series 400k, 600k\n",
    "# exp_dir = 'results/oi3/'\n",
    "# include_checkpoints = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['400k', '600k']) # , '600k'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # oi4 include checkpoints!\n",
    "# # exp_dir = 'results/oi4_perf_cross_time/'\n",
    "# # exp_dir = 'results/oi4_tulu_v1_human_mix/'\n",
    "# # exp_dir = 'results/oi4_flanv2_prune_with_hmv1_model/'\n",
    "# # exp_dir = 'results/oi4_flan2022_1m/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix_ep=3/'\n",
    "# include_checkpoints = True\n",
    "# include_checkpoints = False\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['log_prob_decr', 'el2n_agg=mean_incr', 'logit_margin_decr', 'grad_loraB'])\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "# task_names = task_names_chatfmt # eval alpacafarm only\n",
    "\n",
    "# # oi4 without checkpoint \n",
    "# # exp_dir = 'results/oi4/'\n",
    "# exp_dir = 'results/oi4_flan_v2_vary_subsetsize/'\n",
    "# task_names = task_names_chatfmt\n",
    "\n",
    "\n",
    "\n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    if 'gpt2' in ft_args_model_name_or_path:\n",
    "        tydiqa_max_context_length = 400 # max ctx len without exceeding max_seq_len\n",
    "    else:\n",
    "        tydiqa_max_context_length = 512\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length {tydiqa_max_context_length} \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_61f30_row0_col0, #T_61f30_row0_col1, #T_61f30_row0_col2, #T_61f30_row0_col3, #T_61f30_row1_col0, #T_61f30_row1_col1, #T_61f30_row1_col2, #T_61f30_row1_col3, #T_61f30_row2_col0, #T_61f30_row2_col1, #T_61f30_row2_col2, #T_61f30_row2_col3, #T_61f30_row3_col0, #T_61f30_row3_col1, #T_61f30_row3_col2, #T_61f30_row3_col3, #T_61f30_row4_col0, #T_61f30_row4_col1, #T_61f30_row4_col2, #T_61f30_row4_col3, #T_61f30_row5_col0, #T_61f30_row5_col1, #T_61f30_row5_col2, #T_61f30_row5_col3, #T_61f30_row6_col0, #T_61f30_row6_col1, #T_61f30_row6_col2, #T_61f30_row6_col3, #T_61f30_row7_col0, #T_61f30_row7_col1, #T_61f30_row7_col2, #T_61f30_row7_col3, #T_61f30_row8_col0, #T_61f30_row8_col1, #T_61f30_row8_col2, #T_61f30_row8_col3, #T_61f30_row9_col0, #T_61f30_row9_col1, #T_61f30_row9_col2, #T_61f30_row9_col3, #T_61f30_row10_col0, #T_61f30_row10_col1, #T_61f30_row10_col2, #T_61f30_row10_col3, #T_61f30_row11_col0, #T_61f30_row11_col1, #T_61f30_row11_col2, #T_61f30_row11_col3, #T_61f30_row12_col0, #T_61f30_row12_col1, #T_61f30_row12_col2, #T_61f30_row12_col3, #T_61f30_row13_col0, #T_61f30_row13_col1, #T_61f30_row13_col2, #T_61f30_row13_col3, #T_61f30_row14_col0, #T_61f30_row14_col1, #T_61f30_row14_col2, #T_61f30_row14_col3, #T_61f30_row15_col0, #T_61f30_row15_col1, #T_61f30_row15_col2, #T_61f30_row15_col3, #T_61f30_row16_col0, #T_61f30_row16_col1, #T_61f30_row16_col2, #T_61f30_row16_col3, #T_61f30_row17_col0, #T_61f30_row17_col1, #T_61f30_row17_col2, #T_61f30_row17_col3, #T_61f30_row18_col0, #T_61f30_row18_col1, #T_61f30_row18_col2, #T_61f30_row18_col3, #T_61f30_row19_col0, #T_61f30_row19_col1, #T_61f30_row19_col2, #T_61f30_row19_col3, #T_61f30_row20_col0, #T_61f30_row20_col1, #T_61f30_row20_col2, #T_61f30_row20_col3, #T_61f30_row21_col0, #T_61f30_row21_col1, #T_61f30_row21_col2, #T_61f30_row21_col3, #T_61f30_row22_col0, #T_61f30_row22_col1, #T_61f30_row22_col2, #T_61f30_row22_col3, #T_61f30_row23_col0, #T_61f30_row23_col1, #T_61f30_row23_col2, #T_61f30_row23_col3, #T_61f30_row24_col0, #T_61f30_row24_col1, #T_61f30_row24_col2, #T_61f30_row24_col3, #T_61f30_row25_col0, #T_61f30_row25_col1, #T_61f30_row25_col2, #T_61f30_row25_col3, #T_61f30_row26_col0, #T_61f30_row26_col1, #T_61f30_row26_col2, #T_61f30_row26_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_61f30_row0_col4, #T_61f30_row25_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row0_col5, #T_61f30_row0_col10, #T_61f30_row0_col11, #T_61f30_row1_col8, #T_61f30_row2_col9, #T_61f30_row7_col6, #T_61f30_row7_col7, #T_61f30_row18_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row0_col6, #T_61f30_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row0_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row0_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col4, #T_61f30_row23_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col7, #T_61f30_row2_col8, #T_61f30_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col9, #T_61f30_row5_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col10, #T_61f30_row12_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row2_col4, #T_61f30_row2_col5, #T_61f30_row3_col11, #T_61f30_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f39577;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row2_col7, #T_61f30_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row2_col10, #T_61f30_row5_col9, #T_61f30_row6_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row2_col11, #T_61f30_row4_col10, #T_61f30_row9_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row4_col5, #T_61f30_row21_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row4_col8, #T_61f30_row5_col8, #T_61f30_row6_col8, #T_61f30_row9_col5, #T_61f30_row17_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row4_col9, #T_61f30_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row4_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row5_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #b70d28;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row5_col7, #T_61f30_row14_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row5_col11, #T_61f30_row18_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row6_col4, #T_61f30_row21_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row6_col5, #T_61f30_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row6_col6, #T_61f30_row22_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row6_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row7_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row7_col8, #T_61f30_row16_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row7_col10, #T_61f30_row24_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row7_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row8_col5, #T_61f30_row9_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e46e56;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row8_col8, #T_61f30_row20_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row8_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row8_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row8_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row9_col9, #T_61f30_row9_col11, #T_61f30_row10_col9, #T_61f30_row10_col11, #T_61f30_row11_col9, #T_61f30_row11_col11, #T_61f30_row13_col9, #T_61f30_row13_col11, #T_61f30_row14_col9, #T_61f30_row14_col11, #T_61f30_row15_col9, #T_61f30_row15_col11, #T_61f30_row16_col9, #T_61f30_row16_col11, #T_61f30_row17_col9, #T_61f30_row17_col11, #T_61f30_row18_col9, #T_61f30_row18_col11, #T_61f30_row19_col9, #T_61f30_row19_col11, #T_61f30_row20_col9, #T_61f30_row20_col11, #T_61f30_row21_col9, #T_61f30_row21_col11, #T_61f30_row22_col9, #T_61f30_row22_col11, #T_61f30_row23_col9, #T_61f30_row23_col11, #T_61f30_row25_col9, #T_61f30_row25_col11, #T_61f30_row26_col9, #T_61f30_row26_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row9_col10, #T_61f30_row11_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row10_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row10_col7, #T_61f30_row24_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e1dad6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row10_col10, #T_61f30_row11_col10, #T_61f30_row20_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row11_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row11_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row11_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row11_col8, #T_61f30_row15_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row12_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row12_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row12_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row12_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row12_col8, #T_61f30_row24_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row12_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row12_col11, #T_61f30_row23_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row13_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row13_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row13_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row13_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e9d5cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row13_col10, #T_61f30_row22_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row14_col4, #T_61f30_row17_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row14_col5, #T_61f30_row15_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row14_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row14_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b497;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row14_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row15_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row15_col6, #T_61f30_row17_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row15_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row15_col10, #T_61f30_row17_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row16_col4, #T_61f30_row18_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row16_col5, #T_61f30_row19_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row16_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row16_col7, #T_61f30_row19_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row16_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row17_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row17_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b6cefa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row18_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row18_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row18_col8, #T_61f30_row20_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row19_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row19_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f49a7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row19_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row19_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row20_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row20_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row20_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row21_col4, #T_61f30_row22_col7, #T_61f30_row24_col5, #T_61f30_row24_col9, #T_61f30_row24_col11, #T_61f30_row25_col8, #T_61f30_row26_col6, #T_61f30_row26_col7, #T_61f30_row26_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row21_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row21_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row21_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row22_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row22_col8, #T_61f30_row23_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row22_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row23_col5, #T_61f30_row25_col5, #T_61f30_row26_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row23_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row23_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row24_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row24_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row25_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row25_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #8badfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_61f30_row25_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row26_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_61f30_row26_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #799cf8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_61f30\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_61f30_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_61f30_level0_col1\" class=\"col_heading level0 col1\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_61f30_level0_col2\" class=\"col_heading level0 col2\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_61f30_level0_col3\" class=\"col_heading level0 col3\" >data_args.max_train_samples</th>\n",
       "      <th id=\"T_61f30_level0_col4\" class=\"col_heading level0 col4\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_61f30_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_61f30_level0_col6\" class=\"col_heading level0 col6\" >BBH/CoT</th>\n",
       "      <th id=\"T_61f30_level0_col7\" class=\"col_heading level0 col7\" >TydiQA/GP</th>\n",
       "      <th id=\"T_61f30_level0_col8\" class=\"col_heading level0 col8\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_61f30_level0_col9\" class=\"col_heading level0 col9\" >AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th id=\"T_61f30_level0_col10\" class=\"col_heading level0 col10\" >Average</th>\n",
       "      <th id=\"T_61f30_level0_col11\" class=\"col_heading level0 col11\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_61f30_row0_col0\" class=\"data row0 col0\" >llama-7b_tuluv1_mix_ep=2</td>\n",
       "      <td id=\"T_61f30_row0_col1\" class=\"data row0 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row0_col2\" class=\"data row0 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row0_col3\" class=\"data row0 col3\" >None</td>\n",
       "      <td id=\"T_61f30_row0_col4\" class=\"data row0 col4\" >45.32</td>\n",
       "      <td id=\"T_61f30_row0_col5\" class=\"data row0 col5\" >27.60</td>\n",
       "      <td id=\"T_61f30_row0_col6\" class=\"data row0 col6\" >30.56</td>\n",
       "      <td id=\"T_61f30_row0_col7\" class=\"data row0 col7\" >40.64</td>\n",
       "      <td id=\"T_61f30_row0_col8\" class=\"data row0 col8\" >18.29</td>\n",
       "      <td id=\"T_61f30_row0_col9\" class=\"data row0 col9\" >52.36</td>\n",
       "      <td id=\"T_61f30_row0_col10\" class=\"data row0 col10\" >35.80</td>\n",
       "      <td id=\"T_61f30_row0_col11\" class=\"data row0 col11\" >-3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_61f30_row1_col0\" class=\"data row1 col0\" >llama-7b_tuluv1_mix_ep=1</td>\n",
       "      <td id=\"T_61f30_row1_col1\" class=\"data row1 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row1_col2\" class=\"data row1 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row1_col3\" class=\"data row1 col3\" >None</td>\n",
       "      <td id=\"T_61f30_row1_col4\" class=\"data row1 col4\" >43.46</td>\n",
       "      <td id=\"T_61f30_row1_col5\" class=\"data row1 col5\" >23.00</td>\n",
       "      <td id=\"T_61f30_row1_col6\" class=\"data row1 col6\" >31.94</td>\n",
       "      <td id=\"T_61f30_row1_col7\" class=\"data row1 col7\" >43.75</td>\n",
       "      <td id=\"T_61f30_row1_col8\" class=\"data row1 col8\" >18.90</td>\n",
       "      <td id=\"T_61f30_row1_col9\" class=\"data row1 col9\" >49.32</td>\n",
       "      <td id=\"T_61f30_row1_col10\" class=\"data row1 col10\" >35.06</td>\n",
       "      <td id=\"T_61f30_row1_col11\" class=\"data row1 col11\" >-4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_61f30_row2_col0\" class=\"data row2 col0\" >llama-7b_tuluv1m:50k_log_prob_decr</td>\n",
       "      <td id=\"T_61f30_row2_col1\" class=\"data row2 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row2_col2\" class=\"data row2 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row2_col3\" class=\"data row2 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row2_col4\" class=\"data row2 col4\" >42.77</td>\n",
       "      <td id=\"T_61f30_row2_col5\" class=\"data row2 col5\" >21.80</td>\n",
       "      <td id=\"T_61f30_row2_col6\" class=\"data row2 col6\" >31.57</td>\n",
       "      <td id=\"T_61f30_row2_col7\" class=\"data row2 col7\" >38.85</td>\n",
       "      <td id=\"T_61f30_row2_col8\" class=\"data row2 col8\" >17.68</td>\n",
       "      <td id=\"T_61f30_row2_col9\" class=\"data row2 col9\" >56.77</td>\n",
       "      <td id=\"T_61f30_row2_col10\" class=\"data row2 col10\" >34.91</td>\n",
       "      <td id=\"T_61f30_row2_col11\" class=\"data row2 col11\" >-5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_61f30_row3_col0\" class=\"data row3 col0\" >llama-7b_tuluv1m:50k_el2n_agg=mean_incr</td>\n",
       "      <td id=\"T_61f30_row3_col1\" class=\"data row3 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row3_col2\" class=\"data row3 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row3_col3\" class=\"data row3 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row3_col4\" class=\"data row3 col4\" >43.62</td>\n",
       "      <td id=\"T_61f30_row3_col5\" class=\"data row3 col5\" >21.20</td>\n",
       "      <td id=\"T_61f30_row3_col6\" class=\"data row3 col6\" >31.30</td>\n",
       "      <td id=\"T_61f30_row3_col7\" class=\"data row3 col7\" >37.35</td>\n",
       "      <td id=\"T_61f30_row3_col8\" class=\"data row3 col8\" >17.68</td>\n",
       "      <td id=\"T_61f30_row3_col9\" class=\"data row3 col9\" >51.18</td>\n",
       "      <td id=\"T_61f30_row3_col10\" class=\"data row3 col10\" >33.72</td>\n",
       "      <td id=\"T_61f30_row3_col11\" class=\"data row3 col11\" >-7.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_61f30_row4_col0\" class=\"data row4 col0\" >llama-7b_tuluv1m:50k_logit_margin_decr</td>\n",
       "      <td id=\"T_61f30_row4_col1\" class=\"data row4 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row4_col2\" class=\"data row4 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row4_col3\" class=\"data row4 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row4_col4\" class=\"data row4 col4\" >42.92</td>\n",
       "      <td id=\"T_61f30_row4_col5\" class=\"data row4 col5\" >20.20</td>\n",
       "      <td id=\"T_61f30_row4_col6\" class=\"data row4 col6\" >30.19</td>\n",
       "      <td id=\"T_61f30_row4_col7\" class=\"data row4 col7\" >40.27</td>\n",
       "      <td id=\"T_61f30_row4_col8\" class=\"data row4 col8\" >13.41</td>\n",
       "      <td id=\"T_61f30_row4_col9\" class=\"data row4 col9\" >53.35</td>\n",
       "      <td id=\"T_61f30_row4_col10\" class=\"data row4 col10\" >33.39</td>\n",
       "      <td id=\"T_61f30_row4_col11\" class=\"data row4 col11\" >-7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_61f30_row5_col0\" class=\"data row5 col0\" >llama-7b_tuluv1m:50k_random_s=2</td>\n",
       "      <td id=\"T_61f30_row5_col1\" class=\"data row5 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row5_col2\" class=\"data row5 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row5_col3\" class=\"data row5 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row5_col4\" class=\"data row5 col4\" >41.60</td>\n",
       "      <td id=\"T_61f30_row5_col5\" class=\"data row5 col5\" >17.60</td>\n",
       "      <td id=\"T_61f30_row5_col6\" class=\"data row5 col6\" >32.41</td>\n",
       "      <td id=\"T_61f30_row5_col7\" class=\"data row5 col7\" >39.94</td>\n",
       "      <td id=\"T_61f30_row5_col8\" class=\"data row5 col8\" >13.41</td>\n",
       "      <td id=\"T_61f30_row5_col9\" class=\"data row5 col9\" >54.17</td>\n",
       "      <td id=\"T_61f30_row5_col10\" class=\"data row5 col10\" >33.19</td>\n",
       "      <td id=\"T_61f30_row5_col11\" class=\"data row5 col11\" >-6.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_61f30_row6_col0\" class=\"data row6 col0\" >llama-7b_tuluv1m:50k_random_s=0</td>\n",
       "      <td id=\"T_61f30_row6_col1\" class=\"data row6 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row6_col2\" class=\"data row6 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row6_col3\" class=\"data row6 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row6_col4\" class=\"data row6 col4\" >40.71</td>\n",
       "      <td id=\"T_61f30_row6_col5\" class=\"data row6 col5\" >16.40</td>\n",
       "      <td id=\"T_61f30_row6_col6\" class=\"data row6 col6\" >29.72</td>\n",
       "      <td id=\"T_61f30_row6_col7\" class=\"data row6 col7\" >43.61</td>\n",
       "      <td id=\"T_61f30_row6_col8\" class=\"data row6 col8\" >13.41</td>\n",
       "      <td id=\"T_61f30_row6_col9\" class=\"data row6 col9\" >54.22</td>\n",
       "      <td id=\"T_61f30_row6_col10\" class=\"data row6 col10\" >33.01</td>\n",
       "      <td id=\"T_61f30_row6_col11\" class=\"data row6 col11\" >-8.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_61f30_row7_col0\" class=\"data row7 col0\" >llama-7b_tuluv1_humanmix_ep=1</td>\n",
       "      <td id=\"T_61f30_row7_col1\" class=\"data row7 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row7_col2\" class=\"data row7 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row7_col3\" class=\"data row7 col3\" >None</td>\n",
       "      <td id=\"T_61f30_row7_col4\" class=\"data row7 col4\" >38.48</td>\n",
       "      <td id=\"T_61f30_row7_col5\" class=\"data row7 col5\" >23.20</td>\n",
       "      <td id=\"T_61f30_row7_col6\" class=\"data row7 col6\" >32.59</td>\n",
       "      <td id=\"T_61f30_row7_col7\" class=\"data row7 col7\" >44.98</td>\n",
       "      <td id=\"T_61f30_row7_col8\" class=\"data row7 col8\" >10.98</td>\n",
       "      <td id=\"T_61f30_row7_col9\" class=\"data row7 col9\" >34.14</td>\n",
       "      <td id=\"T_61f30_row7_col10\" class=\"data row7 col10\" >30.73</td>\n",
       "      <td id=\"T_61f30_row7_col11\" class=\"data row7 col11\" >-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_61f30_row8_col0\" class=\"data row8 col0\" >llama-7b_tuluv1m:50k_random_s=1</td>\n",
       "      <td id=\"T_61f30_row8_col1\" class=\"data row8 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row8_col2\" class=\"data row8 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row8_col3\" class=\"data row8 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row8_col4\" class=\"data row8 col4\" >41.86</td>\n",
       "      <td id=\"T_61f30_row8_col5\" class=\"data row8 col5\" >20.40</td>\n",
       "      <td id=\"T_61f30_row8_col6\" class=\"data row8 col6\" >30.00</td>\n",
       "      <td id=\"T_61f30_row8_col7\" class=\"data row8 col7\" >39.05</td>\n",
       "      <td id=\"T_61f30_row8_col8\" class=\"data row8 col8\" >12.20</td>\n",
       "      <td id=\"T_61f30_row8_col9\" class=\"data row8 col9\" >27.08</td>\n",
       "      <td id=\"T_61f30_row8_col10\" class=\"data row8 col10\" >28.43</td>\n",
       "      <td id=\"T_61f30_row8_col11\" class=\"data row8 col11\" >-10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_61f30_row9_col0\" class=\"data row9 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=grad+rp+loraB_k=Kcos</td>\n",
       "      <td id=\"T_61f30_row9_col1\" class=\"data row9 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row9_col2\" class=\"data row9 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row9_col3\" class=\"data row9 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row9_col4\" class=\"data row9 col4\" >37.15</td>\n",
       "      <td id=\"T_61f30_row9_col5\" class=\"data row9 col5\" >20.60</td>\n",
       "      <td id=\"T_61f30_row9_col6\" class=\"data row9 col6\" >30.28</td>\n",
       "      <td id=\"T_61f30_row9_col7\" class=\"data row9 col7\" >39.10</td>\n",
       "      <td id=\"T_61f30_row9_col8\" class=\"data row9 col8\" >9.15</td>\n",
       "      <td id=\"T_61f30_row9_col9\" class=\"data row9 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row9_col10\" class=\"data row9 col10\" >27.25</td>\n",
       "      <td id=\"T_61f30_row9_col11\" class=\"data row9 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_61f30_row10_col0\" class=\"data row10 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=3000_incr</td>\n",
       "      <td id=\"T_61f30_row10_col1\" class=\"data row10 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row10_col2\" class=\"data row10 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row10_col3\" class=\"data row10 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row10_col4\" class=\"data row10 col4\" >39.92</td>\n",
       "      <td id=\"T_61f30_row10_col5\" class=\"data row10 col5\" >16.00</td>\n",
       "      <td id=\"T_61f30_row10_col6\" class=\"data row10 col6\" >31.67</td>\n",
       "      <td id=\"T_61f30_row10_col7\" class=\"data row10 col7\" >38.45</td>\n",
       "      <td id=\"T_61f30_row10_col8\" class=\"data row10 col8\" >9.76</td>\n",
       "      <td id=\"T_61f30_row10_col9\" class=\"data row10 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row10_col10\" class=\"data row10 col10\" >27.16</td>\n",
       "      <td id=\"T_61f30_row10_col11\" class=\"data row10 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_61f30_row11_col0\" class=\"data row11 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=1000_incr</td>\n",
       "      <td id=\"T_61f30_row11_col1\" class=\"data row11 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row11_col2\" class=\"data row11 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row11_col3\" class=\"data row11 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row11_col4\" class=\"data row11 col4\" >39.37</td>\n",
       "      <td id=\"T_61f30_row11_col5\" class=\"data row11 col5\" >16.80</td>\n",
       "      <td id=\"T_61f30_row11_col6\" class=\"data row11 col6\" >32.22</td>\n",
       "      <td id=\"T_61f30_row11_col7\" class=\"data row11 col7\" >38.84</td>\n",
       "      <td id=\"T_61f30_row11_col8\" class=\"data row11 col8\" >8.54</td>\n",
       "      <td id=\"T_61f30_row11_col9\" class=\"data row11 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row11_col10\" class=\"data row11 col10\" >27.15</td>\n",
       "      <td id=\"T_61f30_row11_col11\" class=\"data row11 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_61f30_row12_col0\" class=\"data row12 col0\" >llama-7b_tuluv1m:50k_grad_loraB_l2n_incr</td>\n",
       "      <td id=\"T_61f30_row12_col1\" class=\"data row12 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row12_col2\" class=\"data row12 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row12_col3\" class=\"data row12 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row12_col4\" class=\"data row12 col4\" >36.55</td>\n",
       "      <td id=\"T_61f30_row12_col5\" class=\"data row12 col5\" >8.60</td>\n",
       "      <td id=\"T_61f30_row12_col6\" class=\"data row12 col6\" >28.80</td>\n",
       "      <td id=\"T_61f30_row12_col7\" class=\"data row12 col7\" >29.44</td>\n",
       "      <td id=\"T_61f30_row12_col8\" class=\"data row12 col8\" >4.27</td>\n",
       "      <td id=\"T_61f30_row12_col9\" class=\"data row12 col9\" >54.71</td>\n",
       "      <td id=\"T_61f30_row12_col10\" class=\"data row12 col10\" >27.06</td>\n",
       "      <td id=\"T_61f30_row12_col11\" class=\"data row12 col11\" >-17.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_61f30_row13_col0\" class=\"data row13 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=3000_decr</td>\n",
       "      <td id=\"T_61f30_row13_col1\" class=\"data row13 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row13_col2\" class=\"data row13 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row13_col3\" class=\"data row13 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row13_col4\" class=\"data row13 col4\" >40.22</td>\n",
       "      <td id=\"T_61f30_row13_col5\" class=\"data row13 col5\" >13.00</td>\n",
       "      <td id=\"T_61f30_row13_col6\" class=\"data row13 col6\" >29.35</td>\n",
       "      <td id=\"T_61f30_row13_col7\" class=\"data row13 col7\" >38.55</td>\n",
       "      <td id=\"T_61f30_row13_col8\" class=\"data row13 col8\" >10.37</td>\n",
       "      <td id=\"T_61f30_row13_col9\" class=\"data row13 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row13_col10\" class=\"data row13 col10\" >26.30</td>\n",
       "      <td id=\"T_61f30_row13_col11\" class=\"data row13 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_61f30_row14_col0\" class=\"data row14 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=1000_decr</td>\n",
       "      <td id=\"T_61f30_row14_col1\" class=\"data row14 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row14_col2\" class=\"data row14 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row14_col3\" class=\"data row14 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row14_col4\" class=\"data row14 col4\" >37.03</td>\n",
       "      <td id=\"T_61f30_row14_col5\" class=\"data row14 col5\" >12.60</td>\n",
       "      <td id=\"T_61f30_row14_col6\" class=\"data row14 col6\" >28.70</td>\n",
       "      <td id=\"T_61f30_row14_col7\" class=\"data row14 col7\" >39.98</td>\n",
       "      <td id=\"T_61f30_row14_col8\" class=\"data row14 col8\" >12.80</td>\n",
       "      <td id=\"T_61f30_row14_col9\" class=\"data row14 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row14_col10\" class=\"data row14 col10\" >26.22</td>\n",
       "      <td id=\"T_61f30_row14_col11\" class=\"data row14 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_61f30_row15_col0\" class=\"data row15 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=text+embedding_k=Kcos1np</td>\n",
       "      <td id=\"T_61f30_row15_col1\" class=\"data row15 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row15_col2\" class=\"data row15 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row15_col3\" class=\"data row15 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row15_col4\" class=\"data row15 col4\" >40.24</td>\n",
       "      <td id=\"T_61f30_row15_col5\" class=\"data row15 col5\" >12.60</td>\n",
       "      <td id=\"T_61f30_row15_col6\" class=\"data row15 col6\" >28.98</td>\n",
       "      <td id=\"T_61f30_row15_col7\" class=\"data row15 col7\" >39.66</td>\n",
       "      <td id=\"T_61f30_row15_col8\" class=\"data row15 col8\" >8.54</td>\n",
       "      <td id=\"T_61f30_row15_col9\" class=\"data row15 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row15_col10\" class=\"data row15 col10\" >26.00</td>\n",
       "      <td id=\"T_61f30_row15_col11\" class=\"data row15 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_61f30_row16_col0\" class=\"data row16 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=text+embedding_k=Kcos</td>\n",
       "      <td id=\"T_61f30_row16_col1\" class=\"data row16 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row16_col2\" class=\"data row16 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row16_col3\" class=\"data row16 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row16_col4\" class=\"data row16 col4\" >37.52</td>\n",
       "      <td id=\"T_61f30_row16_col5\" class=\"data row16 col5\" >12.80</td>\n",
       "      <td id=\"T_61f30_row16_col6\" class=\"data row16 col6\" >27.78</td>\n",
       "      <td id=\"T_61f30_row16_col7\" class=\"data row16 col7\" >37.16</td>\n",
       "      <td id=\"T_61f30_row16_col8\" class=\"data row16 col8\" >10.98</td>\n",
       "      <td id=\"T_61f30_row16_col9\" class=\"data row16 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row16_col10\" class=\"data row16 col10\" >25.25</td>\n",
       "      <td id=\"T_61f30_row16_col11\" class=\"data row16 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_61f30_row17_col0\" class=\"data row17 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=3000_incr</td>\n",
       "      <td id=\"T_61f30_row17_col1\" class=\"data row17 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row17_col2\" class=\"data row17 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row17_col3\" class=\"data row17 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row17_col4\" class=\"data row17 col4\" >38.43</td>\n",
       "      <td id=\"T_61f30_row17_col5\" class=\"data row17 col5\" >12.20</td>\n",
       "      <td id=\"T_61f30_row17_col6\" class=\"data row17 col6\" >28.98</td>\n",
       "      <td id=\"T_61f30_row17_col7\" class=\"data row17 col7\" >39.24</td>\n",
       "      <td id=\"T_61f30_row17_col8\" class=\"data row17 col8\" >6.10</td>\n",
       "      <td id=\"T_61f30_row17_col9\" class=\"data row17 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row17_col10\" class=\"data row17 col10\" >24.99</td>\n",
       "      <td id=\"T_61f30_row17_col11\" class=\"data row17 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_61f30_row18_col0\" class=\"data row18 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=1000_decr</td>\n",
       "      <td id=\"T_61f30_row18_col1\" class=\"data row18 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row18_col2\" class=\"data row18 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row18_col3\" class=\"data row18 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row18_col4\" class=\"data row18 col4\" >45.91</td>\n",
       "      <td id=\"T_61f30_row18_col5\" class=\"data row18 col5\" >7.20</td>\n",
       "      <td id=\"T_61f30_row18_col6\" class=\"data row18 col6\" >23.06</td>\n",
       "      <td id=\"T_61f30_row18_col7\" class=\"data row18 col7\" >40.88</td>\n",
       "      <td id=\"T_61f30_row18_col8\" class=\"data row18 col8\" >6.71</td>\n",
       "      <td id=\"T_61f30_row18_col9\" class=\"data row18 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row18_col10\" class=\"data row18 col10\" >24.75</td>\n",
       "      <td id=\"T_61f30_row18_col11\" class=\"data row18 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_61f30_row19_col0\" class=\"data row19 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=grad+rp+loraB_k=Kcos1np</td>\n",
       "      <td id=\"T_61f30_row19_col1\" class=\"data row19 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row19_col2\" class=\"data row19 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row19_col3\" class=\"data row19 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row19_col4\" class=\"data row19 col4\" >40.68</td>\n",
       "      <td id=\"T_61f30_row19_col5\" class=\"data row19 col5\" >8.40</td>\n",
       "      <td id=\"T_61f30_row19_col6\" class=\"data row19 col6\" >28.33</td>\n",
       "      <td id=\"T_61f30_row19_col7\" class=\"data row19 col7\" >35.84</td>\n",
       "      <td id=\"T_61f30_row19_col8\" class=\"data row19 col8\" >7.32</td>\n",
       "      <td id=\"T_61f30_row19_col9\" class=\"data row19 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row19_col10\" class=\"data row19 col10\" >24.11</td>\n",
       "      <td id=\"T_61f30_row19_col11\" class=\"data row19 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_61f30_row20_col0\" class=\"data row20 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=3000_decr</td>\n",
       "      <td id=\"T_61f30_row20_col1\" class=\"data row20 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row20_col2\" class=\"data row20 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row20_col3\" class=\"data row20 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row20_col4\" class=\"data row20 col4\" >44.25</td>\n",
       "      <td id=\"T_61f30_row20_col5\" class=\"data row20 col5\" >3.80</td>\n",
       "      <td id=\"T_61f30_row20_col6\" class=\"data row20 col6\" >24.17</td>\n",
       "      <td id=\"T_61f30_row20_col7\" class=\"data row20 col7\" >38.06</td>\n",
       "      <td id=\"T_61f30_row20_col8\" class=\"data row20 col8\" >6.71</td>\n",
       "      <td id=\"T_61f30_row20_col9\" class=\"data row20 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row20_col10\" class=\"data row20 col10\" >23.40</td>\n",
       "      <td id=\"T_61f30_row20_col11\" class=\"data row20 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_61f30_row21_col0\" class=\"data row21 col0\" >llama-7b</td>\n",
       "      <td id=\"T_61f30_row21_col1\" class=\"data row21 col1\" >huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row21_col2\" class=\"data row21 col2\" >{}</td>\n",
       "      <td id=\"T_61f30_row21_col3\" class=\"data row21 col3\" >None</td>\n",
       "      <td id=\"T_61f30_row21_col4\" class=\"data row21 col4\" >32.82</td>\n",
       "      <td id=\"T_61f30_row21_col5\" class=\"data row21 col5\" >10.80</td>\n",
       "      <td id=\"T_61f30_row21_col6\" class=\"data row21 col6\" >27.41</td>\n",
       "      <td id=\"T_61f30_row21_col7\" class=\"data row21 col7\" >37.21</td>\n",
       "      <td id=\"T_61f30_row21_col8\" class=\"data row21 col8\" >1.83</td>\n",
       "      <td id=\"T_61f30_row21_col9\" class=\"data row21 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row21_col10\" class=\"data row21 col10\" >22.01</td>\n",
       "      <td id=\"T_61f30_row21_col11\" class=\"data row21 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_61f30_row22_col0\" class=\"data row22 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=1000_incr</td>\n",
       "      <td id=\"T_61f30_row22_col1\" class=\"data row22 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row22_col2\" class=\"data row22 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row22_col3\" class=\"data row22 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row22_col4\" class=\"data row22 col4\" >38.66</td>\n",
       "      <td id=\"T_61f30_row22_col5\" class=\"data row22 col5\" >12.40</td>\n",
       "      <td id=\"T_61f30_row22_col6\" class=\"data row22 col6\" >29.72</td>\n",
       "      <td id=\"T_61f30_row22_col7\" class=\"data row22 col7\" >25.35</td>\n",
       "      <td id=\"T_61f30_row22_col8\" class=\"data row22 col8\" >3.05</td>\n",
       "      <td id=\"T_61f30_row22_col9\" class=\"data row22 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row22_col10\" class=\"data row22 col10\" >21.84</td>\n",
       "      <td id=\"T_61f30_row22_col11\" class=\"data row22 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_61f30_row23_col0\" class=\"data row23 col0\" >llama-7b_tuluv1m:50k_el2n_agg=mean_decr</td>\n",
       "      <td id=\"T_61f30_row23_col1\" class=\"data row23 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row23_col2\" class=\"data row23 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row23_col3\" class=\"data row23 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row23_col4\" class=\"data row23 col4\" >43.48</td>\n",
       "      <td id=\"T_61f30_row23_col5\" class=\"data row23 col5\" >3.60</td>\n",
       "      <td id=\"T_61f30_row23_col6\" class=\"data row23 col6\" >23.52</td>\n",
       "      <td id=\"T_61f30_row23_col7\" class=\"data row23 col7\" >28.62</td>\n",
       "      <td id=\"T_61f30_row23_col8\" class=\"data row23 col8\" >3.05</td>\n",
       "      <td id=\"T_61f30_row23_col9\" class=\"data row23 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row23_col10\" class=\"data row23 col10\" >20.45</td>\n",
       "      <td id=\"T_61f30_row23_col11\" class=\"data row23 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_61f30_row24_col0\" class=\"data row24 col0\" >llama-7b_tuluv1m:50k_grad_loraB_l2n_decr</td>\n",
       "      <td id=\"T_61f30_row24_col1\" class=\"data row24 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row24_col2\" class=\"data row24 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row24_col3\" class=\"data row24 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row24_col4\" class=\"data row24 col4\" >42.05</td>\n",
       "      <td id=\"T_61f30_row24_col5\" class=\"data row24 col5\" >3.40</td>\n",
       "      <td id=\"T_61f30_row24_col6\" class=\"data row24 col6\" >19.54</td>\n",
       "      <td id=\"T_61f30_row24_col7\" class=\"data row24 col7\" >38.46</td>\n",
       "      <td id=\"T_61f30_row24_col8\" class=\"data row24 col8\" >4.88</td>\n",
       "      <td id=\"T_61f30_row24_col9\" class=\"data row24 col9\" >7.83</td>\n",
       "      <td id=\"T_61f30_row24_col10\" class=\"data row24 col10\" >19.36</td>\n",
       "      <td id=\"T_61f30_row24_col11\" class=\"data row24 col11\" >-19.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_61f30_row25_col0\" class=\"data row25 col0\" >llama-7b_tuluv1m:50k_log_prob_incr</td>\n",
       "      <td id=\"T_61f30_row25_col1\" class=\"data row25 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row25_col2\" class=\"data row25 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row25_col3\" class=\"data row25 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row25_col4\" class=\"data row25 col4\" >45.30</td>\n",
       "      <td id=\"T_61f30_row25_col5\" class=\"data row25 col5\" >3.60</td>\n",
       "      <td id=\"T_61f30_row25_col6\" class=\"data row25 col6\" >16.94</td>\n",
       "      <td id=\"T_61f30_row25_col7\" class=\"data row25 col7\" >30.11</td>\n",
       "      <td id=\"T_61f30_row25_col8\" class=\"data row25 col8\" >0.00</td>\n",
       "      <td id=\"T_61f30_row25_col9\" class=\"data row25 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row25_col10\" class=\"data row25 col10\" >19.19</td>\n",
       "      <td id=\"T_61f30_row25_col11\" class=\"data row25 col11\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61f30_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_61f30_row26_col0\" class=\"data row26 col0\" >llama-7b_tuluv1m:50k_logit_margin_incr</td>\n",
       "      <td id=\"T_61f30_row26_col1\" class=\"data row26 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_61f30_row26_col2\" class=\"data row26 col2\" >None</td>\n",
       "      <td id=\"T_61f30_row26_col3\" class=\"data row26 col3\" >50000</td>\n",
       "      <td id=\"T_61f30_row26_col4\" class=\"data row26 col4\" >44.76</td>\n",
       "      <td id=\"T_61f30_row26_col5\" class=\"data row26 col5\" >3.60</td>\n",
       "      <td id=\"T_61f30_row26_col6\" class=\"data row26 col6\" >15.74</td>\n",
       "      <td id=\"T_61f30_row26_col7\" class=\"data row26 col7\" >25.40</td>\n",
       "      <td id=\"T_61f30_row26_col8\" class=\"data row26 col8\" >3.66</td>\n",
       "      <td id=\"T_61f30_row26_col9\" class=\"data row26 col9\" >nan</td>\n",
       "      <td id=\"T_61f30_row26_col10\" class=\"data row26 col10\" >18.63</td>\n",
       "      <td id=\"T_61f30_row26_col11\" class=\"data row26 col11\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff78460d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = True\n",
    "\n",
    "# baselines\n",
    "save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "#     ('llama-7b_after_transformers_10.16.2023_merge', '../results/baselines/huggyllama/llama-7b'),\n",
    "#     ('llama-7b_after_09.23.2023_commit_merge', '../results/baselines/huggyllama/llama-7b_after_09.23.2023_commit_merge/'),\n",
    "#     ('llama-7b_before_09.23.2023_commit_merge', '../results/baselines/huggyllama/llama-7b_before_09.23.2023_commit_merge/'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "# #     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1')\n",
    "# ]\n",
    "# use_normalized_preferred_metric = False\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# exp_dir = '../results/ft1'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'), # 2 epochs\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# # exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# use_normalized_preferred_metric = True\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "use_normalized_preferred_metric = False\n",
    "save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "             ('llama-7b_tuluv1_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "             ('llama-7b_tuluv1_mix_ep=1', '../results/ft1_ep=1/llama-7b_tuluv1m'),\n",
    "             ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "            ]\n",
    "save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_tulu_v1_human_mix'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_hmv1_epochs=2', '../results/ft1/llama-7b_humanmix'),]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "# exp_dir = '../results/ft2'\n",
    "# save_dirs = []\n",
    "\n",
    "# run_dirs = [\n",
    "#     'pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-2.8b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-6.9b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "# ]\n",
    "# for run_dir in run_dirs:\n",
    "#     save_dirs += [(os.path.basename(x), x) \n",
    "#                   for x in glob.glob(os.path.join(exp_dir, run_dir, 'checkpoint-*'))]\n",
    "\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:200k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "\n",
    "# # check mmlu chat_format_version\n",
    "# exp_dir = ''\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b-huamnmix', '../results/ft1/llama-7b_humanmix'),\n",
    "# ]\n",
    "\n",
    "# save_dirs = [x for x in save_dirs if 'replace' not in x[1]]\n",
    "\n",
    "\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['', 'ft2', 'oi4']):\n",
    "    chat_fmt = True\n",
    "    ft_args_fields = [\n",
    "        'run_name',\n",
    "        'model_args.model_name_or_path',\n",
    "        'data_args.subsample_mixture',\n",
    "        'data_args.max_train_samples',\n",
    "    ]\n",
    "    print(f'chat_fmt={chat_fmt}')\n",
    "    df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "#     df = get_eval_results(save_dirs, chat_fmt='both', ft_args_fields=ft_args_fields)\n",
    "    cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] #  'ToxiGen/Acc'\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR'] #  'ToxiGen/Acc'\n",
    "\n",
    "    df = df[ft_args_fields + cols]\n",
    "    df['Average'] = df[cols].mean(axis=1)\n",
    "    if sort_rows:\n",
    "        df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "        sort_value_col, sort_value_col_ascending = 'Average', False\n",
    "    #     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "        df = df.sort_values(sort_value_col, ascending=sort_value_col_ascending)\n",
    "    df = df.reset_index(drop=True)\n",
    "else:\n",
    "    ft_args_fields = [\n",
    "        'run_name',\n",
    "        'data_args.subsample_mixture',\n",
    "        'data_args.max_train_samples',\n",
    "    ]\n",
    "    df = get_eval_results(save_dirs, chat_fmt='both', ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "    df = df[ft_args_fields+cols]\n",
    "    \n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "elif any(exp_dir.endswith(x) for x in ['oi4']):\n",
    "    dfc = df.copy()\n",
    "    dfc = dfc.reset_index(drop=True)\n",
    "    if len(dfc):\n",
    "        display(dfc\n",
    "                .style\n",
    "                .set_properties(**{'text-align': 'left'})\n",
    "                .background_gradient(cmap ='coolwarm')\n",
    "                .format(precision=1))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x)]\n",
    "        dfc = dfc.reset_index(drop=True)\n",
    "        if len(dfc):\n",
    "            display(dfc\n",
    "                    .style\n",
    "                    .set_properties(**{'text-align': 'left'})\n",
    "                    .background_gradient(cmap ='coolwarm')\n",
    "                    .format(precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16806ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_97c0f_row0_col0, #T_97c0f_row1_col0, #T_97c0f_row2_col0, #T_97c0f_row3_col0, #T_97c0f_row4_col0, #T_97c0f_row5_col0, #T_97c0f_row6_col0, #T_97c0f_row7_col0, #T_97c0f_row8_col0, #T_97c0f_row9_col0, #T_97c0f_row10_col0, #T_97c0f_row11_col0, #T_97c0f_row12_col0, #T_97c0f_row13_col0, #T_97c0f_row14_col0, #T_97c0f_row15_col0, #T_97c0f_row16_col0, #T_97c0f_row17_col0, #T_97c0f_row18_col0, #T_97c0f_row19_col0, #T_97c0f_row20_col0, #T_97c0f_row21_col0, #T_97c0f_row22_col0, #T_97c0f_row23_col0, #T_97c0f_row24_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_97c0f_row0_col1, #T_97c0f_row23_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row0_col2, #T_97c0f_row0_col7, #T_97c0f_row0_col8, #T_97c0f_row1_col5, #T_97c0f_row2_col6, #T_97c0f_row6_col3, #T_97c0f_row6_col4, #T_97c0f_row16_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row0_col3, #T_97c0f_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row0_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row0_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col1, #T_97c0f_row21_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col4, #T_97c0f_row2_col5, #T_97c0f_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col7, #T_97c0f_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row2_col1, #T_97c0f_row2_col2, #T_97c0f_row3_col8, #T_97c0f_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f39577;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row2_col4, #T_97c0f_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row2_col8, #T_97c0f_row4_col7, #T_97c0f_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row3_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row3_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row4_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row4_col2, #T_97c0f_row19_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row4_col5, #T_97c0f_row7_col2, #T_97c0f_row15_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row5_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row5_col4, #T_97c0f_row16_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row5_col7, #T_97c0f_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row5_col8, #T_97c0f_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row6_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row6_col5, #T_97c0f_row14_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row6_col7, #T_97c0f_row22_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row7_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row7_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row7_col6, #T_97c0f_row7_col8, #T_97c0f_row8_col6, #T_97c0f_row8_col8, #T_97c0f_row9_col6, #T_97c0f_row9_col8, #T_97c0f_row11_col6, #T_97c0f_row11_col8, #T_97c0f_row12_col6, #T_97c0f_row12_col8, #T_97c0f_row13_col6, #T_97c0f_row13_col8, #T_97c0f_row14_col6, #T_97c0f_row14_col8, #T_97c0f_row15_col6, #T_97c0f_row15_col8, #T_97c0f_row16_col6, #T_97c0f_row16_col8, #T_97c0f_row17_col6, #T_97c0f_row17_col8, #T_97c0f_row18_col6, #T_97c0f_row18_col8, #T_97c0f_row19_col6, #T_97c0f_row19_col8, #T_97c0f_row20_col6, #T_97c0f_row20_col8, #T_97c0f_row21_col6, #T_97c0f_row21_col8, #T_97c0f_row23_col6, #T_97c0f_row23_col8, #T_97c0f_row24_col6, #T_97c0f_row24_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row7_col7, #T_97c0f_row9_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row8_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row8_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row8_col4, #T_97c0f_row22_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e1dad6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row8_col7, #T_97c0f_row9_col7, #T_97c0f_row18_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row9_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row9_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row9_col5, #T_97c0f_row13_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row10_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row10_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row10_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row10_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row10_col5, #T_97c0f_row22_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row10_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row10_col8, #T_97c0f_row21_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row11_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row11_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row11_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row11_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row11_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e9d5cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row11_col7, #T_97c0f_row20_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row12_col1, #T_97c0f_row15_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row12_col2, #T_97c0f_row13_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row12_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row12_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row12_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b497;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row12_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row13_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row13_col3, #T_97c0f_row15_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row13_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row13_col7, #T_97c0f_row15_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row14_col1, #T_97c0f_row16_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row14_col2, #T_97c0f_row17_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row14_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row14_col4, #T_97c0f_row17_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row14_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row15_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row15_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b6cefa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row16_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row16_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row16_col5, #T_97c0f_row18_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row17_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row17_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f49a7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row17_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row17_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row18_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row18_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row18_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row18_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row19_col1, #T_97c0f_row20_col4, #T_97c0f_row22_col2, #T_97c0f_row22_col6, #T_97c0f_row22_col8, #T_97c0f_row23_col5, #T_97c0f_row24_col3, #T_97c0f_row24_col4, #T_97c0f_row24_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row19_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row19_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row19_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row19_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row20_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row20_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row20_col5, #T_97c0f_row21_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row20_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row21_col2, #T_97c0f_row23_col2, #T_97c0f_row24_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row21_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row21_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row22_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row22_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row23_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row23_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #8badfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_97c0f_row23_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row24_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_97c0f_row24_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #799cf8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_97c0f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_97c0f_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_97c0f_level0_col1\" class=\"col_heading level0 col1\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_97c0f_level0_col2\" class=\"col_heading level0 col2\" >GSM/CoT</th>\n",
       "      <th id=\"T_97c0f_level0_col3\" class=\"col_heading level0 col3\" >BBH/CoT</th>\n",
       "      <th id=\"T_97c0f_level0_col4\" class=\"col_heading level0 col4\" >TydiQA/GP</th>\n",
       "      <th id=\"T_97c0f_level0_col5\" class=\"col_heading level0 col5\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_97c0f_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th id=\"T_97c0f_level0_col7\" class=\"col_heading level0 col7\" >Average</th>\n",
       "      <th id=\"T_97c0f_level0_col8\" class=\"col_heading level0 col8\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_97c0f_row0_col0\" class=\"data row0 col0\" >llama-7b_tuluv1_mix_ep=2</td>\n",
       "      <td id=\"T_97c0f_row0_col1\" class=\"data row0 col1\" >45.3</td>\n",
       "      <td id=\"T_97c0f_row0_col2\" class=\"data row0 col2\" >27.6</td>\n",
       "      <td id=\"T_97c0f_row0_col3\" class=\"data row0 col3\" >30.6</td>\n",
       "      <td id=\"T_97c0f_row0_col4\" class=\"data row0 col4\" >40.6</td>\n",
       "      <td id=\"T_97c0f_row0_col5\" class=\"data row0 col5\" >18.3</td>\n",
       "      <td id=\"T_97c0f_row0_col6\" class=\"data row0 col6\" >52.4</td>\n",
       "      <td id=\"T_97c0f_row0_col7\" class=\"data row0 col7\" >35.8</td>\n",
       "      <td id=\"T_97c0f_row0_col8\" class=\"data row0 col8\" >-3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_97c0f_row1_col0\" class=\"data row1 col0\" >llama-7b_tuluv1_mix_ep=1</td>\n",
       "      <td id=\"T_97c0f_row1_col1\" class=\"data row1 col1\" >43.5</td>\n",
       "      <td id=\"T_97c0f_row1_col2\" class=\"data row1 col2\" >23.0</td>\n",
       "      <td id=\"T_97c0f_row1_col3\" class=\"data row1 col3\" >31.9</td>\n",
       "      <td id=\"T_97c0f_row1_col4\" class=\"data row1 col4\" >43.7</td>\n",
       "      <td id=\"T_97c0f_row1_col5\" class=\"data row1 col5\" >18.9</td>\n",
       "      <td id=\"T_97c0f_row1_col6\" class=\"data row1 col6\" >49.3</td>\n",
       "      <td id=\"T_97c0f_row1_col7\" class=\"data row1 col7\" >35.1</td>\n",
       "      <td id=\"T_97c0f_row1_col8\" class=\"data row1 col8\" >-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_97c0f_row2_col0\" class=\"data row2 col0\" >llama-7b_tuluv1m:50k_log_prob_decr</td>\n",
       "      <td id=\"T_97c0f_row2_col1\" class=\"data row2 col1\" >42.8</td>\n",
       "      <td id=\"T_97c0f_row2_col2\" class=\"data row2 col2\" >21.8</td>\n",
       "      <td id=\"T_97c0f_row2_col3\" class=\"data row2 col3\" >31.6</td>\n",
       "      <td id=\"T_97c0f_row2_col4\" class=\"data row2 col4\" >38.8</td>\n",
       "      <td id=\"T_97c0f_row2_col5\" class=\"data row2 col5\" >17.7</td>\n",
       "      <td id=\"T_97c0f_row2_col6\" class=\"data row2 col6\" >56.8</td>\n",
       "      <td id=\"T_97c0f_row2_col7\" class=\"data row2 col7\" >34.9</td>\n",
       "      <td id=\"T_97c0f_row2_col8\" class=\"data row2 col8\" >-5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_97c0f_row3_col0\" class=\"data row3 col0\" >llama-7b_tuluv1m:50k_el2n_agg=mean_incr</td>\n",
       "      <td id=\"T_97c0f_row3_col1\" class=\"data row3 col1\" >43.6</td>\n",
       "      <td id=\"T_97c0f_row3_col2\" class=\"data row3 col2\" >21.2</td>\n",
       "      <td id=\"T_97c0f_row3_col3\" class=\"data row3 col3\" >31.3</td>\n",
       "      <td id=\"T_97c0f_row3_col4\" class=\"data row3 col4\" >37.4</td>\n",
       "      <td id=\"T_97c0f_row3_col5\" class=\"data row3 col5\" >17.7</td>\n",
       "      <td id=\"T_97c0f_row3_col6\" class=\"data row3 col6\" >51.2</td>\n",
       "      <td id=\"T_97c0f_row3_col7\" class=\"data row3 col7\" >33.7</td>\n",
       "      <td id=\"T_97c0f_row3_col8\" class=\"data row3 col8\" >-7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_97c0f_row4_col0\" class=\"data row4 col0\" >llama-7b_tuluv1m:50k_logit_margin_decr</td>\n",
       "      <td id=\"T_97c0f_row4_col1\" class=\"data row4 col1\" >42.9</td>\n",
       "      <td id=\"T_97c0f_row4_col2\" class=\"data row4 col2\" >20.2</td>\n",
       "      <td id=\"T_97c0f_row4_col3\" class=\"data row4 col3\" >30.2</td>\n",
       "      <td id=\"T_97c0f_row4_col4\" class=\"data row4 col4\" >40.3</td>\n",
       "      <td id=\"T_97c0f_row4_col5\" class=\"data row4 col5\" >13.4</td>\n",
       "      <td id=\"T_97c0f_row4_col6\" class=\"data row4 col6\" >53.3</td>\n",
       "      <td id=\"T_97c0f_row4_col7\" class=\"data row4 col7\" >33.4</td>\n",
       "      <td id=\"T_97c0f_row4_col8\" class=\"data row4 col8\" >-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_97c0f_row5_col0\" class=\"data row5 col0\" >random_avg (N=3)</td>\n",
       "      <td id=\"T_97c0f_row5_col1\" class=\"data row5 col1\" >41.4</td>\n",
       "      <td id=\"T_97c0f_row5_col2\" class=\"data row5 col2\" >18.1</td>\n",
       "      <td id=\"T_97c0f_row5_col3\" class=\"data row5 col3\" >30.7</td>\n",
       "      <td id=\"T_97c0f_row5_col4\" class=\"data row5 col4\" >40.9</td>\n",
       "      <td id=\"T_97c0f_row5_col5\" class=\"data row5 col5\" >13.0</td>\n",
       "      <td id=\"T_97c0f_row5_col6\" class=\"data row5 col6\" >45.2</td>\n",
       "      <td id=\"T_97c0f_row5_col7\" class=\"data row5 col7\" >31.5</td>\n",
       "      <td id=\"T_97c0f_row5_col8\" class=\"data row5 col8\" >-8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_97c0f_row6_col0\" class=\"data row6 col0\" >llama-7b_tuluv1_humanmix_ep=1</td>\n",
       "      <td id=\"T_97c0f_row6_col1\" class=\"data row6 col1\" >38.5</td>\n",
       "      <td id=\"T_97c0f_row6_col2\" class=\"data row6 col2\" >23.2</td>\n",
       "      <td id=\"T_97c0f_row6_col3\" class=\"data row6 col3\" >32.6</td>\n",
       "      <td id=\"T_97c0f_row6_col4\" class=\"data row6 col4\" >45.0</td>\n",
       "      <td id=\"T_97c0f_row6_col5\" class=\"data row6 col5\" >11.0</td>\n",
       "      <td id=\"T_97c0f_row6_col6\" class=\"data row6 col6\" >34.1</td>\n",
       "      <td id=\"T_97c0f_row6_col7\" class=\"data row6 col7\" >30.7</td>\n",
       "      <td id=\"T_97c0f_row6_col8\" class=\"data row6 col8\" >-7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_97c0f_row7_col0\" class=\"data row7 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=grad+rp+loraB_k=Kcos</td>\n",
       "      <td id=\"T_97c0f_row7_col1\" class=\"data row7 col1\" >37.1</td>\n",
       "      <td id=\"T_97c0f_row7_col2\" class=\"data row7 col2\" >20.6</td>\n",
       "      <td id=\"T_97c0f_row7_col3\" class=\"data row7 col3\" >30.3</td>\n",
       "      <td id=\"T_97c0f_row7_col4\" class=\"data row7 col4\" >39.1</td>\n",
       "      <td id=\"T_97c0f_row7_col5\" class=\"data row7 col5\" >9.1</td>\n",
       "      <td id=\"T_97c0f_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row7_col7\" class=\"data row7 col7\" >27.3</td>\n",
       "      <td id=\"T_97c0f_row7_col8\" class=\"data row7 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_97c0f_row8_col0\" class=\"data row8 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=3000_incr</td>\n",
       "      <td id=\"T_97c0f_row8_col1\" class=\"data row8 col1\" >39.9</td>\n",
       "      <td id=\"T_97c0f_row8_col2\" class=\"data row8 col2\" >16.0</td>\n",
       "      <td id=\"T_97c0f_row8_col3\" class=\"data row8 col3\" >31.7</td>\n",
       "      <td id=\"T_97c0f_row8_col4\" class=\"data row8 col4\" >38.5</td>\n",
       "      <td id=\"T_97c0f_row8_col5\" class=\"data row8 col5\" >9.8</td>\n",
       "      <td id=\"T_97c0f_row8_col6\" class=\"data row8 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row8_col7\" class=\"data row8 col7\" >27.2</td>\n",
       "      <td id=\"T_97c0f_row8_col8\" class=\"data row8 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_97c0f_row9_col0\" class=\"data row9 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=1000_incr</td>\n",
       "      <td id=\"T_97c0f_row9_col1\" class=\"data row9 col1\" >39.4</td>\n",
       "      <td id=\"T_97c0f_row9_col2\" class=\"data row9 col2\" >16.8</td>\n",
       "      <td id=\"T_97c0f_row9_col3\" class=\"data row9 col3\" >32.2</td>\n",
       "      <td id=\"T_97c0f_row9_col4\" class=\"data row9 col4\" >38.8</td>\n",
       "      <td id=\"T_97c0f_row9_col5\" class=\"data row9 col5\" >8.5</td>\n",
       "      <td id=\"T_97c0f_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row9_col7\" class=\"data row9 col7\" >27.2</td>\n",
       "      <td id=\"T_97c0f_row9_col8\" class=\"data row9 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_97c0f_row10_col0\" class=\"data row10 col0\" >llama-7b_tuluv1m:50k_grad_loraB_l2n_incr</td>\n",
       "      <td id=\"T_97c0f_row10_col1\" class=\"data row10 col1\" >36.5</td>\n",
       "      <td id=\"T_97c0f_row10_col2\" class=\"data row10 col2\" >8.6</td>\n",
       "      <td id=\"T_97c0f_row10_col3\" class=\"data row10 col3\" >28.8</td>\n",
       "      <td id=\"T_97c0f_row10_col4\" class=\"data row10 col4\" >29.4</td>\n",
       "      <td id=\"T_97c0f_row10_col5\" class=\"data row10 col5\" >4.3</td>\n",
       "      <td id=\"T_97c0f_row10_col6\" class=\"data row10 col6\" >54.7</td>\n",
       "      <td id=\"T_97c0f_row10_col7\" class=\"data row10 col7\" >27.1</td>\n",
       "      <td id=\"T_97c0f_row10_col8\" class=\"data row10 col8\" >-17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_97c0f_row11_col0\" class=\"data row11 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=3000_decr</td>\n",
       "      <td id=\"T_97c0f_row11_col1\" class=\"data row11 col1\" >40.2</td>\n",
       "      <td id=\"T_97c0f_row11_col2\" class=\"data row11 col2\" >13.0</td>\n",
       "      <td id=\"T_97c0f_row11_col3\" class=\"data row11 col3\" >29.4</td>\n",
       "      <td id=\"T_97c0f_row11_col4\" class=\"data row11 col4\" >38.6</td>\n",
       "      <td id=\"T_97c0f_row11_col5\" class=\"data row11 col5\" >10.4</td>\n",
       "      <td id=\"T_97c0f_row11_col6\" class=\"data row11 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row11_col7\" class=\"data row11 col7\" >26.3</td>\n",
       "      <td id=\"T_97c0f_row11_col8\" class=\"data row11 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_97c0f_row12_col0\" class=\"data row12 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=text+embedding_nc=1000_decr</td>\n",
       "      <td id=\"T_97c0f_row12_col1\" class=\"data row12 col1\" >37.0</td>\n",
       "      <td id=\"T_97c0f_row12_col2\" class=\"data row12 col2\" >12.6</td>\n",
       "      <td id=\"T_97c0f_row12_col3\" class=\"data row12 col3\" >28.7</td>\n",
       "      <td id=\"T_97c0f_row12_col4\" class=\"data row12 col4\" >40.0</td>\n",
       "      <td id=\"T_97c0f_row12_col5\" class=\"data row12 col5\" >12.8</td>\n",
       "      <td id=\"T_97c0f_row12_col6\" class=\"data row12 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row12_col7\" class=\"data row12 col7\" >26.2</td>\n",
       "      <td id=\"T_97c0f_row12_col8\" class=\"data row12 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_97c0f_row13_col0\" class=\"data row13 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=text+embedding_k=Kcos1np</td>\n",
       "      <td id=\"T_97c0f_row13_col1\" class=\"data row13 col1\" >40.2</td>\n",
       "      <td id=\"T_97c0f_row13_col2\" class=\"data row13 col2\" >12.6</td>\n",
       "      <td id=\"T_97c0f_row13_col3\" class=\"data row13 col3\" >29.0</td>\n",
       "      <td id=\"T_97c0f_row13_col4\" class=\"data row13 col4\" >39.7</td>\n",
       "      <td id=\"T_97c0f_row13_col5\" class=\"data row13 col5\" >8.5</td>\n",
       "      <td id=\"T_97c0f_row13_col6\" class=\"data row13 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row13_col7\" class=\"data row13 col7\" >26.0</td>\n",
       "      <td id=\"T_97c0f_row13_col8\" class=\"data row13 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_97c0f_row14_col0\" class=\"data row14 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=text+embedding_k=Kcos</td>\n",
       "      <td id=\"T_97c0f_row14_col1\" class=\"data row14 col1\" >37.5</td>\n",
       "      <td id=\"T_97c0f_row14_col2\" class=\"data row14 col2\" >12.8</td>\n",
       "      <td id=\"T_97c0f_row14_col3\" class=\"data row14 col3\" >27.8</td>\n",
       "      <td id=\"T_97c0f_row14_col4\" class=\"data row14 col4\" >37.2</td>\n",
       "      <td id=\"T_97c0f_row14_col5\" class=\"data row14 col5\" >11.0</td>\n",
       "      <td id=\"T_97c0f_row14_col6\" class=\"data row14 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row14_col7\" class=\"data row14 col7\" >25.2</td>\n",
       "      <td id=\"T_97c0f_row14_col8\" class=\"data row14 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_97c0f_row15_col0\" class=\"data row15 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=3000_incr</td>\n",
       "      <td id=\"T_97c0f_row15_col1\" class=\"data row15 col1\" >38.4</td>\n",
       "      <td id=\"T_97c0f_row15_col2\" class=\"data row15 col2\" >12.2</td>\n",
       "      <td id=\"T_97c0f_row15_col3\" class=\"data row15 col3\" >29.0</td>\n",
       "      <td id=\"T_97c0f_row15_col4\" class=\"data row15 col4\" >39.2</td>\n",
       "      <td id=\"T_97c0f_row15_col5\" class=\"data row15 col5\" >6.1</td>\n",
       "      <td id=\"T_97c0f_row15_col6\" class=\"data row15 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row15_col7\" class=\"data row15 col7\" >25.0</td>\n",
       "      <td id=\"T_97c0f_row15_col8\" class=\"data row15 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_97c0f_row16_col0\" class=\"data row16 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=1000_decr</td>\n",
       "      <td id=\"T_97c0f_row16_col1\" class=\"data row16 col1\" >45.9</td>\n",
       "      <td id=\"T_97c0f_row16_col2\" class=\"data row16 col2\" >7.2</td>\n",
       "      <td id=\"T_97c0f_row16_col3\" class=\"data row16 col3\" >23.1</td>\n",
       "      <td id=\"T_97c0f_row16_col4\" class=\"data row16 col4\" >40.9</td>\n",
       "      <td id=\"T_97c0f_row16_col5\" class=\"data row16 col5\" >6.7</td>\n",
       "      <td id=\"T_97c0f_row16_col6\" class=\"data row16 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row16_col7\" class=\"data row16 col7\" >24.7</td>\n",
       "      <td id=\"T_97c0f_row16_col8\" class=\"data row16 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_97c0f_row17_col0\" class=\"data row17 col0\" >llama-7b_tuluv1m:50k_dppmap_emb=grad+rp+loraB_k=Kcos1np</td>\n",
       "      <td id=\"T_97c0f_row17_col1\" class=\"data row17 col1\" >40.7</td>\n",
       "      <td id=\"T_97c0f_row17_col2\" class=\"data row17 col2\" >8.4</td>\n",
       "      <td id=\"T_97c0f_row17_col3\" class=\"data row17 col3\" >28.3</td>\n",
       "      <td id=\"T_97c0f_row17_col4\" class=\"data row17 col4\" >35.8</td>\n",
       "      <td id=\"T_97c0f_row17_col5\" class=\"data row17 col5\" >7.3</td>\n",
       "      <td id=\"T_97c0f_row17_col6\" class=\"data row17 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row17_col7\" class=\"data row17 col7\" >24.1</td>\n",
       "      <td id=\"T_97c0f_row17_col8\" class=\"data row17 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_97c0f_row18_col0\" class=\"data row18 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=3000_decr</td>\n",
       "      <td id=\"T_97c0f_row18_col1\" class=\"data row18 col1\" >44.3</td>\n",
       "      <td id=\"T_97c0f_row18_col2\" class=\"data row18 col2\" >3.8</td>\n",
       "      <td id=\"T_97c0f_row18_col3\" class=\"data row18 col3\" >24.2</td>\n",
       "      <td id=\"T_97c0f_row18_col4\" class=\"data row18 col4\" >38.1</td>\n",
       "      <td id=\"T_97c0f_row18_col5\" class=\"data row18 col5\" >6.7</td>\n",
       "      <td id=\"T_97c0f_row18_col6\" class=\"data row18 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row18_col7\" class=\"data row18 col7\" >23.4</td>\n",
       "      <td id=\"T_97c0f_row18_col8\" class=\"data row18 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_97c0f_row19_col0\" class=\"data row19 col0\" >llama-7b</td>\n",
       "      <td id=\"T_97c0f_row19_col1\" class=\"data row19 col1\" >32.8</td>\n",
       "      <td id=\"T_97c0f_row19_col2\" class=\"data row19 col2\" >10.8</td>\n",
       "      <td id=\"T_97c0f_row19_col3\" class=\"data row19 col3\" >27.4</td>\n",
       "      <td id=\"T_97c0f_row19_col4\" class=\"data row19 col4\" >37.2</td>\n",
       "      <td id=\"T_97c0f_row19_col5\" class=\"data row19 col5\" >1.8</td>\n",
       "      <td id=\"T_97c0f_row19_col6\" class=\"data row19 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row19_col7\" class=\"data row19 col7\" >22.0</td>\n",
       "      <td id=\"T_97c0f_row19_col8\" class=\"data row19 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_97c0f_row20_col0\" class=\"data row20 col0\" >llama-7b_tuluv1m:50k_kmeansl2_emb=grad+rp+loraB_nc=1000_incr</td>\n",
       "      <td id=\"T_97c0f_row20_col1\" class=\"data row20 col1\" >38.7</td>\n",
       "      <td id=\"T_97c0f_row20_col2\" class=\"data row20 col2\" >12.4</td>\n",
       "      <td id=\"T_97c0f_row20_col3\" class=\"data row20 col3\" >29.7</td>\n",
       "      <td id=\"T_97c0f_row20_col4\" class=\"data row20 col4\" >25.4</td>\n",
       "      <td id=\"T_97c0f_row20_col5\" class=\"data row20 col5\" >3.0</td>\n",
       "      <td id=\"T_97c0f_row20_col6\" class=\"data row20 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row20_col7\" class=\"data row20 col7\" >21.8</td>\n",
       "      <td id=\"T_97c0f_row20_col8\" class=\"data row20 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_97c0f_row21_col0\" class=\"data row21 col0\" >llama-7b_tuluv1m:50k_el2n_agg=mean_decr</td>\n",
       "      <td id=\"T_97c0f_row21_col1\" class=\"data row21 col1\" >43.5</td>\n",
       "      <td id=\"T_97c0f_row21_col2\" class=\"data row21 col2\" >3.6</td>\n",
       "      <td id=\"T_97c0f_row21_col3\" class=\"data row21 col3\" >23.5</td>\n",
       "      <td id=\"T_97c0f_row21_col4\" class=\"data row21 col4\" >28.6</td>\n",
       "      <td id=\"T_97c0f_row21_col5\" class=\"data row21 col5\" >3.0</td>\n",
       "      <td id=\"T_97c0f_row21_col6\" class=\"data row21 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row21_col7\" class=\"data row21 col7\" >20.5</td>\n",
       "      <td id=\"T_97c0f_row21_col8\" class=\"data row21 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_97c0f_row22_col0\" class=\"data row22 col0\" >llama-7b_tuluv1m:50k_grad_loraB_l2n_decr</td>\n",
       "      <td id=\"T_97c0f_row22_col1\" class=\"data row22 col1\" >42.0</td>\n",
       "      <td id=\"T_97c0f_row22_col2\" class=\"data row22 col2\" >3.4</td>\n",
       "      <td id=\"T_97c0f_row22_col3\" class=\"data row22 col3\" >19.5</td>\n",
       "      <td id=\"T_97c0f_row22_col4\" class=\"data row22 col4\" >38.5</td>\n",
       "      <td id=\"T_97c0f_row22_col5\" class=\"data row22 col5\" >4.9</td>\n",
       "      <td id=\"T_97c0f_row22_col6\" class=\"data row22 col6\" >7.8</td>\n",
       "      <td id=\"T_97c0f_row22_col7\" class=\"data row22 col7\" >19.4</td>\n",
       "      <td id=\"T_97c0f_row22_col8\" class=\"data row22 col8\" >-19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_97c0f_row23_col0\" class=\"data row23 col0\" >llama-7b_tuluv1m:50k_log_prob_incr</td>\n",
       "      <td id=\"T_97c0f_row23_col1\" class=\"data row23 col1\" >45.3</td>\n",
       "      <td id=\"T_97c0f_row23_col2\" class=\"data row23 col2\" >3.6</td>\n",
       "      <td id=\"T_97c0f_row23_col3\" class=\"data row23 col3\" >16.9</td>\n",
       "      <td id=\"T_97c0f_row23_col4\" class=\"data row23 col4\" >30.1</td>\n",
       "      <td id=\"T_97c0f_row23_col5\" class=\"data row23 col5\" >0.0</td>\n",
       "      <td id=\"T_97c0f_row23_col6\" class=\"data row23 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row23_col7\" class=\"data row23 col7\" >19.2</td>\n",
       "      <td id=\"T_97c0f_row23_col8\" class=\"data row23 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_97c0f_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_97c0f_row24_col0\" class=\"data row24 col0\" >llama-7b_tuluv1m:50k_logit_margin_incr</td>\n",
       "      <td id=\"T_97c0f_row24_col1\" class=\"data row24 col1\" >44.8</td>\n",
       "      <td id=\"T_97c0f_row24_col2\" class=\"data row24 col2\" >3.6</td>\n",
       "      <td id=\"T_97c0f_row24_col3\" class=\"data row24 col3\" >15.7</td>\n",
       "      <td id=\"T_97c0f_row24_col4\" class=\"data row24 col4\" >25.4</td>\n",
       "      <td id=\"T_97c0f_row24_col5\" class=\"data row24 col5\" >3.7</td>\n",
       "      <td id=\"T_97c0f_row24_col6\" class=\"data row24 col6\" >nan</td>\n",
       "      <td id=\"T_97c0f_row24_col7\" class=\"data row24 col7\" >18.6</td>\n",
       "      <td id=\"T_97c0f_row24_col8\" class=\"data row24 col8\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff78462b60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_average_col_contains_substr\n",
    "\n",
    "Ns = np.unique([x for x in df['data_args.max_train_samples'].to_numpy() if x]).tolist()\n",
    "for N in Ns:\n",
    "    dfc = df.copy()\n",
    "    dfc = dfc[dfc['data_args.max_train_samples'].apply(lambda x: x == N if x else True)]\n",
    "    dfc = pd_average_col_contains_substr(dfc, 'run_name', 'random', substitute=True)\n",
    "#     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "    dfc = dfc.sort_values(['Average'], ascending=False)\n",
    "    dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "    dfc = dfc.reset_index(drop=True)\n",
    "    if len(dfc):\n",
    "        display(dfc\n",
    "                .style\n",
    "                .set_properties(**{'text-align': 'left'})\n",
    "                .background_gradient(cmap ='coolwarm')\n",
    "                .format(precision=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fd168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runif_in_simplex(n):\n",
    "    ''' Return uniformly random vector in the n-simplex '''\n",
    "\n",
    "    k = np.random.exponential(scale=1.0, size=n)\n",
    "    return k / sum(k)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "L = []\n",
    "dataset_name = ['cot', 'dolly', 'flan_v2', 'oasst1']\n",
    "xs = np.arange(4)\n",
    "for _ in range(10):\n",
    "    ys = runif_in_simplex(4)\n",
    "    d = {k: v for k, v in zip(dataset_name, ys)}\n",
    "    L.append(d)\n",
    "    ax.plot(xs, ys)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dirs = []\n",
    "run_dirs = [\n",
    "#     'pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-2.8b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-6.9b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "    ''\n",
    "]\n",
    "for run_dir in run_dirs:\n",
    "    save_dirs += [(os.path.basename(x), x) \n",
    "                  for x in glob.glob(os.path.join('../results/ft2', run_dir, 'checkpoint-*'))]\n",
    "    break\n",
    "\n",
    "    \n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "# df['model'] = ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666b028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from llm.evaluate import get_eval_results_cross_time\n",
    "\n",
    "\n",
    "\n",
    "eval_name_list = [\n",
    "    'MMLU/0-shot',\n",
    "    'MMLU/5-shot',\n",
    "    'GSM/Direct',\n",
    "    'GSM/CoT',\n",
    "    'BBH/Direct',\n",
    "    'BBH/CoT',\n",
    "    'TydiQA/CB',\n",
    "    'TydiQA/GP',\n",
    "    'Codex-Eval/Pass@1',\n",
    "]\n",
    "eval_name_list += [eval_name_list.copy()]\n",
    "\n",
    "\n",
    "N = len(eval_name_list)\n",
    "w = 5\n",
    "fig, axs = plt.subplots(2, 5, figsize=(5*w, 2*w), sharey='row', sharex='col')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = 'tuluv1hm'; exp_dir = '../results/oi4_tulu_v1_human_mix'\n",
    "dataset = 'flan_v2'; exp_dir = '../results/oi4_perf_cross_time'; keep_size = '30k'\n",
    "dataset = 'flan_v2'; exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'; keep_size = '30k'\n",
    "dataset = 'flan2022_1m'; exp_dir = '../results/oi4_flan2022_1m'; keep_size = '100k'\n",
    "filter_fn = lambda x: '100k' in x\n",
    "\n",
    "\n",
    "runs = [x for x in os.listdir(exp_dir) if os.path.isdir(os.path.join(exp_dir, x))]\n",
    "runs = list(filter(filter_fn, runs))\n",
    "def get_name_and_path(x):\n",
    "    name = x.split(':')[-1]\n",
    "    return name, x\n",
    "runs = list(map(get_name_and_path, runs))\n",
    "\n",
    "# runs = [\n",
    "#     ('random', f'llama-7b_{dataset}:{keep_size}_random'),\n",
    "# #     ('dppmap_k=Kcos', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcos'),\n",
    "# #     ('dppmap_k=Kcos1np', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcos1np'),\n",
    "# #     ('dppmap_k=Kcosp', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcosp'),\n",
    "#     ('kmeansl2_nc=3000_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=3000_incr'),\n",
    "#     ('kmeansl2_nc=3000_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=3000_decr'),\n",
    "# #     ('kmeansl2_nc=1000_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=1000_incr'),\n",
    "# #     ('kmeansl2_nc=1000_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=1000_decr'),\n",
    "# #     ('kmeansl2_nc=300_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=300_incr'),\n",
    "# #     ('kmeansl2_nc=300_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=300_decr'),\n",
    "#     ('prob_incr', f'llama-7b_{dataset}:{keep_size}_prob_incr'),\n",
    "#     ('prob_decr', f'llama-7b_{dataset}:{keep_size}_prob_decr'),\n",
    "#     ('el2n_incr', f'llama-7b_{dataset}:{keep_size}_el2n_incr'),\n",
    "#     ('el2n_decr', f'llama-7b_{dataset}:{keep_size}_el2n_decr'),\n",
    "# ]\n",
    "\n",
    "\n",
    "runs = [(x, os.path.join(exp_dir, y)) for x, y in runs]\n",
    "\n",
    "\n",
    "for run_name, save_dir in runs:\n",
    "    df = get_eval_results_cross_time(save_dir, chat_fmt=True)\n",
    "\n",
    "    for axi, eval_name in enumerate(eval_name_list):\n",
    "        ax = axs.flatten()[axi]\n",
    "\n",
    "        xs = df['steps'].to_numpy()\n",
    "        ys = df[eval_name].to_numpy()\n",
    "        if ys.ndim == 2:\n",
    "            ys = ys.mean(-1)\n",
    "\n",
    "        ax.plot(xs, ys, label=run_name)\n",
    "\n",
    "\n",
    "        ax.grid()\n",
    "        ax.set_ylim(0, 55)\n",
    "        ax.set_xlabel('Steps', fontsize=15)\n",
    "        title = eval_name if isinstance(eval_name, str) else 'Avg'\n",
    "        ax.set_title(title, fontsize=20)\n",
    "        \n",
    "        ax.legend()\n",
    "        \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb043d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ylabel = 'llama-7b:600k'\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "ylabel = 'llama-7b_flan_v2:30k'\n",
    "exp_dir = '../results/oi4_perf_cross_time'\n",
    "# exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_decr', 'checkpoint-*'))]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "# add base model performance\n",
    "dfc.loc[dfc['model_args.model_name_or_path']=='huggyllama/llama-7b', 'model_args.model_name_or_path'] = 'checkpoint-0'\n",
    "# get steps \n",
    "dfc.insert(0, 'steps', dfc['model_args.model_name_or_path'].apply(lambda x: int(x.split('-')[-1])))\n",
    "dfc = dfc.sort_values('steps')\n",
    "\n",
    "\n",
    "y_labels_list = [\n",
    "    ['MMLU/0-shot',\n",
    "     'MMLU/0-shot_chatfmt',\n",
    "     'MMLU/5-shot',\n",
    "     'MMLU/5-shot_chatfmt',\n",
    "    ],\n",
    "    ['GSM/Direct',\n",
    "     'GSM/Direct_chatfmt',\n",
    "     'GSM/CoT', \n",
    "     'GSM/CoT_chatfmt', \n",
    "    ],\n",
    "    ['BBH/Direct',\n",
    "     'BBH/Direct_chatfmt',\n",
    "     'BBH/CoT',\n",
    "     'BBH/CoT_chatfmt',\n",
    "    ],\n",
    "    ['TydiQA/CB',\n",
    "     'TydiQA/CB_chatfmt',\n",
    "     'TydiQA/GP',\n",
    "     'TydiQA/GP_chatfmt',\n",
    "    ],\n",
    "    ['Codex-Eval/Pass@1',\n",
    "     'Codex-Eval/Pass@1_chatfmt'],\n",
    "    ['MMLU/0-shot',\n",
    "     'GSM/CoT',\n",
    "     'BBH/CoT',],\n",
    "]\n",
    "\n",
    "N = len(y_labels_list)\n",
    "\n",
    "fig, axs = plt.subplots(1,N,figsize=(5*N,5))\n",
    "\n",
    "axs[0].set_ylabel(ylabel, fontsize=20)\n",
    "\n",
    "for axi, y_labels in enumerate(y_labels_list):\n",
    "    ax = axs[axi]\n",
    "\n",
    "    x = dfc['steps']\n",
    "    y_list = []\n",
    "    for y_label in y_labels:\n",
    "        if y_label not in dfc.columns: continue\n",
    "        y = dfc[y_label].to_numpy()\n",
    "        y_list.append(y)\n",
    "        ax.plot(x, y, label=y_label)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 55)\n",
    "    \n",
    "    \n",
    "# for y_label in ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1']:\n",
    "    \n",
    "#     for chat_fmt in ['', 'chatfmt']:\n",
    "#         col = '_'.join([y_label, chat_fmt]) if chat_fmt else y_label\n",
    "#         y = dfc[col].to_numpy()\n",
    "#         print(f'{col}\\t{y.mean():.2f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_labels = [\n",
    "    'Answer:\\n<|assistant|>\\nThe answer is:',\n",
    "    'Answer:\\n<|assistant|>\\n',\n",
    "    '<|assistant|>\\nAnswer:',\n",
    "    '<|assistant|>\\nThe answer is:',\n",
    "]\n",
    "x_labels = [f'v{i+1}:\\n{x}' for i,x in enumerate(x_labels)]\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc = df.filter(regex='_v|run')\n",
    "\n",
    "runs = dfc['run_name'].to_list()[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for axi, task in enumerate(['MMLU/0-shot', 'MMLU/5-shot']):\n",
    "\n",
    "    ax = axs[axi]\n",
    "    cols = [f'{task}_v{x}' for x in [1, 2, 3, 4]]\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    width = .25\n",
    "    multiplier = 0\n",
    "\n",
    "    for run in runs:\n",
    "        offset = width*multiplier\n",
    "        y = dfc[dfc['run_name']==run][cols].to_numpy().squeeze()\n",
    "        rects = ax.bar(x+offset, y, width, label=run)\n",
    "        ax.bar_label(rects, padding=3, fmt='{:.2f}')\n",
    "        multiplier += 1\n",
    "\n",
    "    ax.set_title(task)\n",
    "    ax.set_xticks(x+width)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(0, 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6ba4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_data_points = 200000 # 10000, 50000, 100000, 200000\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "        )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6323+40966+81933+81933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'results/ft1'\n",
    "\n",
    "# d = {\n",
    "#     'bbh_s=0': 'bbh_s=3',\n",
    "#     'gsm': 'gsm_s=8_cot',\n",
    "#     'mmlu': 'mmlu_s=0',\n",
    "#     'tydiqa_cb': 'tydiqa_s=1_cb',\n",
    "#     'tydiqa_gp': 'tydiqa_s=1_gp',\n",
    "# }\n",
    "\n",
    "# d.update({k+'_chatfmt': v+'_chatfmt' for k,v in d.items()})\n",
    "\n",
    "# for subdir in os.listdir(exp_dir):    \n",
    "#     for task_name_src, task_name_tgt in d.items():\n",
    "#         path_src = os.path.join(exp_dir, subdir, 'eval', task_name_src)\n",
    "#         path_tgt = os.path.join(exp_dir, subdir, 'eval', task_name_tgt)\n",
    "#         if os.path.isdir(path_src):\n",
    "# #             os.rename(path_src, path_tgt)\n",
    "#             print(path_src)\n",
    "#             print(path_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27138820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc = df.copy()\n",
    "dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "    lambda d: sum(list(d.values())) if d else 200000))\n",
    "# dfc[dfc['total_train_samples'].apply(\n",
    "#     lambda x: total_train_samples-500<x<total_train_samples+500)]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat n istoria Uniunii Europene, la drepturile persoanelor care aparin acestor minoriti i la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
