{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 11 21:17:27 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-80GB           On | 00000000:44:00.0 Off |                    0 |\r\n",
      "| N/A   24C    P0               60W / 400W|      0MiB / 81920MiB |      0%   E. Process |\r\n",
      "|                                         |                      |             Disabled |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mit_fm/wpq/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from llm.submit import multiline_to_singleline, submit_job_ccc, get_run_statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b984427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\"\"\"\n",
    "\n",
    "# [ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps.\n",
      "{'job_id': 1809287, 'jbsub_cmd': 'jbsub -queue x86_1h -name finetune -mem 32g -cores 1x20+1 -require a100_80gb -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\"; echo \"======\"; accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1; [ ! -f \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/${LSB_JOBID}.out\" ] || mv \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/${LSB_JOBID}.out\" \"results/huggyllama:llama-7b_human_mix\"'}\n"
     ]
    }
   ],
   "source": [
    "job_name = 'finetune'\n",
    "test_run = False\n",
    "\n",
    "queue = 'x86_1h' # 'x86_12h'\n",
    "num_cpus = 20\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "\n",
    "use_deepspeed = False\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "num_gpus = 1\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''}\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "\n",
    "# things to test to see its effects on (1) eval perf (2) runtime.\n",
    "#\n",
    "# - --mixed_precision bf16. \n",
    "# - with/without LoRA\n",
    "# - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# - batch size\n",
    "# - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=1,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/usr/local/share/zsh/site-functions'), PosixPath('/u/wpq/.oh-my-zsh/completions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1794792.tmpdir/.1689124361.1794792.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}'), PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n",
      "07/11/2023 22:28:51 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "07/11/2023 22:28:51 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.50s/it]\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:54<00:00, 27.15s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Assigning <s> to the bos_token key of the tokenizer\n",
      "Assigning </s> to the eos_token key of the tokenizer\n",
      "Assigning <unk> to the unk_token key of the tokenizer\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/11/2023 22:30:38 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 2097152 || all params: 6740520960 || trainable%: 0.03111261002591705\n",
      "GPU memory occupied: 830 MB.\n",
      "07/11/2023 22:30:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1038c98439ac66d6_*_of_00016.arrow\n",
      "07/11/2023 22:30:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-88ed2eb4a77829ae.arrow\n",
      "07/11/2023 22:30:54 - INFO - __main__ - Sample 1366 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 28491,  1017, 28134,\n",
      "        15964,   304,   679,  1283,   263, 10638,   728, 22588,  1915,   261,\n",
      "          746,  1009, 16286,   471,   508,   346,   839,   322, 10398,   278,\n",
      "         4646,   297,  1009, 22091,   869,    13, 11139,  3034,   675,   278,\n",
      "          263,  1454,   882, 28487,  1426,   297,   263,  2323, 16549, 29889,\n",
      "           13, 29966, 29989,   465, 22137, 29989, 29958,    13,  3364, 21709,\n",
      "        26506,   304,   679,  1283, 10694, 11051,   701,   363,   278,  4646,\n",
      "            2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3364, 21709,\n",
      "        26506,   304,   679,  1283, 10694, 11051,   701,   363,   278,  4646,\n",
      "            2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/11/2023 22:30:55 - INFO - __main__ - Sample 223041 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  2182,   295,   707,\n",
      "          454, 26713, 13019,   316,   752,  6067,  2637,   454,   521,   819,\n",
      "          634,   454, 13563,  1577,    13, 29966, 29989,   465, 22137, 29989,\n",
      "        29958,    13, 14126,   302, 29915, 29891,   263,  2331,   316,  1841,\n",
      "         1713, 12091,   818,   425,  1139,   316,  4048,  6390,  7594,   707,\n",
      "          454, 26713, 13019,   316, 29868,  2637,   454,   521,   819,   634,\n",
      "          454, 13563, 29892,  1559, 18729,  1437, 14081,   553,  7175,  8191,\n",
      "        15001,  4999,   634,   553,  3008, 29877,  1144,   316, 18402,  4348,\n",
      "        29884, 29889,  2664,   521, 11689,  3435, 18484, 16133,  1064,   743,\n",
      "         4191,   553,  3778,  2993,   316, 29868,   658, 29891,  2993,   634,\n",
      "         6602,   434,  1314, 29892, 27093,   712,   966,   521,  1446, 20423,\n",
      "         7848,  2298,  1399,  6430,   355,  1934,   634,  7744,  1849,   818,\n",
      "          553, 18893,   316,  6316,  2298,  1208,  4467, 29889,  1720,   707,\n",
      "         4100,   316,  3060,   275,   381,   443, 13019,   316, 29868,   427,\n",
      "        18165,   316,  1487,  4464,   316,  6316, 29892,   316,  1487,   831,\n",
      "         3535, 26318,   634,   316,  3999, 11101,  7719, 22347,  5908,   818,\n",
      "        23532,   577,   262,   316,  3911, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100, 14126,   302, 29915, 29891,   263,  2331,   316,  1841,\n",
      "         1713, 12091,   818,   425,  1139,   316,  4048,  6390,  7594,   707,\n",
      "          454, 26713, 13019,   316, 29868,  2637,   454,   521,   819,   634,\n",
      "          454, 13563, 29892,  1559, 18729,  1437, 14081,   553,  7175,  8191,\n",
      "        15001,  4999,   634,   553,  3008, 29877,  1144,   316, 18402,  4348,\n",
      "        29884, 29889,  2664,   521, 11689,  3435, 18484, 16133,  1064,   743,\n",
      "         4191,   553,  3778,  2993,   316, 29868,   658, 29891,  2993,   634,\n",
      "         6602,   434,  1314, 29892, 27093,   712,   966,   521,  1446, 20423,\n",
      "         7848,  2298,  1399,  6430,   355,  1934,   634,  7744,  1849,   818,\n",
      "          553, 18893,   316,  6316,  2298,  1208,  4467, 29889,  1720,   707,\n",
      "         4100,   316,  3060,   275,   381,   443, 13019,   316, 29868,   427,\n",
      "        18165,   316,  1487,  4464,   316,  6316, 29892,   316,  1487,   831,\n",
      "         3535, 26318,   634,   316,  3999, 11101,  7719, 22347,  5908,   818,\n",
      "        23532,   577,   262,   316,  3911, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/11/2023 22:30:55 - INFO - __main__ - Sample 128779 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 13696,  1241,   278,\n",
      "         1139, 29889, 11221,   278, 10541,   376, 29909,  2071,   403,  3377,\n",
      "          261,  2599,   263,  8938,  1283,   263, 12616, 14282,  1213,   338,\n",
      "          372,  1565,   393,   376, 29909,  7826,   338,  8743,   297,  1652,\n",
      "          473,  1213, 29973,    13,  1724,   338,   278,  4331, 29899,  1609,\n",
      "        29899, 10568, 24481,  1889,   304, 18331,   472,   278,  1234, 29901,\n",
      "          694, 29973,    13, 29966, 29989,   465, 22137, 29989, 29958,    13,\n",
      "        29909,  2071,   403,  3377,   261,   508, 29915, 29873,   437,   534,\n",
      "         7358,   297,  1652,   473,  1550,  8743,   297,   372, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "        29909,  2071,   403,  3377,   261,   508, 29915, 29873,   437,   534,\n",
      "         7358,   297,  1652,   473,  1550,  8743,   297,   372, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/11/2023 22:30:59 - INFO - __main__ - ***** Running training *****\n",
      "07/11/2023 22:30:59 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) =  Num examples = 270152\n",
      "07/11/2023 22:30:59 - INFO - __main__ -   Num Epochs = 2\n",
      "07/11/2023 22:30:59 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/11/2023 22:30:59 - INFO - __main__ -  128\n",
      "07/11/2023 22:30:59 - INFO - __main__ -   Gradient Accumulation steps = 128\n",
      "07/11/2023 22:30:59 - INFO - __main__ -   Total optimization steps = 4222\n",
      "  0%|                                                  | 0/4222 [00:00<?, ?it/s]before train loop:\n",
      "GPU memory occupied: 14736 MB.\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "0  batch.input_ids:  torch.Size([1, 182])\n",
      "GPU memory occupied: 14736 MB.\n",
      "torch.cuda.memory_allocated():  13552366080\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "1  batch.input_ids:  torch.Size([1, 102])\n",
      "GPU memory occupied: 16698 MB.\n",
      "torch.cuda.memory_allocated():  13590376448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "2  batch.input_ids:  torch.Size([1, 330])\n",
      "GPU memory occupied: 16864 MB.\n",
      "torch.cuda.memory_allocated():  13584328192\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "3  batch.input_ids:  torch.Size([1, 161])\n",
      "GPU memory occupied: 18566 MB.\n",
      "torch.cuda.memory_allocated():  13598916096\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "4  batch.input_ids:  torch.Size([1, 284])\n",
      "GPU memory occupied: 18566 MB.\n",
      "torch.cuda.memory_allocated():  13588102656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "5  batch.input_ids:  torch.Size([1, 163])\n",
      "GPU memory occupied: 18566 MB.\n",
      "torch.cuda.memory_allocated():  13595972096\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "6  batch.input_ids:  torch.Size([1, 757])\n",
      "GPU memory occupied: 18566 MB.\n",
      "torch.cuda.memory_allocated():  13588241408\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "7  batch.input_ids:  torch.Size([1, 211])\n",
      "GPU memory occupied: 25672 MB.\n",
      "torch.cuda.memory_allocated():  13626246144\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "8  batch.input_ids:  torch.Size([1, 110])\n",
      "GPU memory occupied: 25672 MB.\n",
      "torch.cuda.memory_allocated():  13591298048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "9  batch.input_ids:  torch.Size([1, 256])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13584837120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "10  batch.input_ids:  torch.Size([1, 543])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13594581504\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "11  batch.input_ids:  torch.Size([1, 101])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13612547072\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "12  batch.input_ids:  torch.Size([1, 356])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13584264192\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "13  batch.input_ids:  torch.Size([1, 108])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13600862208\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "14  batch.input_ids:  torch.Size([1, 381])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13584712192\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "15  batch.input_ids:  torch.Size([1, 346])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13602184704\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "16  batch.input_ids:  torch.Size([1, 125])\n",
      "GPU memory occupied: 25706 MB.\n",
      "torch.cuda.memory_allocated():  13600862208\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "17  batch.input_ids:  torch.Size([1, 446])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13585801728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "18  batch.input_ids:  torch.Size([1, 220])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13606341632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "19  batch.input_ids:  torch.Size([1, 386])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13591881728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "20  batch.input_ids:  torch.Size([1, 500])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13602507776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "21  batch.input_ids:  torch.Size([1, 135])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13609796096\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "22  batch.input_ids:  torch.Size([1, 278])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13586438656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "23  batch.input_ids:  torch.Size([1, 310])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13595591168\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "24  batch.input_ids:  torch.Size([1, 577])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13597646848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "25  batch.input_ids:  torch.Size([1, 101])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13615542272\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "26  batch.input_ids:  torch.Size([1, 69])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13584258048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "27  batch.input_ids:  torch.Size([1, 79])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13582427648\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "28  batch.input_ids:  torch.Size([1, 282])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13582854656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "29  batch.input_ids:  torch.Size([1, 72])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13595842560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "30  batch.input_ids:  torch.Size([1, 82])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13582402048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "31  batch.input_ids:  torch.Size([1, 228])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13583045120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "32  batch.input_ids:  torch.Size([1, 145])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13592387584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "33  batch.input_ids:  torch.Size([1, 2048])\n",
      "GPU memory occupied: 25714 MB.\n",
      "torch.cuda.memory_allocated():  13587120128\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "34  batch.input_ids:  torch.Size([1, 84])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13708869632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "35  batch.input_ids:  torch.Size([1, 87])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13583925248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "36  batch.input_ids:  torch.Size([1, 148])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13583363584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "37  batch.input_ids:  torch.Size([1, 1256])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13587293696\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "38  batch.input_ids:  torch.Size([1, 37])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13658178560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "39  batch.input_ids:  torch.Size([1, 101])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13580162048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "40  batch.input_ids:  torch.Size([1, 270])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13584262656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "41  batch.input_ids:  torch.Size([1, 104])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13595074560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "42  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13584678912\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "43  batch.input_ids:  torch.Size([1, 722])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13584593408\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44  batch.input_ids:  torch.Size([1, 103])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13624003072\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "45  batch.input_ids:  torch.Size([1, 539])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13584744448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "46  batch.input_ids:  torch.Size([1, 513])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13612301824\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "47  batch.input_ids:  torch.Size([1, 385])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13610634752\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "48  batch.input_ids:  torch.Size([1, 259])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13602439168\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "49  batch.input_ids:  torch.Size([1, 261])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13594375168\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "50  batch.input_ids:  torch.Size([1, 58])\n",
      "GPU memory occupied: 66158 MB.\n",
      "torch.cuda.memory_allocated():  13594497024\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "51  batch.input_ids:  torch.Size([1, 433])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13581513728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "52  batch.input_ids:  torch.Size([1, 682])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13605520384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "53  batch.input_ids:  torch.Size([1, 222])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13621446144\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "54  batch.input_ids:  torch.Size([1, 100])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13592002048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "55  batch.input_ids:  torch.Size([1, 104])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584194048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "56  batch.input_ids:  torch.Size([1, 74])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584678912\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "57  batch.input_ids:  torch.Size([1, 100])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13582530048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "58  batch.input_ids:  torch.Size([1, 230])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584197120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "59  batch.input_ids:  torch.Size([1, 593])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13592526336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "60  batch.input_ids:  torch.Size([1, 1919])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13615790080\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "61  batch.input_ids:  torch.Size([1, 68])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13700613632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "62  batch.input_ids:  torch.Size([1, 115])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13582146048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "63  batch.input_ids:  torch.Size([1, 70])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13585154048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "64  batch.input_ids:  torch.Size([1, 324])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13582280192\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "65  batch.input_ids:  torch.Size([1, 123])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13598765056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "66  batch.input_ids:  torch.Size([1, 165])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13585667584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "67  batch.input_ids:  torch.Size([1, 223])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13588357120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "68  batch.input_ids:  torch.Size([1, 1273])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13592093696\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "69  batch.input_ids:  torch.Size([1, 629])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13659280384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "70  batch.input_ids:  torch.Size([1, 341])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13618057216\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "71  batch.input_ids:  torch.Size([1, 124])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13599618560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "72  batch.input_ids:  torch.Size([1, 260])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13585734656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "73  batch.input_ids:  torch.Size([1, 55])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13594503680\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "74  batch.input_ids:  torch.Size([1, 105])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13581314048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "75  batch.input_ids:  torch.Size([1, 71])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584514048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "76  batch.input_ids:  torch.Size([1, 432])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13582345728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "77  batch.input_ids:  torch.Size([1, 757])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13605457920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "78  batch.input_ids:  torch.Size([1, 500])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13626252288\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "79  batch.input_ids:  torch.Size([1, 123])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13609794560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "80  batch.input_ids:  torch.Size([1, 193])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13585669120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "81  batch.input_ids:  torch.Size([1, 32])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13590144512\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "82  batch.input_ids:  torch.Size([1, 542])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13579893248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "83  batch.input_ids:  torch.Size([1, 417])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13612490752\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "84  batch.input_ids:  torch.Size([1, 111])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13604482560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "85  batch.input_ids:  torch.Size([1, 1173])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584924160\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "86  batch.input_ids:  torch.Size([1, 321])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13652874240\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "87  batch.input_ids:  torch.Size([1, 1935])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13598809600\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "88  batch.input_ids:  torch.Size([1, 252])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13701640704\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "89  batch.input_ids:  torch.Size([1, 96])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13593922048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "90  batch.input_ids:  torch.Size([1, 353])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13583944192\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "91  batch.input_ids:  torch.Size([1, 123])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600862208\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "92  batch.input_ids:  torch.Size([1, 916])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13585686016\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "93  batch.input_ids:  torch.Size([1, 89])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13636419584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "94  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13583490048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "95  batch.input_ids:  torch.Size([1, 305])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584582656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "96  batch.input_ids:  torch.Size([1, 205])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13597317632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "97  batch.input_ids:  torch.Size([1, 176])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13590915584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "98  batch.input_ids:  torch.Size([1, 157])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13589059584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "99  batch.input_ids:  torch.Size([1, 143])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13587843584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "100  batch.input_ids:  torch.Size([1, 179])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13586947584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "101  batch.input_ids:  torch.Size([1, 140])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13589251584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "102  batch.input_ids:  torch.Size([1, 111])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13586754048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "103  batch.input_ids:  torch.Size([1, 228])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584901120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "104  batch.input_ids:  torch.Size([1, 291])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13592390656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "105  batch.input_ids:  torch.Size([1, 298])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13596423168\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "106  batch.input_ids:  torch.Size([1, 303])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13596871168\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "107  batch.input_ids:  torch.Size([1, 204])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13597189632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "108  batch.input_ids:  torch.Size([1, 475])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13590859264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "109  batch.input_ids:  torch.Size([1, 500])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13608203776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "110  batch.input_ids:  torch.Size([1, 415])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13609802240\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "111  batch.input_ids:  torch.Size([1, 101])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13604354560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "112  batch.input_ids:  torch.Size([1, 377])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584264192\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "113  batch.input_ids:  torch.Size([1, 73])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601922560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "114  batch.input_ids:  torch.Size([1, 108])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13582466048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "115  batch.input_ids:  torch.Size([1, 94])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584706048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "116  batch.input_ids:  torch.Size([1, 97])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13583810048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "117  batch.input_ids:  torch.Size([1, 127])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13584002048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "118  batch.input_ids:  torch.Size([1, 82])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13585922048\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "119  batch.input_ids:  torch.Size([1, 614])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13583054336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "120  batch.input_ids:  torch.Size([1, 243])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13617094144\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "121  batch.input_ids:  torch.Size([1, 187])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13593347584\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "122  batch.input_ids:  torch.Size([1, 198])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13589765120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "123  batch.input_ids:  torch.Size([1, 375])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13591168000\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "124  batch.input_ids:  torch.Size([1, 331])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601800704\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "125  batch.input_ids:  torch.Size([1, 341])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13598984704\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "126  batch.input_ids:  torch.Size([1, 329])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13599624704\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "127  batch.input_ids:  torch.Size([1, 2005])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13598896640\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "  0%|                                       | 1/4222 [00:36<42:52:55, 36.57s/it]07/11/2023 22:31:36 - INFO - __main__ -   Step: 1, LR: 1.5873015873015874e-07, Loss: 2.2351760864257812\n",
      "128  batch.input_ids:  torch.Size([1, 118])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13722894336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "129  batch.input_ids:  torch.Size([1, 104])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602123264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "130  batch.input_ids:  torch.Size([1, 364])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601462272\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "131  batch.input_ids:  torch.Size([1, 533])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13617878528\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "132  batch.input_ids:  torch.Size([1, 751])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13628699648\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "133  batch.input_ids:  torch.Size([1, 104])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13642636288\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "134  batch.input_ids:  torch.Size([1, 172])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601228800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "135  batch.input_ids:  torch.Size([1, 430])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13605719552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "136  batch.input_ids:  torch.Size([1, 96])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13622091776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "137  batch.input_ids:  torch.Size([1, 216])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600718336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "138  batch.input_ids:  torch.Size([1, 90])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13608464384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "139  batch.input_ids:  torch.Size([1, 100])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600331264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "140  batch.input_ids:  torch.Size([1, 290])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600975872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141  batch.input_ids:  torch.Size([1, 88])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13613131776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "142  batch.input_ids:  torch.Size([1, 122])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600203264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "143  batch.input_ids:  torch.Size([1, 120])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602379264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "144  batch.input_ids:  torch.Size([1, 538])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602262016\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "145  batch.input_ids:  torch.Size([1, 398])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13629011968\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "146  batch.input_ids:  torch.Size([1, 383])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13620049920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "147  batch.input_ids:  torch.Size([1, 69])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13619083776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "148  batch.input_ids:  torch.Size([1, 127])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13598987264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "149  batch.input_ids:  torch.Size([1, 83])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602699264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "150  batch.input_ids:  torch.Size([1, 445])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13599890944\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "151  batch.input_ids:  torch.Size([1, 296])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13623056384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "152  batch.input_ids:  torch.Size([1, 343])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13613521920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "153  batch.input_ids:  torch.Size([1, 549])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13616534528\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "154  batch.input_ids:  torch.Size([1, 500])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13629717504\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "155  batch.input_ids:  torch.Size([1, 1060])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13626594816\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "156  batch.input_ids:  torch.Size([1, 324])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13662419456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "157  batch.input_ids:  torch.Size([1, 181])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13615543808\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "158  batch.input_ids:  torch.Size([1, 598])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13606167552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "159  batch.input_ids:  torch.Size([1, 869])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13632862720\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "160  batch.input_ids:  torch.Size([1, 78])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13650188800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "161  batch.input_ids:  torch.Size([1, 146])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13599564800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "162  batch.input_ids:  torch.Size([1, 79])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13603915264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "163  batch.input_ids:  torch.Size([1, 998])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13599648768\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "164  batch.input_ids:  torch.Size([1, 555])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13658455552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "165  batch.input_ids:  torch.Size([1, 1014])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13630113792\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "166  batch.input_ids:  torch.Size([1, 369])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13659474944\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "167  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13618187776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "168  batch.input_ids:  torch.Size([1, 96])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601355264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "169  batch.input_ids:  torch.Size([1, 127])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600715264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "170  batch.input_ids:  torch.Size([1, 383])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602705408\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "171  batch.input_ids:  torch.Size([1, 594])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13619096064\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "172  batch.input_ids:  torch.Size([1, 108])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13632588288\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "173  batch.input_ids:  torch.Size([1, 88])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601483264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "174  batch.input_ids:  torch.Size([1, 188])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600204800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "175  batch.input_ids:  torch.Size([1, 115])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13606603264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "176  batch.input_ids:  torch.Size([1, 377])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601937408\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "177  batch.input_ids:  torch.Size([1, 229])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13618702848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "178  batch.input_ids:  torch.Size([1, 152])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13609228800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "179  batch.input_ids:  torch.Size([1, 429])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13604306944\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "180  batch.input_ids:  torch.Size([1, 453])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13622036992\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "181  batch.input_ids:  torch.Size([1, 120])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13623563776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "182  batch.input_ids:  torch.Size([1, 110])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602251264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "183  batch.input_ids:  torch.Size([1, 641])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601625088\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "184  batch.input_ids:  torch.Size([1, 129])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13635597824\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "185  batch.input_ids:  torch.Size([1, 67])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602827264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "186  batch.input_ids:  torch.Size([1, 491])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13598868480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "187  batch.input_ids:  torch.Size([1, 100])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13625995776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "188  batch.input_ids:  torch.Size([1, 270])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600975872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "189  batch.input_ids:  torch.Size([1, 98])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13611851776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190  batch.input_ids:  torch.Size([1, 544])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600854016\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "191  batch.input_ids:  torch.Size([1, 31])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13630089728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "192  batch.input_ids:  torch.Size([1, 139])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13596661248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "193  batch.input_ids:  torch.Size([1, 493])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13603476480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "194  batch.input_ids:  torch.Size([1, 316])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13626128384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "195  batch.input_ids:  torch.Size([1, 95])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13615542272\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "196  batch.input_ids:  torch.Size([1, 151])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13600652800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "197  batch.input_ids:  torch.Size([1, 290])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13604239872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "198  batch.input_ids:  torch.Size([1, 158])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13613133312\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "199  batch.input_ids:  torch.Size([1, 241])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13604686336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "200  batch.input_ids:  torch.Size([1, 185])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13609996800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "201  batch.input_ids:  torch.Size([1, 583])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13606423552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "202  batch.input_ids:  torch.Size([1, 105])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13632319488\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "203  batch.input_ids:  torch.Size([1, 637])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13601303552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "204  batch.input_ids:  torch.Size([1, 299])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13635344896\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "205  batch.input_ids:  torch.Size([1, 394])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13613715456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "206  batch.input_ids:  torch.Size([1, 1230])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13619815424\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "207  batch.input_ids:  torch.Size([1, 38])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13673291776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "208  batch.input_ids:  torch.Size([1, 123])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13597003264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "209  batch.input_ids:  torch.Size([1, 121])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602443264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "210  batch.input_ids:  torch.Size([1, 43])\n",
      "GPU memory occupied: 66218 MB.\n",
      "torch.cuda.memory_allocated():  13602313728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "211  batch.input_ids:  torch.Size([1, 477])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13597332480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "212  batch.input_ids:  torch.Size([1, 435])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13625107456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "213  batch.input_ids:  torch.Size([1, 75])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13622411776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "214  batch.input_ids:  torch.Size([1, 586])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13599754752\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "215  batch.input_ids:  torch.Size([1, 324])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13632325632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "216  batch.input_ids:  torch.Size([1, 273])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13615546880\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "217  batch.input_ids:  torch.Size([1, 376])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612049920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "218  batch.input_ids:  torch.Size([1, 324])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13618641920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "219  batch.input_ids:  torch.Size([1, 120])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13615542272\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "220  batch.input_ids:  torch.Size([1, 137])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13602252800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "221  batch.input_ids:  torch.Size([1, 52])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13603337728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "222  batch.input_ids:  torch.Size([1, 246])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13597902336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "223  batch.input_ids:  torch.Size([1, 432])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13610322944\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "224  batch.input_ids:  torch.Size([1, 205])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13622222848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "225  batch.input_ids:  torch.Size([1, 93])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13607691264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "226  batch.input_ids:  torch.Size([1, 482])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600532480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "227  batch.input_ids:  torch.Size([1, 471])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13625428992\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "228  batch.input_ids:  torch.Size([1, 212])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13624718848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "229  batch.input_ids:  torch.Size([1, 336])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13608601600\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "230  batch.input_ids:  torch.Size([1, 930])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13616095744\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "231  batch.input_ids:  torch.Size([1, 917])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13654112768\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "232  batch.input_ids:  torch.Size([1, 219])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13653263872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "233  batch.input_ids:  torch.Size([1, 608])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13608599552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "234  batch.input_ids:  torch.Size([1, 648])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13633498112\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "235  batch.input_ids:  torch.Size([1, 148])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13636045824\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "236  batch.input_ids:  torch.Size([1, 544])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604054016\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "237  batch.input_ids:  torch.Size([1, 213])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13630094336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "238  batch.input_ids:  torch.Size([1, 828])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13608220160\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239  batch.input_ids:  torch.Size([1, 1041])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13647587840\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "240  batch.input_ids:  torch.Size([1, 181])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13661198848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "241  batch.input_ids:  torch.Size([1, 145])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13606156800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "242  batch.input_ids:  torch.Size([1, 284])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604504064\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "243  batch.input_ids:  torch.Size([1, 1439])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612780032\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "244  batch.input_ids:  torch.Size([1, 104])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13686709248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "245  batch.input_ids:  torch.Size([1, 865])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13601245696\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "246  batch.input_ids:  torch.Size([1, 93])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13649932800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "247  batch.input_ids:  torch.Size([1, 874])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600541696\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "248  batch.input_ids:  torch.Size([1, 122])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13650508800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "249  batch.input_ids:  torch.Size([1, 902])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13602399232\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "250  batch.input_ids:  torch.Size([1, 381])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13652306944\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "251  batch.input_ids:  torch.Size([1, 120])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13618955776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "252  batch.input_ids:  torch.Size([1, 1603])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13602288128\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "253  batch.input_ids:  torch.Size([1, 70])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13697166336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "254  batch.input_ids:  torch.Size([1, 131])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13599052800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "255  batch.input_ids:  torch.Size([1, 1272])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13602982912\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "  0%|                                       | 2/4222 [01:08<39:39:30, 33.83s/it]07/11/2023 22:32:07 - INFO - __main__ -   Step: 2, LR: 3.174603174603175e-07, Loss: 1.9998397827148438\n",
      "256  batch.input_ids:  torch.Size([1, 154])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13675982336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "257  batch.input_ids:  torch.Size([1, 199])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604430336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "258  batch.input_ids:  torch.Size([1, 592])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13607319552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "259  batch.input_ids:  torch.Size([1, 271])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13632464896\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "260  batch.input_ids:  torch.Size([1, 301])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13611920384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "261  batch.input_ids:  torch.Size([1, 341])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13613841920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "262  batch.input_ids:  torch.Size([1, 281])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13616400384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "263  batch.input_ids:  torch.Size([1, 249])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612558848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "264  batch.input_ids:  torch.Size([1, 96])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13610507264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "265  batch.input_ids:  torch.Size([1, 837])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600733696\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "266  batch.input_ids:  torch.Size([1, 396])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13648148480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "267  batch.input_ids:  torch.Size([1, 107])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13619915776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "268  batch.input_ids:  torch.Size([1, 105])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13601419264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "269  batch.input_ids:  torch.Size([1, 1223])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13601428992\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "270  batch.input_ids:  torch.Size([1, 222])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13672848384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "271  batch.input_ids:  torch.Size([1, 100])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13608779264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "272  batch.input_ids:  torch.Size([1, 280])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600975872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "273  batch.input_ids:  torch.Size([1, 529])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612502528\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "274  batch.input_ids:  torch.Size([1, 503])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13628437504\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "275  batch.input_ids:  torch.Size([1, 201])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13626766848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "276  batch.input_ids:  torch.Size([1, 453])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13607444480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "277  batch.input_ids:  torch.Size([1, 94])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13623563776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "278  batch.input_ids:  torch.Size([1, 124])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600587264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "279  batch.input_ids:  torch.Size([1, 97])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13602507264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "280  batch.input_ids:  torch.Size([1, 189])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600780800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "281  batch.input_ids:  torch.Size([1, 312])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13606671872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "282  batch.input_ids:  torch.Size([1, 82])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13615542272\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "283  batch.input_ids:  torch.Size([1, 236])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600859136\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "284  batch.input_ids:  torch.Size([1, 276])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13609679872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "285  batch.input_ids:  torch.Size([1, 286])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612240384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "286  batch.input_ids:  torch.Size([1, 1199])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612901888\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287  batch.input_ids:  torch.Size([1, 126])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13671407616\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "288  batch.input_ids:  torch.Size([1, 158])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13602636800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "289  batch.input_ids:  torch.Size([1, 153])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604684800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "290  batch.input_ids:  torch.Size([1, 1698])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604401664\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "291  batch.input_ids:  torch.Size([1, 1000])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13703267840\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "292  batch.input_ids:  torch.Size([1, 87])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13658572800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "293  batch.input_ids:  torch.Size([1, 210])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600142336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "294  batch.input_ids:  torch.Size([1, 462])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13608670208\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "295  batch.input_ids:  torch.Size([1, 35])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13624138240\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "296  batch.input_ids:  torch.Size([1, 586])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13596823552\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "297  batch.input_ids:  torch.Size([1, 279])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13632324096\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "298  batch.input_ids:  torch.Size([1, 315])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612432384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "299  batch.input_ids:  torch.Size([1, 1054])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13615565312\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "300  batch.input_ids:  torch.Size([1, 516])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13662040064\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "301  batch.input_ids:  torch.Size([1, 563])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13627607040\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "302  batch.input_ids:  torch.Size([1, 592])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13630616576\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "303  batch.input_ids:  torch.Size([1, 138])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13632461824\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "304  batch.input_ids:  torch.Size([1, 305])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13603407872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "305  batch.input_ids:  torch.Size([1, 939])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13614111744\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "306  batch.input_ids:  torch.Size([1, 140])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13654670336\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "307  batch.input_ids:  torch.Size([1, 432])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13603538944\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "308  batch.input_ids:  torch.Size([1, 711])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13622235136\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "309  batch.input_ids:  torch.Size([1, 111])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13640076288\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "310  batch.input_ids:  torch.Size([1, 71])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13601675264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "311  batch.input_ids:  torch.Size([1, 152])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13599116800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "312  batch.input_ids:  torch.Size([1, 183])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604300800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "313  batch.input_ids:  torch.Size([1, 509])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13606292480\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "314  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13627147776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "315  batch.input_ids:  torch.Size([1, 86])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13601355264\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "316  batch.input_ids:  torch.Size([1, 328])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13600081408\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "317  batch.input_ids:  torch.Size([1, 165])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13615565312\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "318  batch.input_ids:  torch.Size([1, 1519])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13605165056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "319  batch.input_ids:  torch.Size([1, 866])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13691808256\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "320  batch.input_ids:  torch.Size([1, 110])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13649996800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "321  batch.input_ids:  torch.Size([1, 274])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13601615872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "322  batch.input_ids:  torch.Size([1, 270])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13612112384\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "323  batch.input_ids:  torch.Size([1, 517])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13611862528\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "324  batch.input_ids:  torch.Size([1, 731])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13627675648\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "325  batch.input_ids:  torch.Size([1, 394])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13641363968\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "326  batch.input_ids:  torch.Size([1, 1625])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13619824640\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "327  batch.input_ids:  torch.Size([1, 151])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13698575872\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "328  batch.input_ids:  torch.Size([1, 33])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13604233728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "329  batch.input_ids:  torch.Size([1, 615])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13597057024\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "330  batch.input_ids:  torch.Size([1, 549])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13633943040\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "331  batch.input_ids:  torch.Size([1, 1804])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13629749760\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "332  batch.input_ids:  torch.Size([1, 311])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13710035456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "333  batch.input_ids:  torch.Size([1, 1520])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13614509568\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "334  batch.input_ids:  torch.Size([1, 139])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13691855360\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "335  batch.input_ids:  torch.Size([1, 145])\n",
      "GPU memory occupied: 66226 MB.\n",
      "torch.cuda.memory_allocated():  13603468800\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336  batch.input_ids:  torch.Size([1, 101])\r\n",
      "GPU memory occupied: 66226 MB.\r\n",
      "torch.cuda.memory_allocated():  13603851264\r\n",
      "model.device: cuda:0\r\n",
      "model.dtype: torch.bfloat16\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune.py\", line 694, in <module>\r\n",
      "    main()\r\n",
      "  File \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune.py\", line 628, in main\r\n",
      "    accelerator.backward(loss)\r\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1821, in backward\r\n",
      "    loss.backward(**kwargs)\r\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\r\n",
      "    torch.autograd.backward(\r\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "KeyboardInterrupt\r\n",
      "\r",
      "  0%|                                       | 2/4222 [01:30<53:03:27, 45.26s/it]\r\n"
     ]
    }
   ],
   "source": [
    "# llama7b+lora: 66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "# \n",
    "\n",
    "\n",
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8d72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "97c5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f9a46f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if any(x in model_name_or_path for x in ['small', 'base', 'medium', 'large']):\n",
    "    cpu_mem = 2\n",
    "elif any(x in model_name_or_path for x in ['3b']):\n",
    "    cpu_mem = 15\n",
    "elif any(x in model_name_or_path for x in ['7b', '11b', 'xl', 'xxl']):\n",
    "    cpu_mem = 64\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/huggyllama/llama-7b --save_dir results/baselines/huggyllama/llama-7b/eval/gsm --eval_batch_size 5 --n_shot 8\n"
     ]
    }
   ],
   "source": [
    "job_name = 'eval.gsm'\n",
    "test_run = False\n",
    "queue = 'x86_1h'\n",
    "num_cpus = 10\n",
    "cpu_mem = 64\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "models += ['huggyllama/llama-7b'] # , 'mosaicml/mpt-7b'\n",
    "\n",
    "models = [os.path.join('results/baselines', x) for x in models]\n",
    "\n",
    "info = {}\n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/gsm'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    python -m eval.gsm.run_eval \\\n",
    "        --data_dir data/eval/gsm/ \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --save_dir {save_dir} \\\n",
    "        --eval_batch_size 5 \\\n",
    "        --n_shot 8\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job_ccc(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        queue=queue,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "    )\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    if not test_run:\n",
    "        info[model_name_or_path] = out['job_id']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "info['results/baselines/t5-small'] = 1763441\n",
    "info['results/baselines/t5-base'] = 1763442\n",
    "info['results/baselines/t5-large'] = 1763443\n",
    "info['results/baselines/t5-3b'] = 1764783\n",
    "info['results/baselines/t5-11b'] = 1763445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "271cdee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>job_id</th>\n",
       "      <th>cpu_time</th>\n",
       "      <th>avg_mem</th>\n",
       "      <th>max_mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>1763441</td>\n",
       "      <td>139.93</td>\n",
       "      <td>0.491738</td>\n",
       "      <td>0.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>1763442</td>\n",
       "      <td>258.60</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.787109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>1763443</td>\n",
       "      <td>76.73</td>\n",
       "      <td>0.957783</td>\n",
       "      <td>1.317383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>1764783</td>\n",
       "      <td>751.98</td>\n",
       "      <td>6.689150</td>\n",
       "      <td>11.693359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>1763445</td>\n",
       "      <td>8.53</td>\n",
       "      <td>5.601807</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name   job_id  cpu_time   avg_mem    max_mem\n",
       "0  t5-small  1763441    139.93  0.491738   0.597656\n",
       "1   t5-base  1763442    258.60  0.729512   0.787109\n",
       "2  t5-large  1763443     76.73  0.957783   1.317383\n",
       "3     t5-3b  1764783    751.98  6.689150  11.693359\n",
       "4    t5-11b  1763445      8.53  5.601807  10.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = []\n",
    "for k, job_id in info.items():\n",
    "    logfile_path = f'{job_id}.out'\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    data.append((k.split('/')[-1], job_id, out['cpu_time'], out['avg_mem'], out['max_mem']))\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['name', 'job_id', 'cpu_time', 'avg_mem', 'max_mem'])\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
