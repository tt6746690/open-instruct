{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 10 16:31:56 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\r\n",
      "| N/A   23C    P0    49W / 400W |      0MiB / 40960MiB |      0%   E. Process |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mit_fm/wpq/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = 'results/mpt-7b_oasst1'\n",
    "output_dir = f\"results/{model_name_or_path.split('/')[-1]}_{train_file_short}\"\n",
    "\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "num_gpus = 1\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "#     --use_deepspeed \\\n",
    "#     --deepspeed_config_file {deepspeed_config_file} \\\n",
    "\n",
    "\n",
    "cmd = f\"\"\"\n",
    "!cd .. && \\\n",
    "accelerate launch \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "print(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd .. && accelerate launch     --mixed_precision bf16     --num_machines 1     --num_processes 1     open_instruct/finetune.py     --model_name_or_path gpt2-Large     --tokenizer_name gpt2-Large     --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl     --max_seq_length 1024     --preprocessing_num_workers 16     --per_device_train_batch_size 2     --gradient_accumulation_steps 64     --learning_rate 2e-5     --lr_scheduler_type linear     --warmup_ratio 0.03     --weight_decay 0.     --num_train_epochs 2     --output_dir results/gpt2-Large_oasst1     --with_tracking     --report_to tensorboard     --logging_steps 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd25f6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --max_num_examples 50 --model_name_or_path results/baselines/gpt2 --save_dir results/baselines/gpt2/eval/gsm/ --eval_batch_size 1 --n_shot 8'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm.submit import multiline_to_singleline\n",
    "model_name_or_path = 'results/baselines/mosaicml/mpt-7b'\n",
    "model_name_or_path = 'results/baselines/gpt2'\n",
    "prefix = '!cd .. && '\n",
    "prefix = ''\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{prefix}python -m eval.gsm.run_eval \\\n",
    "    --data_dir data/eval/gsm/ \\\n",
    "    --max_num_examples 50 \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --save_dir {model_name_or_path}/eval/gsm/ \\\n",
    "    --eval_batch_size 1 \\\n",
    "    --n_shot 8\n",
    "\"\"\"\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0063b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && python -m eval.gsm.run_eval     --data_dir data/eval/gsm/     --max_num_examples 50     --model_name_or_path results/baselines/mosaicml/mpt-7b     --save_dir results/baselines/mosaicml/mpt-7b/eval/gsm/     --eval_batch_size 1     --n_shot 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz=20 for 20\n",
    "# (4*60+36)/20 = 13.8\n",
    "# bsz=10 for 50\n",
    "# (10*60+49)/50 = 12.98\n",
    "# bsz=1 for 50\n",
    "# (12*60+27)/50 = 14.94\n",
    "\n",
    "# 1300 * 14.94 / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_id': 1763441, 'jbsub_cmd': 'jbsub -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-small --save_dir results/baselines/t5-small/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-small --save_dir results/baselines/t5-small/eval/gsm/ --eval_batch_size 10 --n_shot 8', 'stdout': '/opt/share/exec/jbsub8 -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-small --save_dir results/baselines/t5-small/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-small --save_dir results/baselines/t5-small/eval/gsm/ --eval_batch_size 10 --n_shot 8\\n# bsub -q x86_6h -g /wpq/_/default -J eval.gsm -M 10240 -hl -n 10 -R \"rusage[mem=11776] span[ptile=10] affinity[core(1)]\" -oo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -eo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -gpu  num=1:mode=exclusive_process bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-small --save_dir results/baselines/t5-small/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-small --save_dir results/baselines/t5-small/eval/gsm/ --eval_batch_size 10 --n_shot 8\\nJob <1763441> is submitted to queue <x86_6h>.\\n'}\n",
      "{'job_id': 1763442, 'jbsub_cmd': 'jbsub -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-base --save_dir results/baselines/t5-base/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-base --save_dir results/baselines/t5-base/eval/gsm/ --eval_batch_size 10 --n_shot 8', 'stdout': '/opt/share/exec/jbsub8 -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-base --save_dir results/baselines/t5-base/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-base --save_dir results/baselines/t5-base/eval/gsm/ --eval_batch_size 10 --n_shot 8\\n# bsub -q x86_6h -g /wpq/_/default -J eval.gsm -M 10240 -hl -n 10 -R \"rusage[mem=11776] span[ptile=10] affinity[core(1)]\" -oo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -eo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -gpu  num=1:mode=exclusive_process bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-base --save_dir results/baselines/t5-base/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-base --save_dir results/baselines/t5-base/eval/gsm/ --eval_batch_size 10 --n_shot 8\\nJob <1763442> is submitted to queue <x86_6h>.\\n'}\n",
      "{'job_id': 1763443, 'jbsub_cmd': 'jbsub -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-large --save_dir results/baselines/t5-large/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-large --save_dir results/baselines/t5-large/eval/gsm/ --eval_batch_size 10 --n_shot 8', 'stdout': '/opt/share/exec/jbsub8 -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-large --save_dir results/baselines/t5-large/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-large --save_dir results/baselines/t5-large/eval/gsm/ --eval_batch_size 10 --n_shot 8\\n# bsub -q x86_6h -g /wpq/_/default -J eval.gsm -M 10240 -hl -n 10 -R \"rusage[mem=11776] span[ptile=10] affinity[core(1)]\" -oo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -eo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -gpu  num=1:mode=exclusive_process bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-large --save_dir results/baselines/t5-large/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-large --save_dir results/baselines/t5-large/eval/gsm/ --eval_batch_size 10 --n_shot 8\\nJob <1763443> is submitted to queue <x86_6h>.\\n'}\n",
      "{'job_id': 1763444, 'jbsub_cmd': 'jbsub -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-3b --save_dir results/baselines/t5-3b/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-3b --save_dir results/baselines/t5-3b/eval/gsm/ --eval_batch_size 10 --n_shot 8', 'stdout': '/opt/share/exec/jbsub8 -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-3b --save_dir results/baselines/t5-3b/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-3b --save_dir results/baselines/t5-3b/eval/gsm/ --eval_batch_size 10 --n_shot 8\\n# bsub -q x86_6h -g /wpq/_/default -J eval.gsm -M 10240 -hl -n 10 -R \"rusage[mem=11776] span[ptile=10] affinity[core(1)]\" -oo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -eo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -gpu  num=1:mode=exclusive_process bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-3b --save_dir results/baselines/t5-3b/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-3b --save_dir results/baselines/t5-3b/eval/gsm/ --eval_batch_size 10 --n_shot 8\\nJob <1763444> is submitted to queue <x86_6h>.\\n'}\n",
      "{'job_id': 1763445, 'jbsub_cmd': 'jbsub -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-11b --save_dir results/baselines/t5-11b/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-11b --save_dir results/baselines/t5-11b/eval/gsm/ --eval_batch_size 10 --n_shot 8', 'stdout': '/opt/share/exec/jbsub8 -queue x86_6h -name eval.gsm -mem 10g -cores 1x10+1 -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-11b --save_dir results/baselines/t5-11b/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-11b --save_dir results/baselines/t5-11b/eval/gsm/ --eval_batch_size 10 --n_shot 8\\n# bsub -q x86_6h -g /wpq/_/default -J eval.gsm -M 10240 -hl -n 10 -R \"rusage[mem=11776] span[ptile=10] affinity[core(1)]\" -oo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -eo /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out -gpu  num=1:mode=exclusive_process bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-11b --save_dir results/baselines/t5-11b/eval/gsm/ --eval_batch_size 10 --n_shot 8\"; echo \"======\"; python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/t5-11b --save_dir results/baselines/t5-11b/eval/gsm/ --eval_batch_size 10 --n_shot 8\\nJob <1763445> is submitted to queue <x86_6h>.\\n'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\"\"\"\n",
    "\n",
    "submit_kwargs = {\n",
    "    'job_name': 'eval.gsm',\n",
    "    'queue': 'x86_6h',\n",
    "    'num_cpus': 10,\n",
    "    'cpu_mem': 10,\n",
    "    'num_gpus': 1,\n",
    "    'log_dir': '/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/',\n",
    "}\n",
    "\n",
    "models = [\n",
    "    't5-small',\n",
    "    't5-base',\n",
    "    't5-large',\n",
    "    't5-3b',\n",
    "    't5-11b',\n",
    "#     'google/flan-t5-small',\n",
    "#     'google/flan-t5-base',\n",
    "#     'google/flan-t5-large',\n",
    "#     'google/flan-t5-xl',\n",
    "#     'google/flan-t5-xxl',\n",
    "#     'gpt2',\n",
    "#     'gpt2-medium',\n",
    "#     'gpt2-large',\n",
    "#     'gpt2-xl',\n",
    "#     'huggyllama/llama-7b',\n",
    "#     'mosaicml/mpt-7b',\n",
    "]\n",
    "models = [os.path.join('results/baselines', x) for x in models]\n",
    "\n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    cmd = f\"\"\"\n",
    "    {prefix}python -m eval.gsm.run_eval \\\n",
    "        --data_dir data/eval/gsm/ \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --save_dir {model_name_or_path}/eval/gsm/ \\\n",
    "        --eval_batch_size 10 \\\n",
    "        --n_shot 8\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    \n",
    "\n",
    "    \n",
    "for cmd in cmds:\n",
    "    cmd = shell_scripts_template.format(cmd=cmd)\n",
    "    \n",
    "    out = submit_job_ccc(cmd, test=False, **submit_kwargs)\n",
    "    print(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jbsub', '-queue', 'x86_1h', '-name', 'wpq-job', '-mem', '3g', '-cores', '1x1+1', '-require', 'v100', '-out', '/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out', 'bash', '-c', 'python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.is_bf16_supported())\"']\n",
      "['jbsub', '-queue', 'x86_1h', '-name', 'wpq-job', '-mem', '3g', '-cores', '1x1+1', '-require', 'a100_40gb', '-out', '/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out', 'bash', '-c', 'python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.is_bf16_supported())\"']\n",
      "['jbsub', '-queue', 'x86_1h', '-name', 'wpq-job', '-mem', '3g', '-cores', '1x1+1', '-require', 'a100_80gb', '-out', '/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out', 'bash', '-c', 'python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.is_bf16_supported())\"']\n"
     ]
    }
   ],
   "source": [
    "from llm.submit import submit_job_ccc\n",
    "\n",
    "gpu_types = ['v100', 'a100_40gb', 'a100_80gb']\n",
    "for gpu_type in gpu_types:\n",
    "    \n",
    "    out = submit_job_ccc('python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.is_bf16_supported())\"',\n",
    "                  num_gpus=1, require=gpu_type)\n",
    "    print(out['jbsub_cmd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f17f674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jbsub -queue x86_1h -name wpq-job -mem 3g -cores 1x1+1 -require a100_40g -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.is_bf16_supported())\"'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(out['jbsub_cmd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b03b99ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jbsub_cmd': ['jbsub',\n",
       "  '-queue',\n",
       "  'x86_6h',\n",
       "  '-name',\n",
       "  'jpt',\n",
       "  '-mem',\n",
       "  '3g',\n",
       "  '-cores',\n",
       "  '1x20+1',\n",
       "  '-require',\n",
       "  'a100_40g',\n",
       "  '-out',\n",
       "  '/dccstor/mit_fm/wpq/github/mitibm2023/scripts/%J.out',\n",
       "  'bash',\n",
       "  '-c',\n",
       "  'source ~/.profile; cd /dccstor/mit_fm/wpq/github; conda activate wpq-llm; jupyter notebook --no-browser --port=8777 --ip=$(hostname -f)']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shell_scripts = \"source ~/.profile\\ncd /dccstor/mit_fm/wpq/github\\nconda activate wpq-llm\\njupyter notebook --no-browser --port=8777 --ip=$(hostname -f)\"\n",
    "submit_job_ccc(shell_scripts, job_name='jpt', queue='x86_6h', num_cpus=20, num_gpus=1, require='a100_40g', log_dir='/dccstor/mit_fm/wpq/github/mitibm2023/scripts/', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b75f9b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'export OPENAI_API_KEY=$(cat ~/.openai_api_key)\\nexport HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\\n\\nsource /dccstor/mit_fm/miniconda/bin/activate open-instruct\\ncd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\\n\\necho \"Running on $(hostname)\"\\necho \"======\"\\necho \"ls\"\\necho \"======\"\\n\\nls'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\"\"\"\n",
    "shell_scripts = bash_file_template.format(cmd='ls').strip()\n",
    "shell_scripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
