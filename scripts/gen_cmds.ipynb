{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/minoconda3_x86/envs/open-instruct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'x86_64', 'cluster': 'npl'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info\n",
    "\n",
    "info = get_host_info()\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     conda_env='open-instruct',\n",
    "#     cwd=os.path.dirname(os.getcwd()),\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir\n",
    "# )\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '00:33:12'\n",
    "n = 15\n",
    "# total = 1515; nnodes = 1\n",
    "# total = 2083; nnodes = 1\n",
    "total = 1587; nnodes = 1\n",
    "# total = 1041; nnodes = 1\n",
    "# total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_data_points = 200000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up checkpoints `optimizer.bin` to save disk space. \n",
    "# (e.g., 7b model, ~8*7=56GB for storing gradient/momentum in `optimizer.bin`)\n",
    "\n",
    "import glob, os\n",
    "\n",
    "def cleanup_checkpoints(save_dir, test_run=False):\n",
    "\n",
    "    checkpoints = glob.glob(os.path.join(save_dir, 'checkpoint-*'))\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "    checkpoints = checkpoints[:-1]\n",
    "    \n",
    "    if not checkpoints: return\n",
    "\n",
    "    for ckpt_path in checkpoints:\n",
    "        optimizer_bin_path = os.path.join(ckpt_path, 'optimizer.bin')\n",
    "        if os.path.isfile(optimizer_bin_path):\n",
    "            print(optimizer_bin_path)\n",
    "            if not test_run:\n",
    "                os.remove(optimizer_bin_path)\n",
    "        \n",
    "        \n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "exp_dirs = [\n",
    "    '../results/ft1',\n",
    "    '../results/ft2',\n",
    "    '../results/oi3',\n",
    "    '../results/oi4',\n",
    "    '../results/oi4_perf_cross_time',\n",
    "    '../results/oi4_tulu_v1_human_mix',\n",
    "    '../results/oi4_flanv2_prune_with_hmv1_model',\n",
    "    '../results/oi4_flan_v2_vary_subsetsize',\n",
    "]\n",
    "\n",
    "print('Remove extra files (e.g., optimizer.bin) for non-latest checkpoints:')\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    for run_name in os.listdir(exp_dir):\n",
    "        save_dir = os.path.join(exp_dir, run_name)\n",
    "        if os.path.islink(save_dir): continue\n",
    "        cleanup_checkpoints(save_dir, test_run=test_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results/baselines/mistralai/Mistral-7B-v0.1 using 6 GPUs, 2 batch size per GPU, 2 gradient accumulation steps, Effective batch size 120\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_ultrachat:mistral-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_ultrachat:mistral-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpfat7vuqv', 'job_id': 1149281}]\n"
     ]
    }
   ],
   "source": [
    "def compute_mixture_num_samples(mixture, max_train_samples):\n",
    "    s = sum(mixture.values())\n",
    "    mixture = {k: int(max_train_samples*v/s) for k, v in mixture.items()}\n",
    "    return mixture\n",
    "\n",
    "add_hardwarespec_to_dirname = False\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  512 if arch == 'ppc64le' else 64\n",
    "\n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100; save_total_limit = 1\n",
    "\n",
    "\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "# model_name_or_path = 'results/baselines/gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-1.4b'; abbr_model_name = 'pythia-1.4b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-2.8b'; abbr_model_name = 'pythia-2.8b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-6.9b'; abbr_model_name = 'pythia-6.9b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "\n",
    "subsample_mixture_list = []\n",
    "# subsample_mixture_list += [\n",
    "#     {k: max_train_samples} for k in datasets\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     ('humanmix', dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items()))\n",
    "# ] # humanmix mixture.\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot':  0.13568177819252014, 'flan_v2': 0.3957784175872803, \n",
    "#      'dolly': 0.05964866653084755, 'oasst1': 0.4088916480541229}.items())\n",
    "# ] # gpt2-medium_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "# ] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "#         )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_normalized_list = []\n",
    "# subsample_mixture_normalized_list += [('uniform:1200k_doremiv2', # llama-7b_humanmix_uniform:1200k_doremiv2.json\n",
    "#                                        {\"cot\": 0.11419677734375, \"dolly\": 0.1024169921875, \"flan_v2\": 0.204833984375, \"oasst1\": 0.204833984375})]\n",
    "## 10 for trying out datamodels\n",
    "# mixes = [{'cot': 0.37664033529374275,\n",
    "#   'dolly': 0.0874640765523398,\n",
    "#   'flan_v2': 0.39740799933549775,\n",
    "#   'oasst1': 0.1384875888184196},\n",
    "#  {'cot': 0.23064419241874784,\n",
    "#   'dolly': 0.04693354147889885,\n",
    "#   'flan_v2': 0.72121745986295,\n",
    "#   'oasst1': 0.0012048062394032465},\n",
    "#  {'cot': 0.11244721555034376,\n",
    "#   'dolly': 0.21997027355988638,\n",
    "#   'flan_v2': 0.5826671754210359,\n",
    "#   'oasst1': 0.08491533546873392},\n",
    "#  {'cot': 0.27704626812045546,\n",
    "#   'dolly': 0.5712282144637615,\n",
    "#   'flan_v2': 0.024940119654536592,\n",
    "#   'oasst1': 0.12678539776124645},\n",
    "#  {'cot': 0.0024519793352964607,\n",
    "#   'dolly': 0.13274603201304974,\n",
    "#   'flan_v2': 0.012268378167304219,\n",
    "#   'oasst1': 0.8525336104843496},\n",
    "#  {'cot': 0.08065633865016615,\n",
    "#   'dolly': 0.41886215168938545,\n",
    "#   'flan_v2': 0.21723932820070485,\n",
    "#   'oasst1': 0.2832421814597436},\n",
    "#  {'cot': 0.13878643021160036,\n",
    "#   'dolly': 0.05686171157146557,\n",
    "#   'flan_v2': 0.6701353469446995,\n",
    "#   'oasst1': 0.13421651127223455},\n",
    "#  {'cot': 0.2461125374866837,\n",
    "#   'dolly': 0.09774240280444893,\n",
    "#   'flan_v2': 0.13974091986040005,\n",
    "#   'oasst1': 0.5164041398484672},\n",
    "#  {'cot': 0.4069781049152398,\n",
    "#   'dolly': 0.06318759506033228,\n",
    "#   'flan_v2': 0.09504719644992135,\n",
    "#   'oasst1': 0.4347871035745066},\n",
    "#  {'cot': 0.22379693013848484,\n",
    "#   'dolly': 0.30565901275011814,\n",
    "#   'flan_v2': 0.15457716965000887,\n",
    "#   'oasst1': 0.31596688746138824}]\n",
    "\n",
    "# mixes = [\n",
    "#     {'cot': 0.46638974, 'dolly': 0.01456044, 'flan_v2': 0.50886009, 'oasst1': 0.01018973},\n",
    "#     {'cot': 0.39744481, 'dolly': 0.00472114, 'flan_v2': 0.59104177, 'oasst1': 0.00679229},\n",
    "# ]\n",
    "\n",
    "# subsample_mixture_normalized_list += [('', d) for d in mixes]\n",
    "# subsample_mixture_normalized_list += [('humanmix', # humanmix\n",
    "#                                        {'cot': 0.48785105, 'dolly': 0.00732313, 'flan_v2': 0.48785105, 'oasst1': 0.01697478})]\n",
    "# subsample_mixture_normalized_list = [(x[0],  compute_mixture_num_samples(x[1], max_train_samples)) \n",
    "#                                      for x in subsample_mixture_normalized_list]\n",
    "# subsample_mixture_list += subsample_mixture_normalized_list\n",
    "\n",
    "\n",
    "subsample_mixture_list = [('',None)]\n",
    "subsample_inds_file_list = [None]\n",
    "\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "\n",
    "\n",
    "def subsample_inds_file_abbr_fn(x):\n",
    "    s = os.path.basename(x).split('.pkl')[0]\n",
    "    if s.startswith('inds_'):\n",
    "        scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "        pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "        return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "\n",
    "\n",
    "\n",
    "# ft1: reproduce open-instruct table with llama7b\n",
    "# job_name = 'ft1'; num_train_epochs_list = [2]\n",
    "# job_name = 'ft1_ep=1'; num_train_epochs_list = [1] # train for 1 epoch (baseline for comparison.)\n",
    "job_name = 'ft1_ep=2'; num_train_epochs_list = [2]\n",
    "\n",
    "# # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# # train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# # # # train_file = 'data/processed/wpq/cot_flanv2_data.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "# # # # train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'hmv1'\n",
    "# # train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "\n",
    "\n",
    "model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "train_file = 'data/processed/ultrachat/ultrachat_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# # max_train_samples_list = [120]; save_steps = 1; save_total_limit = 100\n",
    "\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "# train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "# train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "# # ft2: test mixture weights\n",
    "# # vary mixture weights\n",
    "# job_name = 'ft2'\n",
    "\n",
    "# # oi3: instruction tuning performance w.r.t. steps.\n",
    "# job_name = 'oi3'\n",
    "\n",
    "# # oi4: data pruning \n",
    "# job_name = 'oi4_flan_v2_vary_subsetsize'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.1, .3, .5]]; num_train_epochs_list = [2]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # oi4_perf_cross_time: perf cross time on flan_v2\n",
    "# job_name = 'oi4_perf_cross_time'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# # ## oi4_flanv2_prune_with_hmv1_model\n",
    "# job_name = 'oi4_flanv2_prune_with_hmv1_model'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b_ft=hmv1/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# ## tulu mix v1.\n",
    "# dataset = 'tulu_v1_human_mix'; train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'tuluv1hm'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [30000]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/{dataset}/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'flan2022_1m'; train_file = 'data/processed/flan2022/flan2022_1m_data.jsonl'; abbr_train_file = 'flan2022_1m'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# ## full data\n",
    "# # max_train_samples_list = [1000000]; num_train_epochs_list = [1]\n",
    "# # subsample_inds_file_list = ['']\n",
    "# # subset\n",
    "# max_train_samples_list = [200000]; num_train_epochs_list = [1]\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "#     # not that helpful\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     ## gradnorm outputs\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "#     ## random baselines.\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "#     ## kmeans on grads\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# # job_name = f'oi4_{dataset}_ep=3'\n",
    "# job_name = f'oi5_tulu_v1_mix:llama-7b' # re-run to see if transformers upgrade altered eval performance.\n",
    "# # save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# max_train_samples_list = [50000]; num_train_epochs_list = [3]\n",
    "# subsample_inds_file_list = [\n",
    "#     # random baselines\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "# #     # correlated statistics\n",
    "# #     os.path.join(data_inds_dir, 'log_prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'log_prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_decr.pkl'),\n",
    "# #     # grad norm\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "# #     # kmeans\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_decr.pkl'),\n",
    "# # #     # kcos only 50k data\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos1np.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# ## oi5: try curriculum learning\n",
    "# # \n",
    "\n",
    "\n",
    "scoring_fn_and_pacing_fn = []\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# M = 150_000; dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "\n",
    "# scoring_fn_list = ['log_prob_neg']\n",
    "# pacing_fn_list = [\n",
    "# #     f'prune_size={M}_ep=3',\n",
    "#     f'singlestep_size={M}_startingfrac=0.05',\n",
    "# #     f'singlestep_size={M}_startingfrac=0.1',\n",
    "# #     f'singlestep_size={M}_startingfrac=0.2',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "# ]\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "M =  50_000; dataset = 'ultrachat'; train_file = 'data/processed/ultrachat/ultrachat_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# scoring_fn_list = ['log_prob_neg', 'el2n_agg=mean', 'logit_margin_neg', 'grad_loraB_l2n',] #  'kmeansl2_emb=text+embedding_nc=3000_incr'\n",
    "scoring_fn_list = ['log_prob_neg']\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep=3',\n",
    "]\n",
    "scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/{abbr_model_name}+lora:r=256:a=256/{dataset}/'\n",
    "num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "dataloader_sampler = 'SequentialSampler'\n",
    "\n",
    "# random baselines\n",
    "# scoring_fn_list = ['random_s=0']; pacing_fn_list = [f'prune_size={M}_ep=1'] # gives advantage to random baselines. # 'random_s=1', 'random_s=2'\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "subsample_inds_file_list = []\n",
    "for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "    p = os.path.join(data_inds_dir, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "    if not os.path.isfile(p):\n",
    "        raise ValueError(f'path={p} does not exists.')\n",
    "    subsample_inds_file_list.append(p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b on 100k. data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b on 100k. data\n",
    "# nodes = 10; num_gpus = 6; gpu_type = 'v100'; job_duration = 36 # llama-7b on 100k. data\n",
    "\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 18 # llama-7b on 400k data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 30 # llama-7b on 600k data\n",
    "\n",
    "# nodes = 1; num_gpus = 1; gpu_type = 'v100'; job_duration = 6  # gpt2\n",
    "# nodes = 2; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-1.4b\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-2.8b|6.9b\n",
    "\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "\n",
    "per_device_train_batch_size = 2; total_batch_size = 128 # 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = True\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_mixture_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     mix_name_and_subsample_mixture,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "    mix_name, subsample_mixture = mix_name_and_subsample_mixture\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "        \n",
    "    if job_name == 'ft2':\n",
    "        if subsample_mixture is not None:\n",
    "            assert(abbr_train_file=='all')\n",
    "            output_dirname += \\\n",
    "                '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "            \n",
    "    if job_name == 'oi3':\n",
    "        output_dirname += '_'+mix_name\n",
    "        \n",
    "#     if job_name.startswith('oi4'):\n",
    "    if subsample_inds_file:\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            '_dtype='+torch_dtype+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=False \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers=16 \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type=linear \\\n",
    "        --warmup_ratio=0.03 \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy=\"no\" \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --report_to=tensorboard \\\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e614b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/mistralai/Mistral-7B-v0.1 --tokenizer_name=results/baselines/mistralai/Mistral-7B-v0.1 --use_fast_tokenizer=False --train_file=data/processed/ultrachat/ultrachat_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=16 --per_device_train_batch_size=2 --gradient_accumulation_steps=2 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=\"no\" --logging_steps=1 --save_strategy=steps --save_steps=100 --save_total_limit=1 --num_train_epochs=1 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"MistralDecoderLayer\" --gradient_checkpointing --report_to=tensorboard --torch_dtype=float32 --dataloader_num_workers=8 --fp16=True --subsample_inds_file=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/mistral-7b+lora:r=256:a=256/ultrachat/random_s=0/inds_prune_size=50000_ep=1.pkl --dataloader_sampler SequentialSampler --output_dir=\"results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=random:s=0_pace=prune:size=50000:ep=1\"\n"
     ]
    }
   ],
   "source": [
    "print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_all_symlinks(directory, verbose=False):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files + dirs:\n",
    "            path = os.path.join(root, name)\n",
    "            if os.path.islink(path):\n",
    "                os.unlink(path)\n",
    "                if verbose:\n",
    "                    print(f\"Removed symlink: {path}\")\n",
    "                \n",
    "import uuid\n",
    "\n",
    "def create_unique_symlinks(file_paths, verbose=False):\n",
    "    \"\"\"Create symlinks for each `file` in `files` in the same directory, with a unique name. \"\"\"\n",
    "    dirs = [os.path.dirname(x) for x in file_paths]\n",
    "\n",
    "    symlink_path_dict = {}\n",
    "    for directory, path in zip(dirs, file_paths):\n",
    "        if os.path.isdir(path):\n",
    "            symlink_name = f\"symlink_{str(uuid.uuid4())[:8]}\"  # Generate a unique symlink name\n",
    "            symlink_path = os.path.join(directory, symlink_name)\n",
    "            try:\n",
    "                os.symlink(os.path.abspath(path), symlink_path)\n",
    "                if verbose:\n",
    "                    print(f\"Created symlink: {symlink_path} -> {path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Failed to create symlink: {path}. Error: {e}\")\n",
    "            symlink_path_dict.update({path: symlink_path})\n",
    "    return symlink_path_dict\n",
    "\n",
    "\n",
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    model_name_or_path = model_name_or_path.lower()\n",
    "    if any(x in model_name_or_path for x in ['gpt2-medium', 'pythia-160m']):\n",
    "        return 50, 1\n",
    "    if any(x in model_name_or_path for x in ['gpt-xl']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp']):\n",
    "            return 16, 1\n",
    "        else:\n",
    "            return 32, 1\n",
    "    if any(x in model_name_or_path for x in ['llama', 'mistral', 'zephyr', 'pythia-1.4b', 'pythia-2.8b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 5, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    if any(x in model_name_or_path for x in ['pythia-6.9b', 'dolly-v2-7b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'mmlu_s=0', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 4, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    return 10, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b68375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mmlu_s=0', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('mmlu_s=5', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('gsm_s=8', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('gsm_s=8_cot', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('bbh_s=3', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('bbh_s=3_cot', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('humaneval', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('tydiqa_s=1_cb', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('tydiqa_s=1_gp', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('mmlu_s=0_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('mmlu_s=5_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('gsm_s=8_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('gsm_s=8_cot_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('bbh_s=3_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('bbh_s=3_cot_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('humaneval_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-alpha')\n",
      "('mmlu_s=5', 'results/baselines/HuggingFaceH4/zephyr-7b-beta')\n",
      "('mmlu_s=5_chatfmt', 'results/baselines/HuggingFaceH4/zephyr-7b-beta')\n",
      "#cmds:  20 \n",
      "\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/mmlu_s=5\" --eval_batch_size 5 --ntrain 5\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 256 --no_cot\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/bbh_s=3\" --eval_batch_size 5 --max_new_tokens 256 --n_shot 3 --no_cot --max_num_examples_per_task 40\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/bbh_s=3_cot\" --eval_batch_size 5 --max_new_tokens 256 --n_shot 3 --max_num_examples_per_task 40\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/humaneval\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --no_context\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/tydiqa_s=1_gp\" --eval_batch_size 5\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/mmlu_s=5_chatfmt\" --eval_batch_size 5 --ntrain 5 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 256 --no_cot --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 256 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/bbh_s=3_chatfmt\" --eval_batch_size 5 --max_new_tokens 256 --n_shot 3 --no_cot --use_chat_format --max_num_examples_per_task 40\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 5 --max_new_tokens 256 --n_shot 3 --use_chat_format --max_num_examples_per_task 40\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --no_context --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-alpha\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-alpha/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 5 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-beta\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-beta/eval/mmlu_s=5\" --eval_batch_size 5 --ntrain 5\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/HuggingFaceH4/zephyr-7b-beta\" --save_dir \"results/baselines/HuggingFaceH4/zephyr-7b-beta/eval/mmlu_s=5_chatfmt\" --eval_batch_size 5 --ntrain 5 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "exp_dir = ''\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "\n",
    "\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "use_slow_tokenizer = False\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0', # \n",
    "    'mmlu_s=5', # ~1hr\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "# task_names = ['alpacafarm_ann=chatgpt']\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## baselines eval \n",
    "subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "#     'gpt2',\n",
    "#     'gpt2-medium',\n",
    "#     'huggyllama/llama-7b', \n",
    "#     'mistralai/Mistral-7B-v0.1',\n",
    "#     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "#     'NousResearch/Llama-2-7b-hf',\n",
    "    'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "    'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "    'HuggingFaceH4/zephyr-7b-alpha',\n",
    "    'HuggingFaceH4/zephyr-7b-beta',\n",
    "#     'EleutherAI/pythia-1.4b',\n",
    "#     'EleutherAI/pythia-2.8b',\n",
    "#     'EleutherAI/pythia-6.9b',\n",
    "#     'databricks/dolly-v2-7b',\n",
    "]]\n",
    "task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # ## baseline re-eval after merge upstream/main\n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b',\n",
    "# ]]\n",
    "# subdir_path_list += ['results/ft1/llama-7b_humanmix']\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1\n",
    "# exp_dir = 'results/ft1'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1_ep=1\n",
    "# exp_dir = 'results/ft1_ep=1'\n",
    "# exp_dir = 'results/ft1_ep=2'\n",
    "# subdir_filter_fn = lambda x: 'ultrachat200k_before' in x\n",
    "# # task_names = task_names_chatfmt\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "# task_names = ['alpacafarm_ann=chatgpt']; task_names = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "# ## ft2\n",
    "# exp_dir = 'results/ft2/'\n",
    "# create_symlinks = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['llama-7b'])\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## llama-7b time-series 400k, 600k\n",
    "# exp_dir = 'results/oi3/'\n",
    "# include_checkpoints = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['400k', '600k']) # , '600k'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # oi4 include checkpoints!\n",
    "# # exp_dir = 'results/oi4_perf_cross_time/'\n",
    "# # exp_dir = 'results/oi4_tulu_v1_human_mix/'\n",
    "# # exp_dir = 'results/oi4_flanv2_prune_with_hmv1_model/'\n",
    "# # exp_dir = 'results/oi4_flan2022_1m/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix_ep=3/'\n",
    "# include_checkpoints = True\n",
    "# include_checkpoints = False\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['log_prob_decr', 'el2n_agg=mean_incr', 'logit_margin_decr', 'grad_loraB'])\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "# task_names = task_names_chatfmt # eval alpacafarm only\n",
    "\n",
    "# # oi4 without checkpoint \n",
    "# # exp_dir = 'results/oi4/'\n",
    "# exp_dir = 'results/oi4_flan_v2_vary_subsetsize/'\n",
    "# task_names = task_names_chatfmt\n",
    "\n",
    "# # oi5\n",
    "# # exp_dir = 'results/oi5_tulu_v1_mix:llama-7b/'\n",
    "# exp_dir = 'results/oi5_ultrachat:mistral-7b'\n",
    "# task_names = ['alpacafarm_ann=chatgpt']; task_names = [x+'_chatfmt' for x in task_names]\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    if 'gpt2' in ft_args_model_name_or_path:\n",
    "        tydiqa_max_context_length = 400 # max ctx len without exceeding max_seq_len\n",
    "    else:\n",
    "        tydiqa_max_context_length = 512\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length {tydiqa_max_context_length} \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_be1d3_row0_col0, #T_be1d3_row1_col0, #T_be1d3_row2_col0, #T_be1d3_row3_col0, #T_be1d3_row4_col0, #T_be1d3_row5_col0, #T_be1d3_row6_col0, #T_be1d3_row7_col0, #T_be1d3_row8_col0, #T_be1d3_row9_col0, #T_be1d3_row10_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_be1d3_row0_col1, #T_be1d3_row0_col2, #T_be1d3_row0_col3, #T_be1d3_row0_col4, #T_be1d3_row0_col5, #T_be1d3_row0_col8, #T_be1d3_row7_col6, #T_be1d3_row7_col8, #T_be1d3_row9_col6, #T_be1d3_row9_col8, #T_be1d3_row10_col6, #T_be1d3_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row0_col6, #T_be1d3_row0_col7, #T_be1d3_row1_col3, #T_be1d3_row1_col8, #T_be1d3_row2_col2, #T_be1d3_row2_col5, #T_be1d3_row3_col1, #T_be1d3_row3_col5, #T_be1d3_row6_col5, #T_be1d3_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row1_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row1_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row1_col7, #T_be1d3_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row2_col1, #T_be1d3_row9_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row2_col3, #T_be1d3_row4_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row3_col2, #T_be1d3_row4_col2, #T_be1d3_row8_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row3_col6, #T_be1d3_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row4_col5, #T_be1d3_row9_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row5_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row5_col4, #T_be1d3_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row5_col6, #T_be1d3_row6_col1, #T_be1d3_row7_col5, #T_be1d3_row8_col3, #T_be1d3_row8_col8, #T_be1d3_row10_col2, #T_be1d3_row10_col4, #T_be1d3_row10_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row7_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row7_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row9_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row9_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row9_col4, #T_be1d3_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_be1d3_row9_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row10_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_be1d3_row10_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_be1d3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_be1d3_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_be1d3_level0_col1\" class=\"col_heading level0 col1\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_be1d3_level0_col2\" class=\"col_heading level0 col2\" >GSM/CoT</th>\n",
       "      <th id=\"T_be1d3_level0_col3\" class=\"col_heading level0 col3\" >BBH/CoT</th>\n",
       "      <th id=\"T_be1d3_level0_col4\" class=\"col_heading level0 col4\" >TydiQA/GP</th>\n",
       "      <th id=\"T_be1d3_level0_col5\" class=\"col_heading level0 col5\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_be1d3_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th id=\"T_be1d3_level0_col7\" class=\"col_heading level0 col7\" >Average</th>\n",
       "      <th id=\"T_be1d3_level0_col8\" class=\"col_heading level0 col8\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_be1d3_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_be1d3_row0_col1\" class=\"data row0 col1\" >nan</td>\n",
       "      <td id=\"T_be1d3_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_be1d3_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "      <td id=\"T_be1d3_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_be1d3_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_be1d3_row0_col6\" class=\"data row0 col6\" >61.66</td>\n",
       "      <td id=\"T_be1d3_row0_col7\" class=\"data row0 col7\" >61.66</td>\n",
       "      <td id=\"T_be1d3_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_be1d3_row1_col0\" class=\"data row1 col0\" >mistral-7b_ultrachat200k_score=grad:loraB:l2n_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_be1d3_row1_col1\" class=\"data row1 col1\" >58.75</td>\n",
       "      <td id=\"T_be1d3_row1_col2\" class=\"data row1 col2\" >34.60</td>\n",
       "      <td id=\"T_be1d3_row1_col3\" class=\"data row1 col3\" >46.76</td>\n",
       "      <td id=\"T_be1d3_row1_col4\" class=\"data row1 col4\" >47.34</td>\n",
       "      <td id=\"T_be1d3_row1_col5\" class=\"data row1 col5\" >28.66</td>\n",
       "      <td id=\"T_be1d3_row1_col6\" class=\"data row1 col6\" >58.08</td>\n",
       "      <td id=\"T_be1d3_row1_col7\" class=\"data row1 col7\" >45.70</td>\n",
       "      <td id=\"T_be1d3_row1_col8\" class=\"data row1 col8\" >-3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_be1d3_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_be1d3_row2_col1\" class=\"data row2 col1\" >58.13</td>\n",
       "      <td id=\"T_be1d3_row2_col2\" class=\"data row2 col2\" >41.80</td>\n",
       "      <td id=\"T_be1d3_row2_col3\" class=\"data row2 col3\" >46.02</td>\n",
       "      <td id=\"T_be1d3_row2_col4\" class=\"data row2 col4\" >37.37</td>\n",
       "      <td id=\"T_be1d3_row2_col5\" class=\"data row2 col5\" >31.10</td>\n",
       "      <td id=\"T_be1d3_row2_col6\" class=\"data row2 col6\" >58.00</td>\n",
       "      <td id=\"T_be1d3_row2_col7\" class=\"data row2 col7\" >45.40</td>\n",
       "      <td id=\"T_be1d3_row2_col8\" class=\"data row2 col8\" >-4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_be1d3_row3_col0\" class=\"data row3 col0\" >mistral-7b_ultrachat200k_score=el2n:agg=mean_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_be1d3_row3_col1\" class=\"data row3 col1\" >59.09</td>\n",
       "      <td id=\"T_be1d3_row3_col2\" class=\"data row3 col2\" >34.00</td>\n",
       "      <td id=\"T_be1d3_row3_col3\" class=\"data row3 col3\" >46.11</td>\n",
       "      <td id=\"T_be1d3_row3_col4\" class=\"data row3 col4\" >37.95</td>\n",
       "      <td id=\"T_be1d3_row3_col5\" class=\"data row3 col5\" >31.10</td>\n",
       "      <td id=\"T_be1d3_row3_col6\" class=\"data row3 col6\" >59.55</td>\n",
       "      <td id=\"T_be1d3_row3_col7\" class=\"data row3 col7\" >44.63</td>\n",
       "      <td id=\"T_be1d3_row3_col8\" class=\"data row3 col8\" >-3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_be1d3_row4_col0\" class=\"data row4 col0\" >mistral-7b_ultrachat200k_score=logit:margin:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_be1d3_row4_col1\" class=\"data row4 col1\" >58.66</td>\n",
       "      <td id=\"T_be1d3_row4_col2\" class=\"data row4 col2\" >34.00</td>\n",
       "      <td id=\"T_be1d3_row4_col3\" class=\"data row4 col3\" >45.93</td>\n",
       "      <td id=\"T_be1d3_row4_col4\" class=\"data row4 col4\" >38.31</td>\n",
       "      <td id=\"T_be1d3_row4_col5\" class=\"data row4 col5\" >28.05</td>\n",
       "      <td id=\"T_be1d3_row4_col6\" class=\"data row4 col6\" >59.57</td>\n",
       "      <td id=\"T_be1d3_row4_col7\" class=\"data row4 col7\" >44.08</td>\n",
       "      <td id=\"T_be1d3_row4_col8\" class=\"data row4 col8\" >-4.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_be1d3_row5_col0\" class=\"data row5 col0\" >mistral-7b_ultrachat200k_score=random:s=0_pace=prune:size=50000:ep=1</td>\n",
       "      <td id=\"T_be1d3_row5_col1\" class=\"data row5 col1\" >58.63</td>\n",
       "      <td id=\"T_be1d3_row5_col2\" class=\"data row5 col2\" >36.00</td>\n",
       "      <td id=\"T_be1d3_row5_col3\" class=\"data row5 col3\" >46.39</td>\n",
       "      <td id=\"T_be1d3_row5_col4\" class=\"data row5 col4\" >41.30</td>\n",
       "      <td id=\"T_be1d3_row5_col5\" class=\"data row5 col5\" >23.17</td>\n",
       "      <td id=\"T_be1d3_row5_col6\" class=\"data row5 col6\" >54.34</td>\n",
       "      <td id=\"T_be1d3_row5_col7\" class=\"data row5 col7\" >43.31</td>\n",
       "      <td id=\"T_be1d3_row5_col8\" class=\"data row5 col8\" >-4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_be1d3_row6_col0\" class=\"data row6 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_be1d3_row6_col1\" class=\"data row6 col1\" >52.42</td>\n",
       "      <td id=\"T_be1d3_row6_col2\" class=\"data row6 col2\" >30.60</td>\n",
       "      <td id=\"T_be1d3_row6_col3\" class=\"data row6 col3\" >38.06</td>\n",
       "      <td id=\"T_be1d3_row6_col4\" class=\"data row6 col4\" >36.52</td>\n",
       "      <td id=\"T_be1d3_row6_col5\" class=\"data row6 col5\" >31.10</td>\n",
       "      <td id=\"T_be1d3_row6_col6\" class=\"data row6 col6\" >59.06</td>\n",
       "      <td id=\"T_be1d3_row6_col7\" class=\"data row6 col7\" >41.29</td>\n",
       "      <td id=\"T_be1d3_row6_col8\" class=\"data row6 col8\" >-6.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_be1d3_row7_col0\" class=\"data row7 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_be1d3_row7_col1\" class=\"data row7 col1\" >57.89</td>\n",
       "      <td id=\"T_be1d3_row7_col2\" class=\"data row7 col2\" >31.60</td>\n",
       "      <td id=\"T_be1d3_row7_col3\" class=\"data row7 col3\" >45.74</td>\n",
       "      <td id=\"T_be1d3_row7_col4\" class=\"data row7 col4\" >49.00</td>\n",
       "      <td id=\"T_be1d3_row7_col5\" class=\"data row7 col5\" >19.51</td>\n",
       "      <td id=\"T_be1d3_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
       "      <td id=\"T_be1d3_row7_col7\" class=\"data row7 col7\" >40.75</td>\n",
       "      <td id=\"T_be1d3_row7_col8\" class=\"data row7 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_be1d3_row8_col0\" class=\"data row8 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_be1d3_row8_col1\" class=\"data row8 col1\" >57.26</td>\n",
       "      <td id=\"T_be1d3_row8_col2\" class=\"data row8 col2\" >29.60</td>\n",
       "      <td id=\"T_be1d3_row8_col3\" class=\"data row8 col3\" >35.56</td>\n",
       "      <td id=\"T_be1d3_row8_col4\" class=\"data row8 col4\" >38.99</td>\n",
       "      <td id=\"T_be1d3_row8_col5\" class=\"data row8 col5\" >20.73</td>\n",
       "      <td id=\"T_be1d3_row8_col6\" class=\"data row8 col6\" >58.64</td>\n",
       "      <td id=\"T_be1d3_row8_col7\" class=\"data row8 col7\" >40.13</td>\n",
       "      <td id=\"T_be1d3_row8_col8\" class=\"data row8 col8\" >-7.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_be1d3_row9_col0\" class=\"data row9 col0\" >mistral-7b_sft-alpha</td>\n",
       "      <td id=\"T_be1d3_row9_col1\" class=\"data row9 col1\" >58.13</td>\n",
       "      <td id=\"T_be1d3_row9_col2\" class=\"data row9 col2\" >29.00</td>\n",
       "      <td id=\"T_be1d3_row9_col3\" class=\"data row9 col3\" >35.74</td>\n",
       "      <td id=\"T_be1d3_row9_col4\" class=\"data row9 col4\" >38.68</td>\n",
       "      <td id=\"T_be1d3_row9_col5\" class=\"data row9 col5\" >28.05</td>\n",
       "      <td id=\"T_be1d3_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
       "      <td id=\"T_be1d3_row9_col7\" class=\"data row9 col7\" >37.92</td>\n",
       "      <td id=\"T_be1d3_row9_col8\" class=\"data row9 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_be1d3_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_be1d3_row10_col0\" class=\"data row10 col0\" >mistral-7b-sft-alpha+dpo</td>\n",
       "      <td id=\"T_be1d3_row10_col1\" class=\"data row10 col1\" >58.92</td>\n",
       "      <td id=\"T_be1d3_row10_col2\" class=\"data row10 col2\" >13.60</td>\n",
       "      <td id=\"T_be1d3_row10_col3\" class=\"data row10 col3\" >40.19</td>\n",
       "      <td id=\"T_be1d3_row10_col4\" class=\"data row10 col4\" >29.38</td>\n",
       "      <td id=\"T_be1d3_row10_col5\" class=\"data row10 col5\" >25.00</td>\n",
       "      <td id=\"T_be1d3_row10_col6\" class=\"data row10 col6\" >nan</td>\n",
       "      <td id=\"T_be1d3_row10_col7\" class=\"data row10 col7\" >33.42</td>\n",
       "      <td id=\"T_be1d3_row10_col8\" class=\"data row10 col8\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f37c392e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "#     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "#     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "#     # llama2\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "#     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "#     # mistral\n",
    "#     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct_10.16update', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "# ]\n",
    "\n",
    "# baselines\n",
    "save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "# #     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1')\n",
    "# ]\n",
    "# use_normalized_preferred_metric = False\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "#     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# # exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# use_normalized_preferred_metric = True\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=1', '../results/ft1_ep=1/llama-7b_tuluv1m'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_tulu_v1_human_mix'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_hmv1_epochs=2', '../results/ft1/llama-7b_humanmix'),]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# exp_dir = '../results/oi5_tulu_v1_mix:llama-7b'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [\n",
    "#     # baselines \n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "# #     ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),\n",
    "#     # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "# #     ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "# ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "use_normalized_preferred_metric = False\n",
    "save_dirs = [\n",
    "    # baselines \n",
    "    ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "    ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "    ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "    ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "    ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "    ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "]\n",
    "save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['', 'ft2', 'oi4']):\n",
    "    chat_fmt = False\n",
    "    chat_fmt = True\n",
    "#     chat_fmt = 'both'\n",
    "    ft_args_fields = [\n",
    "        'run_name',\n",
    "        'model_args.model_name_or_path',\n",
    "        'data_args.subsample_mixture',\n",
    "        'data_args.max_train_samples',\n",
    "    ]\n",
    "    print(f'chat_fmt={chat_fmt}')\n",
    "    df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "#     df = get_eval_results(save_dirs, chat_fmt='both', ft_args_fields=ft_args_fields)\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "    cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] #  'ToxiGen/Acc'\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm(v.Davinci-003)/WR'] #  'ToxiGen/Acc'\n",
    "#     cols = ['MMLU/0-shot', 'GSM/Direct','BBH/Direct']\n",
    "\n",
    "    df = df[ft_args_fields + cols]\n",
    "    df['Average'] = df[cols].mean(axis=1)\n",
    "    if sort_rows:\n",
    "        df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "        sort_value_col, sort_value_col_ascending = 'Average', False\n",
    "    #     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "        df = df.sort_values(sort_value_col, ascending=sort_value_col_ascending)\n",
    "    df = df.reset_index(drop=True)\n",
    "else:\n",
    "    ft_args_fields = [\n",
    "        'run_name',\n",
    "        'data_args.subsample_mixture',\n",
    "        'data_args.max_train_samples',\n",
    "    ]\n",
    "    df = get_eval_results(save_dirs, chat_fmt='both', ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "    df = df[ft_args_fields+cols]\n",
    "    \n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "elif any(exp_dir.endswith(x) for x in ['oi4']):\n",
    "    dfc = df.copy()\n",
    "    dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "    dfc = dfc.reset_index(drop=True)\n",
    "    if len(dfc):\n",
    "        display(dfc\n",
    "                .style\n",
    "                .set_properties(**{'text-align': 'left'})\n",
    "                .background_gradient(cmap ='coolwarm')\n",
    "                .format(precision=1))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x)]\n",
    "        dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "        dfc = dfc.reset_index(drop=True)\n",
    "        if len(dfc):\n",
    "            display(dfc\n",
    "                    .style\n",
    "                    .set_properties(**{'text-align': 'left'})\n",
    "                    .background_gradient(cmap ='coolwarm')\n",
    "                    .format(precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a94cd914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.0.dev0\n",
      "[2]\n",
      "['</s>']\n",
      "[18637, 829, 29879, 29958, 29879, 381]\n",
      "['Hey', '</', 's', '>', 's', 'ir']\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__) # 4.35.0.dev0\n",
    "from transformers import AutoTokenizer \n",
    "s = 'huggyllama/llama-7b'\n",
    "s = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(s) \n",
    "print(tokenizer.encode(\"</s>\", add_special_tokens = False)) # [2]\n",
    "print(tokenizer.tokenize(\"</s>\", add_special_tokens = False)) # ['</s>']\n",
    "print(tokenizer.encode(\"Hey</s>sir\", add_special_tokens = False)) # [18637, 829, 29879, 29958, 29879, 381]\n",
    "print(tokenizer.tokenize(\"Hey</s>sir\", add_special_tokens = False)) # ['Hey', '</', 's', '>', 's', 'ir']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16806ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f51d9_row0_col0, #T_f51d9_row1_col0, #T_f51d9_row2_col0, #T_f51d9_row3_col0, #T_f51d9_row4_col0, #T_f51d9_row5_col0, #T_f51d9_row6_col0, #T_f51d9_row7_col0, #T_f51d9_row8_col0, #T_f51d9_row9_col0, #T_f51d9_row10_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f51d9_row0_col1, #T_f51d9_row0_col2, #T_f51d9_row0_col3, #T_f51d9_row0_col4, #T_f51d9_row0_col5, #T_f51d9_row0_col8, #T_f51d9_row7_col6, #T_f51d9_row7_col8, #T_f51d9_row9_col6, #T_f51d9_row9_col8, #T_f51d9_row10_col6, #T_f51d9_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row0_col6, #T_f51d9_row0_col7, #T_f51d9_row1_col3, #T_f51d9_row1_col8, #T_f51d9_row2_col2, #T_f51d9_row2_col5, #T_f51d9_row3_col1, #T_f51d9_row3_col5, #T_f51d9_row6_col5, #T_f51d9_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row1_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row1_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row1_col7, #T_f51d9_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row2_col1, #T_f51d9_row9_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row2_col3, #T_f51d9_row4_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row3_col2, #T_f51d9_row4_col2, #T_f51d9_row8_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row3_col6, #T_f51d9_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row4_col5, #T_f51d9_row9_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row5_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row5_col4, #T_f51d9_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row5_col6, #T_f51d9_row6_col1, #T_f51d9_row7_col5, #T_f51d9_row8_col3, #T_f51d9_row8_col8, #T_f51d9_row10_col2, #T_f51d9_row10_col4, #T_f51d9_row10_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row7_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row7_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row9_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row9_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row9_col4, #T_f51d9_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f51d9_row9_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row10_col1 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_f51d9_row10_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f51d9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f51d9_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_f51d9_level0_col1\" class=\"col_heading level0 col1\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_f51d9_level0_col2\" class=\"col_heading level0 col2\" >GSM/CoT</th>\n",
       "      <th id=\"T_f51d9_level0_col3\" class=\"col_heading level0 col3\" >BBH/CoT</th>\n",
       "      <th id=\"T_f51d9_level0_col4\" class=\"col_heading level0 col4\" >TydiQA/GP</th>\n",
       "      <th id=\"T_f51d9_level0_col5\" class=\"col_heading level0 col5\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_f51d9_level0_col6\" class=\"col_heading level0 col6\" >AlpacaFarm(v.Davinci-003)/WR</th>\n",
       "      <th id=\"T_f51d9_level0_col7\" class=\"col_heading level0 col7\" >Average</th>\n",
       "      <th id=\"T_f51d9_level0_col8\" class=\"col_heading level0 col8\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f51d9_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_f51d9_row0_col1\" class=\"data row0 col1\" >nan</td>\n",
       "      <td id=\"T_f51d9_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_f51d9_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "      <td id=\"T_f51d9_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_f51d9_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_f51d9_row0_col6\" class=\"data row0 col6\" >61.7</td>\n",
       "      <td id=\"T_f51d9_row0_col7\" class=\"data row0 col7\" >61.7</td>\n",
       "      <td id=\"T_f51d9_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f51d9_row1_col0\" class=\"data row1 col0\" >mistral-7b_ultrachat200k_score=grad:loraB:l2n_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_f51d9_row1_col1\" class=\"data row1 col1\" >58.7</td>\n",
       "      <td id=\"T_f51d9_row1_col2\" class=\"data row1 col2\" >34.6</td>\n",
       "      <td id=\"T_f51d9_row1_col3\" class=\"data row1 col3\" >46.8</td>\n",
       "      <td id=\"T_f51d9_row1_col4\" class=\"data row1 col4\" >47.3</td>\n",
       "      <td id=\"T_f51d9_row1_col5\" class=\"data row1 col5\" >28.7</td>\n",
       "      <td id=\"T_f51d9_row1_col6\" class=\"data row1 col6\" >58.1</td>\n",
       "      <td id=\"T_f51d9_row1_col7\" class=\"data row1 col7\" >45.7</td>\n",
       "      <td id=\"T_f51d9_row1_col8\" class=\"data row1 col8\" >-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f51d9_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_f51d9_row2_col1\" class=\"data row2 col1\" >58.1</td>\n",
       "      <td id=\"T_f51d9_row2_col2\" class=\"data row2 col2\" >41.8</td>\n",
       "      <td id=\"T_f51d9_row2_col3\" class=\"data row2 col3\" >46.0</td>\n",
       "      <td id=\"T_f51d9_row2_col4\" class=\"data row2 col4\" >37.4</td>\n",
       "      <td id=\"T_f51d9_row2_col5\" class=\"data row2 col5\" >31.1</td>\n",
       "      <td id=\"T_f51d9_row2_col6\" class=\"data row2 col6\" >58.0</td>\n",
       "      <td id=\"T_f51d9_row2_col7\" class=\"data row2 col7\" >45.4</td>\n",
       "      <td id=\"T_f51d9_row2_col8\" class=\"data row2 col8\" >-4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f51d9_row3_col0\" class=\"data row3 col0\" >mistral-7b_ultrachat200k_score=el2n:agg=mean_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_f51d9_row3_col1\" class=\"data row3 col1\" >59.1</td>\n",
       "      <td id=\"T_f51d9_row3_col2\" class=\"data row3 col2\" >34.0</td>\n",
       "      <td id=\"T_f51d9_row3_col3\" class=\"data row3 col3\" >46.1</td>\n",
       "      <td id=\"T_f51d9_row3_col4\" class=\"data row3 col4\" >38.0</td>\n",
       "      <td id=\"T_f51d9_row3_col5\" class=\"data row3 col5\" >31.1</td>\n",
       "      <td id=\"T_f51d9_row3_col6\" class=\"data row3 col6\" >59.6</td>\n",
       "      <td id=\"T_f51d9_row3_col7\" class=\"data row3 col7\" >44.6</td>\n",
       "      <td id=\"T_f51d9_row3_col8\" class=\"data row3 col8\" >-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f51d9_row4_col0\" class=\"data row4 col0\" >mistral-7b_ultrachat200k_score=logit:margin:neg_pace=prune:size=50000:ep=3</td>\n",
       "      <td id=\"T_f51d9_row4_col1\" class=\"data row4 col1\" >58.7</td>\n",
       "      <td id=\"T_f51d9_row4_col2\" class=\"data row4 col2\" >34.0</td>\n",
       "      <td id=\"T_f51d9_row4_col3\" class=\"data row4 col3\" >45.9</td>\n",
       "      <td id=\"T_f51d9_row4_col4\" class=\"data row4 col4\" >38.3</td>\n",
       "      <td id=\"T_f51d9_row4_col5\" class=\"data row4 col5\" >28.0</td>\n",
       "      <td id=\"T_f51d9_row4_col6\" class=\"data row4 col6\" >59.6</td>\n",
       "      <td id=\"T_f51d9_row4_col7\" class=\"data row4 col7\" >44.1</td>\n",
       "      <td id=\"T_f51d9_row4_col8\" class=\"data row4 col8\" >-4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f51d9_row5_col0\" class=\"data row5 col0\" >score=random:_avg (N=1)</td>\n",
       "      <td id=\"T_f51d9_row5_col1\" class=\"data row5 col1\" >58.6</td>\n",
       "      <td id=\"T_f51d9_row5_col2\" class=\"data row5 col2\" >36.0</td>\n",
       "      <td id=\"T_f51d9_row5_col3\" class=\"data row5 col3\" >46.4</td>\n",
       "      <td id=\"T_f51d9_row5_col4\" class=\"data row5 col4\" >41.3</td>\n",
       "      <td id=\"T_f51d9_row5_col5\" class=\"data row5 col5\" >23.2</td>\n",
       "      <td id=\"T_f51d9_row5_col6\" class=\"data row5 col6\" >54.3</td>\n",
       "      <td id=\"T_f51d9_row5_col7\" class=\"data row5 col7\" >43.3</td>\n",
       "      <td id=\"T_f51d9_row5_col8\" class=\"data row5 col8\" >-4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f51d9_row6_col0\" class=\"data row6 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_f51d9_row6_col1\" class=\"data row6 col1\" >52.4</td>\n",
       "      <td id=\"T_f51d9_row6_col2\" class=\"data row6 col2\" >30.6</td>\n",
       "      <td id=\"T_f51d9_row6_col3\" class=\"data row6 col3\" >38.1</td>\n",
       "      <td id=\"T_f51d9_row6_col4\" class=\"data row6 col4\" >36.5</td>\n",
       "      <td id=\"T_f51d9_row6_col5\" class=\"data row6 col5\" >31.1</td>\n",
       "      <td id=\"T_f51d9_row6_col6\" class=\"data row6 col6\" >59.1</td>\n",
       "      <td id=\"T_f51d9_row6_col7\" class=\"data row6 col7\" >41.3</td>\n",
       "      <td id=\"T_f51d9_row6_col8\" class=\"data row6 col8\" >-6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f51d9_row7_col0\" class=\"data row7 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_f51d9_row7_col1\" class=\"data row7 col1\" >57.9</td>\n",
       "      <td id=\"T_f51d9_row7_col2\" class=\"data row7 col2\" >31.6</td>\n",
       "      <td id=\"T_f51d9_row7_col3\" class=\"data row7 col3\" >45.7</td>\n",
       "      <td id=\"T_f51d9_row7_col4\" class=\"data row7 col4\" >49.0</td>\n",
       "      <td id=\"T_f51d9_row7_col5\" class=\"data row7 col5\" >19.5</td>\n",
       "      <td id=\"T_f51d9_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
       "      <td id=\"T_f51d9_row7_col7\" class=\"data row7 col7\" >40.7</td>\n",
       "      <td id=\"T_f51d9_row7_col8\" class=\"data row7 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f51d9_row8_col0\" class=\"data row8 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_f51d9_row8_col1\" class=\"data row8 col1\" >57.3</td>\n",
       "      <td id=\"T_f51d9_row8_col2\" class=\"data row8 col2\" >29.6</td>\n",
       "      <td id=\"T_f51d9_row8_col3\" class=\"data row8 col3\" >35.6</td>\n",
       "      <td id=\"T_f51d9_row8_col4\" class=\"data row8 col4\" >39.0</td>\n",
       "      <td id=\"T_f51d9_row8_col5\" class=\"data row8 col5\" >20.7</td>\n",
       "      <td id=\"T_f51d9_row8_col6\" class=\"data row8 col6\" >58.6</td>\n",
       "      <td id=\"T_f51d9_row8_col7\" class=\"data row8 col7\" >40.1</td>\n",
       "      <td id=\"T_f51d9_row8_col8\" class=\"data row8 col8\" >-7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f51d9_row9_col0\" class=\"data row9 col0\" >mistral-7b_sft-alpha</td>\n",
       "      <td id=\"T_f51d9_row9_col1\" class=\"data row9 col1\" >58.1</td>\n",
       "      <td id=\"T_f51d9_row9_col2\" class=\"data row9 col2\" >29.0</td>\n",
       "      <td id=\"T_f51d9_row9_col3\" class=\"data row9 col3\" >35.7</td>\n",
       "      <td id=\"T_f51d9_row9_col4\" class=\"data row9 col4\" >38.7</td>\n",
       "      <td id=\"T_f51d9_row9_col5\" class=\"data row9 col5\" >28.0</td>\n",
       "      <td id=\"T_f51d9_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
       "      <td id=\"T_f51d9_row9_col7\" class=\"data row9 col7\" >37.9</td>\n",
       "      <td id=\"T_f51d9_row9_col8\" class=\"data row9 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f51d9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f51d9_row10_col0\" class=\"data row10 col0\" >mistral-7b-sft-alpha+dpo</td>\n",
       "      <td id=\"T_f51d9_row10_col1\" class=\"data row10 col1\" >58.9</td>\n",
       "      <td id=\"T_f51d9_row10_col2\" class=\"data row10 col2\" >13.6</td>\n",
       "      <td id=\"T_f51d9_row10_col3\" class=\"data row10 col3\" >40.2</td>\n",
       "      <td id=\"T_f51d9_row10_col4\" class=\"data row10 col4\" >29.4</td>\n",
       "      <td id=\"T_f51d9_row10_col5\" class=\"data row10 col5\" >25.0</td>\n",
       "      <td id=\"T_f51d9_row10_col6\" class=\"data row10 col6\" >nan</td>\n",
       "      <td id=\"T_f51d9_row10_col7\" class=\"data row10 col7\" >33.4</td>\n",
       "      <td id=\"T_f51d9_row10_col8\" class=\"data row10 col8\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f37c393e7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_average_col_contains_substr\n",
    "\n",
    "# Ns = np.unique([x for x in df['data_args.max_train_samples'].to_numpy() if x]).tolist()\n",
    "# for N in Ns:\n",
    "dfc = df.copy()\n",
    "dfc = dfc[dfc['data_args.max_train_samples'].apply(lambda x: x == N if x else True)]\n",
    "dfc = pd_average_col_contains_substr(dfc, 'run_name', 'random_', substitute=True)\n",
    "dfc = pd_average_col_contains_substr(dfc, 'run_name', 'score=random:', substitute=True)\n",
    "#     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "dfc = dfc.sort_values(['Average'], ascending=False)\n",
    "dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "if len(dfc):\n",
    "    display(dfc\n",
    "            .style\n",
    "            .set_properties(**{'text-align': 'left'})\n",
    "            .background_gradient(cmap ='coolwarm')\n",
    "            .format(precision=1))\n",
    "\n",
    "# random\n",
    "# log_prob_decr\n",
    "# el2n agg_mean incr\n",
    "# logit_margin_decr\n",
    "# grad l2n incr\n",
    "# kmeansl2_emb=text+embedding_nc=3000_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fd168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runif_in_simplex(n):\n",
    "    ''' Return uniformly random vector in the n-simplex '''\n",
    "\n",
    "    k = np.random.exponential(scale=1.0, size=n)\n",
    "    return k / sum(k)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "L = []\n",
    "dataset_name = ['cot', 'dolly', 'flan_v2', 'oasst1']\n",
    "xs = np.arange(4)\n",
    "for _ in range(10):\n",
    "    ys = runif_in_simplex(4)\n",
    "    d = {k: v for k, v in zip(dataset_name, ys)}\n",
    "    L.append(d)\n",
    "    ax.plot(xs, ys)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dirs = []\n",
    "run_dirs = [\n",
    "#     'pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-2.8b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "#     'pythia-6.9b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394',\n",
    "    ''\n",
    "]\n",
    "for run_dir in run_dirs:\n",
    "    save_dirs += [(os.path.basename(x), x) \n",
    "                  for x in glob.glob(os.path.join('../results/ft2', run_dir, 'checkpoint-*'))]\n",
    "    break\n",
    "\n",
    "    \n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "# df['model'] = ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666b028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from llm.evaluate import get_eval_results_cross_time\n",
    "\n",
    "\n",
    "\n",
    "eval_name_list = [\n",
    "    'MMLU/0-shot',\n",
    "    'MMLU/5-shot',\n",
    "    'GSM/Direct',\n",
    "    'GSM/CoT',\n",
    "    'BBH/Direct',\n",
    "    'BBH/CoT',\n",
    "    'TydiQA/CB',\n",
    "    'TydiQA/GP',\n",
    "    'Codex-Eval/Pass@1',\n",
    "]\n",
    "eval_name_list += [eval_name_list.copy()]\n",
    "\n",
    "\n",
    "N = len(eval_name_list)\n",
    "w = 5\n",
    "fig, axs = plt.subplots(2, 5, figsize=(5*w, 2*w), sharey='row', sharex='col')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = 'tuluv1hm'; exp_dir = '../results/oi4_tulu_v1_human_mix'\n",
    "dataset = 'flan_v2'; exp_dir = '../results/oi4_perf_cross_time'; keep_size = '30k'\n",
    "dataset = 'flan_v2'; exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'; keep_size = '30k'\n",
    "dataset = 'flan2022_1m'; exp_dir = '../results/oi4_flan2022_1m'; keep_size = '100k'\n",
    "filter_fn = lambda x: '100k' in x\n",
    "\n",
    "\n",
    "runs = [x for x in os.listdir(exp_dir) if os.path.isdir(os.path.join(exp_dir, x))]\n",
    "runs = list(filter(filter_fn, runs))\n",
    "def get_name_and_path(x):\n",
    "    name = x.split(':')[-1]\n",
    "    return name, x\n",
    "runs = list(map(get_name_and_path, runs))\n",
    "\n",
    "# runs = [\n",
    "#     ('random', f'llama-7b_{dataset}:{keep_size}_random'),\n",
    "# #     ('dppmap_k=Kcos', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcos'),\n",
    "# #     ('dppmap_k=Kcos1np', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcos1np'),\n",
    "# #     ('dppmap_k=Kcosp', f'llama-7b_{dataset}:{keep_size}_dppmap_k=Kcosp'),\n",
    "#     ('kmeansl2_nc=3000_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=3000_incr'),\n",
    "#     ('kmeansl2_nc=3000_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=3000_decr'),\n",
    "# #     ('kmeansl2_nc=1000_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=1000_incr'),\n",
    "# #     ('kmeansl2_nc=1000_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=1000_decr'),\n",
    "# #     ('kmeansl2_nc=300_incr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=300_incr'),\n",
    "# #     ('kmeansl2_nc=300_decr', f'llama-7b_{dataset}:{keep_size}_kmeansl2_nc=300_decr'),\n",
    "#     ('prob_incr', f'llama-7b_{dataset}:{keep_size}_prob_incr'),\n",
    "#     ('prob_decr', f'llama-7b_{dataset}:{keep_size}_prob_decr'),\n",
    "#     ('el2n_incr', f'llama-7b_{dataset}:{keep_size}_el2n_incr'),\n",
    "#     ('el2n_decr', f'llama-7b_{dataset}:{keep_size}_el2n_decr'),\n",
    "# ]\n",
    "\n",
    "\n",
    "runs = [(x, os.path.join(exp_dir, y)) for x, y in runs]\n",
    "\n",
    "\n",
    "for run_name, save_dir in runs:\n",
    "    df = get_eval_results_cross_time(save_dir, chat_fmt=True)\n",
    "\n",
    "    for axi, eval_name in enumerate(eval_name_list):\n",
    "        ax = axs.flatten()[axi]\n",
    "\n",
    "        xs = df['steps'].to_numpy()\n",
    "        ys = df[eval_name].to_numpy()\n",
    "        if ys.ndim == 2:\n",
    "            ys = ys.mean(-1)\n",
    "\n",
    "        ax.plot(xs, ys, label=run_name)\n",
    "\n",
    "\n",
    "        ax.grid()\n",
    "        ax.set_ylim(0, 55)\n",
    "        ax.set_xlabel('Steps', fontsize=15)\n",
    "        title = eval_name if isinstance(eval_name, str) else 'Avg'\n",
    "        ax.set_title(title, fontsize=20)\n",
    "        \n",
    "        ax.legend()\n",
    "        \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb043d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ylabel = 'llama-7b:600k'\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "ylabel = 'llama-7b_flan_v2:30k'\n",
    "exp_dir = '../results/oi4_perf_cross_time'\n",
    "# exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_decr', 'checkpoint-*'))]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "# add base model performance\n",
    "dfc.loc[dfc['model_args.model_name_or_path']=='huggyllama/llama-7b', 'model_args.model_name_or_path'] = 'checkpoint-0'\n",
    "# get steps \n",
    "dfc.insert(0, 'steps', dfc['model_args.model_name_or_path'].apply(lambda x: int(x.split('-')[-1])))\n",
    "dfc = dfc.sort_values('steps')\n",
    "\n",
    "\n",
    "y_labels_list = [\n",
    "    ['MMLU/0-shot',\n",
    "     'MMLU/0-shot_chatfmt',\n",
    "     'MMLU/5-shot',\n",
    "     'MMLU/5-shot_chatfmt',\n",
    "    ],\n",
    "    ['GSM/Direct',\n",
    "     'GSM/Direct_chatfmt',\n",
    "     'GSM/CoT', \n",
    "     'GSM/CoT_chatfmt', \n",
    "    ],\n",
    "    ['BBH/Direct',\n",
    "     'BBH/Direct_chatfmt',\n",
    "     'BBH/CoT',\n",
    "     'BBH/CoT_chatfmt',\n",
    "    ],\n",
    "    ['TydiQA/CB',\n",
    "     'TydiQA/CB_chatfmt',\n",
    "     'TydiQA/GP',\n",
    "     'TydiQA/GP_chatfmt',\n",
    "    ],\n",
    "    ['Codex-Eval/Pass@1',\n",
    "     'Codex-Eval/Pass@1_chatfmt'],\n",
    "    ['MMLU/0-shot',\n",
    "     'GSM/CoT',\n",
    "     'BBH/CoT',],\n",
    "]\n",
    "\n",
    "N = len(y_labels_list)\n",
    "\n",
    "fig, axs = plt.subplots(1,N,figsize=(5*N,5))\n",
    "\n",
    "axs[0].set_ylabel(ylabel, fontsize=20)\n",
    "\n",
    "for axi, y_labels in enumerate(y_labels_list):\n",
    "    ax = axs[axi]\n",
    "\n",
    "    x = dfc['steps']\n",
    "    y_list = []\n",
    "    for y_label in y_labels:\n",
    "        if y_label not in dfc.columns: continue\n",
    "        y = dfc[y_label].to_numpy()\n",
    "        y_list.append(y)\n",
    "        ax.plot(x, y, label=y_label)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 55)\n",
    "    \n",
    "    \n",
    "# for y_label in ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1']:\n",
    "    \n",
    "#     for chat_fmt in ['', 'chatfmt']:\n",
    "#         col = '_'.join([y_label, chat_fmt]) if chat_fmt else y_label\n",
    "#         y = dfc[col].to_numpy()\n",
    "#         print(f'{col}\\t{y.mean():.2f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_labels = [\n",
    "    'Answer:\\n<|assistant|>\\nThe answer is:',\n",
    "    'Answer:\\n<|assistant|>\\n',\n",
    "    '<|assistant|>\\nAnswer:',\n",
    "    '<|assistant|>\\nThe answer is:',\n",
    "]\n",
    "x_labels = [f'v{i+1}:\\n{x}' for i,x in enumerate(x_labels)]\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc = df.filter(regex='_v|run')\n",
    "\n",
    "runs = dfc['run_name'].to_list()[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for axi, task in enumerate(['MMLU/0-shot', 'MMLU/5-shot']):\n",
    "\n",
    "    ax = axs[axi]\n",
    "    cols = [f'{task}_v{x}' for x in [1, 2, 3, 4]]\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    width = .25\n",
    "    multiplier = 0\n",
    "\n",
    "    for run in runs:\n",
    "        offset = width*multiplier\n",
    "        y = dfc[dfc['run_name']==run][cols].to_numpy().squeeze()\n",
    "        rects = ax.bar(x+offset, y, width, label=run)\n",
    "        ax.bar_label(rects, padding=3, fmt='{:.2f}')\n",
    "        multiplier += 1\n",
    "\n",
    "    ax.set_title(task)\n",
    "    ax.set_xticks(x+width)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(0, 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6ba4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_data_points = 200000 # 10000, 50000, 100000, 200000\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "        )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6323+40966+81933+81933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'results/ft1'\n",
    "\n",
    "# d = {\n",
    "#     'bbh_s=0': 'bbh_s=3',\n",
    "#     'gsm': 'gsm_s=8_cot',\n",
    "#     'mmlu': 'mmlu_s=0',\n",
    "#     'tydiqa_cb': 'tydiqa_s=1_cb',\n",
    "#     'tydiqa_gp': 'tydiqa_s=1_gp',\n",
    "# }\n",
    "\n",
    "# d.update({k+'_chatfmt': v+'_chatfmt' for k,v in d.items()})\n",
    "\n",
    "# for subdir in os.listdir(exp_dir):    \n",
    "#     for task_name_src, task_name_tgt in d.items():\n",
    "#         path_src = os.path.join(exp_dir, subdir, 'eval', task_name_src)\n",
    "#         path_tgt = os.path.join(exp_dir, subdir, 'eval', task_name_tgt)\n",
    "#         if os.path.isdir(path_src):\n",
    "# #             os.rename(path_src, path_tgt)\n",
    "#             print(path_src)\n",
    "#             print(path_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27138820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc = df.copy()\n",
    "dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "    lambda d: sum(list(d.values())) if d else 200000))\n",
    "# dfc[dfc['total_train_samples'].apply(\n",
    "#     lambda x: total_train_samples-500<x<total_train_samples+500)]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat n istoria Uniunii Europene, la drepturile persoanelor care aparin acestor minoriti i la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
