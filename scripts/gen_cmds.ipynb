{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b84d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55k data points, <1k sequence lengths / data point.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem usage (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>0.124</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>stage 3 no offloading</td>\n",
       "      <td>40</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1.500</td>\n",
       "      <td>bf16</td>\n",
       "      <td>stage 3 with offloading</td>\n",
       "      <td>40</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision                deepspeed  \\\n",
       "0        gpt2  0.124            bf16                       no   \n",
       "1  gpt2-Large  0.774            bf16                       no   \n",
       "2  gpt2-Large  0.774            bf16    stage 3 no offloading   \n",
       "3     gpt2-xl  1.500            bf16  stage 3 with offloading   \n",
       "\n",
       "   gpu mem usage (GB)  per-epoch time (hr)  per-iter time (s)  \n",
       "0                  10                  NaN                NaN  \n",
       "1                  36                  2.5               11.0  \n",
       "2                  40                  6.0               25.0  \n",
       "3                  40                 13.0               55.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "]\n",
    "\n",
    "print('a100_40g:')\n",
    "print('55k data points, <1k sequence lengths / data point.')\n",
    "\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gpt2-Large using 1 GPUs, 2 batch size per GPU, 64 gradient accumulation steps.\n",
      "\n",
      "!cd .. && accelerate launch     --mixed_precision bf16     --num_machines 1     --num_processes 1     --use_deepspeed     --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate_setauto.conf     open_instruct/finetune.py     --model_name_or_path gpt2-Large     --tokenizer_name gpt2-Large     --train_file data/processed/oasst1/oasst1_data.jsonl     --max_seq_length 1024     --preprocessing_num_workers 16     --per_device_train_batch_size 2     --gradient_accumulation_steps 64     --learning_rate 2e-5     --lr_scheduler_type linear     --warmup_ratio 0.03     --weight_decay 0.     --num_train_epochs 2     --output_dir results/gpt2-Large_oasst1     --with_tracking     --report_to tensorboard     --logging_steps 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'\n",
    "# output_dir = 'results/mpt-7b_oasst1'\n",
    "output_dir = f\"results/{model_name_or_path.split('/')[-1]}_oasst1\"\n",
    "\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "num_gpus = 1\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "!cd .. && \\\n",
    "accelerate launch \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    --use_deepspeed \\\n",
    "    --deepspeed_config_file {deepspeed_config_file} \\\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "print(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04a2d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/share/zsh/site-functions'), PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/223')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1624786.tmpdir/.1688671405.1624786.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_tpymp1z6/none_0tifimp9/attempt_0/0/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "[2023-07-06 16:31:42,143] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py:358: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "07/06/2023 16:31:42 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: bf16\n",
      "ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 1000000000.0, 'stage3_prefetch_bucket_size': 1000000000.0, 'stage3_param_persistence_threshold': 1000000.0, 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06/2023 16:32:01 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-7597161fffdb2ff7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-Large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-Large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/vocab.json\n",
      "loading file merges.txt from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/merges.txt\n",
      "loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-Large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/model.safetensors\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[2023-07-06 16:32:08,651] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 0.84B parameters\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-Large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/06/2023 16:32:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-7597161fffdb2ff7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3b7da383e21e57e3_*_of_00016.arrow\n",
      "07/06/2023 16:32:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-7597161fffdb2ff7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-89525252e68b4e6c.arrow\n",
      "07/06/2023 16:32:22 - INFO - __main__ - Sample 2540 of the training set: {'input_ids': tensor([   27,    91,  7220,  ...,    13, 12466,    95]), 'labels': tensor([ -100,  -100,  -100,  ...,    13, 12466,    95]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}.\n",
      "07/06/2023 16:32:22 - INFO - __main__ - Sample 23904 of the training set: {'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  9139, 41582, 11033,   260,\n",
      "         5720,    11,   266,   494,   220,   488,   304,   500, 20403,  1350,\n",
      "        26605,   518,    13,   198,    27,    91,   562, 10167,    91,    29,\n",
      "          198, 45896, 16261,   677,   372,   732,   786,   479,  1236,   220,\n",
      "          488,   885,   500,   317, 17990,   403,   701,  1976,   333,  2332,\n",
      "          301,   695,  2150,  6054,  1428,  2034, 30748,   308,  1765,   268,\n",
      "           13,   198,    35,   292, 21522, 49746,   918,    11, 21003,  1142,\n",
      "         3318,  4643,    86,   437,   268,  2322,   265,  5329,   520,  2364,\n",
      "           68,   479,  1236,   308,   891, 11033, 11840, 33467,   384,   259,\n",
      "           11,  3318,  1931,  3841,   861,   569,   669, 30830,  3318,    11,\n",
      "          555,   353, 21039,   301, 11033,   358,   268,    11, 23781,    69,\n",
      "          562,   437,   274, 13614,   545,   471, 11296,   648, 10255, 10564,\n",
      "          268,    11, 23781,  4656, 28799,   372, 29361,   390,  7274,  3318,\n",
      "          843, 11882,  1976,    84,   308,   413, 11033, 11840,   293,   396,\n",
      "          268,    13, 50256,   198,    27,    91,  7220,    91,    29,   198,\n",
      "        24032,   198,  9139, 41582, 11033,   260,  5720,    11,   266,   494,\n",
      "          220,   488,   304,   500, 20403,  1350, 26605,   518,    13,   198,\n",
      "           27,    91,   562, 10167,    91,    29,   198, 23041, 46795,    83,\n",
      "         1646,   354,   798,  1734,   337,  9101,  4743,   488,   365,   270,\n",
      "          268,    11,   304,   500, 20403,  1350,  1976,    84,   275,   559,\n",
      "          268,    13, 47154,   293, 10564,   263,   347,   559,   732, 13254,\n",
      "         1888,  9101,    83,  9324, 12870,  1134, 42690,    11,  5029,  2395,\n",
      "          307,    72,   304,  7749,  6184,    97,    84, 39683, 14226,   412,\n",
      "          259,  2704,  1046,   267,  1082,  4643,    76, 25308,  2150, 10255,\n",
      "          290, 14226, 12870,  1134, 42690, 19278,   302,   363,   494,   918,\n",
      "           13,  1004,  1304,   479,  1236,   220,   488, 26672, 42499,   885,\n",
      "          500,  2429,    64,   518,   260,   317, 17990,   403,   701,  1976,\n",
      "          333,  2332,   301,   695,  2150,   308,  1765,   268,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100, 45896, 16261,   677,   372,   732,   786,   479,  1236,   220,\n",
      "          488,   885,   500,   317, 17990,   403,   701,  1976,   333,  2332,\n",
      "          301,   695,  2150,  6054,  1428,  2034, 30748,   308,  1765,   268,\n",
      "           13,   198,    35,   292, 21522, 49746,   918,    11, 21003,  1142,\n",
      "         3318,  4643,    86,   437,   268,  2322,   265,  5329,   520,  2364,\n",
      "           68,   479,  1236,   308,   891, 11033, 11840, 33467,   384,   259,\n",
      "           11,  3318,  1931,  3841,   861,   569,   669, 30830,  3318,    11,\n",
      "          555,   353, 21039,   301, 11033,   358,   268,    11, 23781,    69,\n",
      "          562,   437,   274, 13614,   545,   471, 11296,   648, 10255, 10564,\n",
      "          268,    11, 23781,  4656, 28799,   372, 29361,   390,  7274,  3318,\n",
      "          843, 11882,  1976,    84,   308,   413, 11033, 11840,   293,   396,\n",
      "          268,    13, 50256,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100, 23041, 46795,    83,\n",
      "         1646,   354,   798,  1734,   337,  9101,  4743,   488,   365,   270,\n",
      "          268,    11,   304,   500, 20403,  1350,  1976,    84,   275,   559,\n",
      "          268,    13, 47154,   293, 10564,   263,   347,   559,   732, 13254,\n",
      "         1888,  9101,    83,  9324, 12870,  1134, 42690,    11,  5029,  2395,\n",
      "          307,    72,   304,  7749,  6184,    97,    84, 39683, 14226,   412,\n",
      "          259,  2704,  1046,   267,  1082,  4643,    76, 25308,  2150, 10255,\n",
      "          290, 14226, 12870,  1134, 42690, 19278,   302,   363,   494,   918,\n",
      "           13,  1004,  1304,   479,  1236,   220,   488, 26672, 42499,   885,\n",
      "          500,  2429,    64,   518,   260,   317, 17990,   403,   701,  1976,\n",
      "          333,  2332,   301,   695,  2150,   308,  1765,   268,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06/2023 16:32:22 - INFO - __main__ - Sample 37597 of the training set: {'input_ids': tensor([   27,    91,  7220,    91,    29,   198,   126,   123, 46141,   415,\n",
      "          418,  9195,  4951,   384,   289,   272,  1171,  4533,  1619,  1960,\n",
      "          273, 19068,  1081, 44273,    30,   198,    27,    91,   562, 10167,\n",
      "           91,    29,   198,  4834,  3350,   695,  5733,   285, 40138,   390,\n",
      "         1679, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  4834,  3350,   695,  5733,   285, 40138,   390,\n",
      "         1679, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/06/2023 16:32:22 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 64 from 1.\n",
      "[2023-07-06 16:32:22,527] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown\n",
      "07/06/2023 16:32:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "07/06/2023 16:32:22 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "[2023-07-06 16:32:22,597] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-07-06 16:32:22,598] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-07-06 16:32:22,598] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-07-06 16:32:22,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2023-07-06 16:32:22,617] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2023-07-06 16:32:22,617] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n",
      "[2023-07-06 16:32:22,796] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-07-06 16:32:22,797] [INFO] [utils.py:786:see_memory_usage] MA 1.75 GB         Max_MA 1.99 GB         CA 2.21 GB         Max_CA 2 GB \n",
      "[2023-07-06 16:32:22,797] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.06 GB, percent = 21.6%\n",
      "[2023-07-06 16:32:22,798] [INFO] [stage3.py:113:__init__] Reduce bucket size 1000000000\n",
      "[2023-07-06 16:32:22,798] [INFO] [stage3.py:114:__init__] Prefetch bucket size 1000000000\n",
      "Using /u/wpq/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /u/wpq/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1322329044342041 seconds\n",
      "[2023-07-06 16:32:23,491] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-07-06 16:32:23,492] [INFO] [utils.py:786:see_memory_usage] MA 1.75 GB         Max_MA 1.75 GB         CA 2.21 GB         Max_CA 2 GB \n",
      "[2023-07-06 16:32:23,492] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.15 GB, percent = 21.7%\n",
      "Parameter Offload: Total persistent parameters: 601600 in 290 params\n",
      "[2023-07-06 16:32:23,634] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-07-06 16:32:23,635] [INFO] [utils.py:786:see_memory_usage] MA 1.74 GB         Max_MA 1.87 GB         CA 2.21 GB         Max_CA 2 GB \n",
      "[2023-07-06 16:32:23,635] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.16 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:23,764] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-07-06 16:32:23,765] [INFO] [utils.py:786:see_memory_usage] MA 1.74 GB         Max_MA 1.74 GB         CA 2.21 GB         Max_CA 2 GB \n",
      "[2023-07-06 16:32:23,765] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.18 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:24,451] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 2\n",
      "[2023-07-06 16:32:24,452] [INFO] [utils.py:786:see_memory_usage] MA 1.72 GB         Max_MA 1.74 GB         CA 1.96 GB         Max_CA 2 GB \n",
      "[2023-07-06 16:32:24,452] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.25 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:24,577] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-07-06 16:32:24,578] [INFO] [utils.py:786:see_memory_usage] MA 1.72 GB         Max_MA 1.72 GB         CA 1.96 GB         Max_CA 2 GB \n",
      "[2023-07-06 16:32:24,578] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.26 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:24,712] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\n",
      "[2023-07-06 16:32:24,713] [INFO] [utils.py:786:see_memory_usage] MA 4.6 GB         Max_MA 6.04 GB         CA 6.29 GB         Max_CA 6 GB \n",
      "[2023-07-06 16:32:24,713] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.28 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:24,842] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-07-06 16:32:24,843] [INFO] [utils.py:786:see_memory_usage] MA 4.6 GB         Max_MA 4.6 GB         CA 6.29 GB         Max_CA 6 GB \n",
      "[2023-07-06 16:32:24,843] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.29 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:25,135] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-07-06 16:32:25,135] [INFO] [utils.py:786:see_memory_usage] MA 10.37 GB         Max_MA 19.01 GB         CA 20.7 GB         Max_CA 21 GB \n",
      "[2023-07-06 16:32:25,136] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.32 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:25,136] [INFO] [stage3.py:366:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2023-07-06 16:32:25,368] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-07-06 16:32:25,369] [INFO] [utils.py:786:see_memory_usage] MA 13.67 GB         Max_MA 13.91 GB         CA 20.7 GB         Max_CA 21 GB \n",
      "[2023-07-06 16:32:25,369] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 218.34 GB, percent = 21.7%\n",
      "[2023-07-06 16:32:25,369] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2023-07-06 16:32:25,369] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-07-06 16:32:25,369] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-07-06 16:32:25,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:953:print] DeepSpeedEngine configuration:\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   amp_enabled .................. False\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   amp_params ................... False\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   bfloat16_enabled ............. True\n",
      "[2023-07-06 16:32:25,370] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x151e4fbe2680>\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   communication_data_type ...... None\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   dataloader_drop_last ......... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   disable_allgather ............ False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   dump_state ................... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   elasticity_enabled ........... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   fp16_auto_cast ............... None\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   fp16_enabled ................. False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   global_rank .................. 0\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   grad_accum_dtype ............. None\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 64\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   gradient_clipping ............ 1.0\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 1\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   load_universal_checkpoint .... False\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   loss_scale ................... 1.0\n",
      "[2023-07-06 16:32:25,371] [INFO] [config.py:957:print]   memory_breakdown ............. False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   optimizer_name ............... None\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   optimizer_params ............. None\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   pld_enabled .................. False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   pld_params ................... False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   prescale_gradients ........... False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   scheduler_name ............... None\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   scheduler_params ............. None\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   sparse_attention ............. None\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   steps_per_print .............. inf\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   train_batch_size ............. 128\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  2\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   use_node_local_storage ....... False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   world_size ................... 1\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  True\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1000000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=1000000000 param_persistence_threshold=1000000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   zero_enabled ................. True\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:957:print]   zero_optimization_stage ...... 3\n",
      "[2023-07-06 16:32:25,372] [INFO] [config.py:943:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.000000e+09, \n",
      "        \"stage3_prefetch_bucket_size\": 1.000000e+09, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+06, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 64, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 128, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Using /u/wpq/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00038743019104003906 seconds\n",
      "07/06/2023 16:32:25 - INFO - __main__ - ***** Running training *****\n",
      "07/06/2023 16:32:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) =  Num examples = 55544\n",
      "07/06/2023 16:32:25 - INFO - __main__ -   Num Epochs = 2\n",
      "07/06/2023 16:32:25 - INFO - __main__ -   Instantaneous batch size per device = 2\n",
      "07/06/2023 16:32:25 - INFO - __main__ -  128\n",
      "07/06/2023 16:32:25 - INFO - __main__ -   Gradient Accumulation steps = 64\n",
      "07/06/2023 16:32:25 - INFO - __main__ -   Total optimization steps = 868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/868 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[2023-07-06 16:33:11,645] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "  0%|                                        | 1/868 [00:46<11:08:38, 46.27s/it]07/06/2023 16:33:11 - INFO - __main__ -   Step: 1, LR: 7.692307692307694e-07, Loss: 2.390869140625\n",
      "  0%|                                         | 2/868 [01:12<8:19:26, 34.60s/it]07/06/2023 16:33:38 - INFO - __main__ -   Step: 2, LR: 1.5384615384615387e-06, Loss: 2.469482421875\n",
      "  0%|▏                                        | 3/868 [01:38<7:20:57, 30.59s/it]07/06/2023 16:34:03 - INFO - __main__ -   Step: 3, LR: 2.307692307692308e-06, Loss: 2.6964111328125\n",
      "  0%|▏                                        | 4/868 [02:04<6:53:40, 28.73s/it]07/06/2023 16:34:29 - INFO - __main__ -   Step: 4, LR: 3.0769230769230774e-06, Loss: 2.63916015625\n",
      "  1%|▏                                        | 5/868 [02:29<6:35:50, 27.52s/it]07/06/2023 16:34:55 - INFO - __main__ -   Step: 5, LR: 3.846153846153847e-06, Loss: 2.6812744140625\n",
      "  1%|▎                                        | 6/868 [02:55<6:24:56, 26.79s/it]07/06/2023 16:35:20 - INFO - __main__ -   Step: 6, LR: 4.615384615384616e-06, Loss: 2.59375\n",
      "  1%|▎                                        | 7/868 [03:20<6:18:36, 26.38s/it]07/06/2023 16:35:46 - INFO - __main__ -   Step: 7, LR: 5.384615384615385e-06, Loss: 2.4986572265625\n",
      "  1%|▍                                        | 8/868 [03:45<6:13:06, 26.03s/it]07/06/2023 16:36:11 - INFO - __main__ -   Step: 8, LR: 6.153846153846155e-06, Loss: 2.7109375\n",
      "  1%|▍                                        | 9/868 [04:11<6:08:30, 25.74s/it]07/06/2023 16:36:36 - INFO - __main__ -   Step: 9, LR: 6.923076923076923e-06, Loss: 2.5308837890625\n",
      "  1%|▍                                       | 10/868 [04:36<6:04:52, 25.52s/it]07/06/2023 16:37:01 - INFO - __main__ -   Step: 10, LR: 7.692307692307694e-06, Loss: 2.321533203125\n",
      "  1%|▌                                       | 11/868 [05:01<6:02:25, 25.37s/it]07/06/2023 16:37:26 - INFO - __main__ -   Step: 11, LR: 8.461538461538462e-06, Loss: 2.59326171875\n",
      "  1%|▌                                       | 12/868 [05:26<6:00:24, 25.26s/it]07/06/2023 16:37:51 - INFO - __main__ -   Step: 12, LR: 9.230769230769232e-06, Loss: 2.5745849609375\n",
      "  1%|▌                                       | 13/868 [05:51<5:59:49, 25.25s/it]07/06/2023 16:38:16 - INFO - __main__ -   Step: 13, LR: 1e-05, Loss: 2.443603515625\n",
      "  2%|▋                                       | 14/868 [06:16<5:59:20, 25.25s/it]07/06/2023 16:38:41 - INFO - __main__ -   Step: 14, LR: 1.076923076923077e-05, Loss: 2.449951171875\n",
      "  2%|▋                                       | 15/868 [06:41<5:59:03, 25.26s/it]07/06/2023 16:39:07 - INFO - __main__ -   Step: 15, LR: 1.1538461538461538e-05, Loss: 2.3995361328125\n",
      "  2%|▋                                       | 16/868 [07:07<5:58:46, 25.27s/it]07/06/2023 16:39:32 - INFO - __main__ -   Step: 16, LR: 1.230769230769231e-05, Loss: 2.406982421875\n",
      "  2%|▊                                       | 17/868 [07:32<5:57:34, 25.21s/it]07/06/2023 16:39:57 - INFO - __main__ -   Step: 17, LR: 1.3076923076923078e-05, Loss: 2.4140625\n",
      "  2%|▊                                       | 18/868 [07:57<5:56:13, 25.15s/it]07/06/2023 16:40:22 - INFO - __main__ -   Step: 18, LR: 1.3846153846153847e-05, Loss: 2.4720458984375\n",
      "  2%|▉                                       | 19/868 [08:22<5:55:50, 25.15s/it]07/06/2023 16:40:47 - INFO - __main__ -   Step: 19, LR: 1.4615384615384615e-05, Loss: 2.4373779296875\n",
      "  2%|▉                                       | 20/868 [08:47<5:55:04, 25.12s/it]07/06/2023 16:41:12 - INFO - __main__ -   Step: 20, LR: 1.5384615384615387e-05, Loss: 2.2572021484375\n",
      "  2%|▉                                       | 21/868 [09:12<5:54:53, 25.14s/it]07/06/2023 16:41:38 - INFO - __main__ -   Step: 21, LR: 1.6153846153846154e-05, Loss: 2.4146728515625\n",
      "  3%|█                                       | 22/868 [09:37<5:54:00, 25.11s/it]07/06/2023 16:42:03 - INFO - __main__ -   Step: 22, LR: 1.6923076923076924e-05, Loss: 2.4559326171875\n",
      "  3%|█                                       | 23/868 [10:02<5:52:37, 25.04s/it]07/06/2023 16:42:27 - INFO - __main__ -   Step: 23, LR: 1.7692307692307694e-05, Loss: 2.3477783203125\n",
      "  3%|█                                       | 24/868 [10:27<5:52:08, 25.03s/it]07/06/2023 16:42:52 - INFO - __main__ -   Step: 24, LR: 1.8461538461538465e-05, Loss: 2.4708251953125\n",
      "  3%|█▏                                      | 25/868 [10:52<5:51:43, 25.03s/it]07/06/2023 16:43:17 - INFO - __main__ -   Step: 25, LR: 1.923076923076923e-05, Loss: 2.3385009765625\n",
      "  3%|█▏                                      | 26/868 [11:17<5:51:38, 25.06s/it]07/06/2023 16:43:43 - INFO - __main__ -   Step: 26, LR: 2e-05, Loss: 2.3751220703125\n",
      "  3%|█▏                                      | 27/868 [11:42<5:51:26, 25.07s/it]07/06/2023 16:44:08 - INFO - __main__ -   Step: 27, LR: 1.9976247030878863e-05, Loss: 2.350341796875\n",
      "  3%|█▎                                      | 28/868 [12:08<5:51:54, 25.14s/it]07/06/2023 16:44:33 - INFO - __main__ -   Step: 28, LR: 1.995249406175772e-05, Loss: 2.13287353515625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/pexpect/spawnbase.py:372\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcd .. && accelerate launch     --mixed_precision bf16     --num_machines 1     --num_processes 1     --use_deepspeed     --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate_setauto.conf     open_instruct/finetune.py     --model_name_or_path gpt2-Large     --tokenizer_name gpt2-Large     --train_file data/processed/oasst1/oasst1_data.jsonl     --max_seq_length 1024     --preprocessing_num_workers 16     --per_device_train_batch_size 2     --gradient_accumulation_steps 64     --learning_rate 2e-5     --lr_scheduler_type linear     --warmup_ratio 0.03     --weight_decay 0.     --num_train_epochs 2     --output_dir results/gpt2-Large_oasst1     --with_tracking     --report_to tensorboard     --logging_steps 1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/ipykernel/zmqshell.py:649\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:177\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Ensure the subprocess really is terminated\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m         \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# add isalive check, to ensure exitstatus is set:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m child\u001b[38;5;241m.\u001b[39misalive()\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:646\u001b[0m, in \u001b[0;36mspawn.terminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill(signal\u001b[38;5;241m.\u001b[39mSIGCONT)\n\u001b[0;32m--> 646\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayafterterminate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misalive():\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!cd .. && accelerate launch     --mixed_precision bf16     --num_machines 1     --num_processes 1     --use_deepspeed     --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate_setauto.conf     open_instruct/finetune.py     --model_name_or_path gpt2-Large     --tokenizer_name gpt2-Large     --train_file data/processed/oasst1/oasst1_data.jsonl     --max_seq_length 1024     --preprocessing_num_workers 16     --per_device_train_batch_size 2     --gradient_accumulation_steps 64     --learning_rate 2e-5     --lr_scheduler_type linear     --warmup_ratio 0.03     --weight_decay 0.     --num_train_epochs 2     --output_dir results/gpt2-Large_oasst1     --with_tracking     --report_to tensorboard     --logging_steps 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc5aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
