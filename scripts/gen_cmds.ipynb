{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da1794b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'x86_64', 'cluster': 'ccc', 'queue': 'alt_7d'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info, move_lsf_job_summary_to_save_dir\n",
    "from note_pruning_analysis import open_instruct_dir\n",
    "import getpass\n",
    "\n",
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "info = get_host_info()\n",
    "info.update({'queue': queue})\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "os.environ['TORCHELASTIC_ERROR_FILE'] = os.path.join(os.getcwd(), 'torchelastic_error_file') \n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = os.path.normpath(os.path.join(os.getcwd(), '../../../../mitibm2023/cache')) \\\n",
    "    if arch == 'ppc64le' else '/dccstor/data-pruning/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True, mode=0o777)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##\n",
    "##\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c421d1",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c09b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/oi2/llama-7b_sharegptv2_ep=2 using 6 GPUs, 1 batch size per GPU, 1 gradient accumulation steps, 30 effective batch size.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp7pssto1b', 'job_id': 1354503}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2p8zx1bt', 'job_id': 1354504}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2_mpillk', 'job_id': 1354505}]\n"
     ]
    }
   ],
   "source": [
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm\n",
    "debug = False\n",
    "if debug:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'INFO'\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "else:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'WARNING'\n",
    "    os.environ['NCCL_DEBUG'] = ''\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  650 if arch == 'ppc64le' else 64\n",
    "\n",
    "preprocessing_num_workers = 32\n",
    "report_to = 'wandb'\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16'\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float32'\n",
    "gradient_checkpointing = True\n",
    "use_fast_tokenizer = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "resume_from_checkpoint = True # resume from latest checkpoint if exists, otherwise train from scratch\n",
    "num_train_epochs = 2\n",
    "checkpointing_steps = 300 # (50_000 / 32) * 2 / 6 ~= 500 (data size of 50k, bsz=32, ep=2, total save 6 times at most)\n",
    "max_train_steps = None\n",
    "subsample_inds_file_list = [None]\n",
    "dataloader_sampler = 'RandomSampler'\n",
    "overwrite_cache = True\n",
    "\n",
    "\n",
    "# #####\n",
    "# job_name = 'dpo1'\n",
    "\n",
    "# # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-410m-deduped'; max_seq_length = 2048; abbr_model_name = 'pythia-410m'\n",
    "# # model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "\n",
    "# train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "\n",
    "\n",
    "# M = 60_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 50_000; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "    (10_000, 10),\n",
    "#     (30_000, 3),\n",
    "]]\n",
    "\n",
    "gen_output_md = 'llama7br512p4096'\n",
    "# gen_output_md = 'llama7b+sharegptv2ep2+r512p4096'\n",
    "# gen_output_model_name = 'all-mpnet-base-v2'\n",
    "\n",
    "scoring_fn_list = []\n",
    "scoring_fn_list += ['random_s=0']\n",
    "# scoring_fn_list += ['random_s=1']\n",
    "scoring_fn_list += [ \n",
    "    f'dppmap_k=vmf_gamma=1_kmd={gen_output_md}_kemb=grad+rp+loraB',\n",
    "    f'dppmap_k=rbf_gamma=1e-3_kmd={gen_output_md}_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1_kmd=mpnet_kemb=text+embedding',\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'dpo2_{dataset}:{abbr_model_name}'\n",
    "    \n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12\n",
    "# nodes = 2; num_gpus = 1; gpu_type = 'v100'; job_duration = 6; cpu_mem = 100; num_cpus = 32; max_train_steps = 5; checkpointing_steps = 2; report_to = 'tensorboard'\n",
    "    \n",
    "#####\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs = 1 # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "use_deepspeed = True\n",
    "deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate.conf'\n",
    "\n",
    "per_device_train_batch_size = 1; total_batch_size = 32\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"{effective_batch_size} effective batch size.\")\n",
    "\n",
    "# reference: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n",
    "launcher = f\"\"\"accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines {nodes} \\\n",
    "    --num_processes {num_gpus*nodes} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''} \\\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''} \\\n",
    "    {'--main_process_ip $master_addr' if use_deepspeed else ''} \\\n",
    "    {'--main_process_port $master_port' if use_deepspeed else ''} \\\n",
    "    {'--machine_rank $SLURM_PROCID' if use_deepspeed else ''} \\\n",
    "    {'--rdzv_backend c10d' if use_deepspeed and nodes>1 else ''} \\\n",
    "    {'--deepspeed_multinode_launcher standard' if use_deepspeed and nodes>1 else ''} \\\n",
    "\"\"\"\n",
    "\n",
    "cmds = []\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_inds_file_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (subsample_inds_file,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{dataset}\"\n",
    "    if any(job_name == y for y in ['dpo1']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "\n",
    "    if subsample_inds_file:\n",
    "        assert(num_train_epochs==1)\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                s = f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "    if subsample_inds_file is not None:\n",
    "        assert(dataloader_sampler=='SequentialSampler')\n",
    "        assert(num_train_epochs==1)\n",
    "    else:\n",
    "        assert(dataloader_sampler=='RandomSampler')\n",
    "\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    wandb_run_name = output_dir.replace('results/', '')\n",
    "\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {f'cd .. && CUDA_VISIBLE_DEVICES={os.environ[\"CUDA_VISIBLE_DEVICES\"]} ' if test_run else ''}{launcher}\n",
    "        open_instruct/dpo_tune.py \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --tokenizer_name {model_name_or_path} \\\n",
    "        {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        --train_file {train_file} \\\n",
    "        --max_seq_length {max_seq_length} \\\n",
    "        {'--subsample_inds_file '+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        --dataloader_sampler {dataloader_sampler} \\\n",
    "        --preprocessing_num_workers {preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "        --learning_rate 5e-7 \\\n",
    "        --lr_scheduler_type linear \\\n",
    "        --warmup_ratio 0.1 \\\n",
    "        --weight_decay 0. \\\n",
    "        --num_train_epochs {num_train_epochs} \\\n",
    "        --with_tracking \\\n",
    "        {'--report_to \"'+str(report_to)+'\"' if report_to else ''} \\\n",
    "        --checkpointing_steps {checkpointing_steps} \\\n",
    "        {'--max_train_steps '+str(max_train_steps) if max_train_steps else ''} \\\n",
    "        {'--resume_from_checkpoint' if resume_from_checkpoint else ''} \\\n",
    "        {'--low_cpu_mem_usage' if not use_deepspeed else ''} \\\n",
    "        {'--overwrite_cache' if overwrite_cache else ''} \\\n",
    "        --logging_steps 1 \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    # if test_run:\n",
    "    #     print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    if test_run:\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prune: {1k@10, 10k@3}, datasets={dolly, stanford_alpaca}, scoring={random, dppmapx2}\n",
    "# need to gen curriculum for 50k sft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b79b9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_dpo.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce604bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=5\n",
      "+ cd ..\n",
      "+ CUDA_VISIBLE_DEVICES=2,5\n",
      "+ accelerate launch --mixed_precision fp16 --num_machines 1 --num_processes 2 --use_deepspeed --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py --model_name_or_path results/baselines/huggyllama/llama-7b --tokenizer_name results/baselines/huggyllama/llama-7b --gradient_checkpointing --train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl --max_seq_length 2048 --preprocessing_num_workers 32 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --learning_rate 5e-7 --lr_scheduler_type linear --warmup_ratio 0.1 --weight_decay 0. --num_train_epochs 2 --with_tracking --report_to tensorboard --checkpointing_steps 500 --logging_steps 1 --output_dir results/dpo1/jpt_llama-7b_ultrafeedback\n",
      "[2024-01-08 20:41:08,547] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-01-08 20:41:17,396] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-08 20:41:17,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:662:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,611] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 251.17it/s]\n",
      "01/08/2024 20:41:25 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-201ebfceee303348/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 252.78it/s]\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:26,493] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.11s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:43,556] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.48B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "01/08/2024 20:42:20 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.\n",
      "[2024-01-08 20:42:20,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.1+23a11a39, git-hash=23a11a39, git-branch=master\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "[2024-01-08 20:42:20,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-01-08 20:42:20,779] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-01-08 20:42:20,891] [INFO] [utils.py:786:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 14.16 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.9 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:118:__init__] Reduce bucket size 16777216\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:119:__init__] Prefetch bucket size 15099494\n",
      "[2024-01-08 20:42:20,995] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 13.64 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2024-01-08 20:42:21,139] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.76 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:21,240] [INFO] [utils.py:786:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.4 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:23,090] [INFO] [utils.py:786:see_memory_usage] After creating fp16 partitions: 4\n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.4 GB         CA 14.6 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.99 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,190] [INFO] [utils.py:786:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.39 GB         CA 14.6 GB         Max_CA 15 GB \n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.81 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,742] [INFO] [utils.py:786:see_memory_usage] After creating fp32 partitions\n",
      "[2024-01-08 20:42:23,743] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 26.61 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,744] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 116.15 GB, percent = 16.7%\n",
      "[2024-01-08 20:42:23,836] [INFO] [utils.py:786:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 25.94 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 117.28 GB, percent = 16.8%\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 1; 31.75 GiB total capacity; 25.93 GiB already allocated; 3.34 GiB free; 27.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 0; 31.75 GiB total capacity; 25.94 GiB already allocated; 3.21 GiB free; 27.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 409110) of binary: /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/python3.10\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/accelerate\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 964, in launch_command\r\n",
      "    deepspeed_launcher(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 687, in deepspeed_launcher\r\n",
      "    distrib_run.run(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "open_instruct/dpo_tune.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "[1]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 1 (local_rank: 1)\r\n",
      "  exitcode  : 1 (pid: 409111)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 409110)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_dpo.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19aebd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results/baselines/huggyllama/llama-7b using 8 GPUs, 1 batch size per GPU, 16 gradient accumulation steps, Effective batch size 128\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 128,\n",
      "    \"cpu_mem\": 768,\n",
      "    \"num_gpus\": 8,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'jbsub -queue alt_7d -name oi2 -mem 768g -cores 1x128+8 -require a100_80gb -out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'echo \"Running on $LSB_DJOB_HOSTFILE\"; echo \"======\"; master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\"); master_port=10002; RDZV_ENDPOINT=$master_addr:$master_port; source /dccstor/data-pruning/.profile; conda activate open-instruct; cd /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct; set -e; set -x; echo \"======\"; torchrun --nnodes 1 --nproc_per_node=8 --rdzv_backend=c10d --master_port=0 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/ultrachat/ultrachat50k_train_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=128 --per_device_train_batch_size=1 --gradient_accumulation_steps=16 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=1000000 --report_to tensorboard wandb --run_name ccc/oi2/llama-7b_ultrachat50k_ep=2 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=1000000 --save_total_limit=1 --num_train_epochs=2 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --torch_dtype=float32 --save_model_torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --use_flash_attn False --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/llama-7b_ultrachat50k_ep=2\"; [ ! -f \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out\" ] && mv /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/$LSB_JOBID*.out /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/llama-7b_ultrachat50k_ep=2 ||:\\'', 'job_id': 1374193}]\n"
     ]
    }
   ],
   "source": [
    "queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "add_hardwarespec_to_dirname = False\n",
    "save_strategy = 'steps'\n",
    "save_steps = 200 if getpass.getuser() in ('PTFMqngp', 'wpq') else 1_000_000\n",
    "save_total_limit = 1\n",
    "preprocessing_num_workers = 32\n",
    "evaluation_strategy = 'no' # set do_eval=False\n",
    "eval_steps = save_steps\n",
    "report_to = 'tensorboard wandb'\n",
    "suffix = None\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.03\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "subsample_inds_file_list = [None]\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "scoring_fn_and_pacing_fn = None\n",
    "\n",
    "\n",
    "# ########### sft baselines\n",
    "\n",
    "\n",
    "# job_name = 'oi2'; num_train_epochs_list = [2] \n",
    "# # model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "\n",
    "# # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; \n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# # train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'\n",
    "# # train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2'; max_train_samples_list=[100_000]\n",
    "\n",
    "# ## 50k sft datasets\n",
    "# # train_file = 'data/processed/flan_v2/flan_v250k_data.jsonl'; abbr_train_file = 'flan_v250k';\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca50k_data.jsonl'; abbr_train_file = 'stanford_alpaca50k'; \n",
    "# # train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# # train_file = 'data/processed/wizardlm/wizardlm50k_data.jsonl'; abbr_train_file = 'wizardlm50k'\n",
    "# # train_file = 'data/processed/sharegpt/sharegpt50k_data.jsonl'; abbr_train_file = 'sharegpt50k'\n",
    "# # train_file = 'data/processed/ultrachat/ultrachat50k_train_data.jsonl'; abbr_train_file = 'ultrachat50k'\n",
    "\n",
    "\n",
    "# # train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2'; max_train_samples_list=[100_000]\n",
    "# ###########\n",
    "\n",
    "\n",
    "# ############ pruning runs\n",
    "\n",
    "# # model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048; gen_output_md = 'mistral7br512p4096'\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048; gen_output_md = 'llama7br512p4096'\n",
    "\n",
    "\n",
    "# # dataset = 'flan_v2'; train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# # dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# dataset = 'stanford_alpaca'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca';\n",
    "# # dataset = 'oasst2'; train_file = 'data/processed/oasst/oasst2_data.jsonl'; abbr_train_file = 'oasst2';\n",
    "# # dataset = 'wizardlmv2'; train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2';\n",
    "# # dataset = 'sharegptv2'; train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2';\n",
    "# # dataset = 'ultrachat200kv2'; train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2';\n",
    "\n",
    "\n",
    "# # dataset = 'oasst1'; train_file = 'data/processed/oasst/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# # dataset = 'open_orca_slim'; train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; \n",
    "# # dataset = 'tulu_v2'; train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2';\n",
    "        \n",
    "# # M = 80_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 40_000\n",
    "# # M = 40_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 20_000\n",
    "# # M = 30_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 10_000\n",
    "# # M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# # M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "# pacing_fn_list = [\n",
    "#     f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "#         (10_000, 10),\n",
    "#         (30_000, 3),\n",
    "# #         (40_000, 2),\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# scoring_fn_list = [\n",
    "#     'random_s=0',\n",
    "# #     'random_s=1',\n",
    "# #     'log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n',\n",
    "# #     'ifd_neg', 'log_pmi_neg',\n",
    "# #     'numtoks_input_neg', 'numtoks_output_neg', 'numtoks_total_neg',\n",
    "# #     f'dppmap_k=vmf_gamma=1_kmd=mpnet', #_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding',\n",
    "#     f'dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB',\n",
    "# ]\n",
    "\n",
    "# scoring_fn_list += [ # vary kernel embedding model \n",
    "# #     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd={kmd}_kemb=grad+rp+loraB'\n",
    "# #     for kmd in ['llama7br256p4096', 'llama7br512p4096', 'pythia1br512p4096']\n",
    "# ]\n",
    "# scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "# job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "# ############ \n",
    "\n",
    "    \n",
    "# add_hardwarespec_to_dirname = True\n",
    "# job_name += '_debug' # wpq debug\n",
    "# max_train_samples_list=[128*2]\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "if arch == 'x86_64':\n",
    "    nodes = 1; num_gpus = 8; gpu_type = 'a100_80gb'; job_duration = 6\n",
    "    num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus); preprocessing_num_workers = 128 # tok takes quite a bit.\n",
    "    per_device_train_batch_size = 1\n",
    "    gradient_checkpointing = False\n",
    "    mixed_precision = 'bf16'; torch_dtype = 'float32'; use_flash_attn = False; use_tf32 = True \n",
    "    save_model_torch_dtype = 'bfloat16' # typically save fp32 weights, but for disk space sake, convert to bf16.\n",
    "else:\n",
    "    nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_checkpointing = True\n",
    "    mixed_precision = 'fp16'; torch_dtype = 'float32'; use_flash_attn = False; use_tf32 = False\n",
    "    save_model_torch_dtype = None\n",
    "\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "total_batch_size = 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "optimizer = 'adamw_hf'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\" \n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 \n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "load_in_8bit = False\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nnodes 1 --nproc_per_node={num_gpus} --rdzv_backend=c10d --master_port=0\" # assigns random port. https://github.com/pytorch/pytorch/issues/73320\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=${'SLURM_JOB_ID' if arch == 'ppcle64' else 'LSB_JOBID'} --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file = os.path.join(open_instruct_dir, 'scripts', 'error_file')\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "            \n",
    "    if any(job_name == y for y in ['oi2']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "        \n",
    "    if subsample_inds_file:\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            else:\n",
    "                return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            ('_dtype='+torch_dtype if torch_dtype is not None else '')+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)+\\\n",
    "            '_ngpus='+str(num_gpus)+\\\n",
    "            ('_fa2' if use_flash_attn else '')\n",
    "    if suffix:\n",
    "        output_dirname += suffix\n",
    "    output_dir = os.path.join(open_instruct_dir, 'results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join(open_instruct_dir, 'results', job_name), exist_ok=True)\n",
    "    if arch == 'x86_64':\n",
    "        wandb_run_name = 'ccc'+output_dir[output_dir.find('results'):][7:] # e.g., ccc/oi2/run_name\n",
    "    else:\n",
    "        wandb_run_name = output_dir.replace('results/', '') # e.g., oi2/run_name\n",
    "    \n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {'cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''} \\\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers={preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type={lr_scheduler_type} \\\n",
    "        --warmup_ratio={warmup_ratio} \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy={evaluation_strategy} \\\n",
    "        {'--eval_steps='+str(eval_steps) if eval_steps else ''} \\\n",
    "        {'--report_to '+str(report_to) if report_to else ''} \\\n",
    "        --run_name {wandb_run_name} \\\n",
    "        --logging_strategy=steps \\\n",
    "        --logging_first_step \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        --ddp_timeout=7200 \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''} \\\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        {'--torch_dtype='+str(torch_dtype) if torch_dtype else ''} \\\n",
    "        {'--save_model_torch_dtype='+str(save_model_torch_dtype) if save_model_torch_dtype else ''} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {f'--tf32=True' if use_tf32 else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --use_flash_attn {'True' if use_flash_attn else 'False'} \\\n",
    "        --low_cpu_mem_usage \\\n",
    "        --overwrite_cache \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #  --overwrite_cache   # if delete a dataset and need to refresh cache\n",
    "\n",
    "    if test_run:\n",
    "        print()\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64':\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        queue=queue,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12fc2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_sft.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed0c2894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ cd ..\n",
      "+ TORCHELASTIC_ERROR_FILE=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/error_file\n",
      "+ TORCH_CPP_LOG_LEVEL=INFO\n",
      "+ NCCL_DEBUG=INFO\n",
      "+ LOGLEVEL=INFO\n",
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/oasst1/oasst1_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=32 --per_device_train_batch_size=1 --gradient_accumulation_steps=128 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=200 --report_to tensorboard wandb --run_name /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=200 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --tf32=True --overwrite_output_dir --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/oasst1/random_s=0/inds_prune_size=10000_ep=10.pkl --dataloader_sampler SequentialSampler --use_flash_attn True --low_cpu_mem_usage --overwrite_cache --output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10\n",
      "[2024-01-19 02:04:37,834] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Saving args dict to /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10.args.json\n",
      "01/19/2024 02:04:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "01/19/2024 02:04:39 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_sampler=SequentialSampler,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=7200,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=200.0,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10/runs/Jan19_02-04-39_cccxc552,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_oasst1:llama-7b_debug/jpt_llama-7b_oasst1_score=random:s=0_pace=prune:size=10000:ep=10,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=200,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Using custom data configuration default-b03eccd42e843020\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Using custom data configuration default-b03eccd42e843020\n",
      "Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset info from data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "01/19/2024 02:04:39 - INFO - datasets.builder - Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/19/2024 02:04:39 - INFO - datasets.info - Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "[INFO|configuration_utils.py:715] 2024-01-19 02:04:39,135 >> loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-01-19 02:04:39,136 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:39,137 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3118] 2024-01-19 02:04:39,229 >> loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1222] 2024-01-19 02:04:39,229 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|modeling_utils.py:1304] 2024-01-19 02:04:39,230 >> You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:791] 2024-01-19 02:04:39,230 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "[INFO|modeling_utils.py:3950] 2024-01-19 02:04:41,778 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3958] 2024-01-19 02:04:41,778 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:749] 2024-01-19 02:04:41,781 >> loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "[INFO|configuration_utils.py:791] 2024-01-19 02:04:41,781 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "01/19/2024 02:04:41 - INFO - __main__ - [wpq] model.dtype=torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-19 02:04:41,785 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1648] 2024-01-19 02:04:41,845 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "Process #16 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #16 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "Process #17 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #17 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "Process #18 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #18 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "Process #19 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #19 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "Process #20 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #20 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "Process #21 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #21 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "Process #22 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #22 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "Process #23 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #23 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "Process #24 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #24 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "Process #25 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #25 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "Process #26 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #26 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "Process #27 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #27 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "Process #28 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #28 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "Process #29 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #29 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "Process #30 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #30 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "Process #31 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "01/19/2024 02:04:51 - INFO - datasets.arrow_dataset - Process #31 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spawning 32 processes\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Spawning 32 processes\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   0%| | 0/33717 [00:Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00000_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00001_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00002_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00003_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   0%| | 22/33717 [00Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00004_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00005_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00006_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00007_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00008_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   1%| | 361/33717 [0Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00009_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00010_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00011_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00012_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00013_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=32):   3%| | 1001/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00014_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00015_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00016_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00017_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00018_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   5%| | 1719/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00019_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00020_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00021_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00022_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):   7%| | 2251/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00023_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00025_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00024_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00026_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00027_of_00032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and reformatting instruction data (num_proc=32):  12%| | 4021/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00028_of_00032.arrow\n",
      "Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00029_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):  14%|▏| 4841/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "01/19/2024 02:04:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00030_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32):  18%|▏| 5957/33717 [Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "01/19/2024 02:04:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-34d81440de676221_00031_of_00032.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=32): 100%|█| 33717/33717 \n",
      "Concatenating 32 shards\n",
      "01/19/2024 02:04:55 - INFO - datasets.arrow_dataset - Concatenating 32 shards\n",
      "Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00000_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00000_of_00016.arrow\n",
      "Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00001_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00001_of_00016.arrow\n",
      "Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00002_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00002_of_00016.arrow\n",
      "Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00003_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00003_of_00016.arrow\n",
      "Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00004_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00004_of_00016.arrow\n",
      "Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00005_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00005_of_00016.arrow\n",
      "Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00006_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00006_of_00016.arrow\n",
      "Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00007_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00007_of_00016.arrow\n",
      "Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00008_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00008_of_00016.arrow\n",
      "Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00009_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00009_of_00016.arrow\n",
      "Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00010_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00010_of_00016.arrow\n",
      "Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00011_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00011_of_00016.arrow\n",
      "Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00012_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00012_of_00016.arrow\n",
      "Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00013_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00013_of_00016.arrow\n",
      "Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00014_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00014_of_00016.arrow\n",
      "Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00015_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_*_of_00016.arrow\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/oasst1/json/default-b03eccd42e843020/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-587744e8d3857964_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "01/19/2024 02:04:56 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "01/19/2024 02:04:56 - INFO - __main__ - Subsample dataset according to indices: /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/oasst1/random_s=0/inds_prune_size=10000_ep=10.pkl\n",
      "01/19/2024 02:04:56 - INFO - __main__ - subsample_inds_file has 10000 indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wpq] Example 0 of train_dataset: \r\n",
      "{'dataset': 'oasst1', 'id': 'oasst1_20480', 'messages': [{'role': 'user', 'content': 'Cómo manejar un carro manual'}, {'role': 'assistant', 'content': 'Lo primero que tienes que hacer, si nunca has conducido un coche manual, es familiarizarte con el embrague y palanca de cambios. Si conduces habitualmente un coche automático, estarás acostumbrado a no utilizar para nada el pie izquierdo ni la palanca del cambio. Encontrarás tres pedales, siendo el embrague el que está situado a la izquierda y el que tendrás que pisar cada vez que cambies de marcha. Por otro lado, la palanca del cambio se ubica siempre en la consola central.\\n\\nPara arrancar un coche manual, es necesario seguir una serie de pasos que, al principio, pueden parecer muchos, pero que, con el tiempo, acabarás haciéndolos sin darte cuenta:\\n\\n1) Comprueba que la palanca del cambio está en punto muerto\\n2) Coloca el pie derecho en el pedal del freno\\n3) Arranca el motor\\n4) Pisa el embrague con el pie izquierdo\\n5) Coloca la palanca del cambio en la primera marcha, sin levantar el pedal del freno\\n6) Suelta el freno de mano\\n7) Suelta el pedal del freno\\nYa estás listo para iniciar la marcha, soltando suavemente el embrague, a medida que aceleras.\\n\\nUna vez que ya estás en marcha, debes hacer un uso correcto del cambio manual para cambiar las marchas de forma correcta. Un uso incorrecto de la caja de cambios manual puede repercutir negativamente en tu seguridad y también afectar gravemente al embrague y a la transmisión, lo que se traduce en serias averías de coste muy elevado. Para evitarlo, te explicamos cómo debes proceder:\\n\\nUna vez que hayas arrancado, pisa el acelerador muy lentamente. Notarás que el régimen del motor aumenta. En ese momento, comienza a soltar suavemente el pedal del embrague. Verás que el motor vuelve a bajar de vueltas. En ese momento, puedes presionar un poco más el acelerador y el coche comenzará a avanzar.\\n\\nAhora llega el momento de meter la segunda marcha. Dependiendo del tipo de coche y combustible, podrás circular a un régimen de giro más bajo o alto. El régimen de giro en coche de gasolina, por lo general, oscila entre loas 2.500 y 3.000 vueltas. Si el motor está sobrealimentado por turbo, te permitirá circular por debajo de ese rango, ya que algunos coches turbos modernos entregan la totalidad de su par motor, incluso por debajo de las 2.000 vueltas.\\n\\nUn coche con motor turbodiésel te permite circular a un régimen muy bajo, por debajo de las 2.000 vueltas, ya que la entrega de par se produce antes que en un motor de gasolina.\\n\\nCuando el coche alcance un régimen de vueltas apropiado, suelta el pedal del acelerador y vuelve a pisar el embrague. Coge la palanca del cambio y baja para meter segunda. Suelta el embrague y presiona nuevamente el acelerador. A partir de aquí, cada vez que quieras cambiar de marcha, deberás repetir el mismo proceso: soltar el acelerador, pisar embrague, meter la marcha, soltar embrague y volver a acelerar.\\n\\n¡Buen viaje!'}], 'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29907, 29980,  4346,\r\n",
      "          767, 29872,  4758,   443,  1559,   307, 12219,    13, 29966, 29989,\r\n",
      "          465, 22137, 29989, 29958,    13,  3410,  1903,  1489,   712,   260,\r\n",
      "          819,   267,   712, 14557, 29892,  1354, 28456,   756, 13417, 13321,\r\n",
      "          443,  1302,  1173, 12219, 29892,   831,  9985,   466, 11908,   378,\r\n",
      "          560,  7232,  1431,   434,   343,  5112,   273,  1113,   316, 10625,\r\n",
      "         2363, 29889,  6101, 13417,   778,  4760, 14162,   443,  1302,  1173,\r\n",
      "         3345, 22054, 29892, 23673,  1569,  1274,   520,   398,  1182,   912,\r\n",
      "          263,   694, 11824,   279,  1702, 25801,   560,  5036,  5951, 16026,\r\n",
      "         1867,  6836,   425,  5112,   273,  1113,   628, 26007, 29889,  1174,\r\n",
      "         9996,   279,  1569,  9941,  8939,  2122, 29892, 18200,   560,  7232,\r\n",
      "         1431,   434,   560,   712,  7919,  2990,   912,   263,   425,  5951,\r\n",
      "        16026,  1388,   343,   560,   712, 10331, 11964,   712, 20066,   279,\r\n",
      "         9747,  7763,   712, 10625,   583,   316,  8575, 29874, 29889,  7102,\r\n",
      "        16994, 19931, 29892,   425,  5112,   273,  1113,   628, 26007,   409,\r\n",
      "        13069,   983, 26692,   427,   425,  1136,  2963,  6555, 29889,    13,\r\n",
      "           13,  2177, 29874,   564,   661,  4287,   443,  1302,  1173, 12219,\r\n",
      "        29892,   831, 16632,  2628,  7025,   381,  1185,  7080,   316,  2331,\r\n",
      "          359,   712, 29892,   394,  3420,   601, 29892, 19796,  9541,  2265,\r\n",
      "        24312, 29892,  7046,   712, 29892,   378,   560, 13924, 29892, 22998,\r\n",
      "          279,  1569,   447,   455, 21183,   324,   359,  4457,  5424,   371,\r\n",
      "        21052, 29901,    13,    13, 29896, 29897,   422,   558,   434,  2291,\r\n",
      "          712,   425,  5112,   273,  1113,   628, 26007,  7919,   427, 15978,\r\n",
      "          286, 15009,    13, 29906, 29897,  1530,  6400,   560,  5036, 14923,\r\n",
      "         1859,   427,   560,  8939,   284,   628,   285, 26155,    13, 29941,\r\n",
      "        29897,   826,   661,  1113,   560, 10992,    13, 29946, 29897,   349,\r\n",
      "         8069,   560,  7232,  1431,   434,   378,   560,  5036,  5951, 16026,\r\n",
      "         1867,    13, 29945, 29897,  1530,  6400,   425,  5112,   273,  1113,\r\n",
      "          628, 26007,   427,   425,  8633,  8575, 29874, 29892,  4457, 14453,\r\n",
      "          424,   279,   560,  8939,   284,   628,   285, 26155,    13, 29953,\r\n",
      "        29897,  2166,  2554,   560,   285, 26155,   316, 24318,    13, 29955,\r\n",
      "        29897,  2166,  2554,   560,  8939,   284,   628,   285, 26155,    13,\r\n",
      "        29979, 29874,   707,  1569,  1051, 29877,  1702, 21855,   279,   425,\r\n",
      "         8575, 29874, 29892,   899, 29873,  1743,   480,   485,  9936,   560,\r\n",
      "         7232,  1431,   434, 29892,   263,  1612,  1458,   712,  1274,  7367,\r\n",
      "          294, 29889,    13,    13, 29965,  1056,  7763,   712,  9343,   707,\r\n",
      "         1569,   427,  8575, 29874, 29892,  2553,   267, 14557,   443, 17448,\r\n",
      "         1959, 29877,   628, 26007, 12219,  1702, 10625,  4447,  1869,  8575,\r\n",
      "          294,   316,  5954,  1959, 29874, 29889,   853, 17448, 10240, 29877,\r\n",
      "          316,   425,   274,  9919,   316, 10625,  2363, 12219, 11493,   337,\r\n",
      "          546,  7582,   381,  3480,  1926,  2503,   427,  5291,  2377,   332,\r\n",
      "         2368,   343,  6196,  2511,   522,   279,  8310,  9936,   394,  7232,\r\n",
      "         1431,   434,   343,   263,   425, 18750, 11861, 29892,   658,   712,\r\n",
      "          409,  3534, 24551,   427,   724,  3173,  4759,  8577,   316,  3438,\r\n",
      "        29872, 12287, 11858,   912, 29889, 12994,  3415,  3673,   417, 29892,\r\n",
      "          734, 28117, 14054, 28810,  4346,  2553,   267,  6449,   261, 29901,\r\n",
      "           13,    13, 29965,  1056,  7763,   712, 14842,   294,   564,   661,\r\n",
      "        29883,   912, 29892,   282,  8069,   560,  1274,  7367,  3136, 12287,\r\n",
      "          301,   296,  2503, 29889,  2216,   279,  1569,   712,   560,  6367,\r\n",
      "        19933,   628, 10992, 19291, 29874, 29889,  1174, 15371, 14341, 29892,\r\n",
      "          419, 24880,   263,   899, 12637,   480,   485,  9936,   560,  8939,\r\n",
      "          284,   628,  7232,  1431,   434, 29889,  1798,  1569,   712,   560,\r\n",
      "        10992, 22126,   345,   263,   289,  1175,   279,   316, 18679,  2152,\r\n",
      "          294, 29889,  1174, 15371, 14341, 29892,  2653, 11696,  2225,   291,\r\n",
      "          279,   443, 14534,  3627,   560,  1274,  7367,  3136,   343,   560,\r\n",
      "         1302,  1173, 19487, 20484,   263,  1029,  4096,   279, 29889,    13,\r\n",
      "           13, 29909, 15255, 10953, 29874,   560, 14341,   316, 11134,   425,\r\n",
      "        17329,  8575, 29874, 29889, 10034,   355, 17008,   628, 13306,   316,\r\n",
      "         1302,  1173,   343,  4145,   504,  1821, 29892,  2532, 11964, 19308,\r\n",
      "          263,   443,  6367, 19933,   316,   330,  3350,  3627, 13085,   288,\r\n",
      "        20478, 29889,  1260,  6367, 19933,   316,   330,  3350,   427,  1302,\r\n",
      "         1173,   316, 10489,   324,  1099, 29892,  1277,   658,  2498, 29892,\r\n",
      "        15199,  4233,  2637,   658,   294, 29871, 29906, 29889, 29945, 29900,\r\n",
      "        29900,   343, 29871, 29941, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29889,  6101,   560, 10992,  7919,  4166,   284,  2073,   912,\r\n",
      "         1277,  7013,   833, 29892,   734, 14257,   381, 29976, 19308,  1277,\r\n",
      "         2553,  7069,   316, 15371,   364,  4524, 29892,  9343,   712, 20071,\r\n",
      "         1302,  6609,  7013, 27737,  5400,   359,   875,  1727,   273,   425,\r\n",
      "         3001,  2368,   316,   480,   610, 10992, 29892,  1343, 10648,  1277,\r\n",
      "         2553,  7069,   316,  1869, 29871, 29906, 29889, 29900, 29900, 29900,\r\n",
      "        18679,  2152,   294, 29889,    13,    13,  2525,  1302,  1173,   378,\r\n",
      "        10992,  7013, 29890, 12143,   743,   295,   734,  3635,   568, 19308,\r\n",
      "          263,   443,  6367, 19933, 12287, 13085, 29892,  1277,  2553,  7069,\r\n",
      "          316,  1869, 29871, 29906, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29892,  9343,   712,   425,   875,  1727, 29874,   316,   610,\r\n",
      "          409,  7738, 12971,   712,   427,   443, 10992,   316, 10489,   324,\r\n",
      "         1099, 29889,    13,    13, 29907, 29884,  1743,   560,  1302,  1173,\r\n",
      "        10747,   749,   443,  6367, 19933,   316, 18679,  2152,   294, 11712,\r\n",
      "         1631,   912, 29892,   480,  2554,   560,  8939,   284,   628,  1274,\r\n",
      "         7367,  3136,   343, 22126,   345,   263, 20066,   279,   560,  7232,\r\n",
      "         1431,   434, 29889,   315, 21317,   425,  5112,   273,  1113,   628,\r\n",
      "        26007,   343,   289,  9919,  1702, 11134, 17329, 29889,  2166,  2554,\r\n",
      "          560,  7232,  1431,   434,   343,  2225, 16017,  8005, 29894,  2503,\r\n",
      "          560,  1274,  7367,  3136, 29889,   319,  8019,   316, 10592, 29983,\r\n",
      "        29892,  9747,  7763,   712,   439,   631,   294, 10625,  4447,   316,\r\n",
      "         8575, 29874, 29892,   316,   495,  1569, 21159,   381,   560, 11329,\r\n",
      "        14177, 29877, 29901,   899, 12637,   560,  1274,  7367,  3136, 29892,\r\n",
      "        20066,   279,  7232,  1431,   434, 29892, 11134,   425,  8575, 29874,\r\n",
      "        29892,   899, 12637,  7232,  1431,   434,   343,  1700,   369,   263,\r\n",
      "         1274,  7367,   279, 29889,    13,    13, 30180,  3727,   264,  3025,\r\n",
      "         1324, 29991,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\r\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\r\n",
      "         -100,  -100,  -100,  -100,  -100,  3410,  1903,  1489,   712,   260,\r\n",
      "          819,   267,   712, 14557, 29892,  1354, 28456,   756, 13417, 13321,\r\n",
      "          443,  1302,  1173, 12219, 29892,   831,  9985,   466, 11908,   378,\r\n",
      "          560,  7232,  1431,   434,   343,  5112,   273,  1113,   316, 10625,\r\n",
      "         2363, 29889,  6101, 13417,   778,  4760, 14162,   443,  1302,  1173,\r\n",
      "         3345, 22054, 29892, 23673,  1569,  1274,   520,   398,  1182,   912,\r\n",
      "          263,   694, 11824,   279,  1702, 25801,   560,  5036,  5951, 16026,\r\n",
      "         1867,  6836,   425,  5112,   273,  1113,   628, 26007, 29889,  1174,\r\n",
      "         9996,   279,  1569,  9941,  8939,  2122, 29892, 18200,   560,  7232,\r\n",
      "         1431,   434,   560,   712,  7919,  2990,   912,   263,   425,  5951,\r\n",
      "        16026,  1388,   343,   560,   712, 10331, 11964,   712, 20066,   279,\r\n",
      "         9747,  7763,   712, 10625,   583,   316,  8575, 29874, 29889,  7102,\r\n",
      "        16994, 19931, 29892,   425,  5112,   273,  1113,   628, 26007,   409,\r\n",
      "        13069,   983, 26692,   427,   425,  1136,  2963,  6555, 29889,    13,\r\n",
      "           13,  2177, 29874,   564,   661,  4287,   443,  1302,  1173, 12219,\r\n",
      "        29892,   831, 16632,  2628,  7025,   381,  1185,  7080,   316,  2331,\r\n",
      "          359,   712, 29892,   394,  3420,   601, 29892, 19796,  9541,  2265,\r\n",
      "        24312, 29892,  7046,   712, 29892,   378,   560, 13924, 29892, 22998,\r\n",
      "          279,  1569,   447,   455, 21183,   324,   359,  4457,  5424,   371,\r\n",
      "        21052, 29901,    13,    13, 29896, 29897,   422,   558,   434,  2291,\r\n",
      "          712,   425,  5112,   273,  1113,   628, 26007,  7919,   427, 15978,\r\n",
      "          286, 15009,    13, 29906, 29897,  1530,  6400,   560,  5036, 14923,\r\n",
      "         1859,   427,   560,  8939,   284,   628,   285, 26155,    13, 29941,\r\n",
      "        29897,   826,   661,  1113,   560, 10992,    13, 29946, 29897,   349,\r\n",
      "         8069,   560,  7232,  1431,   434,   378,   560,  5036,  5951, 16026,\r\n",
      "         1867,    13, 29945, 29897,  1530,  6400,   425,  5112,   273,  1113,\r\n",
      "          628, 26007,   427,   425,  8633,  8575, 29874, 29892,  4457, 14453,\r\n",
      "          424,   279,   560,  8939,   284,   628,   285, 26155,    13, 29953,\r\n",
      "        29897,  2166,  2554,   560,   285, 26155,   316, 24318,    13, 29955,\r\n",
      "        29897,  2166,  2554,   560,  8939,   284,   628,   285, 26155,    13,\r\n",
      "        29979, 29874,   707,  1569,  1051, 29877,  1702, 21855,   279,   425,\r\n",
      "         8575, 29874, 29892,   899, 29873,  1743,   480,   485,  9936,   560,\r\n",
      "         7232,  1431,   434, 29892,   263,  1612,  1458,   712,  1274,  7367,\r\n",
      "          294, 29889,    13,    13, 29965,  1056,  7763,   712,  9343,   707,\r\n",
      "         1569,   427,  8575, 29874, 29892,  2553,   267, 14557,   443, 17448,\r\n",
      "         1959, 29877,   628, 26007, 12219,  1702, 10625,  4447,  1869,  8575,\r\n",
      "          294,   316,  5954,  1959, 29874, 29889,   853, 17448, 10240, 29877,\r\n",
      "          316,   425,   274,  9919,   316, 10625,  2363, 12219, 11493,   337,\r\n",
      "          546,  7582,   381,  3480,  1926,  2503,   427,  5291,  2377,   332,\r\n",
      "         2368,   343,  6196,  2511,   522,   279,  8310,  9936,   394,  7232,\r\n",
      "         1431,   434,   343,   263,   425, 18750, 11861, 29892,   658,   712,\r\n",
      "          409,  3534, 24551,   427,   724,  3173,  4759,  8577,   316,  3438,\r\n",
      "        29872, 12287, 11858,   912, 29889, 12994,  3415,  3673,   417, 29892,\r\n",
      "          734, 28117, 14054, 28810,  4346,  2553,   267,  6449,   261, 29901,\r\n",
      "           13,    13, 29965,  1056,  7763,   712, 14842,   294,   564,   661,\r\n",
      "        29883,   912, 29892,   282,  8069,   560,  1274,  7367,  3136, 12287,\r\n",
      "          301,   296,  2503, 29889,  2216,   279,  1569,   712,   560,  6367,\r\n",
      "        19933,   628, 10992, 19291, 29874, 29889,  1174, 15371, 14341, 29892,\r\n",
      "          419, 24880,   263,   899, 12637,   480,   485,  9936,   560,  8939,\r\n",
      "          284,   628,  7232,  1431,   434, 29889,  1798,  1569,   712,   560,\r\n",
      "        10992, 22126,   345,   263,   289,  1175,   279,   316, 18679,  2152,\r\n",
      "          294, 29889,  1174, 15371, 14341, 29892,  2653, 11696,  2225,   291,\r\n",
      "          279,   443, 14534,  3627,   560,  1274,  7367,  3136,   343,   560,\r\n",
      "         1302,  1173, 19487, 20484,   263,  1029,  4096,   279, 29889,    13,\r\n",
      "           13, 29909, 15255, 10953, 29874,   560, 14341,   316, 11134,   425,\r\n",
      "        17329,  8575, 29874, 29889, 10034,   355, 17008,   628, 13306,   316,\r\n",
      "         1302,  1173,   343,  4145,   504,  1821, 29892,  2532, 11964, 19308,\r\n",
      "          263,   443,  6367, 19933,   316,   330,  3350,  3627, 13085,   288,\r\n",
      "        20478, 29889,  1260,  6367, 19933,   316,   330,  3350,   427,  1302,\r\n",
      "         1173,   316, 10489,   324,  1099, 29892,  1277,   658,  2498, 29892,\r\n",
      "        15199,  4233,  2637,   658,   294, 29871, 29906, 29889, 29945, 29900,\r\n",
      "        29900,   343, 29871, 29941, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29889,  6101,   560, 10992,  7919,  4166,   284,  2073,   912,\r\n",
      "         1277,  7013,   833, 29892,   734, 14257,   381, 29976, 19308,  1277,\r\n",
      "         2553,  7069,   316, 15371,   364,  4524, 29892,  9343,   712, 20071,\r\n",
      "         1302,  6609,  7013, 27737,  5400,   359,   875,  1727,   273,   425,\r\n",
      "         3001,  2368,   316,   480,   610, 10992, 29892,  1343, 10648,  1277,\r\n",
      "         2553,  7069,   316,  1869, 29871, 29906, 29889, 29900, 29900, 29900,\r\n",
      "        18679,  2152,   294, 29889,    13,    13,  2525,  1302,  1173,   378,\r\n",
      "        10992,  7013, 29890, 12143,   743,   295,   734,  3635,   568, 19308,\r\n",
      "          263,   443,  6367, 19933, 12287, 13085, 29892,  1277,  2553,  7069,\r\n",
      "          316,  1869, 29871, 29906, 29889, 29900, 29900, 29900, 18679,  2152,\r\n",
      "          294, 29892,  9343,   712,   425,   875,  1727, 29874,   316,   610,\r\n",
      "          409,  7738, 12971,   712,   427,   443, 10992,   316, 10489,   324,\r\n",
      "         1099, 29889,    13,    13, 29907, 29884,  1743,   560,  1302,  1173,\r\n",
      "        10747,   749,   443,  6367, 19933,   316, 18679,  2152,   294, 11712,\r\n",
      "         1631,   912, 29892,   480,  2554,   560,  8939,   284,   628,  1274,\r\n",
      "         7367,  3136,   343, 22126,   345,   263, 20066,   279,   560,  7232,\r\n",
      "         1431,   434, 29889,   315, 21317,   425,  5112,   273,  1113,   628,\r\n",
      "        26007,   343,   289,  9919,  1702, 11134, 17329, 29889,  2166,  2554,\r\n",
      "          560,  7232,  1431,   434,   343,  2225, 16017,  8005, 29894,  2503,\r\n",
      "          560,  1274,  7367,  3136, 29889,   319,  8019,   316, 10592, 29983,\r\n",
      "        29892,  9747,  7763,   712,   439,   631,   294, 10625,  4447,   316,\r\n",
      "         8575, 29874, 29892,   316,   495,  1569, 21159,   381,   560, 11329,\r\n",
      "        14177, 29877, 29901,   899, 12637,   560,  1274,  7367,  3136, 29892,\r\n",
      "        20066,   279,  7232,  1431,   434, 29892, 11134,   425,  8575, 29874,\r\n",
      "        29892,   899, 12637,  7232,  1431,   434,   343,  1700,   369,   263,\r\n",
      "         1274,  7367,   279, 29889,    13,    13, 30180,  3727,   264,  3025,\r\n",
      "         1324, 29991,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:593] 2024-01-19 02:04:57,333 >> Using auto half precision backend\n",
      "[INFO|trainer.py:738] 2024-01-19 02:04:57,494 >> The following columns in the training set don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: messages, id, dataset. If messages, id, dataset are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1723] 2024-01-19 02:04:57,514 >> ***** Running training *****\n",
      "[INFO|trainer.py:1724] 2024-01-19 02:04:57,514 >>   Num examples = 10,000\n",
      "[INFO|trainer.py:1725] 2024-01-19 02:04:57,514 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1726] 2024-01-19 02:04:57,514 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1729] 2024-01-19 02:04:57,514 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1730] 2024-01-19 02:04:57,514 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1731] 2024-01-19 02:04:57,514 >>   Total optimization steps = 78\n",
      "[INFO|trainer.py:1732] 2024-01-19 02:04:57,515 >>   Number of trainable parameters = 6,738,423,808\n",
      "[INFO|integration_utils.py:718] 2024-01-19 02:04:57,519 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
      "  0%|                                                    | 0/78 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-01-19 02:05:01,563 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,569 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,570 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,572 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,576 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:314] 2024-01-19 02:05:01,577 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.6425, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7168, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.03}        \n",
      "  3%|█▏                                          | 2/78 [00:42<26:54, 21.24s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/spawnbase.py:372\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash gen_cmds_sft.sh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/utils/_process_posix.py:177\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Ensure the subprocess really is terminated\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m         \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# add isalive check, to ensure exitstatus is set:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m child\u001b[38;5;241m.\u001b[39misalive()\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pexpect/pty_spawn.py:646\u001b[0m, in \u001b[0;36mspawn.terminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill(signal\u001b[38;5;241m.\u001b[39mSIGCONT)\n\u001b[0;32m--> 646\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayafterterminate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misalive():\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_sft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "499d6f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mtbench_ann=gpt:4:1106:preview_chatfmt', 'results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('mtbench_ann=gpt:4:1106:preview_chatfmt', 'results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "#cmds:  2 \n",
      "\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mtbench_ann=gpt:4:1106:preview_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mtbench_ann=gpt:4:1106:preview_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gen_cmds_utils import remove_all_symlinks, create_unique_symlinks, get_chat_formatting_function, get_resource_for_task, OPENAI_MODEL_LIST\n",
    "\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "use_slow_tokenizer = True\n",
    "definitely_run_mtbench_on_non_alt7b_queue = False\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    'mmlu_s=5', \n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "#     'alpacafarm_ann=gpt35:turbo:1106',\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "task_names_mtbench = ['mtbench_ann=gpt:4:1106:preview_chatfmt'] # ann=gpt:4, ann=gpt:3.5:turbo:1106, gpt:4:1106:preview (gpt4-turbo)\n",
    "# task_names_mtbench = ['mtbench_ann=gpt:3.5:turbo:1106_chatfmt'] # for debug sake, prefer gpt4\n",
    "# task_names_mtbench = ['mtbench_ann=gpt:4_chatfmt']\n",
    "task_names_alpacafarm = ['alpacafarm_ann=chatgpt_chatfmt']\n",
    "task_names_chateval = task_names_mtbench + task_names_alpacafarm\n",
    "\n",
    "\n",
    "# # ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "# #     'huggyllama/llama-7b', \n",
    "# #     'mistralai/Mistral-7B-v0.1',\n",
    "# #     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "# #     'NousResearch/Llama-2-7b-hf',\n",
    "# #     'NousResearch/Llama-2-7b-chat-hf',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "# #     'HuggingFaceH4/zephyr-7b-alpha',\n",
    "#     'HuggingFaceH4/zephyr-7b-beta',\n",
    "# #     'codellama/CodeLlama-7b-hf',\n",
    "# #     'codellama/CodeLlama-7b-Python-hf',\n",
    "# #     'codellama/CodeLlama-7b-Instruct-hf',\n",
    "# ]]\n",
    "# # task_names = task_names + task_names_chatfmt\n",
    "# task_names = task_names_mtbench; definitely_run_mtbench_on_non_alt7b_queue = True\n",
    "\n",
    "# # oi5\n",
    "# exp_dir = 'results/oi2'\n",
    "# exp_dir = 'results/oi5_flan_v2:llama-7b'\n",
    "exp_dir = 'results/oi5_dolly:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b_debug'\n",
    "# exp_dir = 'results/oi5_stanford_alpaca:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlmv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegptv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat200kv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_tulu_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_open_orca_slim:llama-7b'\n",
    "# exp_dir = 'results/dpo1'\n",
    "# exp_dir = 'results/dpo2_ultrafeedback:llama-7b+sharegptv2ep2'\n",
    "subdir_filter_fn = lambda x: 'size=10000:ep=10' in x\n",
    "# task_names = task_names + task_names_chatfmt\n",
    "# task_names = task_names_alpacafarm; definitely_run_mtbench_on_non_alt7b_queue = False\n",
    "task_names = task_names_mtbench; definitely_run_mtbench_on_non_alt7b_queue = False\n",
    "# task_names = task_names\n",
    "# task_names = ['alpacafarm_ann=gpt35:turbo:1106_chatfmt']\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "num_gpus = 1\n",
    "if arch == 'x86_64': # ccc\n",
    "\n",
    "    if definitely_run_mtbench_on_non_alt7b_queue:\n",
    "        gpu_type = 'v100'; num_cpus = int(32/8*num_gpus); cpu_mem = int(240/8*num_gpus)\n",
    "    else:\n",
    "        gpu_type = 'a100_80gb'; num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus)\n",
    "    use_vllm = True; torch_dtype = 'bfloat16'\n",
    "else:\n",
    "    gpu_type = 'v100'\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    use_vllm = False; torch_dtype = 'float16'\n",
    "    \n",
    "    \n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    queue = None if getpass.getuser() in ('PTFMqngp', 'wpq') else 'alt_7d'\n",
    "\n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    chat_formatting_function = get_chat_formatting_function(model_name_or_path)\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        if 'model_args' in ft_args:\n",
    "            ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "        else:\n",
    "            ft_args_model_name_or_path = ft_args['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 512 \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 3 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length 512 \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_context' if no_context else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314', 'gpt35_turbo_1106']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --max_new_tokens 2048 \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('mtbench'):\n",
    "        assert('chatfmt' in task_name)\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        judge_model = match.group(1).replace(':', '-')\n",
    "        if not judge_model in OPENAI_MODEL_LIST:\n",
    "            raise ValueError('fastchat does not support the judge model.')\n",
    "        os.makedirs(save_dir, exist_ok=True) # since not using python file, make the directory now.\n",
    "        fastchat_mtbench_data_dir = os.path.normpath(os.path.join(open_instruct_dir, '../FastChat/fastchat/llm_judge/data'))\n",
    "        question_file = os.path.join(fastchat_mtbench_data_dir, 'mt_bench/question.jsonl')\n",
    "        rating_file = os.path.join(save_dir, f'{judge_model}_single.jsonl')\n",
    "        question_begin, question_end = (0, 1) if False else (None, None)\n",
    "        model_id = os.path.basename(model_name_or_path) if 'results/baselines' in save_dir else 'tulu'\n",
    "        queue = None if definitely_run_mtbench_on_non_alt7b_queue else queue\n",
    "        cmd = \"\"\n",
    "        cmd += f\"\"\"\n",
    "            python -m fastchat.llm_judge.gen_model_answer \\\n",
    "                --model-path {model_name_or_path} \\\n",
    "                --model-id {model_id} \\\n",
    "                --bench-name mt_bench \\\n",
    "                --question-file {question_file} \\\n",
    "                {'--question-begin '+str(question_begin) if question_begin else ''} \\\n",
    "                {'--question-end '+str(question_end) if question_end else ''} \\\n",
    "                --max-new-token 2048 \\\n",
    "                --answer-file {os.path.join(save_dir, 'model_answer.jsonl')} \\\n",
    "                --dtype {torch_dtype} \\\n",
    "            && \\\n",
    "        \"\"\"\n",
    "        cmd += f\"\"\"\n",
    "            python -m fastchat.llm_judge.gen_judgment \\\n",
    "                --bench-name mt_bench \\\n",
    "                --judge-file {os.path.join(fastchat_mtbench_data_dir, 'judge_prompts.jsonl')} \\\n",
    "                --judge-model {judge_model} \\\n",
    "                --mode single \\\n",
    "                --question-file {question_file} \\\n",
    "                --answer-dir {save_dir} \\\n",
    "                --ref-answer-dir {os.path.join(fastchat_mtbench_data_dir, 'mt_bench/reference_answer')} \\\n",
    "                --output-file {rating_file} \\\n",
    "            && \\\n",
    "            python -m fastchat.llm_judge.show_result \\\n",
    "                --bench-name mt_bench \\\n",
    "                --input-file {rating_file} \\\n",
    "                --mode single \\\n",
    "                --save-to-json\n",
    "        \"\"\"\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "    if test_run:\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd.strip())]))\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    # print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7ac978",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_eval.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3939309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python -m fastchat.llm_judge.gen_model_answer --model-path results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10 --model-id tulu --bench-name mt_bench --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --max-new-token 2048 --answer-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/model_answer.jsonl --dtype bfloat16\n",
      "[2024-01-20 13:17:44,894] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Output to results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/model_answer.jsonl\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 11.67it/s]\n",
      "  0%|                                                    | 0/80 [00:00<?, ?it/s]/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      " 96%|█████████████████████████████████████████▍ | 77/80 [04:22<00:14,  4.96s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████| 80/80 [05:14<00:00,  3.93s/it]\n",
      "+ python -m fastchat.llm_judge.gen_judgment --bench-name mt_bench --judge-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl --judge-model gpt-4-1106-preview --mode single --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --answer-dir results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt --ref-answer-dir /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer --output-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\n",
      "[2024-01-20 13:23:26,885] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Stats:\n",
      "{\n",
      "    \"bench_name\": \"mt_bench\",\n",
      "    \"mode\": \"single\",\n",
      "    \"judge\": \"gpt-4-1106-preview\",\n",
      "    \"baseline\": null,\n",
      "    \"model_list\": [\n",
      "        \"model_answer\"\n",
      "    ],\n",
      "    \"total_num_questions\": 80,\n",
      "    \"total_num_matches\": 160,\n",
      "    \"output_path\": \"results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\"\n",
      "}\n",
      "  0%|                                                   | 0/160 [00:00<?, ?it/s]\n",
      "Num API calls: 1 Prompt Tokens: 371, Completion Tokens: 251\n",
      "Cost [gpt-4-turbo]: 0.011 (per-example: 0.0112)\n",
      "Cost [gpt-4]: 0.026 (per-example: 0.0262)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.001 (per-example: 0.0009)\n",
      "question: 81, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  1%|▎                                          | 1/160 [00:15<39:54, 15.06s/it]\n",
      "Num API calls: 2 Prompt Tokens: 736, Completion Tokens: 431\n",
      "Cost [gpt-4-turbo]: 0.020 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 0.048 (per-example: 0.0240)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.002 (per-example: 0.0008)\n",
      "question: 82, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  1%|▌                                          | 2/160 [00:25<32:10, 12.22s/it]\n",
      "Num API calls: 3 Prompt Tokens: 989, Completion Tokens: 622\n",
      "Cost [gpt-4-turbo]: 0.029 (per-example: 0.0095)\n",
      "Cost [gpt-4]: 0.067 (per-example: 0.0223)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.002 (per-example: 0.0007)\n",
      "question: 83, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  2%|▊                                          | 3/160 [00:35<29:28, 11.27s/it]\n",
      "Num API calls: 4 Prompt Tokens: 1362, Completion Tokens: 839\n",
      "Cost [gpt-4-turbo]: 0.039 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.091 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.003 (per-example: 0.0008)\n",
      "question: 84, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  2%|█                                          | 4/160 [00:50<33:23, 12.85s/it]\n",
      "Num API calls: 5 Prompt Tokens: 1720, Completion Tokens: 947\n",
      "Cost [gpt-4-turbo]: 0.046 (per-example: 0.0091)\n",
      "Cost [gpt-4]: 0.108 (per-example: 0.0217)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.004 (per-example: 0.0007)\n",
      "question: 85, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  3%|█▎                                         | 5/160 [00:54<24:25,  9.45s/it]\n",
      "Num API calls: 6 Prompt Tokens: 1994, Completion Tokens: 1134\n",
      "Cost [gpt-4-turbo]: 0.054 (per-example: 0.0090)\n",
      "Cost [gpt-4]: 0.128 (per-example: 0.0213)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.004 (per-example: 0.0007)\n",
      "question: 86, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  4%|█▌                                         | 6/160 [01:04<25:09,  9.80s/it]\n",
      "Num API calls: 7 Prompt Tokens: 2509, Completion Tokens: 1429\n",
      "Cost [gpt-4-turbo]: 0.068 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.161 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.005 (per-example: 0.0008)\n",
      "question: 87, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  4%|█▉                                         | 7/160 [01:21<30:46, 12.07s/it]\n",
      "Num API calls: 8 Prompt Tokens: 2786, Completion Tokens: 1694\n",
      "Cost [gpt-4-turbo]: 0.079 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.185 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.006 (per-example: 0.0008)\n",
      "question: 88, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  5%|██▏                                        | 8/160 [01:39<35:52, 14.16s/it]\n",
      "Num API calls: 9 Prompt Tokens: 3077, Completion Tokens: 1912\n",
      "Cost [gpt-4-turbo]: 0.088 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.207 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.007 (per-example: 0.0008)\n",
      "question: 89, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  6%|██▍                                        | 9/160 [01:52<34:03, 13.53s/it]\n",
      "Num API calls: 10 Prompt Tokens: 3431, Completion Tokens: 2175\n",
      "Cost [gpt-4-turbo]: 0.100 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.233 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.008 (per-example: 0.0008)\n",
      "question: 90, turn: 1, model: model_answer, score: 8, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  6%|██▋                                       | 10/160 [02:08<36:04, 14.43s/it]\n",
      "Num API calls: 11 Prompt Tokens: 3676, Completion Tokens: 2416\n",
      "Cost [gpt-4-turbo]: 0.109 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.255 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.009 (per-example: 0.0008)\n",
      "question: 91, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  7%|██▉                                       | 11/160 [02:22<35:09, 14.16s/it]\n",
      "Num API calls: 12 Prompt Tokens: 3995, Completion Tokens: 2699\n",
      "Cost [gpt-4-turbo]: 0.121 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 0.282 (per-example: 0.0235)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.009 (per-example: 0.0008)\n",
      "question: 92, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  8%|███▏                                      | 12/160 [02:32<31:50, 12.91s/it]\n",
      "Num API calls: 13 Prompt Tokens: 4313, Completion Tokens: 2978\n",
      "Cost [gpt-4-turbo]: 0.132 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 0.308 (per-example: 0.0237)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.010 (per-example: 0.0008)\n",
      "question: 93, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                      | 13/160 [02:48<33:49, 13.81s/it]\n",
      "Num API calls: 14 Prompt Tokens: 4633, Completion Tokens: 3118\n",
      "Cost [gpt-4-turbo]: 0.140 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.326 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.011 (per-example: 0.0008)\n",
      "question: 94, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  9%|███▋                                      | 14/160 [02:52<26:56, 11.08s/it]\n",
      "Num API calls: 15 Prompt Tokens: 4956, Completion Tokens: 3294\n",
      "Cost [gpt-4-turbo]: 0.148 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.346 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.012 (per-example: 0.0008)\n",
      "question: 95, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      "  9%|███▉                                      | 15/160 [03:05<27:49, 11.51s/it]\n",
      "Num API calls: 16 Prompt Tokens: 5258, Completion Tokens: 3564\n",
      "Cost [gpt-4-turbo]: 0.160 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.372 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.012 (per-example: 0.0008)\n",
      "question: 96, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 10%|████▏                                     | 16/160 [03:22<31:36, 13.17s/it]\n",
      "Num API calls: 17 Prompt Tokens: 5553, Completion Tokens: 3807\n",
      "Cost [gpt-4-turbo]: 0.170 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 0.395 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.013 (per-example: 0.0008)\n",
      "question: 97, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 11%|████▍                                     | 17/160 [03:29<26:44, 11.22s/it]\n",
      "Num API calls: 18 Prompt Tokens: 5809, Completion Tokens: 3970\n",
      "Cost [gpt-4-turbo]: 0.177 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.412 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.014 (per-example: 0.0008)\n",
      "question: 98, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 11%|████▋                                     | 18/160 [03:34<22:46,  9.62s/it]\n",
      "Num API calls: 19 Prompt Tokens: 6074, Completion Tokens: 4172\n",
      "Cost [gpt-4-turbo]: 0.186 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.433 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.014 (per-example: 0.0008)\n",
      "question: 99, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 12%|████▉                                     | 19/160 [03:49<25:50, 11.00s/it]\n",
      "Num API calls: 20 Prompt Tokens: 6291, Completion Tokens: 4389\n",
      "Cost [gpt-4-turbo]: 0.195 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.452 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.015 (per-example: 0.0008)\n",
      "question: 100, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 12%|█████▎                                    | 20/160 [04:01<26:17, 11.27s/it]\n",
      "Num API calls: 21 Prompt Tokens: 6610, Completion Tokens: 4583\n",
      "Cost [gpt-4-turbo]: 0.204 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.473 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.016 (per-example: 0.0008)\n",
      "question: 131, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 13%|█████▌                                    | 21/160 [04:12<26:33, 11.47s/it]\n",
      "Num API calls: 22 Prompt Tokens: 7105, Completion Tokens: 4723\n",
      "Cost [gpt-4-turbo]: 0.213 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.497 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.017 (per-example: 0.0008)\n",
      "question: 132, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 14%|█████▊                                    | 22/160 [04:17<21:30,  9.35s/it]\n",
      "Num API calls: 23 Prompt Tokens: 7621, Completion Tokens: 4925\n",
      "Cost [gpt-4-turbo]: 0.224 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.524 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.017 (per-example: 0.0008)\n",
      "question: 133, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 14%|██████                                    | 23/160 [04:23<19:07,  8.37s/it]\n",
      "Num API calls: 24 Prompt Tokens: 7986, Completion Tokens: 5112\n",
      "Cost [gpt-4-turbo]: 0.233 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.546 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.018 (per-example: 0.0008)\n",
      "question: 134, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 15%|██████▎                                   | 24/160 [04:34<20:58,  9.25s/it]\n",
      "Num API calls: 25 Prompt Tokens: 8306, Completion Tokens: 5361\n",
      "Cost [gpt-4-turbo]: 0.244 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.571 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.019 (per-example: 0.0008)\n",
      "question: 135, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 16%|██████▌                                   | 25/160 [04:49<24:21, 10.83s/it]\n",
      "Num API calls: 26 Prompt Tokens: 8728, Completion Tokens: 5491\n",
      "Cost [gpt-4-turbo]: 0.252 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.591 (per-example: 0.0227)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.020 (per-example: 0.0008)\n",
      "question: 136, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 16%|██████▊                                   | 26/160 [04:53<20:01,  8.97s/it]\n",
      "Num API calls: 27 Prompt Tokens: 9108, Completion Tokens: 5700\n",
      "Cost [gpt-4-turbo]: 0.262 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.615 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.021 (per-example: 0.0008)\n",
      "question: 137, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 17%|███████                                   | 27/160 [05:07<23:15, 10.49s/it]\n",
      "Num API calls: 28 Prompt Tokens: 9608, Completion Tokens: 5935\n",
      "Cost [gpt-4-turbo]: 0.274 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.644 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.021 (per-example: 0.0008)\n",
      "question: 138, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 18%|███████▎                                  | 28/160 [05:16<21:47,  9.91s/it]\n",
      "Num API calls: 29 Prompt Tokens: 10008, Completion Tokens: 6012\n",
      "Cost [gpt-4-turbo]: 0.280 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 0.661 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.022 (per-example: 0.0008)\n",
      "question: 139, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 18%|███████▌                                  | 29/160 [05:21<18:16,  8.37s/it]\n",
      "Num API calls: 30 Prompt Tokens: 10414, Completion Tokens: 6291\n",
      "Cost [gpt-4-turbo]: 0.293 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.690 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.023 (per-example: 0.0008)\n",
      "question: 140, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 19%|███████▉                                  | 30/160 [05:53<33:37, 15.52s/it]\n",
      "Num API calls: 31 Prompt Tokens: 10664, Completion Tokens: 6578\n",
      "Cost [gpt-4-turbo]: 0.304 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.715 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.024 (per-example: 0.0008)\n",
      "question: 141, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 19%|████████▏                                 | 31/160 [06:00<28:03, 13.05s/it]\n",
      "Num API calls: 32 Prompt Tokens: 10904, Completion Tokens: 6822\n",
      "Cost [gpt-4-turbo]: 0.314 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.736 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.025 (per-example: 0.0008)\n",
      "question: 142, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 20%|████████▍                                 | 32/160 [06:08<24:14, 11.36s/it]\n",
      "Num API calls: 33 Prompt Tokens: 11191, Completion Tokens: 7081\n",
      "Cost [gpt-4-turbo]: 0.324 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.761 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.025 (per-example: 0.0008)\n",
      "question: 143, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 21%|████████▋                                 | 33/160 [06:23<26:46, 12.65s/it]\n",
      "Num API calls: 34 Prompt Tokens: 11413, Completion Tokens: 7303\n",
      "Cost [gpt-4-turbo]: 0.333 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.781 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.026 (per-example: 0.0008)\n",
      "question: 144, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 21%|████████▉                                 | 34/160 [06:30<22:40, 10.80s/it]\n",
      "Num API calls: 35 Prompt Tokens: 11690, Completion Tokens: 7512\n",
      "Cost [gpt-4-turbo]: 0.342 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.801 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.027 (per-example: 0.0008)\n",
      "question: 145, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 22%|█████████▏                                | 35/160 [06:44<24:36, 11.81s/it]\n",
      "Num API calls: 36 Prompt Tokens: 12007, Completion Tokens: 7889\n",
      "Cost [gpt-4-turbo]: 0.357 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.834 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.028 (per-example: 0.0008)\n",
      "question: 146, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 22%|█████████▍                                | 36/160 [07:06<30:55, 14.96s/it]\n",
      "Num API calls: 37 Prompt Tokens: 12290, Completion Tokens: 8129\n",
      "Cost [gpt-4-turbo]: 0.367 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.856 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.029 (per-example: 0.0008)\n",
      "question: 147, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 23%|█████████▋                                | 37/160 [07:21<30:38, 14.94s/it]\n",
      "Num API calls: 38 Prompt Tokens: 12531, Completion Tokens: 8390\n",
      "Cost [gpt-4-turbo]: 0.377 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.879 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.029 (per-example: 0.0008)\n",
      "question: 148, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 24%|█████████▉                                | 38/160 [07:29<26:02, 12.81s/it]\n",
      "Num API calls: 39 Prompt Tokens: 12787, Completion Tokens: 8575\n",
      "Cost [gpt-4-turbo]: 0.385 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.898 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.030 (per-example: 0.0008)\n",
      "question: 149, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 24%|██████████▏                               | 39/160 [07:38<23:44, 11.77s/it]\n",
      "Num API calls: 40 Prompt Tokens: 13040, Completion Tokens: 8746\n",
      "Cost [gpt-4-turbo]: 0.393 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 0.916 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.031 (per-example: 0.0008)\n",
      "question: 150, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 25%|██████████▌                               | 40/160 [07:45<20:11, 10.10s/it]\n",
      "Num API calls: 41 Prompt Tokens: 13382, Completion Tokens: 9097\n",
      "Cost [gpt-4-turbo]: 0.407 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.947 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.032 (per-example: 0.0008)\n",
      "question: 151, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 26%|██████████▊                               | 41/160 [08:04<25:39, 12.94s/it]\n",
      "Num API calls: 42 Prompt Tokens: 13610, Completion Tokens: 9316\n",
      "Cost [gpt-4-turbo]: 0.416 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.967 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.032 (per-example: 0.0008)\n",
      "question: 152, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 26%|███████████                               | 42/160 [08:16<24:36, 12.51s/it]\n",
      "Num API calls: 43 Prompt Tokens: 14046, Completion Tokens: 9558\n",
      "Cost [gpt-4-turbo]: 0.427 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 0.995 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.033 (per-example: 0.0008)\n",
      "question: 153, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 27%|███████████▎                              | 43/160 [08:23<21:23, 10.97s/it]\n",
      "Num API calls: 44 Prompt Tokens: 14453, Completion Tokens: 9808\n",
      "Cost [gpt-4-turbo]: 0.439 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.022 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.034 (per-example: 0.0008)\n",
      "question: 154, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 28%|███████████▌                              | 44/160 [08:36<22:31, 11.65s/it]\n",
      "Num API calls: 45 Prompt Tokens: 14901, Completion Tokens: 10094\n",
      "Cost [gpt-4-turbo]: 0.452 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.053 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.035 (per-example: 0.0008)\n",
      "question: 155, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 28%|███████████▊                              | 45/160 [08:53<24:59, 13.04s/it]\n",
      "Num API calls: 46 Prompt Tokens: 15235, Completion Tokens: 10285\n",
      "Cost [gpt-4-turbo]: 0.461 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.074 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.036 (per-example: 0.0008)\n",
      "question: 156, turn: 1, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 29%|████████████                              | 46/160 [09:06<25:11, 13.26s/it]\n",
      "Num API calls: 47 Prompt Tokens: 15439, Completion Tokens: 10627\n",
      "Cost [gpt-4-turbo]: 0.473 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.101 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.037 (per-example: 0.0008)\n",
      "question: 157, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 29%|████████████▎                             | 47/160 [09:42<37:35, 19.96s/it]\n",
      "Num API calls: 48 Prompt Tokens: 15703, Completion Tokens: 10895\n",
      "Cost [gpt-4-turbo]: 0.484 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.125 (per-example: 0.0234)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.037 (per-example: 0.0008)\n",
      "question: 158, turn: 1, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 30%|████████████▌                             | 48/160 [10:00<36:01, 19.30s/it]\n",
      "Num API calls: 49 Prompt Tokens: 16060, Completion Tokens: 11164\n",
      "Cost [gpt-4-turbo]: 0.496 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.152 (per-example: 0.0235)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.038 (per-example: 0.0008)\n",
      "question: 159, turn: 1, model: model_answer, score: 7, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 31%|████████████▊                             | 49/160 [10:16<34:19, 18.55s/it]\n",
      "Num API calls: 50 Prompt Tokens: 16494, Completion Tokens: 11498\n",
      "Cost [gpt-4-turbo]: 0.510 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.185 (per-example: 0.0237)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.039 (per-example: 0.0008)\n",
      "question: 160, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1')\n",
      " 31%|█████████████▏                            | 50/160 [10:27<29:46, 16.24s/it]\n",
      "Num API calls: 51 Prompt Tokens: 16745, Completion Tokens: 11601\n",
      "Cost [gpt-4-turbo]: 0.515 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.198 (per-example: 0.0235)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.040 (per-example: 0.0008)\n",
      "question: 101, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 32%|█████████████▍                            | 51/160 [10:35<24:42, 13.60s/it]\n",
      "Num API calls: 52 Prompt Tokens: 17011, Completion Tokens: 11667\n",
      "Cost [gpt-4-turbo]: 0.520 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.210 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.040 (per-example: 0.0008)\n",
      "question: 102, turn: 1, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 32%|█████████████▋                            | 52/160 [10:39<19:21, 10.76s/it]\n",
      "Num API calls: 53 Prompt Tokens: 17466, Completion Tokens: 11814\n",
      "Cost [gpt-4-turbo]: 0.529 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.233 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.041 (per-example: 0.0008)\n",
      "question: 103, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 33%|█████████████▉                            | 53/160 [10:43<15:42,  8.80s/it]\n",
      "Num API calls: 54 Prompt Tokens: 17662, Completion Tokens: 11921\n",
      "Cost [gpt-4-turbo]: 0.534 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.245 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.042 (per-example: 0.0008)\n",
      "question: 104, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 34%|██████████████▏                           | 54/160 [10:52<15:29,  8.77s/it]\n",
      "Num API calls: 55 Prompt Tokens: 18237, Completion Tokens: 12069\n",
      "Cost [gpt-4-turbo]: 0.544 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.271 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.042 (per-example: 0.0008)\n",
      "question: 105, turn: 1, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-math-v1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34%|██████████████▍                           | 55/160 [10:58<13:48,  7.89s/it]\n",
      "Num API calls: 56 Prompt Tokens: 18497, Completion Tokens: 12141\n",
      "Cost [gpt-4-turbo]: 0.549 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.283 (per-example: 0.0229)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.043 (per-example: 0.0008)\n",
      "question: 106, turn: 1, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 35%|██████████████▋                           | 56/160 [11:02<11:55,  6.88s/it]\n",
      "Num API calls: 57 Prompt Tokens: 18700, Completion Tokens: 12210\n",
      "Cost [gpt-4-turbo]: 0.553 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 1.294 (per-example: 0.0227)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.043 (per-example: 0.0008)\n",
      "question: 107, turn: 1, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 36%|██████████████▉                           | 57/160 [11:06<10:17,  6.00s/it]\n",
      "Num API calls: 58 Prompt Tokens: 18924, Completion Tokens: 12329\n",
      "Cost [gpt-4-turbo]: 0.559 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.307 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.044 (per-example: 0.0008)\n",
      "question: 108, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 36%|███████████████▏                          | 58/160 [11:10<09:04,  5.33s/it]\n",
      "Num API calls: 59 Prompt Tokens: 19305, Completion Tokens: 12492\n",
      "Cost [gpt-4-turbo]: 0.568 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.329 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.044 (per-example: 0.0008)\n",
      "question: 109, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 37%|███████████████▍                          | 59/160 [11:15<08:57,  5.32s/it]\n",
      "Num API calls: 60 Prompt Tokens: 19657, Completion Tokens: 12711\n",
      "Cost [gpt-4-turbo]: 0.578 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.352 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.045 (per-example: 0.0008)\n",
      "question: 110, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 38%|███████████████▊                          | 60/160 [11:30<13:41,  8.21s/it]\n",
      "Num API calls: 61 Prompt Tokens: 20128, Completion Tokens: 12876\n",
      "Cost [gpt-4-turbo]: 0.588 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.376 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.046 (per-example: 0.0008)\n",
      "question: 111, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 38%|████████████████                          | 61/160 [11:42<15:24,  9.34s/it]\n",
      "Num API calls: 62 Prompt Tokens: 20407, Completion Tokens: 13032\n",
      "Cost [gpt-4-turbo]: 0.595 (per-example: 0.0096)\n",
      "Cost [gpt-4]: 1.394 (per-example: 0.0225)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.046 (per-example: 0.0007)\n",
      "question: 112, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 39%|████████████████▎                         | 62/160 [11:48<13:19,  8.15s/it]\n",
      "Num API calls: 63 Prompt Tokens: 20873, Completion Tokens: 13319\n",
      "Cost [gpt-4-turbo]: 0.608 (per-example: 0.0097)\n",
      "Cost [gpt-4]: 1.425 (per-example: 0.0226)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.048 (per-example: 0.0008)\n",
      "question: 113, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 39%|████████████████▌                         | 63/160 [12:03<16:47, 10.38s/it]\n",
      "Num API calls: 64 Prompt Tokens: 21360, Completion Tokens: 13686\n",
      "Cost [gpt-4-turbo]: 0.624 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.462 (per-example: 0.0228)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.049 (per-example: 0.0008)\n",
      "question: 114, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 40%|████████████████▊                         | 64/160 [12:16<17:41, 11.05s/it]\n",
      "Num API calls: 65 Prompt Tokens: 21849, Completion Tokens: 14044\n",
      "Cost [gpt-4-turbo]: 0.640 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.498 (per-example: 0.0230)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.050 (per-example: 0.0008)\n",
      "question: 115, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 41%|█████████████████                         | 65/160 [12:39<23:11, 14.65s/it]\n",
      "Num API calls: 66 Prompt Tokens: 22315, Completion Tokens: 14201\n",
      "Cost [gpt-4-turbo]: 0.649 (per-example: 0.0098)\n",
      "Cost [gpt-4]: 1.522 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.051 (per-example: 0.0008)\n",
      "question: 116, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 41%|█████████████████▎                        | 66/160 [12:48<20:24, 13.02s/it]\n",
      "Num API calls: 67 Prompt Tokens: 22771, Completion Tokens: 14488\n",
      "Cost [gpt-4-turbo]: 0.662 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.552 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.052 (per-example: 0.0008)\n",
      "question: 117, turn: 1, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 42%|█████████████████▌                        | 67/160 [13:04<21:27, 13.84s/it]\n",
      "Num API calls: 68 Prompt Tokens: 23151, Completion Tokens: 14701\n",
      "Cost [gpt-4-turbo]: 0.673 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.577 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.053 (per-example: 0.0008)\n",
      "question: 118, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 42%|█████████████████▊                        | 68/160 [13:17<20:45, 13.54s/it]\n",
      "Num API calls: 69 Prompt Tokens: 23507, Completion Tokens: 14842\n",
      "Cost [gpt-4-turbo]: 0.680 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.596 (per-example: 0.0231)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.053 (per-example: 0.0008)\n",
      "question: 119, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 43%|██████████████████                        | 69/160 [13:25<18:08, 11.96s/it]\n",
      "Num API calls: 70 Prompt Tokens: 23923, Completion Tokens: 15146\n",
      "Cost [gpt-4-turbo]: 0.694 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.626 (per-example: 0.0232)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.054 (per-example: 0.0008)\n",
      "question: 120, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 44%|██████████████████▍                       | 70/160 [13:36<17:43, 11.82s/it]\n",
      "Num API calls: 71 Prompt Tokens: 24420, Completion Tokens: 15330\n",
      "Cost [gpt-4-turbo]: 0.704 (per-example: 0.0099)\n",
      "Cost [gpt-4]: 1.652 (per-example: 0.0233)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.055 (per-example: 0.0008)\n",
      "question: 121, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 44%|██████████████████▋                       | 71/160 [13:43<15:23, 10.37s/it]\n",
      "Num API calls: 72 Prompt Tokens: 25772, Completion Tokens: 15520\n",
      "Cost [gpt-4-turbo]: 0.723 (per-example: 0.0100)\n",
      "Cost [gpt-4]: 1.704 (per-example: 0.0237)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.057 (per-example: 0.0008)\n",
      "question: 122, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 45%|██████████████████▉                       | 72/160 [13:53<15:04, 10.28s/it]\n",
      "Num API calls: 73 Prompt Tokens: 26340, Completion Tokens: 15794\n",
      "Cost [gpt-4-turbo]: 0.737 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.738 (per-example: 0.0238)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.058 (per-example: 0.0008)\n",
      "question: 123, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 46%|███████████████████▏                      | 73/160 [14:13<19:00, 13.11s/it]\n",
      "Num API calls: 74 Prompt Tokens: 26881, Completion Tokens: 15987\n",
      "Cost [gpt-4-turbo]: 0.748 (per-example: 0.0101)\n",
      "Cost [gpt-4]: 1.766 (per-example: 0.0239)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.059 (per-example: 0.0008)\n",
      "question: 124, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 46%|███████████████████▍                      | 74/160 [14:36<22:51, 15.95s/it]\n",
      "Num API calls: 75 Prompt Tokens: 27565, Completion Tokens: 16232\n",
      "Cost [gpt-4-turbo]: 0.763 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.801 (per-example: 0.0240)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.060 (per-example: 0.0008)\n",
      "question: 125, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 47%|███████████████████▋                      | 75/160 [14:43<18:47, 13.27s/it]\n",
      "Num API calls: 76 Prompt Tokens: 28228, Completion Tokens: 16443\n",
      "Cost [gpt-4-turbo]: 0.776 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.833 (per-example: 0.0241)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.061 (per-example: 0.0008)\n",
      "question: 126, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████▉                      | 76/160 [14:54<17:32, 12.53s/it]\n",
      "Num API calls: 77 Prompt Tokens: 28764, Completion Tokens: 16607\n",
      "Cost [gpt-4-turbo]: 0.786 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.859 (per-example: 0.0241)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.062 (per-example: 0.0008)\n",
      "question: 127, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 48%|████████████████████▏                     | 77/160 [15:04<16:33, 11.97s/it]\n",
      "Num API calls: 78 Prompt Tokens: 29316, Completion Tokens: 16804\n",
      "Cost [gpt-4-turbo]: 0.797 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 1.888 (per-example: 0.0242)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.063 (per-example: 0.0008)\n",
      "question: 128, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 49%|████████████████████▍                     | 78/160 [15:17<16:32, 12.10s/it]\n",
      "Num API calls: 79 Prompt Tokens: 29951, Completion Tokens: 17028\n",
      "Cost [gpt-4-turbo]: 0.810 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 1.920 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.064 (per-example: 0.0008)\n",
      "question: 129, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 49%|████████████████████▋                     | 79/160 [15:38<20:07, 14.91s/it]\n",
      "Num API calls: 80 Prompt Tokens: 30388, Completion Tokens: 17372\n",
      "Cost [gpt-4-turbo]: 0.825 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 1.954 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.065 (per-example: 0.0008)\n",
      "question: 130, turn: 1, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1')\n",
      " 50%|█████████████████████                     | 80/160 [15:55<20:42, 15.53s/it]\n",
      "Num API calls: 81 Prompt Tokens: 31001, Completion Tokens: 17494\n",
      "Cost [gpt-4-turbo]: 0.835 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 1.980 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.066 (per-example: 0.0008)\n",
      "question: 81, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 51%|█████████████████████▎                    | 81/160 [16:00<16:07, 12.25s/it]\n",
      "Num API calls: 82 Prompt Tokens: 31436, Completion Tokens: 17675\n",
      "Cost [gpt-4-turbo]: 0.845 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.004 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.067 (per-example: 0.0008)\n",
      "question: 82, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 51%|█████████████████████▌                    | 82/160 [16:17<18:02, 13.88s/it]\n",
      "Num API calls: 83 Prompt Tokens: 31936, Completion Tokens: 17790\n",
      "Cost [gpt-4-turbo]: 0.853 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.025 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.068 (per-example: 0.0008)\n",
      "question: 83, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 52%|█████████████████████▊                    | 83/160 [16:23<14:50, 11.56s/it]\n",
      "Num API calls: 84 Prompt Tokens: 32525, Completion Tokens: 17991\n",
      "Cost [gpt-4-turbo]: 0.865 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.055 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.069 (per-example: 0.0008)\n",
      "question: 84, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 52%|██████████████████████                    | 84/160 [16:35<14:44, 11.64s/it]\n",
      "Num API calls: 85 Prompt Tokens: 33099, Completion Tokens: 18088\n",
      "Cost [gpt-4-turbo]: 0.874 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.078 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.069 (per-example: 0.0008)\n",
      "question: 85, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 53%|██████████████████████▎                   | 85/160 [16:42<12:43, 10.17s/it]\n",
      "Num API calls: 86 Prompt Tokens: 33520, Completion Tokens: 18224\n",
      "Cost [gpt-4-turbo]: 0.882 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.099 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.070 (per-example: 0.0008)\n",
      "question: 86, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 54%|██████████████████████▌                   | 86/160 [16:46<10:19,  8.38s/it]\n",
      "Num API calls: 87 Prompt Tokens: 34413, Completion Tokens: 18315\n",
      "Cost [gpt-4-turbo]: 0.894 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.131 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.071 (per-example: 0.0008)\n",
      "question: 87, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 54%|██████████████████████▊                   | 87/160 [16:53<09:31,  7.83s/it]\n",
      "Num API calls: 88 Prompt Tokens: 34796, Completion Tokens: 18418\n",
      "Cost [gpt-4-turbo]: 0.900 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.149 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.072 (per-example: 0.0008)\n",
      "question: 88, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 55%|███████████████████████                   | 88/160 [16:56<07:43,  6.44s/it]\n",
      "Num API calls: 89 Prompt Tokens: 35195, Completion Tokens: 18588\n",
      "Cost [gpt-4-turbo]: 0.910 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.171 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.072 (per-example: 0.0008)\n",
      "question: 89, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 56%|███████████████████████▎                  | 89/160 [17:06<08:49,  7.46s/it]\n",
      "Num API calls: 90 Prompt Tokens: 35702, Completion Tokens: 18746\n",
      "Cost [gpt-4-turbo]: 0.919 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.196 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.073 (per-example: 0.0008)\n",
      "question: 90, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 56%|███████████████████████▋                  | 90/160 [17:16<09:33,  8.19s/it]\n",
      "Num API calls: 91 Prompt Tokens: 36025, Completion Tokens: 18930\n",
      "Cost [gpt-4-turbo]: 0.928 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.217 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.074 (per-example: 0.0008)\n",
      "question: 91, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 57%|███████████████████████▉                  | 91/160 [17:21<08:31,  7.41s/it]\n",
      "Num API calls: 92 Prompt Tokens: 36398, Completion Tokens: 19155\n",
      "Cost [gpt-4-turbo]: 0.939 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.241 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.075 (per-example: 0.0008)\n",
      "question: 92, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 57%|████████████████████████▏                 | 92/160 [17:34<10:06,  8.93s/it]\n",
      "Num API calls: 93 Prompt Tokens: 36789, Completion Tokens: 19357\n",
      "Cost [gpt-4-turbo]: 0.949 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.265 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.076 (per-example: 0.0008)\n",
      "question: 93, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 58%|████████████████████████▍                 | 93/160 [17:46<11:03,  9.90s/it]\n",
      "Num API calls: 94 Prompt Tokens: 37244, Completion Tokens: 19523\n",
      "Cost [gpt-4-turbo]: 0.958 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.289 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.076 (per-example: 0.0008)\n",
      "question: 94, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 59%|████████████████████████▋                 | 94/160 [17:56<10:50,  9.86s/it]\n",
      "Num API calls: 95 Prompt Tokens: 37673, Completion Tokens: 19681\n",
      "Cost [gpt-4-turbo]: 0.967 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.311 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.077 (per-example: 0.0008)\n",
      "question: 95, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 59%|████████████████████████▉                 | 95/160 [18:03<09:46,  9.02s/it]\n",
      "Num API calls: 96 Prompt Tokens: 38077, Completion Tokens: 19900\n",
      "Cost [gpt-4-turbo]: 0.978 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.336 (per-example: 0.0243)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.078 (per-example: 0.0008)\n",
      "question: 96, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 60%|█████████████████████████▏                | 96/160 [18:11<09:26,  8.85s/it]\n",
      "Num API calls: 97 Prompt Tokens: 38564, Completion Tokens: 20228\n",
      "Cost [gpt-4-turbo]: 0.992 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.371 (per-example: 0.0244)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.079 (per-example: 0.0008)\n",
      "question: 97, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████▍                | 97/160 [18:23<10:16,  9.79s/it]\n",
      "Num API calls: 98 Prompt Tokens: 38937, Completion Tokens: 20480\n",
      "Cost [gpt-4-turbo]: 1.004 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.397 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.080 (per-example: 0.0008)\n",
      "question: 98, turn: 2, model: model_answer, score: 6, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 61%|█████████████████████████▋                | 98/160 [18:33<10:09,  9.83s/it]\n",
      "Num API calls: 99 Prompt Tokens: 39548, Completion Tokens: 20731\n",
      "Cost [gpt-4-turbo]: 1.017 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.430 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.081 (per-example: 0.0008)\n",
      "question: 99, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 62%|█████████████████████████▉                | 99/160 [18:41<09:31,  9.37s/it]\n",
      "Num API calls: 100 Prompt Tokens: 39834, Completion Tokens: 20886\n",
      "Cost [gpt-4-turbo]: 1.025 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.448 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.082 (per-example: 0.0008)\n",
      "question: 100, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 62%|█████████████████████████▋               | 100/160 [18:47<08:13,  8.22s/it]\n",
      "Num API calls: 101 Prompt Tokens: 40224, Completion Tokens: 21141\n",
      "Cost [gpt-4-turbo]: 1.036 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.475 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.083 (per-example: 0.0008)\n",
      "question: 131, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 63%|█████████████████████████▉               | 101/160 [19:01<09:43,  9.89s/it]\n",
      "Num API calls: 102 Prompt Tokens: 40789, Completion Tokens: 21294\n",
      "Cost [gpt-4-turbo]: 1.047 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.501 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.083 (per-example: 0.0008)\n",
      "question: 132, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 64%|██████████████████████████▏              | 102/160 [19:09<09:02,  9.36s/it]\n",
      "Num API calls: 103 Prompt Tokens: 41384, Completion Tokens: 21543\n",
      "Cost [gpt-4-turbo]: 1.060 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.534 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.084 (per-example: 0.0008)\n",
      "question: 133, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 64%|██████████████████████████▍              | 103/160 [19:26<11:13, 11.81s/it]\n",
      "Num API calls: 104 Prompt Tokens: 41796, Completion Tokens: 21774\n",
      "Cost [gpt-4-turbo]: 1.071 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.560 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.085 (per-example: 0.0008)\n",
      "question: 134, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 65%|██████████████████████████▋              | 104/160 [19:35<10:07, 10.84s/it]\n",
      "Num API calls: 105 Prompt Tokens: 42276, Completion Tokens: 21904\n",
      "Cost [gpt-4-turbo]: 1.080 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.583 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.086 (per-example: 0.0008)\n",
      "question: 135, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 66%|██████████████████████████▉              | 105/160 [19:44<09:22, 10.23s/it]\n",
      "Num API calls: 106 Prompt Tokens: 42759, Completion Tokens: 22006\n",
      "Cost [gpt-4-turbo]: 1.088 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.603 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.087 (per-example: 0.0008)\n",
      "question: 136, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 66%|███████████████████████████▏             | 106/160 [19:50<08:04,  8.96s/it]\n",
      "Num API calls: 107 Prompt Tokens: 43228, Completion Tokens: 22189\n",
      "Cost [gpt-4-turbo]: 1.098 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.628 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.088 (per-example: 0.0008)\n",
      "question: 137, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 67%|███████████████████████████▍             | 107/160 [20:00<08:19,  9.42s/it]\n",
      "Num API calls: 108 Prompt Tokens: 43794, Completion Tokens: 22335\n",
      "Cost [gpt-4-turbo]: 1.108 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.654 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.088 (per-example: 0.0008)\n",
      "question: 138, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 68%|███████████████████████████▋             | 108/160 [20:09<07:59,  9.22s/it]\n",
      "Num API calls: 109 Prompt Tokens: 44354, Completion Tokens: 22439\n",
      "Cost [gpt-4-turbo]: 1.117 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.677 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.089 (per-example: 0.0008)\n",
      "question: 139, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 68%|███████████████████████████▉             | 109/160 [20:15<07:05,  8.34s/it]\n",
      "Num API calls: 110 Prompt Tokens: 44853, Completion Tokens: 22645\n",
      "Cost [gpt-4-turbo]: 1.128 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.704 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.090 (per-example: 0.0008)\n",
      "question: 140, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 69%|████████████████████████████▏            | 110/160 [20:27<07:53,  9.47s/it]\n",
      "Num API calls: 111 Prompt Tokens: 45163, Completion Tokens: 22812\n",
      "Cost [gpt-4-turbo]: 1.136 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.724 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.091 (per-example: 0.0008)\n",
      "question: 141, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 69%|████████████████████████████▍            | 111/160 [20:40<08:36, 10.55s/it]\n",
      "Num API calls: 112 Prompt Tokens: 45510, Completion Tokens: 23024\n",
      "Cost [gpt-4-turbo]: 1.146 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.747 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.092 (per-example: 0.0008)\n",
      "question: 142, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 70%|████████████████████████████▋            | 112/160 [20:55<09:26, 11.80s/it]\n",
      "Num API calls: 113 Prompt Tokens: 45994, Completion Tokens: 23351\n",
      "Cost [gpt-4-turbo]: 1.160 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.781 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.093 (per-example: 0.0008)\n",
      "question: 143, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 71%|████████████████████████████▉            | 113/160 [21:05<08:44, 11.15s/it]\n",
      "Num API calls: 114 Prompt Tokens: 46312, Completion Tokens: 23501\n",
      "Cost [gpt-4-turbo]: 1.168 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.799 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.093 (per-example: 0.0008)\n",
      "question: 144, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 71%|█████████████████████████████▏           | 114/160 [21:15<08:25, 10.98s/it]\n",
      "Num API calls: 115 Prompt Tokens: 46687, Completion Tokens: 23738\n",
      "Cost [gpt-4-turbo]: 1.179 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.825 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.094 (per-example: 0.0008)\n",
      "question: 145, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 72%|█████████████████████████████▍           | 115/160 [21:29<08:48, 11.75s/it]\n",
      "Num API calls: 116 Prompt Tokens: 47090, Completion Tokens: 23949\n",
      "Cost [gpt-4-turbo]: 1.189 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 2.850 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.095 (per-example: 0.0008)\n",
      "question: 146, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 72%|█████████████████████████████▋           | 116/160 [21:41<08:44, 11.91s/it]\n",
      "Num API calls: 117 Prompt Tokens: 47502, Completion Tokens: 24108\n",
      "Cost [gpt-4-turbo]: 1.198 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.872 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.096 (per-example: 0.0008)\n",
      "question: 147, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 73%|█████████████████████████████▉           | 117/160 [21:55<08:59, 12.54s/it]\n",
      "Num API calls: 118 Prompt Tokens: 47802, Completion Tokens: 24338\n",
      "Cost [gpt-4-turbo]: 1.208 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.894 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.096 (per-example: 0.0008)\n",
      "question: 148, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████▏          | 118/160 [22:09<08:59, 12.84s/it]\n",
      "Num API calls: 119 Prompt Tokens: 48116, Completion Tokens: 24542\n",
      "Cost [gpt-4-turbo]: 1.217 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.916 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.097 (per-example: 0.0008)\n",
      "question: 149, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 74%|██████████████████████████████▍          | 119/160 [22:19<08:10, 11.96s/it]\n",
      "Num API calls: 120 Prompt Tokens: 48436, Completion Tokens: 24740\n",
      "Cost [gpt-4-turbo]: 1.227 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.937 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.098 (per-example: 0.0008)\n",
      "question: 150, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 75%|██████████████████████████████▊          | 120/160 [22:30<07:53, 11.84s/it]\n",
      "Num API calls: 121 Prompt Tokens: 48919, Completion Tokens: 24985\n",
      "Cost [gpt-4-turbo]: 1.239 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.967 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.099 (per-example: 0.0008)\n",
      "question: 151, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 76%|███████████████████████████████          | 121/160 [22:46<08:29, 13.06s/it]\n",
      "Num API calls: 122 Prompt Tokens: 49251, Completion Tokens: 25243\n",
      "Cost [gpt-4-turbo]: 1.250 (per-example: 0.0102)\n",
      "Cost [gpt-4]: 2.992 (per-example: 0.0245)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.100 (per-example: 0.0008)\n",
      "question: 152, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 76%|███████████████████████████████▎         | 122/160 [22:54<07:13, 11.41s/it]\n",
      "Num API calls: 123 Prompt Tokens: 49839, Completion Tokens: 25507\n",
      "Cost [gpt-4-turbo]: 1.264 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.026 (per-example: 0.0246)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.101 (per-example: 0.0008)\n",
      "question: 153, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 77%|███████████████████████████████▌         | 123/160 [23:02<06:27, 10.47s/it]\n",
      "Num API calls: 124 Prompt Tokens: 50386, Completion Tokens: 25753\n",
      "Cost [gpt-4-turbo]: 1.276 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.057 (per-example: 0.0247)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.102 (per-example: 0.0008)\n",
      "question: 154, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 78%|███████████████████████████████▊         | 124/160 [23:10<05:52,  9.79s/it]\n",
      "Num API calls: 125 Prompt Tokens: 50927, Completion Tokens: 25977\n",
      "Cost [gpt-4-turbo]: 1.289 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.086 (per-example: 0.0247)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.103 (per-example: 0.0008)\n",
      "question: 155, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 78%|████████████████████████████████         | 125/160 [23:17<05:05,  8.74s/it]\n",
      "Num API calls: 126 Prompt Tokens: 51347, Completion Tokens: 26182\n",
      "Cost [gpt-4-turbo]: 1.299 (per-example: 0.0103)\n",
      "Cost [gpt-4]: 3.111 (per-example: 0.0247)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.104 (per-example: 0.0008)\n",
      "question: 156, turn: 2, model: model_answer, score: 2, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 79%|████████████████████████████████▎        | 126/160 [23:28<05:29,  9.68s/it]\n",
      "Num API calls: 127 Prompt Tokens: 51943, Completion Tokens: 26694\n",
      "Cost [gpt-4-turbo]: 1.320 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.160 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.105 (per-example: 0.0008)\n",
      "question: 157, turn: 2, model: model_answer, score: -1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 79%|████████████████████████████████▌        | 127/160 [23:57<08:25, 15.31s/it]\n",
      "Num API calls: 128 Prompt Tokens: 52271, Completion Tokens: 26801\n",
      "Cost [gpt-4-turbo]: 1.327 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.176 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.106 (per-example: 0.0008)\n",
      "question: 158, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 80%|████████████████████████████████▊        | 128/160 [24:02<06:27, 12.12s/it]\n",
      "Num API calls: 129 Prompt Tokens: 52847, Completion Tokens: 26983\n",
      "Cost [gpt-4-turbo]: 1.338 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.204 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.107 (per-example: 0.0008)\n",
      "question: 159, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 81%|█████████████████████████████████        | 129/160 [24:12<05:55, 11.48s/it]\n",
      "Num API calls: 130 Prompt Tokens: 53384, Completion Tokens: 27175\n",
      "Cost [gpt-4-turbo]: 1.349 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.232 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.108 (per-example: 0.0008)\n",
      "question: 160, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-v1-multi-turn')\n",
      " 81%|█████████████████████████████████▎       | 130/160 [24:22<05:31, 11.04s/it]\n",
      "Num API calls: 131 Prompt Tokens: 53837, Completion Tokens: 27329\n",
      "Cost [gpt-4-turbo]: 1.358 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.255 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.108 (per-example: 0.0008)\n",
      "question: 101, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 82%|█████████████████████████████████▌       | 131/160 [24:28<04:36,  9.55s/it]\n",
      "Num API calls: 132 Prompt Tokens: 54282, Completion Tokens: 27494\n",
      "Cost [gpt-4-turbo]: 1.368 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.278 (per-example: 0.0248)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.109 (per-example: 0.0008)\n",
      "question: 102, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 82%|█████████████████████████████████▊       | 132/160 [24:33<03:53,  8.34s/it]\n",
      "Num API calls: 133 Prompt Tokens: 55125, Completion Tokens: 27710\n",
      "Cost [gpt-4-turbo]: 1.383 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.316 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.111 (per-example: 0.0008)\n",
      "question: 103, turn: 2, model: model_answer, score: 3, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 83%|██████████████████████████████████       | 133/160 [24:49<04:48, 10.67s/it]\n",
      "Num API calls: 134 Prompt Tokens: 55453, Completion Tokens: 27842\n",
      "Cost [gpt-4-turbo]: 1.390 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.334 (per-example: 0.0249)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.111 (per-example: 0.0008)\n",
      "question: 104, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 84%|██████████████████████████████████▎      | 134/160 [24:56<04:06,  9.50s/it]\n",
      "Num API calls: 135 Prompt Tokens: 56323, Completion Tokens: 28084\n",
      "Cost [gpt-4-turbo]: 1.406 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.375 (per-example: 0.0250)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.112 (per-example: 0.0008)\n",
      "question: 105, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 84%|██████████████████████████████████▌      | 135/160 [25:07<04:10, 10.02s/it]\n",
      "Num API calls: 136 Prompt Tokens: 56828, Completion Tokens: 28271\n",
      "Cost [gpt-4-turbo]: 1.416 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.401 (per-example: 0.0250)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.113 (per-example: 0.0008)\n",
      "question: 106, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 85%|██████████████████████████████████▊      | 136/160 [25:18<04:08, 10.37s/it]\n",
      "Num API calls: 137 Prompt Tokens: 57586, Completion Tokens: 28480\n",
      "Cost [gpt-4-turbo]: 1.430 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.436 (per-example: 0.0251)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.115 (per-example: 0.0008)\n",
      "question: 107, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 86%|███████████████████████████████████      | 137/160 [25:24<03:26,  8.98s/it]\n",
      "Num API calls: 138 Prompt Tokens: 57941, Completion Tokens: 28700\n",
      "Cost [gpt-4-turbo]: 1.440 (per-example: 0.0104)\n",
      "Cost [gpt-4]: 3.460 (per-example: 0.0251)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.115 (per-example: 0.0008)\n",
      "question: 108, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 86%|███████████████████████████████████▎     | 138/160 [25:29<02:52,  7.86s/it]\n",
      "Num API calls: 139 Prompt Tokens: 58567, Completion Tokens: 28922\n",
      "Cost [gpt-4-turbo]: 1.453 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.492 (per-example: 0.0251)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.116 (per-example: 0.0008)\n",
      "question: 109, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████▌     | 139/160 [25:39<02:57,  8.45s/it]\n",
      "Num API calls: 140 Prompt Tokens: 59517, Completion Tokens: 29145\n",
      "Cost [gpt-4-turbo]: 1.470 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.534 (per-example: 0.0252)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.118 (per-example: 0.0008)\n",
      "question: 110, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 88%|███████████████████████████████████▉     | 140/160 [25:50<03:02,  9.10s/it]\n",
      "Num API calls: 141 Prompt Tokens: 60189, Completion Tokens: 29416\n",
      "Cost [gpt-4-turbo]: 1.484 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.571 (per-example: 0.0253)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.119 (per-example: 0.0008)\n",
      "question: 111, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 88%|████████████████████████████████████▏    | 141/160 [26:00<02:57,  9.36s/it]\n",
      "Num API calls: 142 Prompt Tokens: 60660, Completion Tokens: 29583\n",
      "Cost [gpt-4-turbo]: 1.494 (per-example: 0.0105)\n",
      "Cost [gpt-4]: 3.595 (per-example: 0.0253)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.120 (per-example: 0.0008)\n",
      "question: 112, turn: 2, model: model_answer, score: 10, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 89%|████████████████████████████████████▍    | 142/160 [26:08<02:43,  9.08s/it]\n",
      "Num API calls: 143 Prompt Tokens: 61421, Completion Tokens: 29820\n",
      "Cost [gpt-4-turbo]: 1.509 (per-example: 0.0106)\n",
      "Cost [gpt-4]: 3.632 (per-example: 0.0254)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.121 (per-example: 0.0008)\n",
      "question: 113, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 89%|████████████████████████████████████▋    | 143/160 [26:19<02:43,  9.62s/it]\n",
      "Num API calls: 144 Prompt Tokens: 62519, Completion Tokens: 30070\n",
      "Cost [gpt-4-turbo]: 1.527 (per-example: 0.0106)\n",
      "Cost [gpt-4]: 3.680 (per-example: 0.0256)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.123 (per-example: 0.0009)\n",
      "question: 114, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 90%|████████████████████████████████████▉    | 144/160 [26:32<02:50, 10.65s/it]\n",
      "Num API calls: 145 Prompt Tokens: 63300, Completion Tokens: 30359\n",
      "Cost [gpt-4-turbo]: 1.544 (per-example: 0.0106)\n",
      "Cost [gpt-4]: 3.721 (per-example: 0.0257)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.124 (per-example: 0.0009)\n",
      "question: 115, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 91%|█████████████████████████████████████▏   | 145/160 [26:46<02:52, 11.52s/it]\n",
      "Num API calls: 146 Prompt Tokens: 63977, Completion Tokens: 30550\n",
      "Cost [gpt-4-turbo]: 1.556 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.752 (per-example: 0.0257)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.125 (per-example: 0.0009)\n",
      "question: 116, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 91%|█████████████████████████████████████▍   | 146/160 [26:55<02:31, 10.81s/it]\n",
      "Num API calls: 147 Prompt Tokens: 64763, Completion Tokens: 30806\n",
      "Cost [gpt-4-turbo]: 1.572 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.791 (per-example: 0.0258)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.126 (per-example: 0.0009)\n",
      "question: 117, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 92%|█████████████████████████████████████▋   | 147/160 [27:08<02:28, 11.43s/it]\n",
      "Num API calls: 148 Prompt Tokens: 65340, Completion Tokens: 30967\n",
      "Cost [gpt-4-turbo]: 1.582 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.818 (per-example: 0.0258)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.127 (per-example: 0.0009)\n",
      "question: 118, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 92%|█████████████████████████████████████▉   | 148/160 [27:18<02:11, 10.94s/it]\n",
      "Num API calls: 149 Prompt Tokens: 66123, Completion Tokens: 31203\n",
      "Cost [gpt-4-turbo]: 1.597 (per-example: 0.0107)\n",
      "Cost [gpt-4]: 3.856 (per-example: 0.0259)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.129 (per-example: 0.0009)\n",
      "question: 119, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 93%|██████████████████████████████████████▏  | 149/160 [27:30<02:04, 11.30s/it]\n",
      "Num API calls: 150 Prompt Tokens: 67190, Completion Tokens: 31451\n",
      "Cost [gpt-4-turbo]: 1.615 (per-example: 0.0108)\n",
      "Cost [gpt-4]: 3.903 (per-example: 0.0260)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.130 (per-example: 0.0009)\n",
      "question: 120, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 94%|██████████████████████████████████████▍  | 150/160 [27:37<01:40, 10.04s/it]\n",
      "Num API calls: 151 Prompt Tokens: 68169, Completion Tokens: 31667\n",
      "Cost [gpt-4-turbo]: 1.632 (per-example: 0.0108)\n",
      "Cost [gpt-4]: 3.945 (per-example: 0.0261)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.132 (per-example: 0.0009)\n",
      "question: 121, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 94%|██████████████████████████████████████▋  | 151/160 [27:48<01:32, 10.28s/it]\n",
      "Num API calls: 152 Prompt Tokens: 69967, Completion Tokens: 31990\n",
      "Cost [gpt-4-turbo]: 1.659 (per-example: 0.0109)\n",
      "Cost [gpt-4]: 4.018 (per-example: 0.0264)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.134 (per-example: 0.0009)\n",
      "question: 122, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 95%|██████████████████████████████████████▉  | 152/160 [28:04<01:37, 12.20s/it]\n",
      "Num API calls: 153 Prompt Tokens: 71078, Completion Tokens: 32191\n",
      "Cost [gpt-4-turbo]: 1.677 (per-example: 0.0110)\n",
      "Cost [gpt-4]: 4.064 (per-example: 0.0266)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.135 (per-example: 0.0009)\n",
      "question: 123, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 96%|███████████████████████████████████████▏ | 153/160 [28:15<01:22, 11.76s/it]\n",
      "Num API calls: 154 Prompt Tokens: 72487, Completion Tokens: 32406\n",
      "Cost [gpt-4-turbo]: 1.697 (per-example: 0.0110)\n",
      "Cost [gpt-4]: 4.119 (per-example: 0.0267)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.137 (per-example: 0.0009)\n",
      "question: 124, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 96%|███████████████████████████████████████▍ | 154/160 [28:22<01:01, 10.24s/it]\n",
      "Num API calls: 155 Prompt Tokens: 73783, Completion Tokens: 32651\n",
      "Cost [gpt-4-turbo]: 1.717 (per-example: 0.0111)\n",
      "Cost [gpt-4]: 4.173 (per-example: 0.0269)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.139 (per-example: 0.0009)\n",
      "question: 125, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 97%|███████████████████████████████████████▋ | 155/160 [28:37<00:59, 11.81s/it]\n",
      "Num API calls: 156 Prompt Tokens: 74777, Completion Tokens: 32860\n",
      "Cost [gpt-4-turbo]: 1.734 (per-example: 0.0111)\n",
      "Cost [gpt-4]: 4.215 (per-example: 0.0270)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.140 (per-example: 0.0009)\n",
      "question: 126, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 98%|███████████████████████████████████████▉ | 156/160 [28:44<00:41, 10.39s/it]\n",
      "Num API calls: 157 Prompt Tokens: 75872, Completion Tokens: 33062\n",
      "Cost [gpt-4-turbo]: 1.751 (per-example: 0.0112)\n",
      "Cost [gpt-4]: 4.260 (per-example: 0.0271)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.142 (per-example: 0.0009)\n",
      "question: 127, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 98%|████████████████████████████████████████▏| 157/160 [28:58<00:34, 11.43s/it]\n",
      "Num API calls: 158 Prompt Tokens: 77001, Completion Tokens: 33420\n",
      "Cost [gpt-4-turbo]: 1.773 (per-example: 0.0112)\n",
      "Cost [gpt-4]: 4.315 (per-example: 0.0273)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.144 (per-example: 0.0009)\n",
      "question: 128, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 99%|████████████████████████████████████████▍| 158/160 [29:16<00:26, 13.32s/it]\n",
      "Num API calls: 159 Prompt Tokens: 78221, Completion Tokens: 33632\n",
      "Cost [gpt-4-turbo]: 1.791 (per-example: 0.0113)\n",
      "Cost [gpt-4]: 4.365 (per-example: 0.0274)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.145 (per-example: 0.0009)\n",
      "question: 129, turn: 2, model: model_answer, score: 1, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      " 99%|████████████████████████████████████████▋| 159/160 [29:31<00:13, 13.93s/it]\n",
      "Num API calls: 160 Prompt Tokens: 79023, Completion Tokens: 33817\n",
      "Cost [gpt-4-turbo]: 1.805 (per-example: 0.0113)\n",
      "Cost [gpt-4]: 4.400 (per-example: 0.0275)\n",
      "Cost [gpt-3.5-turbo-1106]: 0.147 (per-example: 0.0009)\n",
      "question: 130, turn: 2, model: model_answer, score: 4, judge: ('gpt-4-1106-preview', 'single-math-v1-multi-turn')\n",
      "100%|█████████████████████████████████████████| 160/160 [29:37<00:00, 11.11s/it]\n",
      "+ python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl --mode single --save-to-json\n",
      "Mode: single\n",
      "Input file: results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\n",
      "\n",
      "########## First turn ##########\n",
      "                   score\n",
      "model        turn       \n",
      "model_answer 1     3.325\n",
      "\n",
      "########## Second turn ##########\n",
      "                      score\n",
      "model        turn          \n",
      "model_answer 2     1.772152\n",
      "\n",
      "########## Average ##########\n",
      "                 score\n",
      "model                 \n",
      "model_answer  2.553459\n",
      "Input file: results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl\n",
      "Save metrics to \n",
      "\tresults/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/metrics.json\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_eval.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6445ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 20 13:17:36 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   24C    P0              58W / 400W |      4MiB / 81920MiB |      0%   E. Process |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859d6d6",
   "metadata": {},
   "source": [
    "# Visualize Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b033ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ./1385767.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/mmlu_s=0_chatfmt\n",
      "Job ./1385854.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385740.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/mmlu_s=0\n",
      "Job ./1385932.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot\n",
      "Job ./1385781.out exited with error code. --save_dir=results/oi2/llama-7b_stanford_alpaca50k_ep=2/eval/bbh_s=3_cot\n",
      "Job ./1385766.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/tydiqa_s=1_gp\n",
      "Job ./1385749.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/mmlu_s=0_chatfmt\n",
      "Job ./1385761.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/gsm_s=8_cot\n",
      "Job ./1385759.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/mmlu_s=5\n",
      "Job ./1385762.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/bbh_s=3\n",
      "Job ./1385907.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb_chatfmt\n",
      "Job ./1385806.out exited with error code. --save_dir=results/oi2/llama-7b_wizardlm50k_ep=2/eval/gsm_s=8_cot_chatfmt\n",
      "Job ./1385878.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot\n",
      "Job ./1385832.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385720.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/tydiqa_s=1_cb_chatfmt\n",
      "Job ./1385764.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/humaneval\n",
      "Job ./1385754.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/bbh_s=3_cot_chatfmt\n",
      "Job ./1385948.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385760.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/gsm_s=8\n",
      "Job ./1385835.out exited with error code. --save_dir=results/oi2/llama-7b_wizardlm50k_ep=2/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385824.out exited with error code. --save_dir=results/oi2/llama-7b_flan_v250k_ep=2/eval/gsm_s=8_cot_chatfmt\n",
      "Job ./1385725.out exited with error code. --save_dir=results/oi2/llama-7b_dolly_ep=2/eval/gsm_s=8_cot\n",
      "Job ./1385741.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/mmlu_s=5\n",
      "Job ./1385872.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385844.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb\n",
      "Job ./1385950.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385706.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/gsm_s=8\n",
      "Job ./1385707.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/gsm_s=8_cot\n",
      "Job ./1385859.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/bbh_s=3\n",
      "Job ./1385834.out exited with error code. --save_dir=results/oi2/llama-7b_stanford_alpaca50k_ep=2/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385730.out exited with error code. --save_dir=results/oi2/llama-7b_dolly_ep=2/eval/tydiqa_s=1_gp\n",
      "Job ./1385926.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385770.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/gsm_s=8_cot_chatfmt\n",
      "Job ./1385829.out exited with error code. --save_dir=results/oi2/llama-7b_flan_v250k_ep=2/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385862.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb\n",
      "Job ./1385884.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8_chatfmt\n",
      "Job ./1385721.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385885.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot_chatfmt\n",
      "Job ./1385823.out exited with error code. --save_dir=results/oi2/llama-7b_flan_v250k_ep=2/eval/gsm_s=8_chatfmt\n",
      "Job ./1385949.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385830.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385711.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/tydiqa_s=1_cb\n",
      "Job ./1385710.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/humaneval\n",
      "Job ./1385708.out exited with error code. --save_dir=results/oi2/llama-7b_oasst2_ep=2/eval/bbh_s=3\n",
      "Job ./1385768.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/mmlu_s=5_chatfmt\n",
      "Job ./1385903.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/gsm_s=8_cot_chatfmt\n",
      "Job ./1385869.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/bbh_s=3_cot_chatfmt\n",
      "Job ./1385758.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/mmlu_s=0\n",
      "Job ./1385841.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/bbh_s=3\n",
      "Job ./1385751.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/gsm_s=8_chatfmt\n",
      "Job ./1385833.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385728.out exited with error code. --save_dir=results/oi2/llama-7b_dolly_ep=2/eval/humaneval\n",
      "Job ./1385825.out exited with error code. --save_dir=results/oi2/llama-7b_flan_v250k_ep=2/eval/bbh_s=3_chatfmt\n",
      "Job ./1385784.out exited with error code. --save_dir=results/oi2/llama-7b_stanford_alpaca50k_ep=2/eval/tydiqa_s=1_gp\n",
      "Job ./1385904.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/bbh_s=3_chatfmt\n",
      "Job ./1385828.out exited with error code. --save_dir=results/oi2/llama-7b_flan_v250k_ep=2/eval/tydiqa_s=1_cb_chatfmt\n",
      "Job ./1385827.out exited with error code. --save_dir=results/oi2/llama-7b_flan_v250k_ep=2/eval/humaneval_chatfmt\n",
      "Job ./1385757.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385944.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385775.out exited with error code. --save_dir=results/oi2/llama-7b_sharegpt50k_ep=2/eval/tydiqa_s=1_gp_chatfmt\n",
      "Job ./1385947.out exited with error code. --save_dir=results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385831.out exited with error code. --save_dir=results/oi2/llama-7b_dolly_ep=2/eval/alpacafarm_ann=chatgpt_chatfmt\n",
      "Job ./1385750.out exited with error code. --save_dir=results/oi2/llama-7b_ultrachat50k_ep=2/eval/mmlu_s=5_chatfmt\n"
     ]
    }
   ],
   "source": [
    "# if job successfully ran, lsf system will generate a summary in log_dir,\n",
    "# call this function to move lsf summary to save_dir if job is successful.\n",
    "move_lsf_job_summary_to_save_dir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4392e9ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/247239774.py:366: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/247239774.py:372: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a7e29 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_a7e29_row0_col0, #T_a7e29_row1_col0, #T_a7e29_row2_col0, #T_a7e29_row3_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a7e29_row0_col1, #T_a7e29_row1_col1, #T_a7e29_row2_col1, #T_a7e29_row3_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a7e29_row0_col2, #T_a7e29_row0_col3, #T_a7e29_row0_col4, #T_a7e29_row0_col5, #T_a7e29_row0_col6, #T_a7e29_row0_col7, #T_a7e29_row0_col8, #T_a7e29_row0_col9, #T_a7e29_row0_col10, #T_a7e29_row0_col13, #T_a7e29_row0_col14, #T_a7e29_row0_col15, #T_a7e29_row1_col10, #T_a7e29_row1_col11, #T_a7e29_row3_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row0_col11, #T_a7e29_row0_col12, #T_a7e29_row1_col4, #T_a7e29_row1_col6, #T_a7e29_row1_col8, #T_a7e29_row1_col9, #T_a7e29_row2_col10, #T_a7e29_row3_col2, #T_a7e29_row3_col3, #T_a7e29_row3_col5, #T_a7e29_row3_col7, #T_a7e29_row3_col13, #T_a7e29_row3_col14, #T_a7e29_row3_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row1_col2, #T_a7e29_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row1_col7, #T_a7e29_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row1_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row1_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row2_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row2_col13, #T_a7e29_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row2_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #4e68d8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row3_col6, #T_a7e29_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c1a9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a7e29_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a7e29_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a7e29\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a7e29_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_a7e29_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_a7e29_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_a7e29_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_a7e29_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_a7e29_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_a7e29_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_a7e29_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_a7e29_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_a7e29_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_a7e29_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_a7e29_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR</th>\n",
       "      <th id=\"T_a7e29_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/ΔWR</th>\n",
       "      <th id=\"T_a7e29_level0_col13\" class=\"col_heading level0 col13\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_a7e29_level0_col14\" class=\"col_heading level0 col14\" >Average</th>\n",
       "      <th id=\"T_a7e29_level0_col15\" class=\"col_heading level0 col15\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a7e29_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a7e29_row0_col0\" class=\"data row0 col0\" >llama-7b_dolly_ep=2</td>\n",
       "      <td id=\"T_a7e29_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_a7e29_row0_col2\" class=\"data row0 col2\" >38.5</td>\n",
       "      <td id=\"T_a7e29_row0_col3\" class=\"data row0 col3\" >38.5</td>\n",
       "      <td id=\"T_a7e29_row0_col4\" class=\"data row0 col4\" >5.4</td>\n",
       "      <td id=\"T_a7e29_row0_col5\" class=\"data row0 col5\" >10.8</td>\n",
       "      <td id=\"T_a7e29_row0_col6\" class=\"data row0 col6\" >34.0</td>\n",
       "      <td id=\"T_a7e29_row0_col7\" class=\"data row0 col7\" >31.3</td>\n",
       "      <td id=\"T_a7e29_row0_col8\" class=\"data row0 col8\" >9.0</td>\n",
       "      <td id=\"T_a7e29_row0_col9\" class=\"data row0 col9\" >44.0</td>\n",
       "      <td id=\"T_a7e29_row0_col10\" class=\"data row0 col10\" >10.4</td>\n",
       "      <td id=\"T_a7e29_row0_col11\" class=\"data row0 col11\" >21.1</td>\n",
       "      <td id=\"T_a7e29_row0_col12\" class=\"data row0 col12\" >-1.0</td>\n",
       "      <td id=\"T_a7e29_row0_col13\" class=\"data row0 col13\" >319.7</td>\n",
       "      <td id=\"T_a7e29_row0_col14\" class=\"data row0 col14\" >22.0</td>\n",
       "      <td id=\"T_a7e29_row0_col15\" class=\"data row0 col15\" >-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7e29_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a7e29_row1_col0\" class=\"data row1 col0\" >llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_a7e29_row1_col1\" class=\"data row1 col1\" >10000</td>\n",
       "      <td id=\"T_a7e29_row1_col2\" class=\"data row1 col2\" >34.7</td>\n",
       "      <td id=\"T_a7e29_row1_col3\" class=\"data row1 col3\" >37.0</td>\n",
       "      <td id=\"T_a7e29_row1_col4\" class=\"data row1 col4\" >3.4</td>\n",
       "      <td id=\"T_a7e29_row1_col5\" class=\"data row1 col5\" >10.0</td>\n",
       "      <td id=\"T_a7e29_row1_col6\" class=\"data row1 col6\" >30.9</td>\n",
       "      <td id=\"T_a7e29_row1_col7\" class=\"data row1 col7\" >30.1</td>\n",
       "      <td id=\"T_a7e29_row1_col8\" class=\"data row1 col8\" >6.4</td>\n",
       "      <td id=\"T_a7e29_row1_col9\" class=\"data row1 col9\" >35.4</td>\n",
       "      <td id=\"T_a7e29_row1_col10\" class=\"data row1 col10\" >10.4</td>\n",
       "      <td id=\"T_a7e29_row1_col11\" class=\"data row1 col11\" >38.1</td>\n",
       "      <td id=\"T_a7e29_row1_col12\" class=\"data row1 col12\" >-0.6</td>\n",
       "      <td id=\"T_a7e29_row1_col13\" class=\"data row1 col13\" >298.0</td>\n",
       "      <td id=\"T_a7e29_row1_col14\" class=\"data row1 col14\" >21.4</td>\n",
       "      <td id=\"T_a7e29_row1_col15\" class=\"data row1 col15\" >-4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7e29_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a7e29_row2_col0\" class=\"data row2 col0\" >llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10</td>\n",
       "      <td id=\"T_a7e29_row2_col1\" class=\"data row2 col1\" >10000</td>\n",
       "      <td id=\"T_a7e29_row2_col2\" class=\"data row2 col2\" >33.3</td>\n",
       "      <td id=\"T_a7e29_row2_col3\" class=\"data row2 col3\" >37.1</td>\n",
       "      <td id=\"T_a7e29_row2_col4\" class=\"data row2 col4\" >4.6</td>\n",
       "      <td id=\"T_a7e29_row2_col5\" class=\"data row2 col5\" >9.4</td>\n",
       "      <td id=\"T_a7e29_row2_col6\" class=\"data row2 col6\" >31.4</td>\n",
       "      <td id=\"T_a7e29_row2_col7\" class=\"data row2 col7\" >28.7</td>\n",
       "      <td id=\"T_a7e29_row2_col8\" class=\"data row2 col8\" >7.3</td>\n",
       "      <td id=\"T_a7e29_row2_col9\" class=\"data row2 col9\" >35.9</td>\n",
       "      <td id=\"T_a7e29_row2_col10\" class=\"data row2 col10\" >7.5</td>\n",
       "      <td id=\"T_a7e29_row2_col11\" class=\"data row2 col11\" >34.4</td>\n",
       "      <td id=\"T_a7e29_row2_col12\" class=\"data row2 col12\" >-0.5</td>\n",
       "      <td id=\"T_a7e29_row2_col13\" class=\"data row2 col13\" >172.6</td>\n",
       "      <td id=\"T_a7e29_row2_col14\" class=\"data row2 col14\" >20.8</td>\n",
       "      <td id=\"T_a7e29_row2_col15\" class=\"data row2 col15\" >-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7e29_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a7e29_row3_col0\" class=\"data row3 col0\" >score=random:s=\\d_pace=prune:size=10000:ep=10_avg (N=1)</td>\n",
       "      <td id=\"T_a7e29_row3_col1\" class=\"data row3 col1\" >10000</td>\n",
       "      <td id=\"T_a7e29_row3_col2\" class=\"data row3 col2\" >30.9</td>\n",
       "      <td id=\"T_a7e29_row3_col3\" class=\"data row3 col3\" >34.8</td>\n",
       "      <td id=\"T_a7e29_row3_col4\" class=\"data row3 col4\" >5.0</td>\n",
       "      <td id=\"T_a7e29_row3_col5\" class=\"data row3 col5\" >8.4</td>\n",
       "      <td id=\"T_a7e29_row3_col6\" class=\"data row3 col6\" >32.9</td>\n",
       "      <td id=\"T_a7e29_row3_col7\" class=\"data row3 col7\" >25.7</td>\n",
       "      <td id=\"T_a7e29_row3_col8\" class=\"data row3 col8\" >8.0</td>\n",
       "      <td id=\"T_a7e29_row3_col9\" class=\"data row3 col9\" >41.0</td>\n",
       "      <td id=\"T_a7e29_row3_col10\" class=\"data row3 col10\" >7.9</td>\n",
       "      <td id=\"T_a7e29_row3_col11\" class=\"data row3 col11\" >28.2</td>\n",
       "      <td id=\"T_a7e29_row3_col12\" class=\"data row3 col12\" >0.0</td>\n",
       "      <td id=\"T_a7e29_row3_col13\" class=\"data row3 col13\" >101.5</td>\n",
       "      <td id=\"T_a7e29_row3_col14\" class=\"data row3 col14\" >20.3</td>\n",
       "      <td id=\"T_a7e29_row3_col15\" class=\"data row3 col15\" >-5.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0cd550460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/247239774.py:366: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/247239774.py:372: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_49ca2 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_49ca2_row0_col0, #T_49ca2_row1_col0, #T_49ca2_row2_col0, #T_49ca2_row3_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_49ca2_row0_col1, #T_49ca2_row1_col1, #T_49ca2_row2_col1, #T_49ca2_row3_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_49ca2_row0_col2, #T_49ca2_row0_col5, #T_49ca2_row0_col8, #T_49ca2_row0_col9, #T_49ca2_row0_col12, #T_49ca2_row0_col13, #T_49ca2_row0_col14, #T_49ca2_row0_col15, #T_49ca2_row1_col3, #T_49ca2_row1_col4, #T_49ca2_row1_col7, #T_49ca2_row1_col11, #T_49ca2_row2_col6, #T_49ca2_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row0_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row0_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row0_col10, #T_49ca2_row0_col11, #T_49ca2_row1_col8, #T_49ca2_row1_col9, #T_49ca2_row1_col10, #T_49ca2_row2_col4, #T_49ca2_row2_col5, #T_49ca2_row2_col7, #T_49ca2_row2_col13, #T_49ca2_row2_col15, #T_49ca2_row3_col2, #T_49ca2_row3_col3, #T_49ca2_row3_col6, #T_49ca2_row3_col7, #T_49ca2_row3_col12, #T_49ca2_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row1_col12, #T_49ca2_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #485fd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row1_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #d24b40;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row1_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row2_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_49ca2_row3_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_49ca2_row3_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_49ca2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_49ca2_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_49ca2_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_49ca2_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_49ca2_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_49ca2_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_49ca2_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_49ca2_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_49ca2_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_49ca2_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_49ca2_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_49ca2_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_49ca2_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR</th>\n",
       "      <th id=\"T_49ca2_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/ΔWR</th>\n",
       "      <th id=\"T_49ca2_level0_col13\" class=\"col_heading level0 col13\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_49ca2_level0_col14\" class=\"col_heading level0 col14\" >Average</th>\n",
       "      <th id=\"T_49ca2_level0_col15\" class=\"col_heading level0 col15\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_49ca2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_49ca2_row0_col0\" class=\"data row0 col0\" >llama-7b_dolly_ep=2</td>\n",
       "      <td id=\"T_49ca2_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_49ca2_row0_col2\" class=\"data row0 col2\" >38.5</td>\n",
       "      <td id=\"T_49ca2_row0_col3\" class=\"data row0 col3\" >38.5</td>\n",
       "      <td id=\"T_49ca2_row0_col4\" class=\"data row0 col4\" >5.4</td>\n",
       "      <td id=\"T_49ca2_row0_col5\" class=\"data row0 col5\" >10.8</td>\n",
       "      <td id=\"T_49ca2_row0_col6\" class=\"data row0 col6\" >34.0</td>\n",
       "      <td id=\"T_49ca2_row0_col7\" class=\"data row0 col7\" >31.3</td>\n",
       "      <td id=\"T_49ca2_row0_col8\" class=\"data row0 col8\" >9.0</td>\n",
       "      <td id=\"T_49ca2_row0_col9\" class=\"data row0 col9\" >44.0</td>\n",
       "      <td id=\"T_49ca2_row0_col10\" class=\"data row0 col10\" >10.4</td>\n",
       "      <td id=\"T_49ca2_row0_col11\" class=\"data row0 col11\" >21.1</td>\n",
       "      <td id=\"T_49ca2_row0_col12\" class=\"data row0 col12\" >-1.0</td>\n",
       "      <td id=\"T_49ca2_row0_col13\" class=\"data row0 col13\" >319.7</td>\n",
       "      <td id=\"T_49ca2_row0_col14\" class=\"data row0 col14\" >22.0</td>\n",
       "      <td id=\"T_49ca2_row0_col15\" class=\"data row0 col15\" >-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49ca2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_49ca2_row1_col0\" class=\"data row1 col0\" >llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_49ca2_row1_col1\" class=\"data row1 col1\" >30000</td>\n",
       "      <td id=\"T_49ca2_row1_col2\" class=\"data row1 col2\" >36.6</td>\n",
       "      <td id=\"T_49ca2_row1_col3\" class=\"data row1 col3\" >39.2</td>\n",
       "      <td id=\"T_49ca2_row1_col4\" class=\"data row1 col4\" >5.6</td>\n",
       "      <td id=\"T_49ca2_row1_col5\" class=\"data row1 col5\" >9.6</td>\n",
       "      <td id=\"T_49ca2_row1_col6\" class=\"data row1 col6\" >33.5</td>\n",
       "      <td id=\"T_49ca2_row1_col7\" class=\"data row1 col7\" >33.1</td>\n",
       "      <td id=\"T_49ca2_row1_col8\" class=\"data row1 col8\" >7.5</td>\n",
       "      <td id=\"T_49ca2_row1_col9\" class=\"data row1 col9\" >37.7</td>\n",
       "      <td id=\"T_49ca2_row1_col10\" class=\"data row1 col10\" >10.4</td>\n",
       "      <td id=\"T_49ca2_row1_col11\" class=\"data row1 col11\" >29.5</td>\n",
       "      <td id=\"T_49ca2_row1_col12\" class=\"data row1 col12\" >-1.4</td>\n",
       "      <td id=\"T_49ca2_row1_col13\" class=\"data row1 col13\" >263.1</td>\n",
       "      <td id=\"T_49ca2_row1_col14\" class=\"data row1 col14\" >21.9</td>\n",
       "      <td id=\"T_49ca2_row1_col15\" class=\"data row1 col15\" >-3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49ca2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_49ca2_row2_col0\" class=\"data row2 col0\" >llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_49ca2_row2_col1\" class=\"data row2 col1\" >30000</td>\n",
       "      <td id=\"T_49ca2_row2_col2\" class=\"data row2 col2\" >36.9</td>\n",
       "      <td id=\"T_49ca2_row2_col3\" class=\"data row2 col3\" >37.1</td>\n",
       "      <td id=\"T_49ca2_row2_col4\" class=\"data row2 col4\" >3.0</td>\n",
       "      <td id=\"T_49ca2_row2_col5\" class=\"data row2 col5\" >9.4</td>\n",
       "      <td id=\"T_49ca2_row2_col6\" class=\"data row2 col6\" >34.1</td>\n",
       "      <td id=\"T_49ca2_row2_col7\" class=\"data row2 col7\" >30.3</td>\n",
       "      <td id=\"T_49ca2_row2_col8\" class=\"data row2 col8\" >8.0</td>\n",
       "      <td id=\"T_49ca2_row2_col9\" class=\"data row2 col9\" >40.9</td>\n",
       "      <td id=\"T_49ca2_row2_col10\" class=\"data row2 col10\" >10.6</td>\n",
       "      <td id=\"T_49ca2_row2_col11\" class=\"data row2 col11\" >27.5</td>\n",
       "      <td id=\"T_49ca2_row2_col12\" class=\"data row2 col12\" >-1.0</td>\n",
       "      <td id=\"T_49ca2_row2_col13\" class=\"data row2 col13\" >260.5</td>\n",
       "      <td id=\"T_49ca2_row2_col14\" class=\"data row2 col14\" >21.5</td>\n",
       "      <td id=\"T_49ca2_row2_col15\" class=\"data row2 col15\" >-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49ca2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_49ca2_row3_col0\" class=\"data row3 col0\" >llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3</td>\n",
       "      <td id=\"T_49ca2_row3_col1\" class=\"data row3 col1\" >30000</td>\n",
       "      <td id=\"T_49ca2_row3_col2\" class=\"data row3 col2\" >36.1</td>\n",
       "      <td id=\"T_49ca2_row3_col3\" class=\"data row3 col3\" >35.0</td>\n",
       "      <td id=\"T_49ca2_row3_col4\" class=\"data row3 col4\" >4.4</td>\n",
       "      <td id=\"T_49ca2_row3_col5\" class=\"data row3 col5\" >10.2</td>\n",
       "      <td id=\"T_49ca2_row3_col6\" class=\"data row3 col6\" >31.2</td>\n",
       "      <td id=\"T_49ca2_row3_col7\" class=\"data row3 col7\" >30.3</td>\n",
       "      <td id=\"T_49ca2_row3_col8\" class=\"data row3 col8\" >8.6</td>\n",
       "      <td id=\"T_49ca2_row3_col9\" class=\"data row3 col9\" >42.1</td>\n",
       "      <td id=\"T_49ca2_row3_col10\" class=\"data row3 col10\" >11.6</td>\n",
       "      <td id=\"T_49ca2_row3_col11\" class=\"data row3 col11\" >28.5</td>\n",
       "      <td id=\"T_49ca2_row3_col12\" class=\"data row3 col12\" >-1.9</td>\n",
       "      <td id=\"T_49ca2_row3_col13\" class=\"data row3 col13\" >277.6</td>\n",
       "      <td id=\"T_49ca2_row3_col14\" class=\"data row3 col14\" >21.5</td>\n",
       "      <td id=\"T_49ca2_row3_col15\" class=\"data row3 col15\" >-3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0cd550460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2455346/247239774.py:366: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  display(dfc\n",
      "/tmp/ipykernel_2455346/247239774.py:372: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(lambda x: 'text-decoration: underline;' \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1cf53 td {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "#T_1cf53_row0_col0 {\n",
       "  max-width: 60ch;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1cf53_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1cf53_row0_col2, #T_1cf53_row0_col3, #T_1cf53_row0_col4, #T_1cf53_row0_col5, #T_1cf53_row0_col6, #T_1cf53_row0_col7, #T_1cf53_row0_col8, #T_1cf53_row0_col9, #T_1cf53_row0_col10, #T_1cf53_row0_col11, #T_1cf53_row0_col12, #T_1cf53_row0_col13, #T_1cf53_row0_col14, #T_1cf53_row0_col15 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1cf53\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1cf53_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_1cf53_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_1cf53_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_1cf53_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_1cf53_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_1cf53_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_1cf53_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_1cf53_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_1cf53_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_1cf53_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_1cf53_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_1cf53_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR</th>\n",
       "      <th id=\"T_1cf53_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/ΔWR</th>\n",
       "      <th id=\"T_1cf53_level0_col13\" class=\"col_heading level0 col13\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_1cf53_level0_col14\" class=\"col_heading level0 col14\" >Average</th>\n",
       "      <th id=\"T_1cf53_level0_col15\" class=\"col_heading level0 col15\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1cf53_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1cf53_row0_col0\" class=\"data row0 col0\" >llama-7b_dolly_ep=2</td>\n",
       "      <td id=\"T_1cf53_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_1cf53_row0_col2\" class=\"data row0 col2\" >38.5</td>\n",
       "      <td id=\"T_1cf53_row0_col3\" class=\"data row0 col3\" >38.5</td>\n",
       "      <td id=\"T_1cf53_row0_col4\" class=\"data row0 col4\" >5.4</td>\n",
       "      <td id=\"T_1cf53_row0_col5\" class=\"data row0 col5\" >10.8</td>\n",
       "      <td id=\"T_1cf53_row0_col6\" class=\"data row0 col6\" >34.0</td>\n",
       "      <td id=\"T_1cf53_row0_col7\" class=\"data row0 col7\" >31.3</td>\n",
       "      <td id=\"T_1cf53_row0_col8\" class=\"data row0 col8\" >9.0</td>\n",
       "      <td id=\"T_1cf53_row0_col9\" class=\"data row0 col9\" >44.0</td>\n",
       "      <td id=\"T_1cf53_row0_col10\" class=\"data row0 col10\" >10.4</td>\n",
       "      <td id=\"T_1cf53_row0_col11\" class=\"data row0 col11\" >21.1</td>\n",
       "      <td id=\"T_1cf53_row0_col12\" class=\"data row0 col12\" >-1.0</td>\n",
       "      <td id=\"T_1cf53_row0_col13\" class=\"data row0 col13\" >319.7</td>\n",
       "      <td id=\"T_1cf53_row0_col14\" class=\"data row0 col14\" >22.0</td>\n",
       "      <td id=\"T_1cf53_row0_col15\" class=\"data row0 col15\" >-2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d0b5aacb20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# sort_rows = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b_12.13update_before', '../results/baselines/huggyllama/llama-7b_12.13update_before/'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "# #     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "# #     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "# #     # llama2\n",
    "#     ('llama2-7b_12.13update_before', '../results/baselines/NousResearch/Llama-2-7b-hf_12.13update_before/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b-chat', '../results/baselines/NousResearch/Llama-2-7b-chat-hf/'),\n",
    "# #     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "# #     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "# #     # mistral\n",
    "# #     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct-v0.1_12.13update_before', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1_12.13update_before'),\n",
    "#     ('mistral-7b-Instruct-v0.1', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     # zephyr\n",
    "#     ('zephyr-7b-beta_12.13update_before', '../results/baselines/HuggingFaceH4/zephyr-7b-beta_12.13update_before'),\n",
    "#     ('zephyr-7b-beta', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "\n",
    "# # baselines\n",
    "# save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b+lima_ep=2', '../results/ft1_ep=2/llama-7b_lima/'),\n",
    "# #     ('mistral-7b+lima_ep=2', '../results/ft1_ep=2/mistral-7b_lima/'), \n",
    "# ]\n",
    "# exp_dir = '../results/oi2/'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# # save_dirs = [\n",
    "# #     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "# #     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# # ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'tuluv1m' in x]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# # exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# # exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# ###### ultrachat\n",
    "# save_dirs = [\n",
    "#     # baselines \n",
    "#     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "#     ('mistral-7b_ultrachat200k_aftersplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k'),\n",
    "#     ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "    \n",
    "#     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "#     ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "#     ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-alpha'),\n",
    "#     ('mistral-7b-sft-beta+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "# # exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_ultrachat200k:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# exp_dir = '../results/oi5_ultrachat15:mistral-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "# dataset = 'stanford_alpaca'\n",
    "# dataset = 'open_orca_slim'\n",
    "# dataset = 'sharegptv2'\n",
    "# dataset = 'ultrachat200kv2'\n",
    "# dataset = 'wizardlm'\n",
    "# dataset = 'wizardlmv2'\n",
    "# dataset = 'tulu_v2'\n",
    "# dataset = 'flan_v2'\n",
    "# dataset = 'oasst1'\n",
    "# dataset = 'dolly'\n",
    "dataset_list = [\n",
    "    'dolly',\n",
    "#     'oasst1', \n",
    "#     'flan_v2', \n",
    "#     'stanford_alpaca', \n",
    "#     'wizardlmv2', \n",
    "#     'sharegptv2', \n",
    "#     'ultrachat200kv2',\n",
    "]; finetune_type = 'sft'\n",
    "# dataset_list = [\n",
    "#     'ultrafeedback',\n",
    "#     'ultrafeedbackfull',\n",
    "# ]; finetune_type = 'pref'\n",
    "\n",
    "## older\n",
    "# dataset = 'tulu_v1_mix'\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b'),\n",
    "#     ('llama-7b_lima_ep=5', '../results/oi2/llama-7b_lima_ep=5/'),\n",
    "#     ('llama-7b_lima_ep=10', '../results/oi2/llama-7b_lima_ep=10/'),\n",
    "]\n",
    "for dataset in dataset_list:\n",
    "    if dataset == 'tulu_v2':\n",
    "        save_dirs += [('llama-7b_tulu_v2:100k_ep=2', '../results/oi2/llama-7b_tulu_v2:100k_ep=2'),]\n",
    "    elif dataset == 'open_orca_slim':\n",
    "        save_dirs += [('llama-7b_openorcaslim:100k_ep=2', '../results/oi2/llama-7b_openorcaslim:100k_ep=2'),]\n",
    "    elif dataset == 'sharegptv2':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_sharegptv2_ep=2', '../results/oi2/llama-7b_sharegptv2_ep=2'),\n",
    "            ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),]\n",
    "    elif dataset == 'tulu_v1_mix':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "            # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "            # ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "        ]\n",
    "    elif dataset == 'ultrafeedback':\n",
    "        exp_dir = f'../results/dpo1/'\n",
    "        save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "    else:\n",
    "        save_dirs += [(f'llama-7b_{dataset}_ep=2', f'../results/oi2/llama-7b_{dataset}_ep=2'),]\n",
    "    \n",
    "    if finetune_type == 'sft':\n",
    "        exp_dir = f'../results/oi5_{dataset}:llama-7b'\n",
    "    elif finetune_type == 'pref':\n",
    "        exp_dir = f'../results/dpo2_{dataset}:llama-7b+sharegptv2ep2/'\n",
    "    save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'dppmapbd' not in x and 'semdedup' not in x]\n",
    "    \n",
    "    if finetune_type == 'pref':\n",
    "        sft_model_dataset = 'sharegptv2'\n",
    "        save_dirs += [('llama-7b_sharegptv2_ep=2', f'../results/oi2/llama-7b_{sft_model_dataset}_ep=2')]\n",
    "# ## just compare dppmap grad vs. text\n",
    "# save_dirs = [x for x in save_dirs if 'prune:size=10000:ep=10' in x[1] and (\n",
    "#         'random' in x[1] or \n",
    "#         'dppmap' in x[1]\n",
    "#     )\n",
    "# ]\n",
    "#####\n",
    "\n",
    "\n",
    "# ##### code instructions\n",
    "# save_dirs = [\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('codellama-7b', '../results/baselines/codellama/CodeLlama-7b-hf/'),\n",
    "#     ('codellama-7b-instruct', '../results/baselines/codellama/CodeLlama-7b-Python-hf/'),\n",
    "#     ('codellama-7b-python', '../results/baselines/codellama/CodeLlama-7b-Instruct-hf/'),\n",
    "# ]\n",
    "\n",
    "# exp_dir = '../results/oi2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'starcoder' in x]\n",
    "# exp_dir = '../results/oi6_starcoder_ep=5'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstr:codellama-7b'\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstrv2:codellama-7b'\n",
    "# exp_dir = '../results/oi5_starcoder_commentinstrv5:codellama-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "###### \n",
    "\n",
    "from llm.evaluate import detect_oom_evals\n",
    "oom_eval_paths = detect_oom_evals([x for l in [glob.glob(os.path.join(x[1], 'eval/*/*.out')) for x in save_dirs] for x in l])\n",
    "if oom_eval_paths: print(oom_eval_paths)\n",
    "    \n",
    "cols_avg_blacklist = ['AlpacaFarm/Len']\n",
    "\n",
    "# chat_fmt = False\n",
    "chat_fmt = True\n",
    "# chat_fmt = 'both'\n",
    "# chat_fmt = 'auto' # base model no chatfmt, tuned model with chatfmt\n",
    "chat_fmt = 'mix'  # non-alpacaeval no chatfmt, alpacaeval chatfmt\n",
    "ft_args_fields = {\n",
    "    'run_name': ('run_name',),\n",
    "    'model_name_or_path': ('model_args.model_name_or_path', 'model_name_or_path'),\n",
    "    'subsample_mixture': ('data_args.subsample_mixture',),\n",
    "    'max_train_samples': ('data_args.max_train_samples', 'max_train_samples'),\n",
    "    'train_file': ('data_args.train_file', 'train_file'),\n",
    "}\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/Direct', 'BBH/Direct', 'TydiQA/CB', 'Codex-Eval/Pass@1']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] #  'ToxiGen/Acc'\n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*', 'AlpacaFarm/Len'] \n",
    "# cols = ['AlpacaFarm/WR', 'AlpacaFarm/ΔWR', 'AlpacaFarm/Len']\n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*'] \n",
    "cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR', 'AlpacaFarm/ΔWR', 'AlpacaFarm/Len'] \n",
    "\n",
    "\n",
    "# cols = [f'BBH {x}' for x in ['reasoning', 'nlu', 'knowledge', 'multilingual']]; cols = [x+'/Direct' for x in cols] + [x+'/CoT' for x in cols]\n",
    "# cols = [f'MMLU {x}' for x in ['STEM', 'humanities', 'social sciences', 'other']]; cols = [x+'/0-shot' for x in cols] + [x+'/5-shot' for x in cols]\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR', 'AlpacaFarm/Rep', 'AlpacaFarm/WR*'] #  'ToxiGen/Acc'\n",
    "#     cols = ['AlpacaFarm/WR', 'AlpacaFarm/Rep', 'AlpacaFarm/WR*']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR'] #  entire, without tydiqa, which has high variance\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', ] #  entire, without tydiqa, which has high variance\n",
    "if 'open_orca_slim' in exp_dir:\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'BBH/Direct', 'BBH/CoT']\n",
    "    cols = ['MMLU/0-shot', 'BBH/Direct']\n",
    "if 'starcoder' in exp_dir:\n",
    "    cols = ['Codex-Eval/Pass@1']\n",
    "    chat_fmt = 'both'\n",
    "print(f'chat_fmt={chat_fmt}')\n",
    "df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "\n",
    "cols = [x for x in cols if x in df.columns]\n",
    "df = df[list(ft_args_fields.keys()) + cols]\n",
    "if chat_fmt == 'both':\n",
    "    for col_lvl2 in ['', 'chatfmt']:\n",
    "        df[('Average', col_lvl2)] = df[list(set(df.columns) & set([(x, col_lvl2) for x in list(set(cols)-set(cols_avg_blacklist))]))].mean(axis=1)\n",
    "else:\n",
    "    df['Average'] = df[list(set(cols)-set(cols_avg_blacklist))].mean(axis=1)\n",
    "if sort_rows:\n",
    "    df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "    sort_value_col, sort_value_col_ascending = ('Average', 'chatfmt') if chat_fmt=='both' else 'Average', False\n",
    "#     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "    df = df.sort_values(by=sort_value_col, ascending=sort_value_col_ascending)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def compute_total_train_samples(x):\n",
    "    match = re.search(r'size=(\\d+)', x['run_name' if chat_fmt!='both' else ('run_name', '')])\n",
    "    total_train_samples = match.group(1) if match else None\n",
    "    return total_train_samples\n",
    "df.insert(1, 'total_train_samples' if chat_fmt!='both' else ('total_train_samples', ''), df.apply(compute_total_train_samples, axis=1))\n",
    "def extract_dataset_from_train_file(x):\n",
    "    if x is None: return None\n",
    "    x = x.split('/')[-1].split('.jsonl')[0]\n",
    "    if x.endswith('_data'): x = x[:-5]\n",
    "    if x.endswith('_train'): x = x[:-6]\n",
    "    return x\n",
    "df.insert(1, 'dataset' if chat_fmt!='both' else ('dataset', ''), df['train_file'].apply(extract_dataset_from_train_file))\n",
    "df = df.drop('train_file', axis=1)\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_name_or_path', 'subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral', 'zephyr']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x.lower())]\n",
    "        if not len(dfc): continue\n",
    "        from rosemary import pd_average_col_contains_substr\n",
    "        Ns = sorted(np.unique([int(x) for x in df['total_train_samples'].to_numpy() if x]).tolist())\n",
    "        datasets = sorted(np.unique([x for x in df['dataset'] if x is not None]).tolist())\n",
    "        for N in Ns+[None]:\n",
    "            for dataset in datasets:\n",
    "                dfc = df.copy()\n",
    "                dfc = dfc[dfc['total_train_samples'].apply(lambda x: int(x) == N if x else True)]\n",
    "                dfc = dfc[dfc['dataset'].apply(lambda x: x == dataset if x else True)]\n",
    "                if not len(dfc): continue\n",
    "                col_runname = 'run_name' if chat_fmt != 'both' else ('run_name', '')\n",
    "                substitute = True\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, '_random_', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=10000:ep=10', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=50000:ep=5', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=3', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=1', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=100000', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=200000', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=400000', substitute=substitute)\n",
    "                #     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "                col = ('Average', 'chatfmt') if chat_fmt == 'both' else 'Average'\n",
    "            #     col = 'AlpacaFarm/WR'\n",
    "            #     col = 'MMLU/0-shot'|\n",
    "            #     col = 'GSM/CoT'\n",
    "            #     col = 'BBH/Direct'\n",
    "            #     col = 'TydiQA/GP'\n",
    "                dfc = dfc.sort_values(by=[col], ascending=False)\n",
    "                dfc = dfc.drop(columns=['model_name_or_path', 'subsample_mixture', 'max_train_samples', 'dataset'], \n",
    "                               axis=1, level=0 if chat_fmt=='both' else None)\n",
    "                dfc = dfc.reset_index(drop=True)\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .applymap(lambda x: f'max-width: 60ch;', subset=['run_name'])\n",
    "                        .set_table_styles([{'selector': 'td', 'props': [('white-space', 'pre-wrap'), ('word-wrap', 'break-word')]}])\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .applymap(lambda x: 'text-decoration: underline;' \\\n",
    "                                  if x in dfc[list(set(dfc.columns) & set([(x, '') for x in cols]))+[col] if chat_fmt=='both' else cols+[col]].values.flatten() and chat_fmt=='both' else '')\n",
    "                        .format(precision=1))\n",
    "\n",
    "# llama-7b_tulu_v1_mix(paper)\n",
    "# MMLU/0-shot, MMLU/5-shot, GSM/Direct, GSM/CoT, BBH/Direct, BBH/CoT, TydiQA/GP, TydiQA/CB, CodexEval/Pass@1, AlpacaEval(vs.Davinci-003)\n",
    "# 44.8       , 47.1       , 7.0       , 25.0   , 38.5      , 38.5   , 43.5,    , 8.0      , 18.6,           , 48.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deeba0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_name',\n",
       " 'nonchat',\n",
       " 'sort_by',\n",
       " 'total_train_samples',\n",
       " 'model_name_or_path',\n",
       " 'subsample_mixture',\n",
       " 'max_train_samples',\n",
       " 'MMLU/0-shot',\n",
       " 'MMLU/5-shot',\n",
       " 'GSM/Direct',\n",
       " 'GSM/CoT',\n",
       " 'BBH/Direct',\n",
       " 'BBH/CoT',\n",
       " 'TydiQA/CB',\n",
       " 'TydiQA/GP',\n",
       " 'Codex-Eval/Pass@1',\n",
       " 'AlpacaFarm/WR',\n",
       " 'AlpacaFarm/ΔWR',\n",
       " 'AlpacaFarm/Len',\n",
       " 'Average',\n",
       " 'ranking']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rosemary import parse_kv_from_string\n",
    "\n",
    "non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', ]\n",
    "# non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', ]\n",
    "\n",
    "def compute_nonchateval_average_performance(row):\n",
    "    L = [row[k] for k in non_chateval_task_names if k in row]\n",
    "    return np.average(L)\n",
    "\n",
    "def parse_prune_subset_size(row):\n",
    "    run_name = row['run_name']\n",
    "    match = re.search(r'(?<=pace=)([^_]+)', run_name)\n",
    "    if match:\n",
    "        pace = match.group(1).replace(':', '_')\n",
    "        kvs = parse_kv_from_string(pace)\n",
    "        return int(kvs['size'] / kvs['ep'])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_sort_by_from_run_name(row):\n",
    "    run_name = row['run_name']\n",
    "    match = re.search(r'score=([^_]+)', run_name)\n",
    "    if match:\n",
    "        sort_by = match.group(1).replace(':', '_')\n",
    "        kvs = parse_kv_from_string(sort_by)\n",
    "    else:\n",
    "        kvs = {}\n",
    "    return kvs\n",
    "\n",
    "\n",
    "def parse_sort_by_type(row):\n",
    "    d = row['sort_by']\n",
    "    if not d:\n",
    "        return None\n",
    "    if d[0] == 'dppmap':\n",
    "        if d['k']=='vmf' and d['kmd']=='mpnet': #and (d['gamma']==1 or d['gamma'] == 'auto1000'):\n",
    "            return 'vmf+text'\n",
    "        elif d['k']=='rbf' and d['kemb']=='text+embedding' and d['kmd'] == 'llama7br512p4096':\n",
    "            return f\"rbf+text_gamma={d['gamma']}\"\n",
    "        elif d['k']=='vmf' and d['kemb']=='grad+rp+loraB' and d['kmd'] == 'llama7br512p4096':\n",
    "            return f\"vmf+grad_gamma={d['gamma']}\"\n",
    "        else:\n",
    "            return None\n",
    "    elif d[0] == 'random':\n",
    "        return 'random'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "dfc.insert(1, 'sort_by' if chat_fmt!='both' else ('sort_by', ''), dfc.apply(parse_sort_by_from_run_name, axis=1))\n",
    "dfc.insert(1, 'sort_by_type' if chat_fmt!='both' else ('sort_by_type', ''), dfc.apply(parse_sort_by_type, axis=1))\n",
    "dfc.insert(1, 'subset_size' if chat_fmt!='both' else ('subset_size', ''), dfc.apply(parse_prune_subset_size, axis=1))\n",
    "dfc.insert(1, 'nonchat' if chat_fmt!='both' else ('nonchat', ''), dfc.apply(compute_nonchateval_average_performance, axis=1))\n",
    "\n",
    "\n",
    "dfc = dfc[dfc['sort_by_type'].notnull()]\n",
    "startswithstrs = ('rbf+text', 'random')\n",
    "startswithstrs = (\n",
    "    'random', \n",
    "    'rbf+text_gamma=0.001', \n",
    "    'vmf+grad_gamma=1',\n",
    "#     'rbf+text_gamma=auto1000', \n",
    "    'vmf+grad_gamma=auto1000',\n",
    ")\n",
    "dfc = dfc[dfc.apply(lambda x: x['sort_by_type'].startswith(startswithstrs)\n",
    "                   , axis=1)]\n",
    "dfc['subset_size'] = dfc['subset_size'].apply(lambda x: int(x) if x else x)\n",
    "dfc = dfc[dfc['subset_size']<=10_000]\n",
    "\n",
    "\n",
    "D = dfc.set_index(['sort_by_type', 'dataset', 'subset_size']).to_dict()\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class DKey:\n",
    "    sort_by_type: str\n",
    "    dataset: str\n",
    "    subset_size: int\n",
    "def convert_key_to_dataclass(d):\n",
    "    return {DKey(*k): v for k, v in d.items()}\n",
    "D = {k: convert_key_to_dataclass(v) for k, v in D.items()}\n",
    "\n",
    "list(D.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "662539cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yaxis_type = 'abs'\n",
    "# yaxis_type = 'delta_random'\n",
    "assert(yaxis_type in ['delta_random', 'abs'])\n",
    "datasets = ['flan_v2', 'dolly', 'stanford_alpaca', 'oasst1', 'ultrachat200kv2', 'wizardlmv2', 'sharegptv2']\n",
    "task_names = ['nonchat', 'MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*',]\n",
    "\n",
    "datasets = ['dolly']\n",
    "task_names = ['nonchat', 'AlpacaFarm/WR', 'AlpacaFarm/ΔWR', 'AlpacaFarm/Len']\n",
    "\n",
    "\n",
    "# task_names = ['AlpacaFarm/WR*',]\n",
    "\n",
    "# datasets = ['stanford_alpaca', 'wizardlmv2']\n",
    "# task_names = ['nonchat', 'MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "\n",
    "\n",
    "\n",
    "w = 2\n",
    "ncols = len(datasets)\n",
    "nrows = len(task_names)\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(w*ncols+3,w*nrows), sharey='row', sharex=True)\n",
    "\n",
    "xs_possible = []\n",
    "\n",
    "for axi, task_name in enumerate(task_names):\n",
    "    d = D[task_name]\n",
    "    for axj, dataset in enumerate(datasets):\n",
    "        ax = axs.reshape(nrows, ncols)[axi, axj]\n",
    "        sort_by_types = sorted(set(x.sort_by_type for x in d.keys()))\n",
    "\n",
    "        for i, sort_by_type in enumerate(sort_by_types):\n",
    "            xs = sorted([x.subset_size for x in d.keys()\n",
    "                         if x.dataset == dataset and x.sort_by_type==sort_by_type])\n",
    "            xs_possible += list(set(xs) - set(xs_possible))\n",
    "            ys = [d[DKey(sort_by_type, dataset, x)] for x in xs]\n",
    "            if yaxis_type == 'delta_random':\n",
    "                if not all(DKey('random', dataset, x) in d for x in xs): continue\n",
    "                ys = [y-d[DKey('random', dataset, x)] for x, y in zip(xs, ys)] \n",
    "            if 'random' in sort_by_type:\n",
    "                marker_style = 'o-'\n",
    "            else:\n",
    "                marker_style = 'o-'\n",
    "            ax.plot(xs, ys, marker_style, label=sort_by_type)\n",
    "        \n",
    "#         ax.set_yscale('log')\n",
    "            \n",
    "for axi, task_name in enumerate(task_names):\n",
    "    axs.reshape(nrows, ncols)[axi, 0].set_ylabel('△ '+task_name if yaxis_type.startswith('delta') else task_name, fontsize=13)\n",
    "    axs.reshape(nrows, ncols)[axi, -1].legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "for axj, dataset in enumerate(datasets):\n",
    "    axs.reshape(nrows, ncols)[0, axj].set_title(dataset, fontsize=15)\n",
    "    axs.reshape(nrows, ncols)[0, axj].set_xticks(xs_possible, xs_possible)\n",
    "\n",
    "space = 0.05\n",
    "fig.subplots_adjust(wspace=space, hspace=space)  # Adjust the value as needed\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5fdb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e2f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39874bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
