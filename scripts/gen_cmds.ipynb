{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 17 13:38:10 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:0A:00.0 Off |                    0 |\r\n",
      "| N/A   24C    P0    54W / 400W |      0MiB / 40960MiB |      0%   E. Process |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mit_fm/wpq/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from llm.submit import multiline_to_singleline, submit_job_ccc, get_run_statistics\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b984427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}*.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}*.out\" \"{save_dir}\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 2 GPUs, 2 batch size per GPU, 32 gradient accumulation steps.\n",
      "\n",
      "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 2 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 2 --gradient_accumulation_steps 32 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir jpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft'\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 20\n",
    "num_gpus = 1\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "if test_run:\n",
    "    output_dir = 'jpt_' + output_dir\n",
    "\n",
    "use_deepspeed = False\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''}\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "\n",
    "# things to test to see its effects on (1) eval perf (2) runtime.\n",
    "#\n",
    "# - int8\n",
    "# - mixed_precision bf16 or no\n",
    "# - with/without LoRA\n",
    "# - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# - batch size\n",
    "# - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions'), PosixPath('/usr/local/share/zsh/site-functions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1832148.tmpdir/.1689173487.1832148.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/12/2023 12:59:09 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "07/12/2023 12:59:09 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 292.59it/s]\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:50<00:00, 25.41s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Assigning <s> to the bos_token key of the tokenizer\n",
      "Assigning </s> to the eos_token key of the tokenizer\n",
      "Assigning <unk> to the unk_token key of the tokenizer\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/12/2023 13:00:49 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 2097152 || all params: 6740520960 || trainable%: 0.03111261002591705\n",
      "GPU memory occupied: 837 MB.\n",
      "07/12/2023 13:01:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1038c98439ac66d6_*_of_00016.arrow\n",
      "07/12/2023 13:01:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-88ed2eb4a77829ae.arrow\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 134049 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29925,  1745,   895,\n",
      "        29901,   376,  1576,  7306,   347,   310,   385, 14890, 18881,  3815,\n",
      "          338,  1641, 15569,   373,   408,  3196,  5144,   310,   670,  3815,\n",
      "         6505,  1213,    13, 29933,  1463,   373,   445,  5188,   895, 29892,\n",
      "          508,   591, 17668,   393,   278, 20051,   376, 29909,  7306,   347,\n",
      "         3907, 27401,  1213,   338,  1565, 29973,    13,  5856, 29901,    13,\n",
      "        29899,  4874,    13, 29899,   372,   338,   451,  1950,   304,  2649,\n",
      "           13, 29899,   694,  2567, 29892,  1235, 29915, 29879,   367, 16232,\n",
      "          408,  1950, 29889,  3834,  7291,   937, 29901,    13, 29966, 29989,\n",
      "          465, 22137, 29989, 29958,    13, 29909,  7306,   347,  2609,   367,\n",
      "         3907,  4078,   565,   540,   338,  1641, 15569,   373, 29889,  1105,\n",
      "        29892,   278,  1234,   338,   694, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100, 29909,  7306,   347,  2609,   367,\n",
      "         3907,  4078,   565,   540,   338,  1641, 15569,   373, 29889,  1105,\n",
      "        29892,   278,  1234,   338,   694, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 180142 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  5618,   338,   278,\n",
      "        11564,   310,  1554,  1641,   619,  1717,   304,  1554,  1683, 29973,\n",
      "           13,  5856, 29901,    13, 29899,  2313,   824,    13, 29899, 20820,\n",
      "           13, 29899,  1539,   481,  2015,    13, 29899,  2313,  5632, 16976,\n",
      "           13, 29899, 12814,    13, 12024, 29915, 29879,  4505,   372,  5232,\n",
      "        29889,  1281,   509,   579,   338,   263,  2106,   310,  1641, 19492,\n",
      "        11687,  1422,   515,  1554,  1683,   297,  3623,   486,   481,  4490,\n",
      "          470,  3802, 15477, 29889,  1281,   509,   579,   338,   278, 11564,\n",
      "          310,  1554,  1641,   619,  1717,   304,  1554,  1683,  3045,  1105,\n",
      "          278,  1234,   338, 12814, 29889,    13,    13, 11921,   297,   596,\n",
      "        19546, 12713,   508,   366,  3013,   366,  2343, 19531, 29973,    13,\n",
      "         5856, 29901,    13, 29899,  3762,    13, 29899,  2919,  3271,    13,\n",
      "        29899,  3699,    13, 29899,  3056,  1153,   384,    13, 29899,  4694,\n",
      "          300,    13, 12024, 29915, 29879,  4505,   372,  5232, 29889, 12252,\n",
      "        19531,   508,   367,  8126,   472,  3056,  1153,   384, 29889,  1670,\n",
      "         1795,   367,  3056,  1153,   384,   338,   297,   596, 19546, 12713,\n",
      "         3045,  1105,   278,  1234,   338,  3056,  1153,   384, 29889,    13,\n",
      "           13,  1576, 13013,   471,  6820, 18872,   304, 13175, 29892,   825,\n",
      "          892,   278, 13175, 16743,   411, 29973,    13,  5856, 29901,    13,\n",
      "        29899, 12045,  5622,    13, 29899,   437,   290,    13, 29899, 19912,\n",
      "         4045,    13, 29899,  2898,  3527,    13, 29899,  8444,  2264,    13,\n",
      "        12024, 29915, 29879,  4505,   372,  5232, 29889, 21882, 25700,  2367,\n",
      "        18872,   304,  6460,   322,   817, 29891, 13175, 29889,  3929,   272,\n",
      "          322,   817, 29891,  5304,  1090,  2898,  3527,  3045,  1105,   278,\n",
      "         1234,   338,  2898,  3527, 29889,    13,    13,  1576, 12736,   368,\n",
      "          471,  1925,   964,   263,   260,  4003, 29892,   988,   471,   372,\n",
      "         3216, 29973,    13,  5856, 29901,    13, 29899,  1016,  8842,    13,\n",
      "        29899,  1591,    13, 29899,  1236,   273,   329,   541,   357,    13,\n",
      "        29899, 14631,    13, 29899,   337,  1341,  4087,  1061,    13, 29966,\n",
      "        29989,   465, 22137, 29989, 29958,    13, 12024, 29915, 29879,  4505,\n",
      "          372,  5232, 29889,  3872,  8842,   338,   263,  2319,   285,  1255,\n",
      "          274,  1296,   310,  7901,  6302,   287,   270,   820, 29892, 12234,\n",
      "          297,   278,  8267,   310,   263,  8287,   470,  9228, 29889,   450,\n",
      "        12736,   368,   471,  1925,   964,   263,   260,  4003, 29936,   372,\n",
      "          471,  3216,   297,  1016,  8842,  3045,  1105,   278,  1234,   338,\n",
      "         1016,  8842, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100, 12024, 29915, 29879,  4505,\n",
      "          372,  5232, 29889,  3872,  8842,   338,   263,  2319,   285,  1255,\n",
      "          274,  1296,   310,  7901,  6302,   287,   270,   820, 29892, 12234,\n",
      "          297,   278,  8267,   310,   263,  8287,   470,  9228, 29889,   450,\n",
      "        12736,   368,   471,  1925,   964,   263,   260,  4003, 29936,   372,\n",
      "          471,  3216,   297,  1016,  8842,  3045,  1105,   278,  1234,   338,\n",
      "         1016,  8842, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 26934 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  4300,  9632,   304,\n",
      "         5176, 29901,    13,    13, 29930,   830,   790,  6742,   363, 16905,\n",
      "         9590, 29889,    13, 29966, 29989,   465, 22137, 29989, 29958,    13,\n",
      "        29930,   405,  6223,   585,   260,  3055,   479,  1671,  1153, 14125,\n",
      "        13698, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "        29930,   405,  6223,   585,   260,  3055,   479,  1671,  1153, 14125,\n",
      "        13698, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 13:01:07 - INFO - __main__ - ***** Running training *****\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) =  Num examples = 270152\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Num Epochs = 2\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/12/2023 13:01:07 - INFO - __main__ -  128\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Gradient Accumulation steps = 128\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Total optimization steps = 4222\n",
      "  0%|                                                  | 0/4222 [00:00<?, ?it/s]before train loop:\n",
      "GPU memory occupied: 14747 MB.\n",
      "torch.cuda.memory_allocated():  13552361472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                       | 1/4222 [00:31<37:07:52, 31.67s/it]07/12/2023 13:01:39 - INFO - __main__ -   Step: 1, LR: 1.5873015873015874e-07, Loss: 2.1495587825775146\n",
      "GPU memory occupied: 64167 MB.\n",
      "torch.cuda.memory_allocated():  13856768512\n",
      "  0%|                                       | 2/4222 [01:01<35:51:27, 30.59s/it]07/12/2023 13:02:09 - INFO - __main__ -   Step: 2, LR: 3.174603174603175e-07, Loss: 2.2048227787017822\n",
      "GPU memory occupied: 64941 MB.\n",
      "torch.cuda.memory_allocated():  13600457216\n",
      "  0%|                                       | 3/4222 [01:25<32:11:57, 27.48s/it]07/12/2023 13:02:33 - INFO - __main__ -   Step: 3, LR: 4.7619047619047623e-07, Loss: 2.066133737564087\n",
      "GPU memory occupied: 64941 MB.\n",
      "torch.cuda.memory_allocated():  13647059968\n",
      "  0%|                                       | 4/4222 [01:50<30:56:53, 26.41s/it]07/12/2023 13:02:57 - INFO - __main__ -   Step: 4, LR: 6.34920634920635e-07, Loss: 2.149609088897705\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13616460800\n",
      "  0%|                                       | 5/4222 [02:12<29:27:35, 25.15s/it]07/12/2023 13:03:20 - INFO - __main__ -   Step: 5, LR: 7.936507936507937e-07, Loss: 2.120454788208008\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13791988224\n",
      "  0%|                                       | 6/4222 [02:41<30:40:37, 26.19s/it]07/12/2023 13:03:49 - INFO - __main__ -   Step: 6, LR: 9.523809523809525e-07, Loss: 1.988757848739624\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13606858752\n",
      "  0%|                                       | 7/4222 [03:04<29:43:35, 25.39s/it]07/12/2023 13:04:12 - INFO - __main__ -   Step: 7, LR: 1.111111111111111e-06, Loss: 2.1727051734924316\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13607242752\n",
      "  0%|                                       | 8/4222 [03:26<28:09:31, 24.06s/it]07/12/2023 13:04:34 - INFO - __main__ -   Step: 8, LR: 1.26984126984127e-06, Loss: 2.1439826488494873\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13618892800\n",
      "  0%|                                       | 9/4222 [03:47<27:16:21, 23.30s/it]07/12/2023 13:04:55 - INFO - __main__ -   Step: 9, LR: 1.4285714285714286e-06, Loss: 2.0475826263427734\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13606730752\n",
      "  0%|                                      | 10/4222 [04:09<26:37:32, 22.76s/it]07/12/2023 13:05:17 - INFO - __main__ -   Step: 10, LR: 1.5873015873015873e-06, Loss: 2.175341844558716\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13640785920\n",
      "  0%|                                      | 11/4222 [04:31<26:17:33, 22.48s/it]07/12/2023 13:05:39 - INFO - __main__ -   Step: 11, LR: 1.746031746031746e-06, Loss: 2.0671279430389404\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13609802752\n",
      "  0%|                                      | 12/4222 [04:51<25:24:28, 21.73s/it]07/12/2023 13:05:59 - INFO - __main__ -   Step: 12, LR: 1.904761904761905e-06, Loss: 1.9760054349899292\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13613132800\n",
      "  0%|                                      | 13/4222 [05:15<26:19:37, 22.52s/it]07/12/2023 13:06:23 - INFO - __main__ -   Step: 13, LR: 2.0634920634920634e-06, Loss: 2.154667377471924\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13631056384\n",
      "  0%|▏                                     | 14/4222 [05:36<25:46:28, 22.05s/it]07/12/2023 13:06:44 - INFO - __main__ -   Step: 14, LR: 2.222222222222222e-06, Loss: 2.0096664428710938\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13611347456\n",
      "  0%|▏                                     | 15/4222 [05:57<25:20:09, 21.68s/it]07/12/2023 13:07:05 - INFO - __main__ -   Step: 15, LR: 2.380952380952381e-06, Loss: 1.9334514141082764\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13617740800\n",
      "  0%|▏                                     | 16/4222 [06:16<24:27:22, 20.93s/it]07/12/2023 13:07:24 - INFO - __main__ -   Step: 16, LR: 2.53968253968254e-06, Loss: 2.1369714736938477\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13622862336\n",
      "  0%|▏                                     | 17/4222 [06:40<25:29:42, 21.83s/it]07/12/2023 13:07:48 - INFO - __main__ -   Step: 17, LR: 2.6984126984126986e-06, Loss: 2.1751840114593506\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13624782336\n",
      "  0%|▏                                     | 18/4222 [07:00<25:00:18, 21.41s/it]07/12/2023 13:08:08 - INFO - __main__ -   Step: 18, LR: 2.8571428571428573e-06, Loss: 2.0311388969421387\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13606090752\n",
      "  0%|▏                                     | 19/4222 [07:21<24:51:09, 21.29s/it]07/12/2023 13:08:29 - INFO - __main__ -   Step: 19, LR: 3.015873015873016e-06, Loss: 1.9443316459655762\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607370752\n",
      "  0%|▏                                     | 20/4222 [07:41<24:15:36, 20.78s/it]07/12/2023 13:08:49 - INFO - __main__ -   Step: 20, LR: 3.1746031746031746e-06, Loss: 2.0267834663391113\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13599817216\n",
      "  0%|▏                                     | 21/4222 [08:04<24:57:26, 21.39s/it]07/12/2023 13:09:12 - INFO - __main__ -   Step: 21, LR: 3.3333333333333333e-06, Loss: 2.293339490890503\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13608522752\n",
      "  1%|▏                                     | 22/4222 [08:26<25:18:40, 21.70s/it]07/12/2023 13:09:34 - INFO - __main__ -   Step: 22, LR: 3.492063492063492e-06, Loss: 2.053500175476074\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13696606720\n",
      "  1%|▏                                     | 23/4222 [08:48<25:24:58, 21.79s/it]07/12/2023 13:09:56 - INFO - __main__ -   Step: 23, LR: 3.6507936507936507e-06, Loss: 2.0365335941314697\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13631952384\n",
      "  1%|▏                                     | 24/4222 [09:08<24:48:56, 21.28s/it]07/12/2023 13:10:16 - INFO - __main__ -   Step: 24, LR: 3.80952380952381e-06, Loss: 2.125044584274292\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13643729920\n",
      "  1%|▏                                     | 25/4222 [09:33<25:56:59, 22.26s/it]07/12/2023 13:10:41 - INFO - __main__ -   Step: 25, LR: 3.968253968253968e-06, Loss: 2.2882473468780518\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13639505920\n",
      "  1%|▏                                     | 26/4222 [09:54<25:39:15, 22.01s/it]07/12/2023 13:11:02 - INFO - __main__ -   Step: 26, LR: 4.126984126984127e-06, Loss: 2.098587989807129\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13633360384\n",
      "  1%|▏                                     | 27/4222 [10:15<25:03:45, 21.51s/it]07/12/2023 13:11:22 - INFO - __main__ -   Step: 27, LR: 4.2857142857142855e-06, Loss: 2.2885560989379883\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13599177216\n",
      "  1%|▎                                     | 28/4222 [10:39<26:05:23, 22.39s/it]07/12/2023 13:11:47 - INFO - __main__ -   Step: 28, LR: 4.444444444444444e-06, Loss: 1.9634195566177368\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13646931968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▎                                     | 29/4222 [11:00<25:30:06, 21.90s/it]07/12/2023 13:12:08 - INFO - __main__ -   Step: 29, LR: 4.603174603174604e-06, Loss: 2.1846425533294678\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13609034752\n",
      "  1%|▎                                     | 30/4222 [11:24<26:26:01, 22.70s/it]07/12/2023 13:12:32 - INFO - __main__ -   Step: 30, LR: 4.761904761904762e-06, Loss: 2.024883270263672\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13641553920\n",
      "  1%|▎                                     | 31/4222 [11:49<27:12:57, 23.38s/it]07/12/2023 13:12:57 - INFO - __main__ -   Step: 31, LR: 4.920634920634921e-06, Loss: 2.1247711181640625\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13728229888\n",
      "  1%|▎                                     | 32/4222 [12:10<26:22:46, 22.67s/it]07/12/2023 13:13:18 - INFO - __main__ -   Step: 32, LR: 5.07936507936508e-06, Loss: 2.2556421756744385\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13748969472\n",
      "  1%|▎                                     | 33/4222 [12:32<26:05:45, 22.43s/it]07/12/2023 13:13:40 - INFO - __main__ -   Step: 33, LR: 5.2380952380952384e-06, Loss: 2.040525197982788\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13721059840\n",
      "  1%|▎                                     | 34/4222 [12:56<26:30:54, 22.79s/it]07/12/2023 13:14:04 - INFO - __main__ -   Step: 34, LR: 5.396825396825397e-06, Loss: 1.9864712953567505\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13655893504\n",
      "  1%|▎                                     | 35/4222 [13:20<26:50:51, 23.08s/it]07/12/2023 13:14:28 - INFO - __main__ -   Step: 35, LR: 5.555555555555557e-06, Loss: 2.0973119735717773\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13807221760\n",
      "  1%|▎                                     | 36/4222 [13:41<26:15:48, 22.59s/it]07/12/2023 13:14:49 - INFO - __main__ -   Step: 36, LR: 5.7142857142857145e-06, Loss: 1.9493706226348877\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13621966336\n",
      "  1%|▎                                     | 37/4222 [14:03<26:08:18, 22.48s/it]07/12/2023 13:15:11 - INFO - __main__ -   Step: 37, LR: 5.873015873015874e-06, Loss: 2.276886463165283\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13666007552\n",
      "  1%|▎                                     | 38/4222 [14:26<26:20:50, 22.67s/it]07/12/2023 13:15:34 - INFO - __main__ -   Step: 38, LR: 6.031746031746032e-06, Loss: 2.124443292617798\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607626752\n",
      "  1%|▎                                     | 39/4222 [14:50<26:49:37, 23.09s/it]07/12/2023 13:15:58 - INFO - __main__ -   Step: 39, LR: 6.1904761904761914e-06, Loss: 2.173035144805908\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13665239552\n",
      "  1%|▎                                     | 40/4222 [15:13<26:40:05, 22.96s/it]07/12/2023 13:16:21 - INFO - __main__ -   Step: 40, LR: 6.349206349206349e-06, Loss: 2.138033628463745\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13613388800\n",
      "  1%|▎                                     | 41/4222 [15:34<25:58:14, 22.36s/it]07/12/2023 13:16:42 - INFO - __main__ -   Step: 41, LR: 6.507936507936509e-06, Loss: 2.203645706176758\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607626752\n",
      "  1%|▍                                     | 42/4222 [15:56<25:45:23, 22.18s/it]07/12/2023 13:17:04 - INFO - __main__ -   Step: 42, LR: 6.666666666666667e-06, Loss: 2.1471872329711914\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13617868800\n",
      "  1%|▍                                     | 43/4222 [16:15<24:49:39, 21.39s/it]07/12/2023 13:17:23 - INFO - __main__ -   Step: 43, LR: 6.825396825396826e-06, Loss: 2.0088717937469482\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13643089920\n",
      "  1%|▍                                     | 44/4222 [16:39<25:42:27, 22.15s/it]07/12/2023 13:17:47 - INFO - __main__ -   Step: 44, LR: 6.984126984126984e-06, Loss: 2.1421897411346436\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13605578752\n",
      "  1%|▍                                     | 45/4222 [17:02<26:03:43, 22.46s/it]07/12/2023 13:18:10 - INFO - __main__ -   Step: 45, LR: 7.1428571428571436e-06, Loss: 2.163747549057007\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13642193920\n",
      "  1%|▍                                     | 46/4222 [17:27<26:37:44, 22.96s/it]07/12/2023 13:18:35 - INFO - __main__ -   Step: 46, LR: 7.301587301587301e-06, Loss: 1.9889566898345947\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13806709760\n",
      "  1%|▍                                     | 47/4222 [17:49<26:25:59, 22.79s/it]07/12/2023 13:18:57 - INFO - __main__ -   Step: 47, LR: 7.460317460317461e-06, Loss: 2.114711046218872\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13665111552\n",
      "  1%|▍                                     | 48/4222 [18:14<27:03:40, 23.34s/it]07/12/2023 13:19:22 - INFO - __main__ -   Step: 48, LR: 7.61904761904762e-06, Loss: 1.8832544088363647\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13607882752\n",
      "  1%|▍                                     | 49/4222 [18:43<29:03:37, 25.07s/it]07/12/2023 13:19:51 - INFO - __main__ -   Step: 49, LR: 7.77777777777778e-06, Loss: 2.012969732284546\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13758701056\n",
      "  1%|▍                                     | 50/4222 [19:06<28:23:18, 24.50s/it]07/12/2023 13:20:14 - INFO - __main__ -   Step: 50, LR: 7.936507936507936e-06, Loss: 1.881207823753357\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13611852800\n",
      "  1%|▍                                     | 51/4222 [19:29<27:54:22, 24.09s/it]07/12/2023 13:20:37 - INFO - __main__ -   Step: 51, LR: 8.095238095238097e-06, Loss: 1.9218477010726929\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13642321920\n",
      "  1%|▍                                     | 52/4222 [19:55<28:30:00, 24.60s/it]07/12/2023 13:21:03 - INFO - __main__ -   Step: 52, LR: 8.253968253968254e-06, Loss: 2.3106133937835693\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13809271808\n",
      "  1%|▍                                     | 53/4222 [20:20<28:34:53, 24.68s/it]07/12/2023 13:21:28 - INFO - __main__ -   Step: 53, LR: 8.412698412698414e-06, Loss: 1.8654088973999023\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13638353920\n",
      "  1%|▍                                     | 54/4222 [20:44<28:25:02, 24.54s/it]07/12/2023 13:21:52 - INFO - __main__ -   Step: 54, LR: 8.571428571428571e-06, Loss: 2.1221580505371094\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13630800384\n",
      "  1%|▍                                     | 55/4222 [21:07<28:02:57, 24.23s/it]07/12/2023 13:22:15 - INFO - __main__ -   Step: 55, LR: 8.730158730158731e-06, Loss: 2.2923853397369385\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13611347456\n",
      "  1%|▌                                     | 56/4222 [21:30<27:21:38, 23.64s/it]07/12/2023 13:22:38 - INFO - __main__ -   Step: 56, LR: 8.888888888888888e-06, Loss: 2.0254714488983154\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13610058752\n",
      "  1%|▌                                     | 57/4222 [21:51<26:32:53, 22.95s/it]07/12/2023 13:22:59 - INFO - __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.0491456985473633\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13601737216\n",
      "  1%|▌                                     | 58/4222 [22:12<26:02:31, 22.51s/it]07/12/2023 13:23:20 - INFO - __main__ -   Step: 58, LR: 9.206349206349207e-06, Loss: 2.109394073486328\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13667543552\n",
      "  1%|▌                                     | 59/4222 [22:34<25:46:58, 22.30s/it]07/12/2023 13:23:42 - INFO - __main__ -   Step: 59, LR: 9.365079365079366e-06, Loss: 2.172915458679199\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13666903552\n",
      "  1%|▌                                     | 60/4222 [22:58<26:08:58, 22.62s/it]07/12/2023 13:24:06 - INFO - __main__ -   Step: 60, LR: 9.523809523809525e-06, Loss: 2.035609483718872\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13603402752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▌                                     | 61/4222 [23:21<26:16:36, 22.73s/it]07/12/2023 13:24:29 - INFO - __main__ -   Step: 61, LR: 9.682539682539683e-06, Loss: 1.951716661453247\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13657301504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|▏         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 2 GPUs, 1 batch size per GPU, 64 gradient accumulation steps.\n",
      "[{'jbsub_cmd': 'jbsub -queue x86_12h -name ft-trainer -mem 64g -cores 1x32+2 -require a100_80gb -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo torchrun --nproc_per_node=2 --master_port=10001 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 64 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 100 --save_total_limit 1 --num_train_epochs 2 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap \"LlamaDecoderLayer\" --gradient_checkpointing --bf16 True --tf32 True --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/huggyllama:llama-7b_humanmix\"; echo \"======\"; torchrun --nproc_per_node=2 --master_port=10001 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 64 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 100 --save_total_limit 1 --num_train_epochs 2 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap \"LlamaDecoderLayer\" --gradient_checkpointing --bf16 True --tf32 True --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/huggyllama:llama-7b_humanmix\"; [ ! -f \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/${LSB_JOBID}*.out\" ] || mv \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/${LSB_JOBID}*.out\" \"results/huggyllama:llama-7b_humanmix\"\\'', 'job_id': 1944172}]\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft-trainer'\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "job_duration = None\n",
    "shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 32\n",
    "cpu_mem = 64\n",
    "num_gpus = 2; require = 'a100_80gb'\n",
    "\n",
    "overwrite_output_dir = False\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "\n",
    "num_train_epochs = 2\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "\n",
    "fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"\n",
    "if 'gpt2' in model_name_or_path: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in model_name_or_path: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 128 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "gradient_checkpointing = True\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "abbr_model_name = model_name_or_path.replace('/', ':')\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "if test_run:\n",
    "    output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "exe = 'python' if num_gpus == 1 else f\"torchrun --nproc_per_node={num_gpus} --master_port=10001\"\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}{exe}\n",
    "    open_instruct/finetune_trainer.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    {'--lora_rank '+str(lora_rank) if use_lora else ''}\n",
    "    {'--lora_alpha '+str(lora_alpha) if use_lora else ''}\n",
    "    {'--lora_dropout '+str(lora_dropout) if use_lora else ''}\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --save_strategy {save_strategy} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --save_total_limit 1 \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    {'--fsdp \"'+fsdp+'\"' if fsdp else ''}\n",
    "    {'--fsdp_transformer_layer_cls_to_wrap \"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "        if fsdp else ''}\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "    --bf16 True \\\n",
    "    --tf32 True \\\n",
    "    --report_to tensorboard \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    --output_dir \"{output_dir}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    "    job_duration=job_duration,\n",
    "    shell_scripts_modification_fn=shell_scripts_modification_fn,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30877747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/usr/local/share/zsh/site-functions'), PosixPath('/u/wpq/.oh-my-zsh/completions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1886966.tmpdir/.1689346154.1886966.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}'), PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/14/2023 14:10:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "07/14/2023 14:10:12 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix/runs/Jul14_14-10-12_cccxc552,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=200,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14/2023 14:10:13 - INFO - datasets.builder - Using custom data configuration default-247ebf1b4910b0d3\n",
      "07/14/2023 14:10:13 - INFO - datasets.info - Loading Dataset Infos from /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "07/14/2023 14:10:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "07/14/2023 14:10:13 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "07/14/2023 14:10:13 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "07/14/2023 14:10:13 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.29s/it]\n",
      "[INFO|configuration_utils.py:669] 2023-07-14 14:10:24,678 >> loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-07-14 14:10:24,684 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,725 >> loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,725 >> loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,726 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,726 >> loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,726 >> loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2578] 2023-07-14 14:10:25,375 >> loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-07-14 14:10:25,381 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:577] 2023-07-14 14:10:25,381 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:56<00:00, 58.13s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-07-14 14:13:08,348 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-07-14 14:13:08,348 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:539] 2023-07-14 14:13:08,395 >> loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-07-14 14:13:08,396 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/14/2023 14:13:15 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 268435456 || all params: 7006859264 || trainable%: 3.8310382139280827\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00000_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00001_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00002_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00003_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00004_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00005_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00006_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00007_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00008_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00009_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00010_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00011_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00012_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00013_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00014_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00015_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14/2023 14:13:52 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_*_of_00016.arrow\n",
      "07/14/2023 14:13:52 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "[INFO|trainer.py:776] 2023-07-14 14:14:10,575 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: dataset, messages, id. If dataset, messages, id are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1786] 2023-07-14 14:14:10,604 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2023-07-14 14:14:10,604 >>   Num examples = 270,679\n",
      "[INFO|trainer.py:1788] 2023-07-14 14:14:10,604 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1789] 2023-07-14 14:14:10,604 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1790] 2023-07-14 14:14:10,605 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1791] 2023-07-14 14:14:10,605 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1792] 2023-07-14 14:14:10,605 >>   Total optimization steps = 4,228\n",
      "[INFO|trainer.py:1793] 2023-07-14 14:14:10,606 >>   Number of trainable parameters = 268,435,456\n",
      "  0%|                                                  | 0/4228 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-07-14 14:14:11,413 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,454 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,467 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,481 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,510 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,522 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,553 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,592 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 2.1112, 'learning_rate': 1.5748031496062994e-07, 'epoch': 0.0}         \n",
      "{'loss': 1.9091, 'learning_rate': 3.149606299212599e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.3673, 'learning_rate': 4.724409448818898e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.3104, 'learning_rate': 6.299212598425198e-07, 'epoch': 0.0}          \n",
      "{'loss': 1.9968, 'learning_rate': 7.874015748031496e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.2831, 'learning_rate': 9.448818897637796e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.1034, 'learning_rate': 1.1023622047244096e-06, 'epoch': 0.0}         \n",
      "{'loss': 3.4951, 'learning_rate': 1.2598425196850396e-06, 'epoch': 0.0}         \n",
      "{'loss': 3.7316, 'learning_rate': 1.4173228346456693e-06, 'epoch': 0.0}         \n",
      "{'loss': 2.0593, 'learning_rate': 1.5748031496062992e-06, 'epoch': 0.0}         \n",
      "{'loss': 1.9925, 'learning_rate': 1.7322834645669292e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.2873, 'learning_rate': 1.8897637795275591e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.0844, 'learning_rate': 2.0472440944881893e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.2608, 'learning_rate': 2.2047244094488192e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.9228, 'learning_rate': 2.362204724409449e-06, 'epoch': 0.01}         \n",
      "{'loss': 4.0822, 'learning_rate': 2.519685039370079e-06, 'epoch': 0.01}         \n",
      "{'loss': 4.6403, 'learning_rate': 2.677165354330709e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.5807, 'learning_rate': 2.8346456692913386e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.0202, 'learning_rate': 2.992125984251969e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.544, 'learning_rate': 3.1496062992125985e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.1635, 'learning_rate': 3.307086614173229e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.9579, 'learning_rate': 3.4645669291338583e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.9001, 'learning_rate': 3.6220472440944887e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.9951, 'learning_rate': 3.7795275590551182e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.8361, 'learning_rate': 3.937007874015748e-06, 'epoch': 0.01}         \n",
      "{'loss': 3.3521, 'learning_rate': 4.0944881889763785e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.8555, 'learning_rate': 4.251968503937008e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.0398, 'learning_rate': 4.4094488188976384e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.5598, 'learning_rate': 4.566929133858268e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7405, 'learning_rate': 4.724409448818898e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.8333, 'learning_rate': 4.881889763779528e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7384, 'learning_rate': 5.039370078740158e-06, 'epoch': 0.02}         \n",
      "{'loss': 1.8449, 'learning_rate': 5.196850393700788e-06, 'epoch': 0.02}         \n",
      "  1%|▎                                     | 33/4228 [12:43<27:08:04, 23.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd .. && python open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 256 --lora_alpha 256 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 1 --num_train_epochs 2 --bf16 True --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "g\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c6efdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/huggyllama:llama-7b_human_mix-trainer_savebystep/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m [glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint*\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m models \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m      4\u001b[0m models\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/huggyllama:llama-7b_human_mix-trainer_savebystep/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint*\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m models]\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m models \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m      4\u001b[0m models\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "models = ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "models = [x for l in models for x in l]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acfe2f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mmlu, 0-shot\n",
    "# gsm, 8-shot\n",
    "# bbh, 0-shot\n",
    "# bbh, 3-shot\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path results/baselines/huggyllama/llama-7b --save_dir results/baselines/huggyllama/llama-7b/eval/humaneval --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1\n"
     ]
    }
   ],
   "source": [
    "# task_name = 'bbh_s=0'\n",
    "# task_name = 'bbh_s=3'\n",
    "# task_name = 'gsm'\n",
    "# task_name = 'mmlu'\n",
    "task_name = 'humaneval'\n",
    "job_name = f'eval.{task_name}'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "batch_size = 10\n",
    "if task_name == 'gsm':\n",
    "    queue = 'x86_6h'\n",
    "if task_name == 'bbh_s=0':\n",
    "    queue = 'x86_1h'\n",
    "if task_name == 'bbh_s=3':\n",
    "    queue = 'x86_6h'\n",
    "    batch_size = 5 # for longer prompts.\n",
    "if task_name == 'mmlu':\n",
    "    queue = 'x86_1h'\n",
    "    batch_size = 10\n",
    "if task_name == 'humaneval':\n",
    "    queue = 'x86_1h' # pass@1: 10min, pass@10: 100min\n",
    "    batch_size = 10\n",
    "    \n",
    "num_cpus = 10\n",
    "cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "\n",
    "use_chat_format = False\n",
    "load_in_8bit = False\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "models += ['huggyllama/llama-7b'] # , 'mosaicml/mpt-7b'\n",
    "models = [os.path.join('results/baselines', x) for x in models]\n",
    "\n",
    "\n",
    "# models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "# models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "# models = [x for l in models for x in l]\n",
    "# models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400']\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    if use_chat_format:\n",
    "        save_dir += '_chatfmt'\n",
    "    \n",
    "    if task_name == 'mmlu':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --save_dir {model_name_or_path}/eval/mmlu/ \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain 0 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name == 'gsm':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --save_dir {save_dir} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --n_shot 8 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('bbh'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --save_dir {save_dir} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_cot' if 's=0' in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path {model_name_or_path} \\\n",
    "            --save_dir {save_dir} \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "            {'--load_in_8bit' if load_in_8bit else ''}\n",
    "        \"\"\"\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job_ccc(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        queue=queue,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "    )\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbh_s=0', 'bbh_s=3', 'humaneval', 'gsm', 'mmlu']\n",
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'bbh_s=3', 'gsm_chatfmt', 'gsm', 'mmlu']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th>model</th>\n",
       "      <th>mmlu</th>\n",
       "      <th>gsm</th>\n",
       "      <th>bbh_s=0</th>\n",
       "      <th>bbh_s=3</th>\n",
       "      <th>humaneval</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "      <th>average_acc</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>average_exact_match</th>\n",
       "      <th>average_exact_match</th>\n",
       "      <th>pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/baselines/huggyllama/llama-7b/</td>\n",
       "      <td>0.318687</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.316671</td>\n",
       "      <td>0.334654</td>\n",
       "      <td>0.140244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.100834</td>\n",
       "      <td>0.332591</td>\n",
       "      <td>0.343629</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task_name                                                                        model  \\\n",
       "metrics                                                                                  \n",
       "0                                            ../results/baselines/huggyllama/llama-7b/   \n",
       "0          ../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400   \n",
       "\n",
       "task_name        mmlu         gsm             bbh_s=0             bbh_s=3  \\\n",
       "metrics   average_acc exact_match average_exact_match average_exact_match   \n",
       "0            0.318687    0.109932            0.316671            0.334654   \n",
       "0            0.333001    0.100834            0.332591            0.343629   \n",
       "\n",
       "task_name humaneval  \n",
       "metrics      pass@1  \n",
       "0          0.140244  \n",
       "0               NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        \n",
    "    def get_result_df(self):\n",
    "\n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "\n",
    "            index = [(task_name, k) for k in metrics.keys()]\n",
    "            multi_columns = pd.MultiIndex.from_tuples(index, names=['task_name', 'metrics'])\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=multi_columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        cols = [\n",
    "            ('mmlu', 'average_acc'),\n",
    "            ('gsm', 'exact_match'),\n",
    "            ('bbh_s=0', 'average_exact_match'),\n",
    "            ('bbh_s=3', 'average_exact_match'),\n",
    "            ('humaneval', 'pass@1')\n",
    "        ]\n",
    "        cols = [col for col in cols if col in df]\n",
    "        print(task_names)\n",
    "        df = df[cols]\n",
    "        df.insert(0, ('model', ''), [self.save_dir])\n",
    "        return df\n",
    "\n",
    "\n",
    "save_dirs = [\n",
    "    '../results/baselines/huggyllama/llama-7b/',\n",
    "    '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400',\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for save_dir in save_dirs:\n",
    "    r = EvalResults(save_dir)\n",
    "    df = r.get_result_df()\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cpu_time (hr)</th>\n",
       "      <th>avg_mem</th>\n",
       "      <th>max_mem</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.491738</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>0.071833</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.018196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.957783</td>\n",
       "      <td>1.317383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>0.208883</td>\n",
       "      <td>6.689150</td>\n",
       "      <td>11.693359</td>\n",
       "      <td>0.013647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>0.458394</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>33.018555</td>\n",
       "      <td>0.009098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huggyllama/llama-7b</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.639141</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.109932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  cpu_time (hr)   avg_mem    max_mem  exact_match\n",
       "0             t5-small       0.038869  0.491738   0.597656     0.014405\n",
       "1              t5-base       0.071833  0.729512   0.787109     0.018196\n",
       "2             t5-large       0.021314  0.957783   1.317383     0.000000\n",
       "3                t5-3b       0.208883  6.689150  11.693359     0.013647\n",
       "4               t5-11b       0.458394  0.872705  33.018555     0.009098\n",
       "5  huggyllama/llama-7b       0.790400  0.639141   0.710938     0.109932"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38e2e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = [\n",
    "    # human-authored\n",
    "    'SuperNI',\n",
    "    'CoT',\n",
    "    'Flan V2',\n",
    "    'Dolly',\n",
    "    'Open Assistant 1',\n",
    "    # gpt-generated\n",
    "    'Self-instruct',\n",
    "    'Unnatural Instructions',\n",
    "    'Alpaca',\n",
    "    'Code-Alpaca',\n",
    "    'GPT4-Alpaca',\n",
    "    'Baize',\n",
    "    'ShareGPT',\n",
    "]\n",
    "N = len(datasets)\n",
    "\n",
    "def encode_input(dataset):\n",
    "    if dataset == 'LLaMa-13B':\n",
    "        return [0]*N\n",
    "    if dataset == 'Human data mix':\n",
    "        # FLAN V2, CoT, Dolly, and Open Assistant 1\n",
    "        return [0,1,1,1,1,0,0,0,0,0,0,0]\n",
    "    if dataset == 'Human+GPT data mix.':\n",
    "        # FLAN V2, CoT, Dolly, and Open Assistant 1, GPT4-Alpaca, Code-Alpaca, and ShareGPT.\n",
    "        return [0,1,1,1,1,0,0,0,1,1,0,1]\n",
    "    x = [0]*N\n",
    "    i = datasets.index(dataset)\n",
    "    x[i] = 1\n",
    "    return x\n",
    "\n",
    "import numpy as np\n",
    "X = [encode_input(x[0]) for x in data]\n",
    "X = np.array(X)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e63f5842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>GSM</th>\n",
       "      <th>BBH</th>\n",
       "      <th>TydiQA</th>\n",
       "      <th>Codex-Eval</th>\n",
       "      <th>AlpacaFarm</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLaMa-13B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SuperNI</td>\n",
       "      <td>7.3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-34.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-13.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CoT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flan V2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dolly</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-10.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>18.3</td>\n",
       "      <td>30.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Open Assistant 1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>55.2</td>\n",
       "      <td>37.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Self-instruct</td>\n",
       "      <td>-12.2</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-13.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Unnatural Instructions</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-4.1</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alpaca</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-14.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Code-Alpaca</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>21.3</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GPT4-Alpaca</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>63.6</td>\n",
       "      <td>36.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Baize</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-13.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>33.9</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ShareGPT</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69.1</td>\n",
       "      <td>39.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Human data mix</td>\n",
       "      <td>7.9</td>\n",
       "      <td>22.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>38.5</td>\n",
       "      <td>39.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Human+GPT data mix.</td>\n",
       "      <td>6.7</td>\n",
       "      <td>22.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>57.2</td>\n",
       "      <td>44.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  MMLU   GSM   BBH TydiQA Codex-Eval  AlpacaFarm  \\\n",
       "0                LLaMa-13B   0.0   0.0   0.0    0.0        0.0         NaN   \n",
       "1                  SuperNI   7.3 -10.0 -34.1    4.0      -13.5         5.0   \n",
       "2                      CoT   2.0  25.5   2.1    4.8       -3.3         4.7   \n",
       "3                  Flan V2   8.2   7.0   2.3    0.1      -10.4         5.3   \n",
       "4                    Dolly   2.8   3.0 -10.9   -0.6        4.8        18.3   \n",
       "5         Open Assistant 1   0.6   2.0   1.6   -9.1        5.2        55.2   \n",
       "6            Self-instruct -12.2  -5.0  -7.3   -7.0      -13.2         7.3   \n",
       "7   Unnatural Instructions   3.7  -6.5  -4.1   -8.1       -1.8        10.8   \n",
       "8                   Alpaca   2.6  -6.0  -2.4  -14.6        1.0        33.2   \n",
       "9              Code-Alpaca   0.1  -2.0  -0.3   -6.1        7.9        21.3   \n",
       "10             GPT4-Alpaca   4.5   0.0   1.4  -23.0        5.9        63.6   \n",
       "11                   Baize   1.0  -5.5  -0.2  -13.5        0.7        33.9   \n",
       "12                ShareGPT   6.7   2.0   3.2  -17.3        5.0        69.1   \n",
       "13          Human data mix   7.9  22.5   2.5    2.4       -2.9        38.5   \n",
       "14     Human+GPT data mix.   6.7  22.5   5.9   -1.3        8.4        57.2   \n",
       "\n",
       "    Average  \n",
       "0       NaN  \n",
       "1      21.0  \n",
       "2      33.9  \n",
       "3      30.0  \n",
       "4      30.8  \n",
       "5      37.1  \n",
       "6      21.7  \n",
       "7      26.9  \n",
       "8      30.2  \n",
       "9      31.4  \n",
       "10     36.6  \n",
       "11     30.6  \n",
       "12     39.3  \n",
       "13     39.7  \n",
       "14     44.5  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    ['LLaMa-13B', 42.5, 14.0, 36.9, 47.4, 26.6, None, None],\n",
    "    ['SuperNI', 49.8, 4.0, 2.8, 51.4, 13.1, 5.0, 21.0],\n",
    "    ['CoT', 44.5, 39.5, 39.0, 52.2, 23.3, 4.7, 33.9],\n",
    "    ['Flan V2', 50.7, 21.0, 39.2, 47.5, 16.2, 5.3, 30.0],\n",
    "    ['Dolly', 45.3, 17.0, 26.0, 46.8, 31.4, 18.3, 30.8],\n",
    "    ['Open Assistant 1', 43.1, 16.0, 38.5, 38.3, 31.8, 55.2, 37.1],\n",
    "    ['Self-instruct', 30.3, 9.0, 29.6, 40.4, 13.4, 7.3, 21.7],\n",
    "    ['Unnatural Instructions', 46.2, 7.5, 32.8, 39.3, 24.8, 10.8, 26.9],\n",
    "    ['Alpaca', 45.1, 8.0, 34.5, 32.8, 27.6, 33.2, 30.2],\n",
    "    ['Code-Alpaca', 42.6, 12.0, 36.6, 41.3, 34.5, 21.3, 31.4],\n",
    "    ['GPT4-Alpaca', 47.0, 14.0, 38.3, 24.4, 32.5, 63.6, 36.6],\n",
    "    ['Baize', 43.5, 8.5, 36.7, 33.9, 27.3, 33.9, 30.6],\n",
    "    ['ShareGPT', 49.2, 16.0, 40.1, 30.1, 31.6, 69.1, 39.3],\n",
    "    ['Human data mix', 50.4, 36.5, 39.4, 49.8, 23.7, 38.5, 39.7],\n",
    "    ['Human+GPT data mix.', 49.2, 36.5, 42.8, 46.1, 35.0, 57.2, 44.5]\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    'Model', 'MMLU', 'GSM', 'BBH', 'TydiQA', 'Codex-Eval', 'AlpacaFarm',\n",
    "    'Average'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "tasks = ['MMLU', 'GSM', 'BBH', 'TydiQA', 'Codex-Eval']\n",
    "base_model_perf = df.iloc[0][tasks]\n",
    "df[tasks] = df[tasks].subtract(base_model_perf)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4053138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some todo:\n",
    "# 1. think about how to normalize input/output.\n",
    "# 2. think about normalize w.r.t. number of training tokens, data points.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93caf497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=Ridge(fit_intercept=False, random_state=123))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=Ridge(fit_intercept=False, random_state=123))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(fit_intercept=False, random_state=123)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(fit_intercept=False, random_state=123)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=Ridge(fit_intercept=False, random_state=123))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# # y = 1 * x_0 + 2 * x_1 + 3\n",
    "# y = np.dot(X, np.array([1, 2])) + 3\n",
    "# reg = LinearRegression().fit(X, y)\n",
    "# reg.score(X, y)\n",
    "# reg.coef_\n",
    "# reg.intercept_\n",
    "# reg.predict(np.array([[3, 5]]))\n",
    "\n",
    "X_train = X[1:-2]\n",
    "y_train = df[tasks].to_numpy()[1:-2]\n",
    "\n",
    "X_test = X[-2:]\n",
    "y_test = df[tasks].to_numpy()[-2:]\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import load_linnerud\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regr = MultiOutputRegressor(Ridge(random_state=123, fit_intercept=False)).fit(X_train, y_train)\n",
    "regr\n",
    "# regr.predict(X[[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bba68335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.axis.XTick at 0x14f358cbb670>,\n",
       " <matplotlib.axis.XTick at 0x14f358cbb6d0>,\n",
       " <matplotlib.axis.XTick at 0x14f35f11ee60>,\n",
       " <matplotlib.axis.XTick at 0x14f3590cf4c0>,\n",
       " <matplotlib.axis.XTick at 0x14f359087490>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcYAAAJACAYAAAAXY8ZxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8rElEQVR4nO3deVxN+f8H8NdtX26btCEtCiEpWZJdKfs69qWxzIxlMBiEjD37vhvEmLHN2IaxNjK2QciaLaVmFL6oJEr1+f1hOr+ublSqW3o9H4/zmO5ZPud9bsPb+3M+53xkQggBIiIiAgCoqToAIiKi4oSJkYiIKAsmRiIioiyYGImIiLJgYiQiIsqCiZGIiCgLJkYiIqIsmBiJiIiy0FB1AESqlpGRgUePHsHAwAAymUzV4RAVKSEEXr58iXLlykFN7f9rpTdv3iA1NTVfbWppaUFHR6egQixyTIxU6j169AjW1taqDoNIpWJiYlChQgUA75Kina4u4vLZlqWlJSIjI0tscmRipFLPwMAAABAzbx4MdXVVHE12V136qToEpRo3VnUEOUv47biqQ1DqYIqXqkPIJjk5EYMGWUt/DgAgNTUVcQBiZDIY5rG9RADWcXFITU1lYiQqqTK7Tw11dYtlYpTL8/pXExnq66s6BKX01Ivv71LZbQRDAIZ5vb3wGbx+m4mRiIiUU1MD8pMY09MLJ54iwlGpREREWbBiJCIi5VgxEhEREStGIiJSLr8VYwnHipGIiCgLVoxERKRcKa0YmRiJiEi5UpoY2ZVKRESUBStGIiJSjhUjERERsWIkIiLlWDFSaff06VMMGTIEFStWhLa2NiwtLeHj44MzZ86oOjQA715yrKOjg4cPHyqs79ixI/z8/KTPfn5+6NixY9EGR0SfDVaMJOnSpQtSU1OxefNm2Nvb4/HjxwgODsazZ89UGldqaiq0tLQAvEuOU6ZMwebNm1UaE1GpwIqRSrP4+HicOnUKc+fORbNmzWBjY4O6devC398f7du3R1RUFGQyGcLCwhSOkclkCAkJAQCEhIRAJpPh4MGDqFmzJnR0dFC/fn3cuHFD4VynT59Go0aNoKurC2tra4wYMQKvXr2Sttva2mLGjBno168fDA0N8dVXX0nbhg8fjq1bt2Zrk4iooDAxEgBALpdDLpdj7969SElJ+aS2vv/+eyxcuBAXL16EmZkZ2rVrh7dv3wIAIiIi4Ovriy5duuDatWvYsWMHTp8+jeHDhyu0sWDBAri4uODKlSsICAiQ1nt6eqJt27aYMGFCvuNLSUlBYmKiwkJESshk76rGvCx5rTCLISZGAgBoaGggKCgImzdvhrGxMTw9PTFx4kRcu3Ytz2398MMP8Pb2hrOzMzZv3ozHjx9jz549AIDAwED07t0bo0aNgqOjIxo0aIBly5Zhy5YtePPmjdRG8+bNMWbMGFSqVAmVKlVSaD8wMBCHDx/GqVOn8nWtgYGBMDIykhZra+t8tUNEnycmRpJ06dIFjx49wv79++Hr64uQkBC4ubkhKCgoT+14eHhIP5cpUwZVqlRBeHg4AODq1asICgqSKlS5XA4fHx9kZGQgMjJSOs7d3T3H9qtVq4Z+/frlu2r09/dHQkKCtMTExOSrHaLPXl6rxcylhOPgG1Kgo6MDb29veHt7IyAgAIMGDcIPP/wgVWciy431zO7RvEhKSsLXX3+NESNGZNtWsWJF6Wd9ff0PtjNt2jRUrlwZe/fuzXMM2tra0NbWzvNxRKXOZ5Lo8qr0XTHlSbVq1fDq1SuYmZkBAGJjY6VtWQfiZPX3339LP7948QJ3796Fk5MTAMDNzQ23bt2Cg4NDtiVz5GluWFtbY/jw4Zg4cSLSS/ikqERUvDAxEgDg2bNnaN68ObZu3Ypr164hMjISu3btwrx589ChQwfo6uqifv36mDNnDsLDw3Hy5ElMnjxZaVvTp09HcHAwbty4AT8/P5QtW1Z6rnD8+PE4e/Yshg8fjrCwMNy7dw/79u3LNvgmN/z9/fHo0SMcP378Uy6diHJSSrtSS/4VUIGQy+WoV68eFi9ejMaNG6NGjRoICAjA4MGDsWLFCgDAxo0bkZaWhtq1a2PUqFGYOXOm0rbmzJmDkSNHonbt2oiLi8Pvv/8uVYM1a9bEyZMncffuXTRq1Aiurq6YMmUKypUrl+eYy5Qpg/HjxysM2iEi+lQyIT6DpzGpWAgJCUGzZs3w4sULGBsbqzqcXEtMTISRkRESli+Hoa6uqsPJ5orbQFWHoJSbm6ojyJk4fETVISi1742PqkPIJjk5Eb16GSEhIQGGhoYAsvyZKFcOhnmsABMzMmD06JFCeyUNK0YiIqIsOCqViIiU+0zuGeYVEyMVmKZNm4I980RU0jExEhGRcqwYiYiIsiilibH0XTEREdEHsGIkIiLlWDESERERK0YiIlIucz7GvPgMRqazYiQiIsqCFSMRESmXn3uMrBiJiIg+L6wYiTJVqwZ8ZIJkVXB1k6k6BKVElnk3i5244jnjSoeWr1UdQjaJiR+IqQgqxr/++gvz58/HpUuXEBsbiz179kjT1L1rTuCHH37A+vXrER8fD09PT6xevRqOjo55iysPWDESEZFyRTAf46tXr+Di4oKVK1cq3T5v3jwsW7YMa9aswfnz56Gvrw8fH59CnW6OFSMREalMq1at0KpVK6XbhBBYsmQJJk+ejA4dOgAAtmzZAgsLC+zduxc9evQolJhYMRIRkXKfUDEmJiYqLCkpKXk+fWRkJOLi4uDl5SWtMzIyQr169XDu3LkCu8z3MTESEVGBs7a2hpGRkbQEBgbmuY24uDgAgIWFhcJ6CwsLaVthYFcqEREp9wmDb2JiYmBoaCit1tbWLsjIChUrRiIiKnCGhoYKS34So6WlJQDg8ePHCusfP34sbSsMTIxERKRcEYxK/RA7OztYWloiODhYWpeYmIjz58/Dw8OjwM7zPnalEhGRyiQlJeH+/fvS58jISISFhaFMmTKoWLEiRo0ahZkzZ8LR0RF2dnYICAhAuXLlFJ51LGhMjEREpFwRPOAfGhqKZs2aSZ9Hjx4NAOjfvz+CgoIwbtw4vHr1Cl999RXi4+PRsGFDHD58GDo6OnmLKw+YGImISLkiSIxNmzaF+MAxMpkM06dPx/Tp0/MWxyfgPUYiIqIsWDESEZFy+ZmPMSOjcGIpQqwYiYiIsmDFSEREyuXnHmMBPq6hKiX/CqhEi4uLw7fffgt7e3toa2vD2toa7dq1U3huKSdNmzaFTCbLcWnatGnhXwARfXZYMZLKREVFwdPTE8bGxpg/fz6cnZ3x9u1bHDlyBMOGDcPt27c/ePzu3buRmpoK4N3rp+rWrYvjx4+jevXqAAAtLa1Cvwaiz1oprRiZGEllhg4dCplMhgsXLkA/ywTB1atXx4ABAwAA0dHR+PbbbxEcHAw1NTX4+vpi+fLlsLCwQJkyZaRjMudmMzU1LdRXRRHR56/kp3YqkZ4/f47Dhw9j2LBhCkkxk7GxMTIyMtChQwc8f/4cJ0+exLFjx/DgwQN07979k86dkpKSbUocIlJCxa+EUxVWjKQS9+/fhxACVatWzXGf4OBgXL9+HZGRkbC2tgbwbpLS6tWr4+LFi6hTp06+zh0YGIhp06bl61iiUqWUdqWW/CugEulDb7rIFB4eDmtraykpAkC1atVgbGyM8PDwfJ/b398fCQkJ0hITE5Pvtojo88OKkVTC0dERMpnsowNsCoO2tnaJmhuOSGVYMRIVnTJlysDHxwcrV67Eq1evsm2Pj4+Hk5MTYmJiFCq6W7duIT4+HtWqVSvKcImoFGFiJJVZuXIl0tPTUbduXfz222+4d+8ewsPDsWzZMnh4eMDLywvOzs7o3bs3Ll++jAsXLqBfv35o0qQJ3N3dVR0+0eevlA6+KflXQCWWvb09Ll++jGbNmmHMmDGoUaMGvL29ERwcjNWrV0Mmk2Hfvn0wMTFB48aN4eXlBXt7e+zYsUPVoRPRZ4z3GEmlrKyssGLFCqxYsULp9ooVK2Lfvn0fbcfW1jZXA3qIKA94j5GIiIhYMRIRkXKltGJkYiQiIuXyMx+jTFY4sRShkp/aiYiIChArRiIiUq6UdqWW/CsgIiIqQKwYiYhIOVaMRERExIqRiIiUY8VIRERErBiJiEi5UloxMjESEZFypTQxlvwrICIiKkCsGIkyXboE6OqqOorsNm5UdQTKZWSoOoKcFdOJrC/eKH7/fyUlvc15IytGIiIiYsVIRETKsWIkIiIiVoxERKQcK0YiIiJixUhERMqV0omKmRiJiEg5dqUSERERK0YiIlKOFSMRERGxYiQiIuVYMRIRERErRiIiUo4VI1HuNG3aFKNGjVJ1GEREhYKJkbLx8/ODTCbLtty/f79Izv/48WNoampi+/btSrcPHDgQbm5uAID169ejUaNGMDExgYmJCby8vHDhwoUiiZPos5dZMeZ1KeFK/hVQofD19UVsbKzCYmdnVyTntrCwQJs2bbBRyTyEr169ws6dOzFw4EAAQEhICHr27IkTJ07g3LlzsLa2RsuWLfHvv/8WSaxE9PlhYiSltLW1YWlpqbCoq6sr3fenn36Cu7s7DAwMYGlpiV69euHJkyfS9pCQEMhkMgQHB8Pd3R16enpo0KAB7ty5k+P5Bw4ciODgYERHRyus37VrF9LS0tC7d28AwM8//4yhQ4eiVq1aqFq1Kn788UdkZGQgODi4AL4FolKOFSNR/rx9+xYzZszA1atXsXfvXkRFRcHPzy/bfpMmTcLChQsRGhoKDQ0NDBgwIMc2W7duDQsLCwQFBSms37RpEzp37gxjY2OlxyUnJ+Pt27coU6ZMjm2npKQgMTFRYSEiJZgYif7fgQMHIJfLpeWLL77Icd8BAwagVatWsLe3R/369bFs2TIcOnQISUlJCvvNmjULTZo0QbVq1TBhwgScPXsWb968Udqmuro6+vfvj6CgIAghAAARERE4derUBxPq+PHjUa5cOXh5eeW4T2BgIIyMjKTF2tr6Q18FEZUyTIykVLNmzRAWFiYty5Yty3HfS5cuoV27dqhYsSIMDAzQpEkTAMjWDVqzZk3pZysrKwBQ6HJ934ABAxAZGYkTJ04AeFct2traonnz5kr3nzNnDrZv3449e/ZAR0cnx3b9/f2RkJAgLTExMTnuS1SqldKKkc8xklL6+vpwcHD46H6vXr2Cj48PfHx88PPPP8PMzAzR0dHw8fFBamqqwr6amprSz7L/pqbJyMjIsW1HR0c0atQImzZtQtOmTbFlyxYMHjxYOjarBQsWYM6cOTh+/LhCAlZGW1sb2traH702IiqdmBjpk9y+fRvPnj3DnDlzpC7J0NDQAmt/4MCBGDJkCNq3b49///1X6b3LefPmYdasWThy5Ajc3d0L7NxEpV4pnY+x5Ne8pFIVK1aElpYWli9fjgcPHmD//v2YMWNGgbX/xRdfQFNTE19//TVatmyZ7X7g3LlzERAQgI0bN8LW1hZxcXGIi4vLdn+TiCi3mBjpk5iZmSEoKAi7du1CtWrVMGfOHCxYsKDA2tfT00OPHj3w4sULpYNuVq9ejdTUVHTt2hVWVlbSUpAxEJVapfQeo0xkDvkjKqUSExNhZGSEhHnzYKirq+pwstPXV3UEylWtquoIcla2rKojUOpivKOqQ8gmKSkRzZsbISEhAYaGhgCy/JkYMgSGebwfn5iSAqPVqxXaK2l4j5GIiJQrpS8RZ2IkIiLlSmliLPlXQEREVIBYMRIRkXKsGImIiIgVIxERKceKkYiIiFgxEhGRcqwYiYiIiBUjEREpV0orRiZGIiJSrpQmxpJ/BURERAWIFSNRJmfn4vnC7g9M5qxK5+Ch6hBy5KH1UNUhKFXn0T5Vh5BNYnJyzhs5HyMREVHRmTp1KmQymcJStRjM2sKKkYiIlCuCe4zVq1fH8ePHpc8aGqpPS6qPgIiISi0NDQ1YWlqqOgwF7EolIiLlMivGvC54N9lx1iUlJUXpKe7du4dy5crB3t4evXv3RnR0dFFeoVJMjEREVOCsra1hZGQkLYGBgdn2qVevHoKCgnD48GGsXr0akZGRaNSoEV6+fKmCiP8fu1KJiEi5T7jHGBMTA0NDQ2m1trZ2tl1btWol/VyzZk3Uq1cPNjY22LlzJwYOHJi/mAsAEyMRESn3CYnR0NBQITHmhrGxMSpXroz79+/n7ZwFjF2pRERULCQlJSEiIgJWVlYqjYOJkYiIlPuEwTe5MXbsWJw8eRJRUVE4e/YsOnXqBHV1dfTs2bMQL+rj2JVKREQq8c8//6Bnz5549uwZzMzM0LBhQ/z9998wMzNTaVxMjEREpFwhP+C/ffv2PAZUNNiVSkRElAUrRiIiUo7TThEVP1OnTkWtWrWkz35+fujYsaPK4iGizx8TIxUKPz8/6W35mpqasLCwgLe3NzZu3IiMYjqNEhG9p5BHpRZXJf8KqNjy9fVFbGwsoqKicOjQITRr1gwjR45E27ZtkZaWpurwiOhjMudjzMvC+RiJcqatrQ1LS0uUL18ebm5umDhxIvbt24dDhw4hKCgIABAdHY0OHTpALpfD0NAQ3bp1w+PHj3PV/pYtW2Bqaprt5cQdO3ZE3759C/pyiKiUYGKkItW8eXO4uLhg9+7dyMjIQIcOHfD8+XOcPHkSx44dw4MHD9C9e/dctfXFF18gPT0d+/fvl9Y9efIEBw8exIABA3I8LiUlJdub/4lICXalEhWNqlWrIioqCsHBwbh+/Tp++eUX1K5dG/Xq1cOWLVtw8uRJXLx48aPt6OrqolevXti0aZO0buvWrahYsSKaNm2a43GBgYEKb/23trYuiMsios8EEyMVOSEEZDIZwsPDYW1trZCYqlWrBmNjY4SHh+eqrcGDB+Po0aP4999/AQBBQUHSwJ+c+Pv7IyEhQVpiYmI+7YKIPleltGLkc4xU5MLDw2FnZ1cgbbm6usLFxQVbtmxBy5YtcfPmTRw8ePCDx2hrayudAoeICGBipCL2559/4vr16/juu+9QoUIFxMTEICYmRqoab926hfj4eFSrVi3XbQ4aNAhLlizBv//+Cy8vL3aNEhWUUvqAPxMjFZqUlBTExcUhPT0djx8/xuHDhxEYGIi2bduiX79+UFNTg7OzM3r37o0lS5YgLS0NQ4cORZMmTeDu7p7r8/Tq1Qtjx47F+vXrsWXLlkK8IiIqDUp+aqdi6/Dhw7CysoKtrS18fX1x4sQJLFu2DPv27YO6ujpkMhn27dsHExMTNG7cGF5eXrC3t8eOHTvydB4jIyN06dIFcrmcb8UhKkil9B6jTAghVB0E0adq0aIFqlevjmXLluX52MTERBgZGSHh0CEY6usXQnSfqJi+KeicVhNVh5Ajj3IPVR2CcmFhqo4gm8TkZBj16oWEhAQYGhq+W5f5Z2L5chjq6uatvdevYfTttwrtlTTsSqUS7cWLFwgJCUFISAhWrVql6nCI6DPAxEglmqurK168eIG5c+eiSpUqqg6H6PPCwTdEJU9UVJSqQyCizwwTIxERKVdKK8aSfwVEREQFiBUjEREpx4qRiIiIWDESEZFymRMV5/WYEo6JkYiIlGNXKhEREbFiJCIi5VgxEhEREStGIiJSrpRWjEyMRJmMjQG5XNVRZPfqlaojUMpD45KqQ8hReHJtVYeglFPGZVWHkF0xnb1FlZgYiYhIuVJaMZb8KyAiIipArBiJiEi5UloxMjESEZFypTQxlvwrICIiKkCsGImISDlWjERERMSKkYiIlGPFSERERKwYiYhIOVaMRERExIqRiIiUk8nyXgHKZIUTSxFixUhERJQFEyMp8PPzQ8eOHVUdBhEVB5n3GPO6lHAl/woKWExMDAYMGIBy5cpBS0sLNjY2GDlyJJ49e6bq0LKpWrUqtLW1ERcXV2BtLl26FEFBQbnatzCTaNOmTTFq1KiP7rd79260bNkSpqamkMlkCAsLK5R4iEolJkZ68OAB3N3dce/ePWzbtg3379/HmjVrEBwcDA8PDzx//lzVIUpOnz6N169fo2vXrti8eXOBtWtkZARjY+MCa6+wvXr1Cg0bNsTcuXNVHQoRfSaYGLMYNmwYtLS0cPToUTRp0gQVK1ZEq1atcPz4cfz777+YNGmStK+trS1mzJiBnj17Ql9fH+XLl8fKlSsV2ouPj8egQYNgZmYGQ0NDNG/eHFevXpW2T506FbVq1cJPP/0EW1tbGBkZoUePHnj58uVHY92wYQN69eqFvn37YuPGjdm2r1q1Co6OjtDR0YGFhQW6du0qbfv111/h7OwMXV1dmJqawsvLC6/+mwz3/Sowp32nTp2KzZs3Y9++fZDJZJDJZAgJCQEAjB8/HpUrV4aenh7s7e0REBCAt2/f5vq6/fz8cPLkSSxdulRqOyoqSun30LdvX0yZMgVeXl4f/c6IKI9YMZZuz58/x5EjRzB06FDo6uoqbLO0tETv3r2xY8cOCCGk9fPnz4eLiwuuXLmCCRMmYOTIkTh27Ji0/YsvvsCTJ09w6NAhXLp0CW5ubmjRooVC5RkREYG9e/fiwIEDOHDgAE6ePIk5c+Z8MNaXL19i165d6NOnD7y9vZGQkIBTp05J20NDQzFixAhMnz4dd+7cweHDh9G4cWMAQGxsLHr27IkBAwYgPDwcISEh6Ny5s8J1ZfrQvmPHjkW3bt3g6+uL2NhYxMbGokGDBgAAAwMDBAUF4datW1i6dCnWr1+PxYsXK7T9oeteunQpPDw8MHjwYKlta2vrD34neZGSkoLExESFhYgoEx/X+M+9e/cghICTk5PS7U5OTnjx4gWePn0Kc3NzAICnpycmTJgAAKhcuTLOnDmDxYsXw9vbG6dPn8aFCxfw5MkTaGtrAwAWLFiAvXv34tdff8VXX30FAMjIyEBQUBAMDAwAvKuAgoODMWvWrBxj3b59OxwdHVG9enUAQI8ePbBhwwY0atQIABAdHQ19fX20bdsWBgYGsLGxgaurK4B3yS4tLQ2dO3eGjY0NAMDZ2VnpeT62r66uLlJSUmBpaalw3OTJk6WfbW1tMXbsWGzfvh3jxo2T1n/ouo2MjKClpQU9Pb1sbReEwMBATJs2rcDbJfrs8AF/AqC0csqJh4dHts/h4eEAgKtXryIpKQmmpqaQy+XSEhkZiYiICOkYW1tbKTkAgJWVFZ48efLB827cuBF9+vSRPvfp0we7du2SuiK9vb1hY2MDe3t79O3bFz///DOSk5MBAC4uLmjRogWcnZ3xxRdfYP369Xjx4oXS8+Rl36x27NgBT09PWFpaQi6XY/LkyYiOjlbYJz/XXVD8/f2RkJAgLTExMUVyXiIqGZgY/+Pg4ACZTCYltveFh4fDxMQEZmZmuWovKSkJVlZWCAsLU1ju3LmD77//XtpPU1NT4TiZTIaMjIwc27116xb+/vtvjBs3DhoaGtDQ0ED9+vWRnJyM7du3A3jXlXn58mVs27YNVlZWmDJlClxcXBAfHw91dXUcO3YMhw4dQrVq1bB8+XJUqVIFkZGR2c6Vl30znTt3Dr1790br1q1x4MABXLlyBZMmTUJqaqrCfnm97oKkra0NQ0NDhYWIlOA9xtLN1NQU3t7eWLVqFV6/fq2wLS4uDj///DO6d+8OWZa3Ovz9998K+/39999SV6ybmxvi4uKgoaEBBwcHhaVs2bL5jnPDhg1o3Lgxrl69qpBwR48ejQ0bNkj7aWhowMvLC/PmzcO1a9cQFRWFP//8E8C7JOTp6Ylp06bhypUr0NLSwp49e5Se70P7amlpIT09XWH/s2fPwsbGBpMmTYK7uzscHR3x8OHDPF+nsraJiIoC7zFmsWLFCjRo0AA+Pj6YOXMm7OzscPPmTXz//fcoX758tvt+Z86cwbx589CxY0ccO3YMu3btwsGDBwEAXl5e8PDwQMeOHTFv3jxUrlwZjx49wsGDB9GpUye4u7vnOb63b9/ip59+wvTp01GjRg2FbYMGDcKiRYtw8+ZNREZG4sGDB2jcuDFMTEzwxx9/ICMjA1WqVMH58+cRHByMli1bwtzcHOfPn8fTp0+V3lv92L62trY4cuQI7ty5A1NTUxgZGcHR0RHR0dHYvn076tSpg4MHD+aYdD/E1tYW58+fR1RUFORyOcqUKQM1Jf8Sff78OaKjo/Ho0SMAwJ07dwC8GzBVGPcniUoV3mMkR0dHhIaGwt7eHt26dUOlSpXw1VdfoVmzZjh37hzKlCmjsP+YMWMQGhoKV1dXzJw5E4sWLYKPjw+Ad5XWH3/8gcaNG+PLL79E5cqV0aNHDzx8+BAWFhb5im///v149uwZOnXqlG2bk5MTnJycsGHDBhgbG2P37t1o3rw5nJycsGbNGmzbtg3Vq1eHoaEh/vrrL7Ru3RqVK1fG5MmTsXDhQrRq1Spbmx/bd/DgwahSpQrc3d1hZmaGM2fOoH379vjuu+8wfPhw1KpVC2fPnkVAQECer3Xs2LFQV1dHtWrVYGZmlu0eZdbvxNXVFW3atAHwbiCSq6sr1qxZk+dzEtF7SmlXqkzkZbQJSWxtbTFq1KhcvZ2FirfExEQYGRkh4dw5GMrlqg4nu/+eMS12NIpvh1O4Xm1Vh6CU0+28954UtsTkZBj16YOEhATpfrv0Z+LYMRjq6+etvVevYPTfY2Ql9f598f0/m4iIVItdqURERMSKMZ9yekUZEdFng/MxEhEREStGIiJSjvcYiYiIiBUjEREpV0orRiZGIiJSrpQmxpJ/BURERAWIFSMRESnHipGIiIhYMRIRkXKltGJkYiT6T8iTatB/VfxeetywoaojUO75c1VHkLMyxfTv5t81ss+Mo2rJGomqDqHYKab/+xARkcoV0bRTK1euhK2tLXR0dFCvXj1cuHChEC4m95gYiYhIZXbs2IHRo0fjhx9+wOXLl+Hi4gIfHx88efJEZTExMRIRkXJFUDEuWrQIgwcPxpdffolq1aphzZo10NPTw8aNGwvpoj6OiZGIiJT7hMSYmJiosKSkpGRrPjU1FZcuXYKXl1eWU6rBy8sL586dK7LLfB8TIxERFThra2sYGRlJS2BgYLZ9/ve//yE9PR0WFhYK6y0sLBAXF1dUoWbDUalERKTcJ8zHGBMTA0PD/x/lra2tXZCRFSomRiIiKnCGhoYKiVGZsmXLQl1dHY8fP1ZY//jxY1haWhZmeB/ErlQiIlKukAffaGlpoXbt2ggODpbWZWRkIDg4GB4eHoVxRbnCipGIiFRm9OjR6N+/P9zd3VG3bl0sWbIEr169wpdffqmymJgYiYhIuSJ4JVz37t3x9OlTTJkyBXFxcahVqxYOHz6cbUBOUWJiJCIilRo+fDiGDx+u6jAkTIxERKQcXyJORESURSlNjCX/CihHU6dORa1atbKts7CwgEwmw969e3N1DBFRacLEWIw9ffoUQ4YMQcWKFaGtrQ1LS0v4+PjgzJkz+WovPDwc06ZNw9q1axEbG4tWrVpl22fs2LEKQ6c/VdOmTTFq1KgCa+9DoqKiIJPJEBYWViTnI/rsFdHsGsUNu1KLsS5duiA1NRWbN2+Gvb09Hj9+jODgYDx79ixf7UVERAAAOnToANl/b6d4n1wuh1wuz3fM+SGEQHp6OjQ0+L8jEaleyU/tn6n4+HicOnUKc+fORbNmzWBjY4O6devC398f7du3l/YZNGgQzMzMYGhoiObNm+Pq1atK25s6dSratWsH4N1LenNKjO93pfr5+aFjx45YsGABrKysYGpqimHDhuHt27fSPqtWrYKjoyN0dHRgYWGBrl27SseePHkSS5cuhUwmg0wmQ1RUFEJCQiCTyXDo0CHUrl0b2traOH36tHSurEaNGoWmTZtKnzMyMjBv3jw4ODhAW1sbFStWxKxZswAAdnZ2AABXV1fIZDKF44goH1gxUnGSWbnt3bsX9evXV/qewS+++AK6uro4dOgQjIyMsHbtWrRo0QJ3795FmTJlFPYdO3YsbG1t8eWXXyI2NjZPsZw4cQJWVlY4ceIE7t+/j+7du6NWrVoYPHgwQkNDMWLECPz0009o0KABnj9/jlOnTgEAli5dirt376JGjRqYPn06AMDMzAxRUVEAgAkTJmDBggWwt7eHiYlJrmLx9/fH+vXrsXjxYjRs2BCxsbG4ffs2AODChQuoW7cujh8/jurVq0NLS0tpGykpKQpv+k9M5AzmRPT/mBiLKQ0NDQQFBWHw4MFYs2YN3Nzc0KRJE/To0QM1a9bE6dOnceHCBTx58kRKmgsWLMDevXvx66+/4quvvlJoTy6Xw9jYGADy/A5CExMTrFixAurq6qhatSratGmD4OBgDB48GNHR0dDX10fbtm1hYGAAGxsbuLq6AgCMjIygpaUFPT09peecPn06vL29cx3Hy5cvsXTpUqxYsQL9+/cHAFSqVAkNGzYE8C7pAoCpqekHrzEwMBDTpk3L9XmJSi2OSqXipkuXLnj06BH2798PX19fhISEwM3NDUFBQbh69SqSkpJgamoqVZdyuRyRkZHSvcSPyXrcN998k+N+1atXh7q6uvTZyspKml3b29sbNjY2sLe3R9++ffHzzz8jOTk5V+d3d3fP1X6ZwsPDkZKSghYtWuTpuPf5+/sjISFBWmJiYj6pPSL6vLBiLOZ0dHTg7e0Nb29vBAQEYNCgQfjhhx8wdOhQWFlZISQkJNsxmZXhx2Qdvfmht+BramoqfJbJZMjIyAAAGBgY4PLlywgJCcHRo0cxZcoUTJ06FRcvXvxoHPr6+gqf1dTUIIRQWJf1Xqauru4H28stbW3tEjUFDpHKsGKkkqBatWp49eoV3NzcEBcXBw0NDTg4OCgsZcuWzVVbWY8xNzfPd0waGhrw8vLCvHnzcO3aNURFReHPP/8E8O7t+enp6blqx8zMLNv9z6zJ29HREbq6ujk+TpJ5TzG35yOij8icjzEvSw4D+0oSVozF1LNnz/DFF19gwIABqFmzJgwMDBAaGop58+ahQ4cO8PLygoeHBzp27Ih58+ahcuXKePToEQ4ePIhOnTrluZsyvw4cOIAHDx6gcePGMDExwR9//IGMjAxUqVIFAGBra4vz588jKioKcrk826CgrJo3b4758+djy5Yt8PDwwNatW3Hjxg3pnqWOjg7Gjx+PcePGQUtLC56ennj69Clu3ryJgQMHwtzcHLq6ujh8+DAqVKgAHR0dGBkZFcn3QESfj3xVjNOnT1d6H+n169fS6EP6NHK5HPXq1cPixYvRuHFj1KhRAwEBARg8eDBWrFgBmUyGP/74A40bN8aXX36JypUro0ePHnj48GGRvpXe2NgYu3fvRvPmzeHk5IQ1a9Zg27ZtqF69OoB3o2HV1dVRrVo1mJmZITo6Ose2fHx8EBAQgHHjxqFOnTp4+fIl+vXrp7BPQEAAxowZgylTpsDJyQndu3eX7ndqaGhg2bJlWLt2LcqVK4cOHToU3oUTlQal9HENmXj/pk4uqKurIzY2Nlv327Nnz2Bubs6uLCpREhMTYWRkhH37EqCv/+EZx1Xhv0G3xc7z56qOIGfF9e/mCxdUHUF2ycmJ6NHDCAkJCdJYg8w/EwnR0R8cf6BMYmIijCpWVGivpMlXV6oQQukD4levXv1gVxkREZUgpXTwTZ4So4mJifQGk8qVKyskx/T0dCQlJX1w2D8REVFxl6fEuGTJEgghMGDAAEybNk1hYIOWlhZsbW3h4eFR4EESEZEKsGL8uMy3jdjZ2aFBgwbZnm8jIiIq6fJ1j9HOzu6D79usWLFivgMiIqJighVj7tna2uY4OwPAB6yJiD4LTIy5d+XKFYXPb9++xZUrV7Bo0SJpCiAiIqKSKF+J0cXFJds6d3d3lCtXDvPnz0fnzp0/OTAiIlKxUloxFugVVKlSBRcvXizIJomIiIpUvirG9yd2FUIgNjYWU6dOhaOjY4EERkREKlZKK8Z8JUZjY+Nsg2+EELC2tsb27dsLJDAiIiJVyFdiPHHihMJnNTU1mJmZwcHBARoanLCDiOizwIox95o0aVLQcRARERUL+S7v7ty5g+XLlyM8PBwA4OTkhOHDh6Nq1aoFFhxRUWqa8ScMM/RVHUY2z5K8VR2CUlaWeZ6Yp8hERhXPyXLb1X+q6hCySXz5MsdtAjII5O27zOv+xVG+at7ffvsNNWrUwKVLl+Di4gIXFxdcvnwZzs7O+O233wo6RiIiUoGMjPwtJV2+KsZx48bB398/26TEP/zwA8aNG4cuXboUSHBERERFLV8VY2xsbLaZ1QGgT58+H3yHKhERlRyltWLMV2Js2rQpTp06lW396dOn0ahRo08OioiISFXy1ZXavn17jB8/HpcuXUL9+vUBAH///Td27dqFadOmYf/+/Qr7EhFRyZOfCvBzqBhlQog8Dy1Ty+VzKjKZjDNtULGXmJgIIyMjJOzZA0P9Yjgq1a14jko1LcNRqXllJy+eo1KNKlVCQkICDA0N363778/Ekyf/vy7X7SUmwtzcSKG9kiZfFWPG5/BPAiIi+qDSWjGW/FcUEBERFaB8P+AfHByM4OBgPHnyJFsFuXHjxk8OjIiIVKu0Voz5SozTpk3D9OnT4e7uDisrq2wvFCciopKPiTEP1qxZg6CgIPTt27eg4yEiIlKpfCXG1NRUNGjQoKBjISKiYqS0Voz5GnwzaNAg/PLLLwUdCxERkcrlq2J88+YN1q1bh+PHj6NmzZrQ1NRU2L5o0aICCa40sLW1xahRozBq1ChVh1Jk/Pz8EB8fj71796o6FCL6AFaMeXDt2jXUqlULampquHHjBq5cuaKw5FbTpk2VJoSgoCAYGxvnJ7RPMnXqVNSqVavIz/shBR1TTt95YYiKioJMJkNYWJjC+qVLlyIoKKhIYiAiyqt8VYwnTpwo6Dg+K0IIpKenQ0Mj30/DqFRhx29kZFQo7RJRwRIi7xVg3t+lVvzkqWLs3LnzR5fCmHLKz88PHTt2xIIFC2BlZQVTU1MMGzYMb9++lfaxtbXF7NmzMWDAABgYGKBixYpYt26dQjvjx49H5cqVoaenB3t7ewQEBEhtBAUFYdq0abh69SpkMhlkMhmCgoKUVj3x8fGQyWQICQkBAISEhEAmk+HQoUOoXbs2tLW1cfr0aURERKBDhw6wsLCAXC5HnTp1cPz48UL/LlatWgVHR0fo6OjAwsICXbt2lY49efIkli5dKl1jVFRUjvFnniurUaNGoWnTptLnjIwMzJs3Dw4ODtDW1kbFihUxa9YsAICdnR0AwNXVFTKZTDru/XZTUlIwYsQImJubQ0dHBw0bNsTFixel7ZnxBQcHw93dHXp6emjQoAHu3Lkj7XP16lU0a9YMBgYGMDQ0RO3atREaGvpJ3zURlU55KglU+S/9EydOwMrKCidOnMD9+/fRvXt31KpVC4MHD5b2WbhwIWbMmIGJEyfi119/xZAhQ9CkSRNUqVIFAGBgYICgoCCUK1cO169fx+DBg2FgYIBx48ahe/fuuHHjBg4fPiwlLyMjIzx+/DjXMU6YMAELFiyAvb09TExMEBMTg9atW2PWrFnQ1tbGli1b0K5dO9y5cwcVK1YslO8iNDQUI0aMwE8//YQGDRrg+fPn0kwoS5cuxd27d1GjRg1pLk0zMzNERUUpjT83/P39sX79eixevBgNGzZEbGwsbt++DQC4cOEC6tati+PHj6N69erQ0tJS2sa4cePw22+/YfPmzbCxscG8efPg4+OD+/fvo0yZMtJ+kyZNwsKFC2FmZoZvvvkGAwYMwJkzZwAAvXv3hqurK1avXg11dXWEhYVlu/edKSUlBSkpKdLnxMTEXF0rUWlTWu8x5ikxbtq0qbDi+CgTExOsWLEC6urqqFq1Ktq0aYPg4GCFxNi6dWsMHToUwLvqcPHixThx4oSUGCdPnizta2tri7Fjx2L79u0YN24cdHV1IZfLoaGhAUtLy3zFOH36dHh7//8Ln8uUKQMXFxfp84wZM7Bnzx7s378fw4cPz9c5gA9/F9HR0dDX10fbtm1hYGAAGxsbuLq6AniX6LW0tKCnp6f0Gt+P/2NevnyJpUuXYsWKFejfvz8AoFKlSmjYsCGAd0kXAExNTXP8Tl+9eoXVq1cjKCgIrVq1AgCsX78ex44dw4YNG/D9999L+86aNQtNmjQB8C6Jt2nTBm/evIGOjg6io6Px/fffo2rVqgAAR0fHHOMODAzEtGnTcn2dRFS6lJh3pVavXh3q6urSZysrKzx58kRhn5o1a0o/y2QyWFpaKuyzY8cOeHp6wtLSEnK5HJMnT0Z0dHSBxeju7q7wOSkpCWPHjoWTkxOMjY0hl8sRHh7+yef80Hfh7e0NGxsb2Nvbo2/fvvj555+RnJycr/g/Jjw8HCkpKWjRokWejssqIiICb9++haenp7ROU1MTdevWRXh4uMK+WX+/VlZWACBd9+jRozFo0CB4eXlhzpw5iIiIyPGc/v7+SEhIkJaYmJh8x0/0OeNExSpgaGiIhISEbOvj4+Ozddu+3y0mk8myvaP1Q/ucO3cOvXv3RuvWrXHgwAFcuXIFkyZNQmpq6gdjzJxiK+vsXFnv52Wl/96URWPHjsWePXswe/ZsnDp1CmFhYXB2dv7oOT/mQ9dpYGCAy5cvY9u2bbCyssKUKVPg4uKC+Pj4j7b7fvxqamp4f1ayrNeuq6ubzyvIn6zXnfkawszrnjp1Km7evIk2bdrgzz//RLVq1bBnzx6l7Whra8PQ0FBhIaLsmBhVoEqVKrh8+XK29ZcvX0blypUL9Fxnz56FjY0NJk2aBHd3dzg6OuLhw4cK+2hpaWWbPzKzOzA2NlZa9/7jBzk5c+YM/Pz80KlTJzg7O8PS0lK6n1eYNDQ04OXlhXnz5uHatWuIiorCn3/+CUD5NebEzMxM4boBxWt3dHSErq4ugoODlR6feU/xQ+erVKkStLS0pHuFwLvke/HiRVSrVi1XcWaqXLkyvvvuOxw9ehSdO3dWadc/EZVcKn2eYMiQIVixYgVGjBiBQYMGQVtbGwcPHsS2bdvw+++/F+i5HB0dER0dje3bt6NOnTo4ePBgtorC1tYWkZGRCAsLQ4UKFWBgYABdXV3Ur18fc+bMgZ2dHZ48eaJwr/Jj59y9ezfatWsHmUyGgICAQp/L8sCBA3jw4AEaN24MExMT/PHHH8jIyJDus9ra2uL8+fOIioqCXC5XGNzyvubNm2P+/PnYsmULPDw8sHXrVty4cUO6Z6mjo4Px48dj3Lhx0NLSgqenJ54+fYqbN29i4MCBMDc3h66uLg4fPowKFSpAR0cnW0+Avr4+hgwZgu+//x5lypRBxYoVMW/ePCQnJ2PgwIG5uubXr1/j+++/R9euXWFnZ4d//vkHFy9eLJQR0kSlSWkdfKPSitHe3h5//fUXbt++DS8vL9SrVw87d+7Erl274OvrW6Dnat++Pb777jsMHz4ctWrVwtmzZxEQEKCwT5cuXeDr64tmzZrBzMwM27ZtA/BuGq20tDTUrl0bo0aNwsyZM3N1zkWLFsHExAQNGjRAu3bt4OPjAzc3twK9rvcZGxtj9+7daN68OZycnLBmzRps27YN1atXB/Cue1ddXR3VqlWDmZnZB+93+vj4ICAgAOPGjUOdOnXw8uVL9OvXT2GfgIAAjBkzBlOmTIGTkxO6d+8u3ffT0NDAsmXLsHbtWpQrVw4dOnRQep45c+agS5cu6Nu3L9zc3HD//n0cOXIk1yNj1dXV8ezZM/Tr1w+VK1dGt27d0KpVKw6wIaJ8kYn3byIRlTKJiYkwMjJCwp49MHzvPmtx8Mwt9yOFi5JpmeL7V0dkVPGcCs9O/lTVIWST+PIljCpVQkJCgnS/PfPPxM2bCTAwyNs9+JcvE1G9upFCeyVNiRmVSkREVBRK5jvLiIio0PEeIxEREbFiJCIi5UprxcjESERESpXWxMiuVCIioixYMRIRkVKcj5GIiIhYMRIRkXK8x0hERESsGImISDlWjERERMSKkUiiq/tuKWY0iuuf0lxMfq0qf/+du5lZippdjThVh5BdUlKOm0prxVhc/8gREZGKldbEyK5UIiIqEWxtbSGTyRSWOXPmFPh5WDESEZFSxbFinD59OgYPHix9NjAwKPBzMDESEVGJYWBgAEtLy0I9B7tSiYhIqcyKMa8LACQmJiosKSkpBRLTnDlzYGpqCldXV8yfPx9paWkF0m5WrBiJiKjAWVtbK3z+4YcfMHXq1E9qc8SIEXBzc0OZMmVw9uxZ+Pv7IzY2FosWLfqkdt/HxEhEREp9yj3GmJgYGBoaSuu1tbWV7j9hwgTMnTv3g22Gh4ejatWqGD16tLSuZs2a0NLSwtdff43AwMAc288PJkYiIipwhoaGCokxJ2PGjIGfn98H97G3t1e6vl69ekhLS0NUVBSqVKmSnzCVYmIkIiKlimJUqpmZGczMzPJ20H/CwsKgpqYGc3PzfB2fEyZGIiJSqjjNx3ju3DmcP38ezZo1g4GBAc6dO4fvvvsOffr0gYlJwb7piImRiIiKPW1tbWzfvh1Tp05FSkoK7Ozs8N133yncdywoTIxERKRUcXrA383NDX///XfhNP4ePsdIRESUBRMjFYiQkBDIZDLEF+MZF4gobz7lAf+SjImR8uTcuXNQV1dHmzZtVB0KEVGhYGKkPNmwYQO+/fZb/PXXX3j06JGqwyGiQsSKkegjkpKSsGPHDgwZMgRt2rRBUFBQjvsGBQXB2NgYe/fuhaOjI3R0dODj44OYmBhpn4iICHTo0AEWFhaQy+WoU6cOjh8/rtBOSkoKxo8fD2tra2hra8PBwQEbNmwAAKSnp2PgwIGws7ODrq4uqlSpgqVLlxbKtRNR6cHESLm2c+dOVK1aFVWqVEGfPn2wceNGiA88tJScnIxZs2Zhy5YtOHPmDOLj49GjRw9pe1JSElq3bo3g4GBcuXIFvr6+aNeuHaKjo6V9+vXrh23btmHZsmUIDw/H2rVrIZfLAQAZGRmoUKECdu3ahVu3bmHKlCmYOHEidu7c+cHrSElJyfaCYyLKrrRWjHxcg3Jtw4YN6NOnDwDA19cXCQkJOHnyJJo2bap0/7dv32LFihWoV68eAGDz5s1wcnLChQsXULduXbi4uMDFxUXaf8aMGdizZw/279+P4cOH4+7du9i5cyeOHTsGLy8vAIqvhtLU1MS0adOkz3Z2djh37hx27tyJbt265XgdgYGBCscRkXLF6XGNosSKkXLlzp07uHDhAnr27AkA0NDQQPfu3aVuTWU0NDRQp04d6XPVqlVhbGyM8PBwAO8qxrFjx8LJyQnGxsaQy+UIDw+XKsawsDCoq6ujSZMmOZ5j5cqVqF27NszMzCCXy7Fu3TqFilMZf39/JCQkSEvW7l0iIlaMlCsbNmxAWloaypUrJ60TQkBbWxsrVqzIV5tjx47FsWPHsGDBAjg4OEBXVxddu3ZFamoqAEBXV/eDx2/fvh1jx47FwoUL4eHhAQMDA8yfPx/nz5//4HHa2toF+iZ+os9Vaa0YmRjpo9LS0rBlyxYsXLgQLVu2VNjWsWNHbNu2DVWrVlV6XGhoKOrWrQvgXdUZHx8PJycnAMCZM2fg5+eHTp06AXhXQUZFRUnHOzs7IyMjAydPnpS6UrM6c+YMGjRogKFDh0rrIiIiPvl6iah0Y1cqfdSBAwfw4sULDBw4EDVq1FBYunTpkmN3qqamJr799lucP38ely5dgp+fH+rXry8lSkdHR+zevRthYWG4evUqevXqhYws/9y0tbVF//79MWDAAOzduxeRkZEICQmRBtc4OjoiNDQUR44cwd27dxEQEICLFy8W/hdCVEqU1sE3TIz0URs2bICXlxeMjIyybevSpQtCQ0Nx7dq1bNv09PQwfvx49OrVC56enpDL5dixY4e0fdGiRTAxMUGDBg3Qrl07+Pj4wM3NTaGN1atXo2vXrhg6dCiqVq2KwYMH49WrVwCAr7/+Gp07d0b37t1Rr149PHv2TKF6JCLKD5n40Hh7onwKCgrCqFGjSsQr4hITE2FkZISEw4dhqK+v6nCySXBuqOoQlDLKeKHqEHK07XDBTkNUUHrWuK7qELJJTEqCUYMGSEhIkCYWzvwz8dtvCdDX//hkw1m9epWILl2MFNoraVgxEhERZcHBN0REpFRxmqi4KLFipELh5+dXIrpRiShnHHxDRERE7EolIiLlSusD/qwYiYiIsmDFSERESrFiJCIiIlaMRESkHCtGIiIiYsVIRETKldaKkYmRiIiUKq2JkV2pREREWbBiJMpkbg7I5aqOIhujtGeqDkG5YvzKv9ati+fsGvjltKojyO716xw3sWIkIiIiVoxERKQcK0YiIiJixUhERMqxYiQiIiJWjEREpJwQea8AhSicWIoSEyMRESnFrlQiIiJixUhERMqxYiQiIiJWjEREpBwrRiIiImLFSEREyrFipM/S1KlTUatWrSI5l0wmw969e4vkXEREhYWJsRiJi4vDt99+C3t7e2hra8Pa2hrt2rVDcHCwqkMDALx+/RplypRB2bJlkZKSoupwiKiQZVaMeV1KOnalFhNRUVHw9PSEsbEx5s+fD2dnZ7x9+xZHjhzBsGHDcPv2bVWHiN9++w3Vq1eHEAJ79+5F9+7dVR0SEVGBY8VYTAwdOhQymQwXLlxAly5dULlyZVSvXh2jR4/G33//DQCIjo5Ghw4dIJfLYWhoiG7duuHx48cK7cyZMwcWFhYwMDDAwIED8ebNm2zn+vHHH+Hk5AQdHR1UrVoVq1atylWMGzZsQJ8+fdCnTx9s2LDhg/tGRUVBJpNh+/btaNCgAXR0dFCjRg2cPHlS2ic9PR0DBw6EnZ0ddHV1UaVKFSxdujRbWxs3bkT16tWhra0NKysrDB8+XNq2aNEiODs7Q19fH9bW1hg6dCiSkpJydT1E9GGltWJkYiwGnj9/jsOHD2PYsGHQ19fPtt3Y2BgZGRno0KEDnj9/jpMnT+LYsWN48OCBQtW2c+dOTJ06FbNnz0ZoaCisrKyyJb2ff/4ZU6ZMwaxZsxAeHo7Zs2cjICAAmzdv/mCMEREROHfuHLp164Zu3brh1KlTePjw4Uev7fvvv8eYMWNw5coVeHh4oF27dnj27N2M9BkZGahQoQJ27dqFW7duYcqUKZg4cSJ27twpHb969WoMGzYMX331Fa5fv479+/fDwcFB2q6mpoZly5bh5s2b2Lx5M/7880+MGzfugzGlpKQgMTFRYSGi7EprYmRXajFw//59CCFQtWrVHPcJDg7G9evXERkZCWtrawDAli1bUL16dVy8eBF16tTBkiVLMHDgQAwcOBAAMHPmTBw/flyhavzhhx+wcOFCdO7cGQBgZ2eHW7duYe3atejfv3+O59+4cSNatWoFExMTAICPjw82bdqEqVOnfvDahg8fji5dugB4l+QOHz6MDRs2YNy4cdDU1MS0adOkfe3s7HDu3Dns3LkT3bp1k65hzJgxGDlypLRfnTp1pJ9HjRol/Wxra4uZM2fim2+++WAVHBgYqHBeIqKsWDEWAyIXr6MPDw+HtbW1lBQBoFq1ajA2NkZ4eLi0T7169RSO8/DwkH5+9eoVIiIiMHDgQMjlcmmZOXMmIiIiAACtWrWS1levXh3Auy7PzZs3o0+fPlJbffr0QVBQEDI+8s/DrOfX0NCAu7u7FC8ArFy5ErVr14aZmRnkcjnWrVuH6OhoAMCTJ0/w6NEjtGjRIsf2jx8/jhYtWqB8+fIwMDBA37598ezZMyQnJ+d4jL+/PxISEqQlJibmg9dAVFqxYiSVcXR0hEwmK/QBNpn33tavX58tgaqrqwN4d//x9evXAABNTU0AwJEjR/Dvv/9mG2yTnp6O4OBgeHt75yue7du3Y+zYsVi4cCE8PDxgYGCA+fPn4/z58wAAXV3dDx4fFRWFtm3bYsiQIZg1axbKlCmD06dPY+DAgUhNTYWenp7S47S1taGtrZ2vmIno88eKsRgoU6YMfHx8sHLlSrx69Srb9vj4eDg5OSEmJkahurl16xbi4+NRrVo1AICTk5OUVDJlDtwBAAsLC5QrVw4PHjyAg4ODwmJnZwcAKF++vLTOxsYGwLtBNz169EBYWJjC0qNHj48Owsl6/rS0NFy6dAlOTk4AgDNnzqBBgwYYOnQoXF1d4eDgIFWuAGBgYABbW9scH1e5dOkSMjIysHDhQtSvXx+VK1fGo0ePPhgPEeVe5nyMeVk4HyMVmJUrV8LT0xN169bF9OnTUbNmTaSlpeHYsWNYvXo1bt26BWdnZ/Tu3RtLlixBWloahg4diiZNmsDd3R0AMHLkSPj5+cHd3R2enp74+eefcfPmTdjb20vnmTZtGkaMGAEjIyP4+voiJSUFoaGhePHiBUaPHp0trqdPn+L333/H/v37UaNGDYVt/fr1Q6dOnfD8+XOUKVMmx+tydHSEk5MTFi9ejBcvXmDAgAEA3lXKW7ZswZEjR2BnZ4effvoJFy9elJI08O4FBd988w3Mzc3RqlUrvHz5EmfOnMG3334LBwcHvH37FsuXL0e7du1w5swZrFmz5pN/F0RUurFiLCbs7e1x+fJlNGvWDGPGjEGNGjXg7e2N4OBgrF69GjKZDPv27YOJiQkaN24MLy8v2NvbY8eOHVIb3bt3R0BAAMaNG4fatWvj4cOHGDJkiMJ5Bg0ahB9//BGbNm2Cs7MzmjRpgqCgIIVklNWWLVugr6+v9D5fixYtoKuri61bt+Z4XXPmzMGcOXPg4uKC06dPY//+/ShbtiwA4Ouvv0bnzp3RvXt31KtXD8+ePcPQoUMVju/fvz+WLFmCVatWoXr16mjbti3u3bsHAHBxccGiRYswd+5c1KhRAz///DMCAwNz94UT0UeV1nuMMpGbkR9EeRQVFQU7OztcuXKlyF5Jl1+JiYkwMjJCwuXLMJTLVR1OdjlU4yoXH6/qCHKUULaSqkNQyuiX1aoOIZvE169hNGYMEhISYGho+G7df38mZs9OgI6OYZ7ae/MmERMnGim0V9KwK5WIiJQqrS8RZ2IkIiKlmBiJCpCtrW2uns8kIipumBiJiEip0loxclQqERFRFqwYiYhIKVaMRERExIqRiIiUY8VIRERErBiJiEi50loxMjESEZFSpTUxsiuViIgoC1aMRJnu3QNymNxYpapUUXUEyv038XVxNGK6qiNQbnPrYvhC+OTkHDdlzseYF5/DC69YMRIREWXBxEhEREoVt/kYZ82ahQYNGkBPTw/GxsZK94mOjkabNm2gp6cHc3NzfP/990hLS8vTediVSkREJUJqaiq++OILeHh4YMOGDdm2p6eno02bNrC0tMTZs2cRGxuLfv36QVNTE7Nnz871eZgYiYhIqeI2KnXatGkAgKCgIKXbjx49ilu3buH48eOwsLBArVq1MGPGDIwfPx5Tp06FlpZWrs7DrlQiIipwiYmJCktKSkqhn/PcuXNwdnaGhYWFtM7HxweJiYm4efNmrtthYiQiIqU+5R6jtbU1jIyMpCUwMLDQ442Li1NIigCkz3Fxcbluh12pRESk1Kd0pcbExMDQ0FBar62trXT/CRMmYO7cuR9sMzw8HFWrVs1bIJ+AiZGIiAqcoaGhQmLMyZgxY+Dn5/fBfezt7XN1TktLS1y4cEFh3ePHj6VtucXESEREShXF4BszMzOYmZnl7aAceHh4YNasWXjy5AnMzc0BAMeOHYOhoSGqVauW63aYGImIqESIjo7G8+fPER0djfT0dISFhQEAHBwcIJfL0bJlS1SrVg19+/bFvHnzEBcXh8mTJ2PYsGE5duUqw8RIRERKFbfHNaZMmYLNmzdLn11dXQEAJ06cQNOmTaGuro4DBw5gyJAh8PDwgL6+Pvr374/p0/P2jkAmRiIiKhGCgoJyfIYxk42NDf74449POg8TIxERKVXcKsaiwucYqcCEhIRAJpMhPj5e1aEQEeUbE2MxEhcXh5EjR8LBwQE6OjqwsLCAp6cnVq9ejeT/poaxtbWFTCaDTCaDvr4+3NzcsGvXrmzblC3vD4lOSUlBrVq1IJPJpJvYH3Pu3Dmoq6ujTZs2BXnpRFQMFbeXiBcVdqUWEw8ePICnpyeMjY0xe/ZsODs7Q1tbG9evX8e6detQvnx5tG/fHgAwffp0DB48GImJiVi4cCG6d++O8uXL4+LFi0hPTwcAnD17Fl26dMGdO3ekZ4l0dXUVzjlu3DiUK1cOV69ezXWcGzZswLfffosNGzbg0aNHKFeuXAF9A0RU3HA+RlKpoUOHQkNDA6GhoejWrRucnJxgb2+PDh064ODBg2jXrp20r4GBASwtLVG5cmWsXLkSurq6+P3332FmZgZLS0tYWlqiTJl3E6Kam5tL64yMjKQ2Dh06hKNHj2LBggW5jjEpKQk7duzAkCFD0KZNm4/eBA8KCoKxsTH27t0LR0dH6OjowMfHBzExMdI+ERER6NChAywsLCCXy1GnTh0cP35coZ2UlBSMHz8e1tbW0NbWhoODg/Rm/fT0dAwcOBB2dnbQ1dVFlSpVsHTp0lxfExHR+5gYi4Fnz57h6NGjGDZsGPT19ZXuI5PJlK7X0NCApqYmUlNTc32+x48fY/Dgwfjpp5+gl4cZ63fu3ImqVauiSpUq6NOnDzZu3AjxkX8eJicnY9asWdiyZQvOnDmD+Ph49OjRQ9qelJSE1q1bIzg4GFeuXIGvry/atWuH6OhoaZ9+/fph27ZtWLZsGcLDw7F27VrI5XIAQEZGBipUqIBdu3bh1q1bmDJlCiZOnIidO3fmGFNKSkq2FxwTUXbsSiWVuX//PoQQqFKlisL6smXL4s2bNwCAYcOGZXufYGpqKhYuXIiEhAQ0b948V+cSQsDPzw/ffPMN3N3dERUVles4N2zYgD59+gAAfH19kZCQgJMnT6Jp06Y5HvP27VusWLEC9erVAwBs3rwZTk5OuHDhAurWrQsXFxe4uLhI+8+YMQN79uzB/v37MXz4cNy9exc7d+7EsWPH4OXlBUDx9VCamprSVDQAYGdnh3PnzmHnzp3o1q2b0pgCAwMVjiEiyooVYzF24cIFhIWFoXr16gpTtowfPx5yuRx6enqYO3cu5syZk+vBMMuXL8fLly/h7++f4z5yuVxavvnmGwDAnTt3cOHCBfTs2RPAu0q1e/fuSicLzUpDQwN16tSRPletWhXGxsYIDw8H8K5iHDt2LJycnGBsbAy5XI7w8HCpYgwLC4O6ujqaNGmS4zlWrlyJ2rVrw8zMDHK5HOvWrVOoON/n7++PhIQEacnatUtE/48VI6mMg4MDZDIZ7ty5o7A+szJ6f9DM999/Dz8/P8jlclhYWOTYzarMn3/+iXPnzmV7PZK7uzt69+6NzZs3K4xQzRy4s2HDBqSlpSkMthFCQFtbGytWrFC4f5kXY8eOxbFjx7BgwQI4ODhAV1cXXbt2lbqG37/2923fvh1jx47FwoUL4eHhAQMDA8yfPx/nz5/P8Rhtbe08vR6KiEoXJsZiwNTUFN7e3lixYgW+/fbbHO8zZipbtiwcHBzyda5ly5Zh5syZ0udHjx7Bx8cHO3bskLo73287LS0NW7ZswcKFC9GyZUuFbR07dsS2bdukyvJ9aWlpCA0NRd26dQG8qzzj4+Ph5OQEADhz5gz8/PzQqVMnAO8qyKzdu87OzsjIyMDJkyelrtSszpw5gwYNGmDo0KHSuoiIiNx+HUT0AaX1AX8mxmJi1apV8PT0hLu7O6ZOnYqaNWtCTU0NFy9exO3bt1G7du0COU/FihUVPmcOYqlUqRIqVKig9JgDBw7gxYsXGDhwYLbKsEuXLtiwYUOOiVFTUxPffvstli1bBg0NDQwfPhz169eXEqWjoyN2796Ndu3aQSaTISAgABlZ/mTZ2tqif//+GDBgAJYtWwYXFxc8fPgQT548Qbdu3eDo6IgtW7bgyJEjsLOzw08//YSLFy/Czs4u398REZVuvMdYTFSqVAlXrlyBl5cX/P394eLiAnd3dyxfvhxjx47FjBkzVBbbhg0b4OXlpbS7tEuXLggNDcW1a9eUHqunp4fx48ejV69e8PT0hFwux44dO6TtixYtgomJCRo0aIB27drBx8cHbm5uCm2sXr0aXbt2xdChQ1G1alUMHjwYr169AgB8/fXX6Ny5M7p374569erh2bNnCtUjEeVfab3HKBMfG29PlE9BQUEYNWpUsX9FXGJiIoyMjJCwYwcM8/D4SpF5b7RysZGUpOoIctR/iauqQ1Bqc+sdH9+piCUmJ8NowAAkJCRIYwoy/0z06ZMALa2PTzacVWpqIrZuNVJor6RhxUhERJQF7zESEZFSpXXwDStGKjR+fn7FvhuViOh9rBiJiEgpVoxERETEipGIiJRjxUhERESsGImISLnSOlExEyMRESnFrlQiIiJixUhERMqxYiQiIiJWjEREpFxprRiZGIkyWVsD/81PWZwkWTmqOgSl5I+L74TQnTurOoIcVKqu6giyK8azpKgKEyMRESlVWitG3mMkIiLKghUjEREpVVorRiZGIiJSqrQmRnalEhERZcGKkYiIlGLFSERERKwYiYhIOVaMRERExIqRiIiUK63zMbJiJCIiyoIVIxERKcV7jERERMTESMXL1KlTUatWLVWHQUT4/4oxr0tJx8RIBcbPzw8ymUxaTE1N4evri2vXruW6jbFjxyI4OLgQoySi3GJiJCoAvr6+iI2NRWxsLIKDg6GhoYG2bdvm+ni5XA5TU9NCjJCI6MOYGKlAaWtrw9LSEpaWlqhVqxYmTJiAmJgYPH36FAAwfvx4VK5cGXp6erC3t0dAQADevn0rHf9+V2rWCjRzsbW1lbbfuHEDrVq1glwuh4WFBfr27Yv//e9/RXW5RJ81VoxEBSwpKQlbt26Fg4ODVAUaGBggKCgIt27dwtKlS7F+/XosXrw4xzYyq8/Y2Fjcv38fDg4OaNy4MQAgPj4ezZs3h6urK0JDQ3H48GE8fvwY3bp1+2BcKSkpSExMVFiIiDLxcQ0qUAcOHIBcLgcAvHr1ClZWVjhw4ADU1N79G2zy5MnSvra2thg7diy2b9+OcePGKW3P0tISACCEQJcuXWBkZIS1a9cCAFasWAFXV1fMnj1b2n/jxo2wtrbG3bt3UblyZaVtBgYGYtq0aZ9+sUSfOT6uQVQAmjVrhrCwMISFheHChQvw8fFBq1at8PDhQwDAjh074OnpCUtLS8jlckyePBnR0dEfbXfixIk4d+4c9u3bB11dXQDA1atXceLECcjlcmmpWrUqACAiIiLHtvz9/ZGQkCAtMTExBXDlRPS5YMVIBUpfXx8ODg7S5x9//BFGRkZYv3492rRpg969e2PatGnw8fGBkZERtm/fjoULF36wza1bt2Lx4sUICQlB+fLlpfVJSUlo164d5s6dm+0YKyurHNvT1taGtrZ2Pq6OqHQprRUjEyMVKplMBjU1Nbx+/Rpnz56FjY0NJk2aJG3PrCRzcu7cOQwaNAhr165F/fr1Fba5ubnht99+g62tLTQ0+L8yERUMdqVSgUpJSUFcXBzi4uIQHh6Ob7/9VqrsHB0dER0dje3btyMiIgLLli3Dnj17cmwrLi4OnTp1Qo8ePeDj4yO1mznCddiwYXj+/Dl69uyJixcvIiIiAkeOHMGXX36J9PT0orpkos8WR6USFYDDhw/DysoKVlZWqFevHi5evIhdu3ahadOmaN++Pb777jsMHz4ctWrVwtmzZxEQEJBjW7dv38bjx4+xefNmqU0rKyvUqVMHAFCuXDmcOXMG6enpaNmyJZydnTFq1CgYGxtLg32IKP9Ka2KUCfE5TBJClH+JiYkwMjJCwtmzMPxvRG1xkmTnrOoQlJI/znmAk6rtu1FJ1SEo1aHSDVWHkE1iUhKMPDyQkJAAQ0PDd+v++zPh7p4ADQ3DPLWXlpaI0FAjhfZKGt6YISIipTgfIxEREbFiJCIi5TIyAJks78eUdKwYiYiIsmDFSERESrFiJCIiIlaMRESkXGmtGJkYiYhIqdKaGNmVSkRElAUrRiIiUooVIxEREbFiJCIi5UprxcjESJTp6lVAV1fVUWQjT05WdQjKFde4AERFFc+XiAPF8MXrxfj3qCpMjEREpFRprRh5j5GIiCgLJkYiIlKquE1UPGvWLDRo0AB6enowNjZWuo9MJsu2bN++PU/nYVcqEREpVdzmY0xNTcUXX3wBDw8PbNiwIcf9Nm3aBF9fX+lzTkk0J0yMRERUIkybNg0AEBQU9MH9jI2NYWlpme/zsCuViIiU+pSu1MTERIUlJSWlyOIeNmwYypYti7p162Ljxo0QeSxjmRiJiKjAWVtbw8jISFoCAwOL5LzTp0/Hzp07cezYMXTp0gVDhw7F8uXL89QGu1KJiEip/AykyTwmJiYGhoaG0nptbW2l+0+YMAFz5879YJvh4eGoWrVqrs4fEBAg/ezq6opXr15h/vz5GDFiRK6OB5gYiYioEBgaGiokxpyMGTMGfn5+H9zH3t4+33HUq1cPM2bMQEpKSo7J+X1MjEREpNSnVIy5ZWZmBjMzs7yfKJfCwsJgYmKS66QIMDESEVEJER0djefPnyM6Ohrp6ekICwsDADg4OEAul+P333/H48ePUb9+fejo6ODYsWOYPXs2xo4dm6fzMDESEZFSRVEx5sWUKVOwefNm6bOrqysA4MSJE2jatCk0NTWxcuVKfPfddxBCwMHBAYsWLcLgwYPzdB4mRiIiUqq4JcagoKAPPsPo6+ur8GB/fvFxjVJKJpNh7969qg6DiKjYYWL8TD19+hRDhgxBxYoVoa2tDUtLS/j4+ODMmTOqDg3Au4d/AwICUL16dejq6sLU1BR16tTBvHnz8OLFC2m/pk2bSu871NHRQbVq1bBq1aps25QtTZs2VdHVEX0eitu7UosKu1I/U126dEFqaio2b94Me3t7PH78GMHBwXj27FmhnTM1NRVaWlof3e/58+do2LAhEhMTMWPGDNSuXRtGRka4c+cONm3ahF9++QXDhg2T9h88eDCmT5+O5ORkbNmyBcOGDYOJiQl2796N1NRUAO+emapbty6OHz+O6tWrA0CuYiEieh8T42coPj4ep06dQkhICJo0aQIAsLGxQd26dRX2+9///odOnTrhyJEjKF++PBYuXIj27dsDANLT0/HVV1/hzz//RFxcHCpWrIihQ4di5MiR0vF+fn6Ij49HnTp1sHLlSmhrayMyMhIxMTEYM2YMjh49CjU1NTRq1AhLly6Fra0tAGDixImIjo7G3bt3Ua5cOak9GxsbtGzZMtvrm/T09KT3Hk6dOhW//PIL9u/fj549e0r7vHnzBgBgamr6Se9IJKL/V9zuMRYVdqV+huRyOeRyOfbu3fvB9xNOmzYN3bp1w7Vr19C6dWv07t0bz58/BwBkZGSgQoUK2LVrF27duoUpU6Zg4sSJ2Llzp0IbwcHBuHPnDo4dO4YDBw7g7du38PHxgYGBAU6dOoUzZ85ALpfD19cXqampyMjIwI4dO9CnTx+FpJiV7CMzo+rq6kqVYn6kpKRke48jEVEmJsbPkIaGBoKCgrB582YYGxvD09MTEydOxLVr1xT28/PzQ8+ePeHg4IDZs2cjKSkJFy5cAABoampi2rRpcHd3h52dHXr37o0vv/wyW2LU19fHjz/+iOrVq6N69erYsWMHMjIy8OOPP8LZ2RlOTk7YtGkToqOjERISgqdPnyI+Ph5VqlRRaKd27dpSQs9aCWaVnp6OrVu34tq1a2jevHm+v5/AwECFdzhaW1vnuy2iz1lpvcfIxPiZ6tKlCx49eoT9+/fD19cXISEhcHNzUxjqXLNmTelnfX19GBoa4smTJ9K6lStXonbt2jAzM4NcLse6desQHR2tcB5nZ2eFe3lXr17F/fv3YWBgICW6MmXK4M2bN4iIiMgx3j179iAsLAw+Pj54/fq1wrZVq1ZBLpdDV1cXgwcPxnfffYchQ4bk96uBv78/EhISpCUmJibfbRHR54f3GD9jOjo68Pb2hre3NwICAjBo0CD88MMP0nsJNTU1FfaXyWTI+O+fe9u3b8fYsWOxcOFCeHh4wMDAAPPnz8f58+cVjtHX11f4nJSUhNq1a+Pnn3/OFo+ZmRkMDAxgbGyMO3fuKGyrWLEiAMDAwADx8fEK23r37o1JkyZBV1cXVlZWUFP7tH/PaWtr5+n1UESlVXGbqLioMDGWItWqVcv1s4tnzpxBgwYNMHToUGndhyq+TG5ubtixYwfMzc1zfIFwt27dsHXrVkyZMiXH+4xZGRkZwcHBIVdxE1HBycgAPnLLP5vPITGyK/Uz9OzZMzRv3ly6HxcZGYldu3Zh3rx56NChQ67acHR0RGhoKI4cOYK7d+8iICAAFy9e/OhxvXv3RtmyZdGhQwecOnUKkZGRCAkJwYgRI/DPP/8AAGbPno3y5ctLk4heu3YNERER2LNnD86dOwd1dfVPun4iok/BivEzJJfLUa9ePSxevBgRERF4+/YtrK2tMXjwYEycODFXbXz99de4cuUKunfvDplMhp49e2Lo0KE4dOjQB4/T09PDX3/9hfHjx6Nz5854+fIlypcvjxYtWkgVpKmpKS5cuIC5c+di/vz5iIyMhJqaGhwdHdG9e3eMGjXqU78CIioApbVilIn3HxojKmUSExNhZGSEhNWrYairq+pwssvlBK1FLjlZ1RHkaOm1ZqoOQamRtvtUHUI2icnJMOrVCwkJCdI/XjP/TBgYJEAm+/icilkJkYiXL40U2itpWDESEZFSpbVi5D1GIiKiLFgxEhGRUqwYiYiIiBUjEREpV1orRiZGIiJSqrQmRnalEhERZcGKkYiIlGLFSERERKwYiYhIOVaMRERExIqRiIiUE+LzqADziomRSr3M9+gnvn6t4khy8OqVqiNQrhi/RPzNm0RVh6BUYjH8zjJjUj6fRH6+x+L53ecFZ9egUu+ff/6BtbW1qsMgUqmYmBhUqFABAPDmzRvY2dkhLi4uX21ZWloiMjISOjo6BRlikWFipFIvIyMDjx49goGBAWR5HWnwnsTERFhbWyMmJqZYTblTXOMCim9sxTUuoGBjE0Lg5cuXKFeuHNTU/n/YyZs3b5CampqvNrW0tEpsUgTYlUoENTU16V/KBcXQ0LDY/WUKFN+4gOIbW3GNCyi42IyMjLKt09HRKdHJ7VNwVCoREVEWTIxERERZMDESFSBtbW388MMP0NbWVnUoCoprXEDxja24xgUU79g+Bxx8Q0RElAUrRiIioiyYGImIiLJgYiQiIsqCiZGIiCgLJkaiPHj48CFiYmJUHUaJFhUVpbLv8OTJk1iyZIlKzk0lBxMjUS7du3cPXbt2haurK5NjPt27dw/dunWDm5sbrl27VuTnj42NxejRo7F+/foiPzeQ04u6qbjhK+GIcuHOnTuYOHEiNDU18fz5c4SGhhbLF4//888/iIqKQlRUFGrXrg0LCwuUKVNG1WEBAO7evYuRI0fCysoKtra2qF+/Ps6dOwcXF5ciOb8QAj169ICRkRHatm0LTU1N+Pn5Fdr5/v33Xzx8+BBRUVGoVasWLC0tUaZMGWRkZCi8k5SKIUFEH3T37l3RqlUr0bFjR+Hm5iZat26t6pCySUtLE7/++qswNzcX2traQl1dXVSqVEk0btxY3LhxQ9Xhifv37wtvb2/Rvn17cfr0aSGEEBMnThRmZmbizp07RR7PoEGDhEwmE1u2bCnwttPT08Vvv/0mzM3NhY6OjtDQ0BCVKlUSDRs2lH4X6enpBX5eKjj8ZwvRB9y7dw/Dhw9Heno6+vXrBzc3N/Tv3x8AkJKSouLo/t+PP/6IXr16oX79+vjll1/w9u1brF+/HjVr1kSjRo1w8+ZNlcV27949DB06FKdPn4a5uTk8PT0BALNmzYKnpyeWL19eZF2MCQkJWLt2Lf744w94eXlhwIABWLduXYGeY/369dLvYuvWrUhNTcXGjRvh5uaGhg0b4tatW1BTU2O3anGm6sxMVFzdvXtX+Pr6ipYtW4o7d+6IPXv2CBsbm2wVzokTJ8S///6roiiF+Pvvv4WGhobo27eviIqKktZnViWTJ08WLi4uIiYmpshjy/wOW7VqJRYsWCDMzc1FYGCgFF+PHj1Ep06dREpKSqHHkpCQINauXSvKli0rhg4dKoQQ4sCBA0Imk4lNmzYVyDnOnz//wd/FxIkThaenp3jy5EmBnI8KBxMjkRLh4eGibdu2wsfHR5w+fVokJSWJpk2birVr1wohhHjy5Ik4ePCgGDBggFBTUxNly5YVkZGRRRpjRkaGSEtLE507dxZVq1YVN2/elNZnlZCQIPr06SO2bt1apPHdvn1b+g7/+usvIYQQR44cEXp6eqJ79+5i+PDhQlNTU8yePVs65p9//imUWLImxeHDh0vrDx8+LEJDQ8XUqVPFq1ev8t1+5u+ia9euCr8LIRS7TR8+fCh8fX3FxYsX830uKnzsSiV6T2pqKgICAvC///0PkydPhqenJ65fvw5jY2MYGxvj999/R+PGjbFr1y5cvnwZtWvXhomJCSIiIoo0TplMBnV1dURERKBBgwaoVq2atP6ff/7BmjVrALybs69cuXJISkoqstjevn2LcePG4cKFCwgMDESjRo0ghEDLli3h7++PO3fuQENDA9OmTYO/vz8AoGbNmrC2tsb169cLNJbExERs374dkyZNQq9evbB8+XIA77qfO3fujPLly2PChAnQ09PL9zkyfxf379+Hh4eH9LsAoDDQpkKFCtDT0+Pgm2KOo1KJ3qOlpYWFCxfi0aNHqF+/PoB398MOHjyItLQ0JCQkYPjw4XB0dMTLly+RkpKCyZMno1mzZli6dCnatGkDBweHIovXwMAAmpqaCusiIyMxYsQIJCQkYPz48Zg7dy7i4uKQkJCA9PR0aGtrQ19fv9Bi0tTUxOzZs+Hq6opjx47B1dUVaWlp0NTUhJWVFd68eYNp06ZJk+zWq1cP6enp6NmzJzw8PHDmzJkCGa0aHx+PXbt2YfLkyejVqxeWLl0qbbtx4wZat24NPT29ApulwsDAQJrcVwgBmUwmbcscjbpz506oq6sXyPmokKi6ZCUqCX766SdRr1498ccff4gnT56IJ0+eiM6dO4smTZqI3bt3i7S0NDFz5kxhbGwsKlSoIJ4+fVroMWV2mU6YMEG0bdtWJCQkCCH+v+tu5cqVQiaTif379wshhFizZo0wNzcXbm5uomXLluLWrVuFHmNoaKjQ0dERAQEB4p9//hEPHz4UTk5OokWLFtI+jRo1Ek5OTtJo1cmTJwsjIyMRHh7+yeffu3evkMlkYtSoUQrrL1y4ILS0tMS+ffuUHpfXUaOZv4vx48eL9u3bK3TLpqWlKex78eJFcf36dXHv3j2FY6n4YGIkyqXMASK3bt0SnTt3Fp6enmLv3r0iPT1dTJ06VbRt21bY29uLcuXKifv37xdZXPfu3ROWlpbi+++/l9bduHFDeHt7i+bNm4tjx46JmzdvCjU1NeHv7y8OHz4sRo4cKUxMTMT169cLPb7Lly8LOzs74eLiImQymbC1tZWStaenp5DJZGLIkCEKxwwePFj06tVLvHnz5pPOnZaWJpYsWSJ9TklJEenp6WLChAmiS5cuCknr4cOH4ty5c590vrt37wpzc3Ph7++fbdvp06fFF198ITQ1NYW9vb2oWLGi+PXXX4UQTI7FDRMjUS5k/sV18+ZN0blzZ9GwYUOxb98+kZ6eLqZPny7q1asn+vbtK5ydncWaNWuKPL7Lly8LExMTMWbMGLFu3TrRokUL0bp1a7F3714hhBD79+8XNjY2CseMGzdO1KhRQzx8+LDQ44uOjha7du0Sy5cvl87XtGlTUalSJbF48WJRtmxZMW/ePGl/Pz8/0bJly08659u3bxU+p6amCiGEePPmjbC3txfr168XQgjx4sULMX78eNG0aVMhk8k+eWDM5cuXhY2NjRgzZoxYtmyZePXqlbhw4YLw9fUV1apVE3///bcICwsT69evF3K5XKqUqfhgYiTKpdu3bwsvLy/RrFkzhaRYq1Yt8cMPP4iQkBDRrVs3cfnyZSHE//9FXFTVwI0bN8QXX3wh6tSpIxo0aCBVZUIIERUVJXR1dcX8+fOldQ8fPhQtWrQQ27ZtK5L4smrcuLGwtbWVRqseP35c6OnpiZEjR4oxY8aIxo0bi6FDh4rXr1/n+xxZv/esP0+fPl3UrFlT3L59WwwdOlTUrl1bWFpaipUrV4pTp05layc/D+PfuXNHTJgwQXh7e4vw8HDRu3dvUaVKFRERESHFk5GRIZo0aSLmzp2bj6ujwsTESJRLMTExonXr1mLnzp1SUqxdu7bUbdahQwfRoUMHhWP++ecfsWjRIukvxMKWmJgo4uLixF9//SXOnj2rsG3x4sXC2dlZ/Pnnn9K6w4cPixcvXhRJbJnJqUuXLkImk4nLly8rJJ1169YJOzs70atXLzFixAjpHtybN2/EkSNHxIMHD/J13qzPE7548UK0bt1amJqaCrlcLnx8fMSCBQsU7gm+ePFCnD17VoSFhUnr3q8+cyMjI0O8efNGPH36VGhra4vg4GDpejLVq1dPTJgwQdqfigcmRqI8ePnypXSPqn79+mL06NEiJSVFnDt3Tnh7e4vo6GghxLtBH/Pnzxc2NjZCJpOJcuXKibi4uCKL886dO6JTp05i9+7d0rpbt24JX19fMX78eKXH7Ny5U9y+fbvQY7ty5YowMDAQAQEBCut/+ukn4eTkJFJSUqRE9Pr1a3Hw4EFhZmYmunTpkudzJSUlie7duwt/f38xffp00alTJ6GjoyP69OkjVcqZyfn69eti8uTJwtTUVJiZmQkzMzMxbtw4qa179+5JLwbIi4cPHwoHBwdx8uRJhfXbtm0TMplMHD16NM9tUuHi4xpEeSCXy/G///0P58+fh6urK2bPng0tLS389ddfkMlkCAkJwZEjR3Dp0iX4+vrCxsYG9vb2KFu2LF6/fl1kcZqamuL169f466+/0KlTJ2RkZMDJyQnNmzfHvHnzMG7cOIWXi2/evBlffvklACAuLg7m5uaFFlutWrUQEhKCBg0aAABGjx6NxMREnD59Gq9evUJCQgLMzMyQkpKC48ePY+zYsXBzc8Ovv/6a53Pp6+tj/Pjx6NWrFypUqAAbGxucP38e1apVg4bGu7/+1NTUEBERgeXLl+Py5csYN24cunXrhtu3b2Pw4MHw8vJClSpV0KZNGwgh8OLFC5iYmOQ6hszHQY4ePYr69etDS0sLv/zyC0aMGIERI0bA29sb8fHxePz4MeLi4mBlZYXKlSsjLS0NGhoaSE9P5+MdRU3VmZmoJHr8+LE0SjU2NlYYGBgImUwmevfuLfz8/MS///4rFi1aJFq1aiW+/PJLqRJLSEgokkc5hHh3z7FMmTIiICBAJCUlifT0dDFixAhRq1YtkZSUJO23fPlyIZPJhJ2dnWjRokW+ug3z49KlS8LW1lbUqVNHWFlZCQ0NDWlAzOvXr8X+/fuFi4uLaN++vXRMfmNLTEwUQvz/fd/MKjGzvfXr1wtHR0exYcMGheOmTZsmli5dKqpWrSoaNWokvfovr6+wu3z5sihbtqyoXbu2qFGjhlBTUxOjR48Wb968EZGRkaJJkybCwcFBWFpaCnNzc7Fjxw4hhBCvXr0Sy5YtExs3bszXdVP+MDESfYLMv2C3bdsmZs6cKf73v/8JIYTYvn27qFOnjvjyyy+l5wXj4+PFt99+K1q2bFlkyfHKlSvC1tZWtG3bVjg6OoqaNWuKb7/9VkoQy5cvF2pqamLVqlWiZ8+eYvz48eLt27dFdr/rwYMHYtWqVWLs2LHSPbjU1FTx+++/ixo1agiZTCbmzJkjhHj36EXmIoQQycnJuT5P5vUou66MjAxRo0YNMWjQoGz79+rVS8hkMtGkSROF9+GuW7dONGjQIE/XGhERIdatWycWLFggDfKJjIwUFSpUEK1atRK//vqruHfvnli5cqWoWbOmuHbtmtiwYYNwcXERrVu35owcRYiJkaiArVmzRtStW1cMGDBAqhTj4+PFhAkThKenp/j222+zPfRdmCIiIsTy5cvFqFGjxObNm6XqadmyZUImk4mVK1eKX375RZiamioMVFGF169fiwMHDojKlSsLHx8fcezYMWFmZiZmzZolhPj/hHX8+HFhaWkpYmNjC+ScDRs2FCtWrFBYHxYWJiwsLETr1q2lF7Bn/t7evHkjdu3a9UnnTUhIEL6+vsLHx0fh3arJycmie/fu4ocffhAeHh6id+/eRT7CubRjYiQqQKtXrxYeHh5Kk2LmCMTMv1xVWQEsXbpUyGQyERgYKN6+fStGjRolpk6dqrBPZGSk2Lp1a5G8IUeId5Xib7/9JqpXry68vb2l9X/88YcoU6aM9LLxv/76S+jp6YnOnTuLR48eFci527ZtK5o0aSISExNFbGysOH36tLCzsxMNGzaUKsXk5GTh7+8vZsyYUSDnjI6OFg4ODiIoKEhh/Zs3b4SDg4MwMTER3bp1k5JiUf5jqrRjYiQqIBs2bBBVq1YVgwcPlpLiixcvpKTo7++fLSmqIjnOnTtXaGtrS8/P3blzR5iZmYnTp0+LlJQU8fDhQ9GnTx/RoUMHIZPJhEwmK5Jpkl6+fCnq1Kkj6tWrJ63LvAd46NAhYWFhIQYMGCA0NTXFl19+WSBTfWVWYKmpqaJ27dqif//+omrVqkJXV1c0atRISrzJycli9uzZQiaTiYULF+bYTl6EhoaKChUqiKtXr0rrEhISRGBgoDAxMREDBw4ssvu9pIiJkaiAREZGiuHDh0vdYrlNitHR0YU23dL7nj9/LsqUKSOmT58uhHiXeL755hvRo0cPcePGDdGjRw9Rv3590bp1azF58mQhk8nEwIEDpaqlsGV9C09mUsj83ubOnSu9Pq4g57/MPE96eroICwsTMplMdOnSRaFSnDlzppDJZGLVqlVSTMnJyeLKlSvi0aNH+bpn/PjxY1GxYkXx/fffi2fPnon4+HgxY8YMUatWLdGzZ0/pO09OThZv376VXnbA7tTCx8RIVIAy/zJ78eKFmDRpkpQUs/7lm/W/UVFRolGjRqJDhw7Svb/C9uzZM+nn58+fi6pVqwpzc3PpHZ9Hjx4VoaGhwsrKSvTr16/IkmJWmckw878nTpwQmpqa4quvviqU+6CZv4+0tDTx9ddfS8+jJicnixkzZkj3YoV4V9lOnjxZ1KxZU8jlcmFhYSG8vLzy9QahS5cuiTp16oguXboIW1tbYWhoqHBPMSgoSPTv31/Uq1dP9O7dWxw8eFAIweRY2JgYiQpQ5qu+Zs6cKSwsLBTuKb5fMUZGRoq2bdsKLy8v8eOPP6os5h07doiFCxdKXXonTpwQ5ubm4osvvlDpfa3M7+nkyZMF2n2ak/ev9dWrV2LWrFnSqF0h3j320bp1a1G2bFnh7e0tgoODxa5du8SSJUuErq5utsc9ciMuLk4sXLhQaGtri6+//lpKivPnzxcymUyMGzdO7NixQ8yaNUvo6OjkOCMIFRwmRqJCcOfOHaGjoyPNGpG1IhHi/5Ni06ZNxdatW6XjVJWIMiuQ4OBgYWRkJHr06FEsHg84ffq0kMlkYtCgQYWaFJXZtGmTkMlk0rOVKSkpYtCgQcLMzExMnDgx27OMO3fuFBUrVszXS8ijo6PFl19+KSXFn376SchkMlG/fn1hZ2cn3etctGiR8PLyEs+fP//Eq6MPYWIkKiSXLl0Senp6YtiwYSIlJUVKPlFRUR9NiqoYdBESEiKMjY1Fz549i0VSFOLda+yGDRsmdW0Wpbdv3ypUZzdv3hTly5cXvXv3lp6hzHof9MWLF8LT01N6OD+/Xr9+Lfz8/ISfn59ISkoSgwcPFuXLlxdPnjwRJ0+eFHZ2duLx48efdA76MDVVv3mH6HPl5uaG06dPY8OGDRg5ciSEEHj+/DmmTp2KpKQkDBo0CL179wYAhdd+3bt3D+PHj8f9+/eLLNbTp0+jWbNmaNeuHbZu3Qo1teLxV4OTkxOWLl0Ka2vrIj1v5uvY2rdvL627cOEC4uPjMWvWLOjq6iIjI0N6rZy6ujqMjY3x8OFD3Lhx45POraOjg9jYWKipqUFfXx9r165F69at4ebmhoMHD8LV1RW6urqIj4/HtWvXPulcpFzx+L+f6DPl6uqK06dP4+TJk/jrr78QFRWFv//+G506dVKaFB88eIAlS5Zg8eLFiIyMLLI4zczMMHDgQGzatKnYJMVMqnhPaGbCy0oIAT09PWhqagKA9D1lZGQAAG7duoX4+HiYmZl90rkzMjLg4uKCtLQ0AIBMJsO6devQqVMnzJ8/H7Vq1YKBgQGeP3+O9u3b486dO590PlJC1SUrUWkQFxcnUlJSxOLFi4W5ubm0PmuXaUREhPjqq6+EkZGR+OOPP4QQHH1YnISEhAh3d3fpGdX09HSF30/Xrl2FhYWFNItK1m157Zq+deuWMDIyEhMnTpReMyiEEMeOHZN+PnjwoDA3N5emzOL/KwWneP3TkOgzZWFhAS0tLVSsWBHOzs548eIFUlNTpcrkwYMHmDdvHnbs2IGff/4ZrVq1ghACMplMxZFTpoYNG0Iul2P06NF48+YN1NTUIJPJ8OLFC/Ts2RN79+6Fv78/ypQpg7S0NIXfXXJycp7O5eTkhBMnTmDHjh0YN24cdu7cCQDw8vKS9nn8+DFMTU2hp6cHANL5hBCfeqmlHhMjURGqXbs2Hj58iGXLlklJ8cqVKwgMDMT27dvx888/S9MbMSkWHxkZGVBXV8fRo0dx584dNGrUCIMHD0a3bt1Qu3ZtHD58GHPnzkX37t2hqakp/W6PHj2K9evXw9HREUFBQXk6p6urK44ePQp3d3fY2NgobEtISMD169dRpUoV7NixA4sWLUK7du3w559/8v+bAiAT/OcFUZEKCwtD69at0bx5czx9+hQpKSmIiYnBypUr4evry6RYTGUOyBk+fDj27t2Lbt264f79+zA1NcWYMWNgb28PPT09JCcn49KlS1i9ejXu3buHunXrYtOmTejcuTO2bt2ar3P/888/+OOPP3Dr1i08ffoUV65cwe3bt2FtbY3Xr1+jVq1aMDU1hbOzM/z9/fn/zyfiRMVERSxzot7du3fj2rVrqFOnDho1agR3d3cmxWIsswq0tLSEl5cXFi1apLA9KioKERERGD9+PKytrZGYmIhffvkF8fHxePv2LQYMGJDvc79+/RpBQUGQyWRo3LgxvL29MW/ePHz11Vfo3bv3Jw/4IUWsGImI8uDQoUNISkrCF198AQC4e/cuDh8+jF9//RVPnz7FnTt3cOHCBbi7uwMAFi1ahF27dmH37t2wsrLK93lfvXoFfX19AO9GwNapUwcXLlxA9erVFfbLrGwp/3iPkYgoD1q1aoVWrVoBeHfvceHChRg1ahRatmyJefPmwdfXV6oO4+LisGrVKowdOxZWVlafNDAmMykCQGxsLMzMzFCmTBlp3apVq/DPP/9AQ0MD6enp2Y5/+fKlFDN9GBMjEVEeyeVyAO+eZVy1ahUePnyIyZMno127dvjpp59gaGiImjVr4ujRo6hRowbq168PAAXWTX7q1CmYmpoqVKAPHz5E/fr18fDhQ6irqyskx5UrV6JSpUq4du0a1NTUmBw/gomRiCifMl/OYG1tLSUiU1NTHD58GObm5vDz84OTkxPKly9foOft1asX3NzckJ6eLlWhc+bMQbt27VC3bl08fvxYejHCmjVrMGbMGJQvXx4NGzbE1atXmRw/gh3RRET5lPWtPFl/lsvlaNmyJdTU1DBmzBgA77owC+qtQpUrV8a6desgk8mkxCiTybBy5UpUqVIFWlpaAICNGzdi2LBhmD17NsaPH49JkybBw8MD586dg4uLS4HG9DlhYiQiKgTBwcGwsbGBoaEhABR4Asrsls3aPaumpoZRo0YBAFavXo3hw4ejadOmGD9+PABg1qxZUFNTg6urKy5fvoxatWoVaEyfCyZGIqICtn37dhw7dgyRkZHQ0tIq8sdw1qxZg+HDh6Nv3744dOgQ5syZgwkTJgAAZsyYgbdv3+Lw4cOoWbMmK0YlmBiJiApY165d8eTJE5QtW7bIuyvXrl2LYcOGYdasWZgwYQKOHTuGHj16QE1NDePGjQPw7n5kSkoKk2IO+BwjEVEBUuVzhLt27UL37t2xdOlSDB8+XKpSjx07hrZt22LKlCmYNGkSACA+Ph7GxsZ8qYQS/OcCEVEBUuXD9TVq1AAAPHv2TOGl4t7e3ti/fz9cXFwAvHsBuaOjI65evcqkqAS7UomIPhNOTk4IDQ1FgwYNIITAtGnTpMTn4+MD4N0jJuvWrYONjQ0qVaqkynCLLSZGIqLPiJubG86cOYMmTZogLS0Ns2bNAgCFLtM///wTjRs3ho6ODrtSlWBiJCL6zNSuXRshISGoX78+PDw80LZtWyn5LVy4EFeuXMGOHTsUun0TExNhaGjIRAneYyQi+iy5u7sjMjISlStXltZFR0cjODgY48aNg7m5OQAgKSkJ/v7+aN++PU6dOlXqkyLAipGI6LNlbW2t8PnGjRsICwvDypUrcfv2bRw4cABLliyBtbU1XF1dYWtrq5pAixk+rkFEVAo8e/YMTZs2haGhIfr164eJEyeiVq1aqF+/PgICAqCpqQl1dXXcuXMH6enpqFatWqntVmViJCIqBZ4+fYoaNWogLS0NVlZWGDt2LJo1awYbGxtpnydPnmDixIk4f/48Zs6ciQ4dOpTK5MjESERUSty/fx/Xrl1D3bp1UaFCBWn969evoaamBm1tbSQmJmLHjh0YOXIkfvvtN2nuydKEiZGIqBT7448/sHPnToSHh8PZ2RlTpkxBxYoVMWXKFNy6dQubN2+Gvr6+9Gq70lBBclQqEVEpJITAs2fPsGbNGjx8+BCtWrVCcnIyWrRogRcvXsDAwAARERHQ1tYG8G7mjvT0dEydOhWbN29WcfSFi6NSiYhKIZlMhuTkZFy+fBkLFixAjx498Pr1a4wePRpVq1aFh4cHWrRogfT0dGhoaCA9PR2LFi3CoUOHUKVKFfTo0UNKmp8bVoxERKWUrq4ujI2NcfnyZaSnp0NXVxd9+vSBiYkJHjx4gDZt2kBbWxvp6elYsGABdu/ejbp162L58uWfbVIEeI+RiKhUu3z5Mjp37gwfHx88f/4cN2/eBAAsWLAArVu3RlpaGhYsWIB9+/bB1dUVgYGBMDIyUnHUhYsVIxFRKebm5obTp0/D2dkZp06dghACCxculJLi/PnzS1VSBFgxEhER3j3n2LFjR3z//ffo2LGjlBT3799fqpIiwMRIRET/efnyJQwMDJCWloaFCxdi7969pS4pAuxKJSKi/8jlcgDA8uXLERQUBHd391KXFAE+rkFERP/JfHC/U6dO+PfffzFp0qRSlxQBdqUSEZESpeENNzlhVyoREWVTWpMiwMRIRESkgImRiIgoCyZGIiKiLJgYiYiIsmBiJCIiyoKJkYiIKAsmRiIioiyYGIkoT2JiYjBgwACUK1cOWlpasLGxwciRI/Hs2bNctxEVFQWZTIawsLBCiVEmk2Hv3r2F0jZ9/pgYiSjXHjx4AHd3d9y7dw/btm3D/fv3sWbNGgQHB8PDwwPPnz9XdYhEn4yJkYhybdiwYdDS0sLRo0fRpEkTVKxYEa1atcLx48eld2sCyis2Y2NjBAUFAQDs7OwAAK6urpDJZGjatCkAwM/PDx07dsS0adNgZmYGQ0NDfPPNN0hNTZXasbW1xZIlSxTarlWrFqZOnSptB96971Mmk0mfiXKLiZGIcuX58+c4cuQIhg4dCl1dXYVtlpaW6N27N3bs2IHcvH75woULAIDjx48jNjYWu3fvlrYFBwcjPDwcISEh2LZtG3bv3o1p06blOs6LFy8CADZt2oTY2FjpM1FuMTESUa7cu3cPQgg4OTkp3e7k5IQXL17g6dOnH23LzMwMAGBqagpLS0uUKVNG2qalpYWNGzeievXqaNOmDaZPn45ly5YhIyMjV3Fmtm1sbAxLS0vpM1FuMTESUZ4U9oQ8Li4u0NPTkz57eHggKSkJMTExhXpeokxMjESUKw4ODpDJZAgPD1e6PTw8HCYmJjAzM4NMJsuWQN++fVsgcaipqRVa20QAEyMR5ZKpqSm8vb2xatUqvH79WmFbXFwcfv75Z3Tv3h0ymQxmZmaIjY2Vtt+7dw/JycnSZy0tLQBAenp6tvNcvXpVof2///4bcrkc1tbWAJCt7cTERERGRiq0oampqbRtotxgYiSiXFuxYgVSUlLg4+ODv/76CzExMTh8+DC8vb1Rvnx5zJo1CwDQvHlzrFixAleuXEFoaCi++eYbaGpqSu2Ym5tDV1cXhw8fxuPHj5GQkCBtS01NxcCBA3Hr1i388ccf+OGHHzB8+HCoqalJbf/00084deoUrl+/jv79+0NdXV0hTltbWwQHByMuLg4vXrwogm+GPidMjESUa46OjggNDYW9vT26deuGSpUq4auvvkKzZs1w7tw5aRDNwoULYW1tjUaNGqFXr14YO3aswn1DDQ0NLFu2DGvXrkW5cuXQoUMHaVuLFi3g6OiIxo0bo3v37mjfvr30KAYA+Pv7o0mTJmjbti3atGmDjh07olKlSgpxLly4EMeOHYO1tTVcXV0L90uhz45MFPaddCKiXPLz80N8fDzfWkMqxYqRiIgoCyZGIiKiLNiVSkRElAUrRiIioiyYGImIiLJgYiQiIsqCiZGIiCgLJkYiIqIsmBiJiIiyYGIkIiLKgomRiIgoCyZGIiKiLP4Pi7HhRVQfnzIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import plt_scaled_colobar_ax\n",
    "# (n_input, n_output)\n",
    "W = np.stack([regr.estimators_[i].coef_ for i in range(len(regr.estimators_))]).T\n",
    "b = [regr.estimators_[i].intercept_ for i in range(len(regr.estimators_))]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(3,6))\n",
    "im = ax.imshow(W, cmap='bwr')\n",
    "fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "ax.set_ylabel('Input')\n",
    "ax.set_xlabel('Output')\n",
    "ax.set_yticks(list(range(len(datasets))), datasets)\n",
    "ax.set_xticks(list(range(len(tasks))), tasks, rotation=-45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6253b9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.8 ,  18.75,  -2.45,  -2.4 ,  -1.85],\n",
       "       [ 12.45,  18.75,  -0.3 , -25.6 ,   7.55]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = regr.predict(X[-2:])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8a4dcfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.899999999999999, 22.5, 2.5, 2.3999999999999986,\n",
       "        -2.900000000000002],\n",
       "       [6.700000000000003, 22.5, 5.899999999999999, -1.2999999999999972,\n",
       "        8.399999999999999]], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4576003e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMLU</th>\n",
       "      <th>GSM</th>\n",
       "      <th>BBH</th>\n",
       "      <th>TydiQA</th>\n",
       "      <th>Codex-Eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50.4</td>\n",
       "      <td>36.5</td>\n",
       "      <td>39.4</td>\n",
       "      <td>49.8</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49.2</td>\n",
       "      <td>36.5</td>\n",
       "      <td>42.8</td>\n",
       "      <td>46.1</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MMLU   GSM   BBH TydiQA Codex-Eval\n",
       "13  50.4  36.5  39.4   49.8       23.7\n",
       "14  49.2  36.5  42.8   46.1       35.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
