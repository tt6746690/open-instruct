{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ppc64le', 'dcs')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "    get_run_statistics)\n",
    "import pandas as pd\n",
    "import json\n",
    "import platform\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import socket\n",
    "\n",
    "arch = platform.uname().processor\n",
    "hostname = socket.gethostname()\n",
    "cluster = 'ccc' if hostname.startswith('ccc') else ('dcs' if hostname.startswith('dcs') else 'npl')\n",
    "arch, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir)\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|‚ñè         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "# !cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"Running on $SLURM_JOB_NODELIST\"\n",
      "echo \"======\"\n",
      "\n",
      "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
      "master_port=10002\n",
      "RDZV_ENDPOINT=$master_addr:$master_port\n",
      "\n",
      "source ~/.profile\n",
      "conda activate open-instruct\n",
      "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
      "\n",
      "set -e\n",
      "set -x\n",
      "echo \"======\"\n",
      "srun {cmd}\n",
      "\n",
      "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=256), micro-bsz=1,  grad-ckpt,  a100_80gb, 37s/it, 43hrs# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n",
    "\n",
    "# aimos\n",
    "# shared: humanmix, max_sequence_length=2048. 1 node = 6x v100_32gb\n",
    "\n",
    "# take-aways: \n",
    "#   (1) fsdp (v4.28.1, v4.32.0.dev0 are pretty similar in terms of speed. don't use v4.31.0)\n",
    "#   (2) micro-bsz=1->2 seems to be the best here. leads to ~30% reduction in runtime.\n",
    "#   (3) increasing #nodes almost linear reduction in time, e.g., Use 4x nodes cost 30% time (25% if linear.)\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=none, nodes=1, 74s/it, 89hrs, loss=0\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 72s/it, 86hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=none, nodes=1, 52s/it, 66hrs, loss=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 49s/it, 61hrs\n",
    "# - resume from lastest checkpt (trained 4.28.1, resume 4.32.0.dev0), did\n",
    "#   did not found `optimizer.bin` \n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# # nodes > 1\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=4, 14s/it, 18hrs\n",
    "\n",
    "# take-away: \n",
    "#   (1) torch_dtype=float16 gives loss=0. setting torch_dtype=float32 solves the issue but takes more memory and compute\n",
    "#   (2) mbsz=2 oom for nodes=1 but works fine with nodes=2. more nodes -> potentially larger mbsz.\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, 139s/it, 166hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=2, 33s/it, 41hrs, loss!=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 17s/it, 21hrs\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 18s/it, 21hrs\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=5, 4s/it, 21.7hrs\n",
    "\n",
    "\n",
    "#\n",
    "# deepspeed\n",
    "# shared: llama7b, deepspeed, grad-ckpt\n",
    "# take-aways\n",
    "#   (1) with deepspeed, using mixed-precision gives x50% speed improvement\n",
    "#   (2) no loss overflow issues. deepspeed has good mixed-precision integration, loss_scaler handles it.\n",
    "# \n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 161s/it, 192hrs, loss ok, loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=none, nodes=1, 259s/it, 309hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=fp16, nodes=1, 169s/it, 202hrs, loss ok. loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=none, nodes=1, 258s/it, 307hrs, loss ok.\n",
    "#\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 108s/it, 135hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3,offload), float32, mp=fp16, nodes=1,  96s/it, 120hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3), float32, mp=fp16, nodes=1, 123s/it, 147hrs, loss ok.\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=1, 66s/it, 83hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=1, oom\n",
    "#\n",
    "# nodes>1\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=2, error with downloading config.json\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=4, \n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=4, 16s/it, 21hrs\n",
    "# llama7b, micro-bsz=6, deepspeed(s=3), float32, mp=fp16, nodes=4,  3s/it, 17hrs\n",
    "\n",
    "\n",
    "# 1 node, 1 v100_32gb, no-grad-ckpt, nodes=1\n",
    "#\n",
    "# gpt2, micro-bsz=1, float32, mp=16, 7s/it, 8.6hrs\n",
    "# gpt2, micro-bsz=2, float32, mp=16, 5s/it, 5.6hrs\n",
    "# gpt2, micro-bsz=4, float32, mp=16, 5s/it, 6.3hrs\n",
    "# gpt2, micro-bsz=8, float32, mp=16, 6s/it, 6.6hrs\n",
    "# gpt2, micro-bsz=16, float32, mp=16, oom\n",
    "# gpt2-medium, micro-bsz=2, float32, mp=16, 14s/it, 17.0hrs\n",
    "# gpt2-medium, micro-bsz=2, float32, mp=16, 10s/it, 12.2hrs\n",
    "# gpt2-medium, micro-bsz=4, float32, mp=16, oom\n",
    "# gpt2-medium, micro-bsz=8, float32, mp=16, oom\n",
    "# \n",
    "\n",
    "\n",
    "shell_scripts_template_slurm = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template_lsf = \"\"\"\n",
    "echo \"Running on $LSB_DJOB_HOSTFILE\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\")\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$LSB_JOBID*.out\" ] && mv {log_dir}/$LSB_JOBID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf\n",
    "\n",
    "print(shell_scripts_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10s/it, 12.2hrs\n"
     ]
    }
   ],
   "source": [
    "t = '0:08:49'\n",
    "n = 51\n",
    "total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a669464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot\n",
      "sharegpt\n",
      "dolly\n",
      "gpt4_alpaca\n",
      "flan_v2\n",
      "super_ni\n",
      "stanford_alpaca\n",
      "baize\n",
      "code_alpaca\n",
      "self_instruct\n",
      "unnatural_instructions\n",
      "oasst1\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "p = '../data/processed/'\n",
    "for x in os.listdir(p):\n",
    "    y = os.path.join(p, x)\n",
    "    if os.path.isdir(y):\n",
    "        d = os.path.join(y, os.listdir(y)[0])\n",
    "    else:\n",
    "        continue\n",
    "    d = d[3:]\n",
    "    if 'shuffled' in d:\n",
    "        continue\n",
    "#     print(f\"train_file = '{d}'; abbr_train_file = '{x}'\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'baize': 100000},\n",
       " {'code_alpaca': 100000},\n",
       " {'cot': 100000},\n",
       " {'dolly': 100000},\n",
       " {'gpt4_alpaca': 100000},\n",
       " {'oasst1': 100000},\n",
       " {'self_instruct': 100000},\n",
       " {'sharegpt': 100000},\n",
       " {'stanford_alpaca': 100000},\n",
       " {'super_ni': 100000},\n",
       " {'unnatural_instructions': 100000},\n",
       " {'cot': 25000, 'flan_v2': 25000, 'dolly': 25000, 'oasst1': 25000},\n",
       " {'cot': 40031, 'flan_v2': 40031, 'dolly': 6009, 'oasst1': 13928},\n",
       " {'baize': 8333,\n",
       "  'code_alpaca': 8333,\n",
       "  'cot': 8333,\n",
       "  'dolly': 8333,\n",
       "  'flan_v2': 8333,\n",
       "  'gpt4_alpaca': 8333,\n",
       "  'oasst1': 8333,\n",
       "  'self_instruct': 8333,\n",
       "  'sharegpt': 8333,\n",
       "  'stanford_alpaca': 8333,\n",
       "  'super_ni': 8333,\n",
       "  'unnatural_instructions': 8333}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_mixture_samples = 100000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_mixture_samples/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: n for k, n in zip(['cot', 'flan_v2', 'dolly', 'oasst1'], [40031, 40031, 6009, 13928])}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_mixture_samples/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gpt2-medium using 1 GPUs, 2 batch size per GPU, 64 gradient accumulation steps.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"ft2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=ft2 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:1 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpwwejg9q3', 'job_id': 705158}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ft1: reproduce open-instruct table with llama7b\n",
    "job_name = 'ft1'\n",
    "\n",
    "# ft2: test mixture weights\n",
    "# vary mixture weights\n",
    "job_name = 'ft2'\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "job_duration = 6 if arch == 'ppc64le' else 12\n",
    "job_duration = 6\n",
    "# shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "nodes = 1 # 128/(5*6*2)~=2.1\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem = 512 if arch == 'ppc64le' else 64\n",
    "num_gpus = 1; gpu_type = 'a100_80gb'\n",
    "num_gpus = 6; gpu_type = 'v100'\n",
    "num_gpus = 1; gpu_type = 'v100'\n",
    "debug_mode = test_run\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; abbr_model_name = 'gpt2'; max_seq_length = 1024\n",
    "model_name_or_path = 'gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "# subsample_mixture = {'flan_v2': 100000}\n",
    "# subsample_mixture = dict(sorted(subsample_mixture.items()))\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets\n",
    "]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(total_mixture_samples/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: n for k, n in zip(['cot', 'flan_v2', 'dolly', 'oasst1'], [40031, 40031, 6009, 13928])}\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(total_mixture_samples/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list = [dict(sorted(d.items())) for d in subsample_mixture_list]\n",
    "\n",
    "\n",
    "# subsample_mixture_list = [None]\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "# train_file = 'data/processed/dolly_oasst1.jsonl'; abbr_train_file = 'dolly:oasst1'\n",
    "# train_file = 'data/processed/cot_flanv2.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "\n",
    "# train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "\n",
    "# train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "\n",
    "# train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "# train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "num_train_epochs = 1\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128 # 128\n",
    "gradient_acc_steps = int(total_batch_size/(num_gpus*nodes)/batch_size_per_gpu)\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'fp16' if arch == 'ppc64le' else 'bf16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = False\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_mixture_list,\n",
    ")\n",
    "\n",
    "for (subsample_mixture,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "    if subsample_mixture is not None:\n",
    "        assert(abbr_train_file=='all')\n",
    "        output_dirname += \\\n",
    "            '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "    #         ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "    #         ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "    #         '_mbsz='+str(batch_size_per_gpu)+\\\n",
    "    #         '_dtype='+torch_dtype+\\\n",
    "    #         ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "    #         '_seqlen='+str(max_seq_length)+\\\n",
    "    #         '_nodes='+str(nodes)\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers=16 \\\n",
    "        --per_device_train_batch_size={batch_size_per_gpu} \\\n",
    "        --gradient_accumulation_steps={gradient_acc_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type=linear \\\n",
    "        --warmup_ratio=0.03 \\\n",
    "        --optim={optimizer} \\\n",
    "        --weight_decay=0. \\\n",
    "        --evaluation_strategy=\"no\" \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit=1 \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --report_to=tensorboard \\\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    if 'gpt2' in model_name_or_path:\n",
    "        return 50, 1\n",
    "    \n",
    "    batch_size = 10\n",
    "    if task_name == 'gsm':\n",
    "        job_duration = 1 # 10min for n=200\n",
    "    elif task_name == 'bbh_s=0':\n",
    "        job_duration = 1\n",
    "    elif task_name == 'bbh_s=3':\n",
    "        job_duration = 1\n",
    "        batch_size = 5 # for longer prompts.\n",
    "    elif task_name == 'mmlu':\n",
    "        job_duration = 1\n",
    "    elif task_name == 'humaneval':\n",
    "        job_duration = 1 # pass@1: 10min, pass@10: 100min\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        job_duration = 1\n",
    "    else:\n",
    "        job_duration = 1\n",
    "    return batch_size, job_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/baselines/gpt2-medium\" --save_dir \"results/baselines/gpt2-medium/eval/humaneval_chatfmt\" --eval_batch_size 50 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def finished_training(path):\n",
    "    return os.path.isfile(os.path.join(path, 'config.json'))\n",
    "\n",
    "task_name = 'mmlu'\n",
    "task_name = 'gsm'\n",
    "task_name = 'bbh_s=0'\n",
    "task_name = 'bbh_s=3'\n",
    "task_name = 'humaneval'\n",
    "task_name = 'tydiqa_cb'\n",
    "task_name = 'tydiqa_gp'\n",
    "job_name = f'eval.{task_name}'\n",
    "job_name = 'eval'\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "use_chat_format = True\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "models = []\n",
    "models += [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b', 'mosaicml/mpt-7b', 'NousResearch/Llama-2-7b-hf',\n",
    "#     'gpt2', \n",
    "    'gpt2-medium',\n",
    "]]\n",
    "\n",
    "# exp_dir = 'results/ft1'\n",
    "# exp_dir = 'results/ft2'\n",
    "# models += [x for x in os.listdir(exp_dir) if finished_training(os.path.join(exp_dir, x))]\n",
    "\n",
    "\n",
    "#     'mmlu_s=0',\n",
    "#     'mmlu_s=5',\n",
    "#     'gsm_s=8',\n",
    "#     'gsm_s=8_cot',\n",
    "#     'bbh_s=3',\n",
    "#     'bbh_s=3_cot',\n",
    "\n",
    "task_names = [\n",
    "#     'mmlu_s=0',\n",
    "#     'gsm_s=8_cot',\n",
    "#     'bbh_s=3',\n",
    "    'humaneval',\n",
    "#     'tydiqa_cb',\n",
    "#     'tydiqa_gp',\n",
    "]\n",
    "\n",
    "options_list = itertools.product(\n",
    "    task_names,\n",
    "    models,\n",
    ")\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    job_name = f'eval.{task_name}'\n",
    "    batch_size, job_duration = get_resource_for_task(task_name, model_name_or_path)\n",
    "    \n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    if use_chat_format:\n",
    "        save_dir += '_chatfmt'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 200 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "            {'--load_in_8bit' if load_in_8bit else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot 1 \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length 512 \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "            {'--load_in_8bit' if load_in_8bit else ''}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    \n",
    "\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c96f67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on pattern to match 's=' followed by one or more digits\n",
    "\n",
    "# # Using re.search() to find the pattern in the input_string\n",
    "# match = re.search(pattern, input_string)\n",
    "\n",
    "# if match:\n",
    "#     extracted_number = int(match.group(1))\n",
    "#     print(extracted_number)\n",
    "# else:\n",
    "#     print(\"No match found.\")\n",
    "import re\n",
    "n_shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e9a23e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26 16:32:07,845] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Loading data...\n",
      "Loading model and tokenizer...\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.22s/it]\n",
      "Generating Completions:   0%|                           | 0/200 [00:00<?, ?it/s]/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd .. && CUDA_VISIBLE_DEVICES=0 python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_cot\" --save_dir \"results/llama-7b_cot/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'gsm_chatfmt', 'bbh_s=3_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'gsm', 'tydiqa_cb', 'mmlu', 'humaneval_chatfmt']\n",
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'mmlu_chatfmt', 'gsm', 'tydiqa_cb', 'mmlu', 'humaneval_chatfmt']\n",
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'mmlu_chatfmt', 'gsm', 'tydiqa_cb', 'mmlu', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'gsm', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'tydiqa_gp', 'humaneval', 'tydiqa_gp_chatfmt', 'tydiqa_cb_chatfmt', 'mmlu_chatfmt', 'tydiqa_cb', 'humaneval_chatfmt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MMLU/0-shot</th>\n",
       "      <th colspan=\"2\" halign=\"left\">GSM/CoT</th>\n",
       "      <th colspan=\"2\" halign=\"left\">BBH/Direct</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Codex-Eval/Pass@1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">TydiQA/CB</th>\n",
       "      <th colspan=\"2\" halign=\"left\">TydiQA/GP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>31.9</td>\n",
       "      <td>32.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.8</td>\n",
       "      <td>39.4</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-7b</td>\n",
       "      <td>40.7</td>\n",
       "      <td>37.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>32.8</td>\n",
       "      <td>33.1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama2-7b+humanmix</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-7b+super_ni</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>7.9</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>47.6</td>\n",
       "      <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-7b+cot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.9</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>43.8</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-7b+flan_v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>45.1</td>\n",
       "      <td>44.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-7b+dolly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>10.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>44.9</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama-7b+oasst1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.7</td>\n",
       "      <td>9.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-7b+gpt4_alpaca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>39.4</td>\n",
       "      <td>25.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-7b+stanford_alpaca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>41.8</td>\n",
       "      <td>36.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-7b+code_alpaca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>14.6</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-7b+baize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.9</td>\n",
       "      <td>14.6</td>\n",
       "      <td>11.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>36.9</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-7b+self_instruct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.4</td>\n",
       "      <td>9.1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>9.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>39.4</td>\n",
       "      <td>38.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama-7b+sharegpt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.7</td>\n",
       "      <td>12.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>8.4</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-7b+humanmix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.1</td>\n",
       "      <td>11.6</td>\n",
       "      <td>10.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>42.8</td>\n",
       "      <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama-7b+dolly:oasst1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>39.7</td>\n",
       "      <td>35.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llama-7b+cot:flanv2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.7</td>\n",
       "      <td>44.4</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model MMLU/0-shot         GSM/CoT         BBH/Direct  \\\n",
       "                                         chatfmt         chatfmt              \n",
       "0                   llama-7b        31.9    32.5    10.5    11.0       31.8   \n",
       "1                  llama2-7b        40.7    37.5    12.0    16.5       32.8   \n",
       "2         llama2-7b+humanmix        26.0    24.4     0.0     1.0       10.8   \n",
       "3          llama-7b+super_ni         NaN    43.5     NaN     3.0        NaN   \n",
       "4               llama-7b+cot         NaN    37.0     NaN    28.0        NaN   \n",
       "5           llama-7b+flan_v2         NaN    43.2     NaN    12.0        NaN   \n",
       "6             llama-7b+dolly         NaN    37.2     NaN    13.0        NaN   \n",
       "7            llama-7b+oasst1         NaN    34.1     NaN     7.5        NaN   \n",
       "8       llama-7b+gpt4_alpaca         NaN    39.2     NaN     8.5        NaN   \n",
       "9   llama-7b+stanford_alpaca         NaN    41.7     NaN    10.5        NaN   \n",
       "10      llama-7b+code_alpaca         NaN    34.9     NaN    11.0        NaN   \n",
       "11            llama-7b+baize         NaN    38.7     NaN    10.0        NaN   \n",
       "12    llama-7b+self_instruct         NaN    35.7     NaN     6.5        NaN   \n",
       "13         llama-7b+sharegpt         NaN    43.2     NaN     8.0        NaN   \n",
       "14         llama-7b+humanmix         NaN    43.5     NaN    29.0        NaN   \n",
       "15     llama-7b+dolly:oasst1         NaN    37.9     NaN     7.0        NaN   \n",
       "16       llama-7b+cot:flanv2         NaN    42.7     NaN    19.0        NaN   \n",
       "\n",
       "           Codex-Eval/Pass@1         TydiQA/CB         TydiQA/GP          \n",
       "   chatfmt                   chatfmt           chatfmt           chatfmt  \n",
       "0     33.0              10.4     4.3      11.5     9.8      39.4    37.5  \n",
       "1     33.1              12.8     0.0      15.5     NaN      52.1     NaN  \n",
       "2      9.7               0.0     0.0       4.0     NaN      11.1     NaN  \n",
       "3     34.1               8.5     7.9       8.5    10.1      47.6    43.5  \n",
       "4     34.9              10.4    10.4       9.8     9.1      43.8    44.1  \n",
       "5     34.4              11.6    12.8       9.2    10.2      45.1    44.2  \n",
       "6     30.6              12.2    11.6      10.4    11.0      44.9    44.0  \n",
       "7     29.7               9.1     2.4       9.6     9.6      40.0    34.9  \n",
       "8     31.7              13.4    12.2       9.1     6.9      39.4    25.3  \n",
       "9     32.9              12.2    11.0      10.3     8.9      41.8    36.1  \n",
       "10    31.6              15.2    14.6      10.1     9.2      38.0    35.2  \n",
       "11    31.9              14.6    11.6      10.0     8.3      36.9    33.9  \n",
       "12    32.4               9.1     7.3       9.1     8.5      39.4    38.9  \n",
       "13    28.7              12.2     2.4       9.6     8.4      39.0    22.7  \n",
       "14    36.1              11.6    10.4       9.3    10.4      42.8    43.5  \n",
       "15    30.2              11.0     9.1      10.0    10.5      39.7    35.6  \n",
       "16    35.2              11.6     8.5       8.9     8.7      44.4    41.8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir, run_name=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        self.run_name = run_name if run_name else self.save_dir\n",
    "        \n",
    "    def get_result_df(self):\n",
    "        \n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "        print(task_names)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "            if task_name.startswith('tydiqa'):\n",
    "                metrics['average_f1'] = metrics['average']['f1']\n",
    "            columns = [f'{task_name}/{k}' for k in metrics.keys()]\n",
    "            # columns = pd.MultiIndex.from_tuples(columns)\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        mapper = {\n",
    "            'mmlu/average_acc': 'MMLU/0-shot',\n",
    "            'mmlu_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'gsm/exact_match': 'GSM/CoT',\n",
    "            'gsm_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            'bbh_s=0/average_exact_match': 'BBH/Direct',\n",
    "            'bbh_s=0_chatfmt/average_exact_match': 'BBH/Direct_chatfmt',\n",
    "            'bbh_s=3/average_exact_match': 'BBH/CoT', \n",
    "            'bbh_s=3_chatfmt/average_exact_match': 'BBH/CoT_chatfmt',\n",
    "            'humaneval/pass@1': 'Codex-Eval/Pass@1',\n",
    "            'humaneval_chatfmt/pass@1': 'Codex-Eval/Pass@1_chatfmt',\n",
    "            'tydiqa_cb/average_f1': 'TydiQA/CB',\n",
    "            'tydiqa_cb_chatfmt/average_f1': 'TydiQA/CB_chatfmt',\n",
    "            'tydiqa_gp/average_f1': 'TydiQA/GP',\n",
    "            'tydiqa_gp_chatfmt/average_f1': 'TydiQA/GP_chatfmt',\n",
    "        }\n",
    "        cols = [col for col in mapper.keys() if col in df]\n",
    "        for col in cols:\n",
    "            if 'tydiqa' not in col:\n",
    "                df[col] = df[col]*100\n",
    "        df = df[cols]\n",
    "        df = df.rename(columns=mapper)\n",
    "        df.insert(0, 'Model', [self.run_name])\n",
    "        return df\n",
    "\n",
    "# get_last_checkpoint(v)\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "    ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "    ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "#     ('llama-7b+humanmix', '../results/huggyllama:llama-7b_humanmix'),\n",
    "#     ('llama-7b+lora(r=8,a=8)', '../results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix'),\n",
    "#     ('llama-7b+lora(r=128,a=128)', '../results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix'),\n",
    "]\n",
    "datasets = [\n",
    "    'super_ni', 'cot', 'flan_v2', 'dolly', 'oasst1',\n",
    "    'gpt4_alpaca', 'stanford_alpaca', 'code_alpaca', 'baize', 'self_instruct', \n",
    "    'sharegpt', 'humanmix',\n",
    "#     'unnatural_instructions',\n",
    "    'dolly:oasst1',\n",
    "    'cot:flanv2'\n",
    "]\n",
    "\n",
    "save_dirs += [(f'llama-7b+{x}', f'../results/llama-7b_{x}') for x in datasets]\n",
    "\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model_name, save_dir in save_dirs:\n",
    "    r = EvalResults(save_dir, model_name)\n",
    "    df = r.get_result_df()\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df = df.reset_index(drop=True)\n",
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    dfc = df.copy()\n",
    "    cols = [x.split('_') for x in df.columns]\n",
    "    cols = [x+[''] if len(x)==1 else x for x in cols]\n",
    "    dfc.columns = pd.MultiIndex.from_tuples(cols)\n",
    "    display(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6ec921210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3618: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3619: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff9c6f3250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6ec19eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>32.459764</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.970313</td>\n",
       "      <td>5.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-7b+dolly</td>\n",
       "      <td>37.231164</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.603476</td>\n",
       "      <td>11.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-7b+oasst1</td>\n",
       "      <td>34.147557</td>\n",
       "      <td>7.5</td>\n",
       "      <td>29.692361</td>\n",
       "      <td>2.439024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-7b+dolly:oasst1</td>\n",
       "      <td>37.900584</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.150571</td>\n",
       "      <td>9.146341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  MMLU/0-shot  GSM/CoT  BBH/Direct  Codex-Eval/Pass@1\n",
       "0                llama-7b    32.459764     11.0   32.970313           5.487805\n",
       "4          llama-7b+dolly    37.231164     13.0   30.603476          11.585366\n",
       "5         llama-7b+oasst1    34.147557      7.5   29.692361           2.439024\n",
       "14  llama-7b+dolly:oasst1    37.900584      7.0   30.150571           9.146341"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima datƒÉ √Æn istoria Uniunii Europene, la drepturile persoanelor care apar≈£in acestor minoritƒÉ≈£i ≈üi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
