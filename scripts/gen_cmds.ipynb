{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 11 23:17:04 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:C4:00.0 Off |                    0 |\r\n",
      "| N/A   33C    P0    61W / 400W |      0MiB / 81920MiB |      0%   E. Process |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from llm.submit import multiline_to_singleline, submit_job_ccc, get_run_statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b984427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\"\"\"\n",
    "\n",
    "# [ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'ft'\n",
    "test_run = True\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 20\n",
    "num_gpus = 2\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "\n",
    "use_deepspeed = False\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''}\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "\n",
    "# things to test to see its effects on (1) eval perf (2) runtime.\n",
    "#\n",
    "# - int8\n",
    "# - mixed_precision bf16 or no\n",
    "# - with/without LoRA\n",
    "# - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# - batch size\n",
    "# - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/share/zsh/site-functions'), PosixPath('/u/wpq/.oh-my-zsh/completions'), PosixPath('/u/wpq/.oh-my-zsh/functions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1809498.tmpdir/.1689131691.1809498.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/11/2023 23:20:18 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "07/11/2023 23:20:18 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.93s/it]\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:09<00:00, 34.69s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Assigning <s> to the bos_token key of the tokenizer\n",
      "Assigning </s> to the eos_token key of the tokenizer\n",
      "Assigning <unk> to the unk_token key of the tokenizer\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "GPU memory occupied: 837 MB.\n",
      "07/11/2023 23:22:45 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1038c98439ac66d6_*_of_00016.arrow\n",
      "07/11/2023 23:22:46 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-88ed2eb4a77829ae.arrow\n",
      "07/11/2023 23:22:47 - INFO - __main__ - Sample 15006 of the training set: {'input_ids': tensor([    1,   529, 29989,  ...,  1359, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  ...,  1359, 29889,     2]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}.\n",
      "07/11/2023 23:22:47 - INFO - __main__ - Sample 89871 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,   797,   445,  3414,\n",
      "        29892,   366,   526,  2183,   263, 10541,   322,   263,  6351, 29889,\n",
      "          450, 10541, 26649,  1023,  2600, 10964, 29901,   697, 29915, 29879,\n",
      "        23346,   338,  2893, 28677,   773,   278, 23346,   287, 11504,  1309,\n",
      "        29879,   297,   278,  1426,   322,   278,   916, 29915, 29879, 23346,\n",
      "          338,   443,  1693, 28677, 29889,   887,   526,  3806,   304,   736,\n",
      "         3692,   278,  2183,  6351, 29915, 29879, 23346,   338,  2893, 28677,\n",
      "          470,   443,  1693, 28677, 29889,    13,    13,  1252, 10567, 29901,\n",
      "           13, 29903,   296,   663, 29901,   450,   286,   957,  7124,   278,\n",
      "        12464,   272,  1363,   540,  8389,   297,   278,  2224, 29889,    13,\n",
      "         6175,  1211, 29901,   286,   957,    13,    13,  1252, 10604, 29901,\n",
      "           13,  2525,  1693, 28677,    13,    13,    13,  1252, 10567, 29901,\n",
      "           13, 29903,   296,   663, 29901,   450,   286,   957,  1395,   300,\n",
      "          287,   278,   619,  1182, 13956,   322,  4433,  1075,   988,   278,\n",
      "         8277,   892, 29889,    13,  6175,  1211, 29901,   286,   957,    13,\n",
      "           13,  1252, 10604, 29901,    13,  2525,  1693, 28677,    13,    13,\n",
      "           13,  1252, 10567, 29901,    13, 29903,   296,   663, 29901,   450,\n",
      "        10212,   261,  5429,   278, 15703,   393,   540,  4312,   304,  7726,\n",
      "          301,   283,   672,   297,   770, 29889,    13,  6175,  1211, 29901,\n",
      "        10212,   261,    13,    13,  1252, 10604, 29901,    13, 29966, 29989,\n",
      "          465, 22137, 29989, 29958,    13,  2525,  1693, 28677,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  2525,  1693, 28677,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/11/2023 23:22:47 - INFO - __main__ - Sample 126744 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  9760,   310,   278,\n",
      "         2211,  3879,  2122,  9883, 29879,  1914, 29879,   385, 24841,  4071,\n",
      "          345,   393,   756, 29871, 29896, 29896, 29900, 10697, 29889,  7806,\n",
      "        24841,  5447, 13880, 18350, 29899, 29879,  1891,   470,  6916, 29889,\n",
      "         2180,  4023, 10147,   931, 29892,  9899,   374,  3100, 29915, 29879,\n",
      "         4071,   345,  7371, 29871, 29953, 29900, 29900,   470,  6916,   639,\n",
      "         5447, 29892,  1550,   838,  2291,  4023, 29894,  2868, 29871, 29946,\n",
      "        29900, 29900,   639,  5447, 29889,  7806,   310,  1085,   293,  3100,\n",
      "        29915, 29879, 10697,  7371, 29871, 29945, 29900, 29900,   470,  6916,\n",
      "        29889,  2973,  1316,   263, 12176,  4023, 10147, 29892,   278,  9883,\n",
      "        29879,   526,  7291,   310,  3907, 24841,  3623,   625,   363, 14686,\n",
      "        29889,   960,   372,  4893,  2211, 18350, 29899, 29879,  1891,   470,\n",
      "         6916,   304,  1207, 29871, 29896, 18002,   310,  3623,   625, 29892,\n",
      "          322,   769,   896, 19417,  1269, 18002,   363,   395, 29946, 29892,\n",
      "          920,  1568,  6909,   674,   896,  1207, 29973, 25538,   592,  9590,\n",
      "        29892,  1434, 22862,   278,  1139,    13, 29966, 29989,   465, 22137,\n",
      "        29989, 29958,    13, 29954,   370,   374,  3100, 29915, 29879,  4071,\n",
      "          345,  7371, 29871, 29953, 29900, 29900,   470,  6916,   847,  5447,\n",
      "          334, 29871, 29896, 29896, 29900, 10697,   353, 29871, 29953, 29953,\n",
      "        29900, 29900, 29900,   470,  6916, 29889,   838,  2291, 30010, 29879,\n",
      "         4071,   345,  7371, 29871, 29946, 29900, 29900,   470,  6916,   847,\n",
      "         5447,   334, 29871, 29896, 29896, 29900, 10697,   353, 29871, 29946,\n",
      "        29946, 29900, 29900, 29900,   470,  6916, 29889,  1085,   293,  3100,\n",
      "        29915, 29879, 10697,  7371, 29871, 29945, 29900, 29900,   470,  6916,\n",
      "          847,  5447,   334, 29871, 29896, 29896, 29900, 10697,   353, 29871,\n",
      "        29945, 29945, 29900, 29900, 29900,   470,  6916, 29889,   450,  3879,\n",
      "         2122,  9883, 29879,  5480,  4023, 29894,  2868,   263,  3001,   310,\n",
      "        29871, 29953, 29953, 29900, 29900, 29900,   470,  6916,   718, 29871,\n",
      "        29946, 29946, 29900, 29900, 29900,   470,  6916,   718, 29871, 29945,\n",
      "        29945, 29900, 29900, 29900,   470,  6916,   353, 29871, 29896, 29953,\n",
      "        29945, 29900, 29900, 29900,   470,  6916, 29889,   450,   470,  6916,\n",
      "          674,  5480,  7709, 29871, 29896, 29953, 29945, 29900, 29900, 29900,\n",
      "          470,  6916,   847, 29871, 29941,   470,  6916,   847, 18002,   353,\n",
      "        29871, 29945, 29945, 29900, 29900, 29900,  2723,   567,   310,  3623,\n",
      "          625, 29889, 14990, 16538,   674,   367, 29871, 29945, 29945, 29900,\n",
      "        29900, 29900,  2723,   567,   334,   395, 29946,   847, 18002,   353,\n",
      "          395, 29906, 29906, 29900, 29900, 29900, 29900, 29889,  1105,   278,\n",
      "         2186,  1234,   338, 29871, 29906, 29906, 29900, 29900, 29900, 29900,\n",
      "        29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100, 29954,   370,   374,  3100, 29915, 29879,  4071,\n",
      "          345,  7371, 29871, 29953, 29900, 29900,   470,  6916,   847,  5447,\n",
      "          334, 29871, 29896, 29896, 29900, 10697,   353, 29871, 29953, 29953,\n",
      "        29900, 29900, 29900,   470,  6916, 29889,   838,  2291, 30010, 29879,\n",
      "         4071,   345,  7371, 29871, 29946, 29900, 29900,   470,  6916,   847,\n",
      "         5447,   334, 29871, 29896, 29896, 29900, 10697,   353, 29871, 29946,\n",
      "        29946, 29900, 29900, 29900,   470,  6916, 29889,  1085,   293,  3100,\n",
      "        29915, 29879, 10697,  7371, 29871, 29945, 29900, 29900,   470,  6916,\n",
      "          847,  5447,   334, 29871, 29896, 29896, 29900, 10697,   353, 29871,\n",
      "        29945, 29945, 29900, 29900, 29900,   470,  6916, 29889,   450,  3879,\n",
      "         2122,  9883, 29879,  5480,  4023, 29894,  2868,   263,  3001,   310,\n",
      "        29871, 29953, 29953, 29900, 29900, 29900,   470,  6916,   718, 29871,\n",
      "        29946, 29946, 29900, 29900, 29900,   470,  6916,   718, 29871, 29945,\n",
      "        29945, 29900, 29900, 29900,   470,  6916,   353, 29871, 29896, 29953,\n",
      "        29945, 29900, 29900, 29900,   470,  6916, 29889,   450,   470,  6916,\n",
      "          674,  5480,  7709, 29871, 29896, 29953, 29945, 29900, 29900, 29900,\n",
      "          470,  6916,   847, 29871, 29941,   470,  6916,   847, 18002,   353,\n",
      "        29871, 29945, 29945, 29900, 29900, 29900,  2723,   567,   310,  3623,\n",
      "          625, 29889, 14990, 16538,   674,   367, 29871, 29945, 29945, 29900,\n",
      "        29900, 29900,  2723,   567,   334,   395, 29946,   847, 18002,   353,\n",
      "          395, 29906, 29906, 29900, 29900, 29900, 29900, 29889,  1105,   278,\n",
      "         2186,  1234,   338, 29871, 29906, 29906, 29900, 29900, 29900, 29900,\n",
      "        29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/11/2023 23:22:49 - INFO - __main__ - ***** Running training *****\n",
      "07/11/2023 23:22:49 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) =  Num examples = 270152\n",
      "07/11/2023 23:22:49 - INFO - __main__ -   Num Epochs = 2\n",
      "07/11/2023 23:22:49 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/11/2023 23:22:49 - INFO - __main__ -  128\n",
      "07/11/2023 23:22:49 - INFO - __main__ -   Gradient Accumulation steps = 128\n",
      "07/11/2023 23:22:49 - INFO - __main__ -   Total optimization steps = 4222\n",
      "  0%|                                                  | 0/4222 [00:00<?, ?it/s]before train loop:\n",
      "GPU memory occupied: 14739 MB.\n",
      "torch.cuda.memory_allocated():  13543972864\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "0  batch.input_ids:  torch.Size([1, 558])\n",
      "GPU memory occupied: 14739 MB.\n",
      "torch.cuda.memory_allocated():  13543986688\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "1  batch.input_ids:  torch.Size([1, 105])\n",
      "GPU memory occupied: 33847 MB.\n",
      "torch.cuda.memory_allocated():  27109290496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "2  batch.input_ids:  torch.Size([1, 403])\n",
      "GPU memory occupied: 34411 MB.\n",
      "torch.cuda.memory_allocated():  27051312128\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "3  batch.input_ids:  torch.Size([1, 451])\n",
      "GPU memory occupied: 34663 MB.\n",
      "torch.cuda.memory_allocated():  27089459200\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "4  batch.input_ids:  torch.Size([1, 40])\n",
      "GPU memory occupied: 35397 MB.\n",
      "torch.cuda.memory_allocated():  27096582656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "5  batch.input_ids:  torch.Size([1, 215])\n",
      "GPU memory occupied: 35409 MB.\n",
      "torch.cuda.memory_allocated():  27042987520\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "6  batch.input_ids:  torch.Size([1, 931])\n",
      "GPU memory occupied: 35409 MB.\n",
      "torch.cuda.memory_allocated():  27065404928\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "7  batch.input_ids:  torch.Size([1, 202])\n",
      "GPU memory occupied: 44473 MB.\n",
      "torch.cuda.memory_allocated():  27157404672\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "8  batch.input_ids:  torch.Size([1, 417])\n",
      "GPU memory occupied: 44473 MB.\n",
      "torch.cuda.memory_allocated():  27063728640\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "9  batch.input_ids:  torch.Size([1, 216])\n",
      "GPU memory occupied: 44473 MB.\n",
      "torch.cuda.memory_allocated():  27091245056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "10  batch.input_ids:  torch.Size([1, 1501])\n",
      "GPU memory occupied: 44473 MB.\n",
      "torch.cuda.memory_allocated():  27065546752\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "11  batch.input_ids:  torch.Size([1, 148])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27230803456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "12  batch.input_ids:  torch.Size([1, 98])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27056808960\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "13  batch.input_ids:  torch.Size([1, 251])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27050449920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "14  batch.input_ids:  torch.Size([1, 136])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27069994496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "15  batch.input_ids:  torch.Size([1, 108])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27055272960\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "16  batch.input_ids:  torch.Size([1, 389])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27051696128\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "17  batch.input_ids:  torch.Size([1, 57])\n",
      "GPU memory occupied: 64391 MB.\n",
      "torch.cuda.memory_allocated():  27087656448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "18  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27045160448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "19  batch.input_ids:  torch.Size([1, 127])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27051432448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "20  batch.input_ids:  torch.Size([1, 897])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27054661120\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "21  batch.input_ids:  torch.Size([1, 211])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27152687104\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "22  batch.input_ids:  torch.Size([1, 723])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27065142272\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "23  batch.input_ids:  torch.Size([1, 76])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27130411008\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "24  batch.input_ids:  torch.Size([1, 251])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27047595520\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "25  batch.input_ids:  torch.Size([1, 573])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27070003712\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "26  batch.input_ids:  torch.Size([1, 571])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27111221248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "27  batch.input_ids:  torch.Size([1, 92])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27110954496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "28  batch.input_ids:  torch.Size([1, 273])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27050451456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "29  batch.input_ids:  torch.Size([1, 84])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27072809472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "30  batch.input_ids:  torch.Size([1, 121])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27048616448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "31  batch.input_ids:  torch.Size([1, 895])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27053370880\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "32  batch.input_ids:  torch.Size([1, 500])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27152436736\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "33  batch.input_ids:  torch.Size([1, 1528])\n",
      "GPU memory occupied: 64401 MB.\n",
      "torch.cuda.memory_allocated():  27101899776\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "34  batch.input_ids:  torch.Size([1, 82])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27233454080\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "35  batch.input_ids:  torch.Size([1, 134])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27048361984\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "36  batch.input_ids:  torch.Size([1, 406])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27055024640\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "37  batch.input_ids:  torch.Size([1, 739])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27089849344\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "38  batch.input_ids:  torch.Size([1, 165])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27132460544\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "39  batch.input_ids:  torch.Size([1, 170])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27058986496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "40  batch.input_ids:  torch.Size([1, 327])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27059631104\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "41  batch.input_ids:  torch.Size([1, 84])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27079806976\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "42  batch.input_ids:  torch.Size([1, 95])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27048616448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "43  batch.input_ids:  torch.Size([1, 116])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27050446848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44  batch.input_ids:  torch.Size([1, 1203])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27052738560\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "45  batch.input_ids:  torch.Size([1, 1029])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27191876096\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "46  batch.input_ids:  torch.Size([1, 98])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27169580544\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "47  batch.input_ids:  torch.Size([1, 253])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27050449920\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "48  batch.input_ids:  torch.Size([1, 436])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27070256640\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "49  batch.input_ids:  torch.Size([1, 575])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27094497792\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "50  batch.input_ids:  torch.Size([1, 567])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27111477248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "51  batch.input_ids:  torch.Size([1, 271])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27110447104\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "52  batch.input_ids:  torch.Size([1, 496])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27072562688\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "53  batch.input_ids:  torch.Size([1, 129])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27101355520\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "54  batch.input_ids:  torch.Size([1, 121])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27054641152\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "55  batch.input_ids:  torch.Size([1, 107])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27053352448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "56  batch.input_ids:  torch.Size([1, 118])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27051560448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "57  batch.input_ids:  torch.Size([1, 469])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27052977664\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "58  batch.input_ids:  torch.Size([1, 267])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27098685952\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "59  batch.input_ids:  torch.Size([1, 121])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27072041472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "60  batch.input_ids:  torch.Size([1, 287])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27053357056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "61  batch.input_ids:  torch.Size([1, 414])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27074609152\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "62  batch.input_ids:  torch.Size([1, 509])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27090867200\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "63  batch.input_ids:  torch.Size([1, 1257])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27103045632\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "64  batch.input_ids:  torch.Size([1, 426])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27198772736\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "65  batch.input_ids:  torch.Size([1, 515])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27092404736\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "66  batch.input_ids:  torch.Size([1, 175])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27103788032\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "67  batch.input_ids:  torch.Size([1, 1479])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27060298752\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "68  batch.input_ids:  torch.Size([1, 1246])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27227209728\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "69  batch.input_ids:  torch.Size([1, 128])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27197357056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "70  batch.input_ids:  torch.Size([1, 209])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27054644224\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "71  batch.input_ids:  torch.Size([1, 444])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27065134592\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "72  batch.input_ids:  torch.Size([1, 75])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27094697984\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "73  batch.input_ids:  torch.Size([1, 576])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27047475200\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "74  batch.input_ids:  torch.Size([1, 184])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27111596032\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "75  batch.input_ids:  torch.Size([1, 596])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27061429248\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "76  batch.input_ids:  torch.Size([1, 121])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27114154496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "77  batch.input_ids:  torch.Size([1, 95])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27053352448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "78  batch.input_ids:  torch.Size([1, 126])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27050446848\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "79  batch.input_ids:  torch.Size([1, 116])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27054641152\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "80  batch.input_ids:  torch.Size([1, 80])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27052712448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "81  batch.input_ids:  torch.Size([1, 1882])\n",
      "GPU memory occupied: 74411 MB.\n",
      "torch.cuda.memory_allocated():  27048147456\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "82  batch.input_ids:  torch.Size([1, 179])\n",
      "GPU memory occupied: 75037 MB.\n",
      "torch.cuda.memory_allocated():  27279037952\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "83  batch.input_ids:  torch.Size([1, 110])\n",
      "GPU memory occupied: 75037 MB.\n",
      "torch.cuda.memory_allocated():  27060776960\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "84  batch.input_ids:  torch.Size([1, 466])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27051953664\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "85  batch.input_ids:  torch.Size([1, 239])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27097517056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "86  batch.input_ids:  torch.Size([1, 201])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27068460032\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "87  batch.input_ids:  torch.Size([1, 95])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27063592960\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "88  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27050024448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "89  batch.input_ids:  torch.Size([1, 48])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27051430912\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "90  batch.input_ids:  torch.Size([1, 145])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27044009984\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "91  batch.input_ids:  torch.Size([1, 174])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27056426496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "92  batch.input_ids:  torch.Size([1, 171])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27060138496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "93  batch.input_ids:  torch.Size([1, 285])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27059757568\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94  batch.input_ids:  torch.Size([1, 228])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27074348544\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "95  batch.input_ids:  torch.Size([1, 774])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27067065856\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "96  batch.input_ids:  torch.Size([1, 280])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27136944128\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "97  batch.input_ids:  torch.Size([1, 236])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27073708544\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "98  batch.input_ids:  torch.Size([1, 1530])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27068106752\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "99  batch.input_ids:  torch.Size([1, 278])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27233714688\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "100  batch.input_ids:  torch.Size([1, 144])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27073451008\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "101  batch.input_ids:  torch.Size([1, 328])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27056303104\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "102  batch.input_ids:  torch.Size([1, 618])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27079861760\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "103  batch.input_ids:  torch.Size([1, 439])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27116978176\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "104  batch.input_ids:  torch.Size([1, 231])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27094061056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "105  batch.input_ids:  torch.Size([1, 174])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27067434496\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "106  batch.input_ids:  torch.Size([1, 643])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27060150784\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "107  batch.input_ids:  torch.Size([1, 882])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27120189440\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "108  batch.input_ids:  torch.Size([1, 873])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27150781952\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "109  batch.input_ids:  torch.Size([1, 226])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27149614592\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "110  batch.input_ids:  torch.Size([1, 904])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27066812928\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "111  batch.input_ids:  torch.Size([1, 94])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27153580032\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "112  batch.input_ids:  torch.Size([1, 94])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27049896448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "113  batch.input_ids:  torch.Size([1, 266])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27049901056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "114  batch.input_ids:  torch.Size([1, 106])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27071913472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "115  batch.input_ids:  torch.Size([1, 124])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27051432448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "116  batch.input_ids:  torch.Size([1, 314])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27054645760\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "117  batch.input_ids:  torch.Size([1, 1316])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27078086656\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "118  batch.input_ids:  torch.Size([1, 74])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27206317568\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "119  batch.input_ids:  torch.Size([1, 83])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27047336448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "120  batch.input_ids:  torch.Size([1, 597])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27048500736\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "121  batch.input_ids:  torch.Size([1, 59])\n",
      "GPU memory occupied: 75327 MB.\n",
      "torch.cuda.memory_allocated():  27114280960\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "122  batch.input_ids:  torch.Size([1, 324])\n",
      "GPU memory occupied: 75365 MB.\n",
      "torch.cuda.memory_allocated():  27046258688\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "123  batch.input_ids:  torch.Size([1, 60])\n",
      "GPU memory occupied: 75365 MB.\n",
      "torch.cuda.memory_allocated():  27079805440\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "124  batch.input_ids:  torch.Size([1, 76])\n",
      "GPU memory occupied: 75365 MB.\n",
      "torch.cuda.memory_allocated():  27046252544\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "125  batch.input_ids:  torch.Size([1, 304])\n",
      "GPU memory occupied: 75365 MB.\n",
      "torch.cuda.memory_allocated():  27047597056\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "126  batch.input_ids:  torch.Size([1, 115])\n",
      "GPU memory occupied: 75365 MB.\n",
      "torch.cuda.memory_allocated():  27076777472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "127  batch.input_ids:  torch.Size([1, 84])\n",
      "GPU memory occupied: 75365 MB.\n",
      "torch.cuda.memory_allocated():  27052584448\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "Traceback (most recent call last):\n",
      "  File \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune.py\", line 701, in <module>\n",
      "    main()\n",
      "  File \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune.py\", line 632, in main\n",
      "    optimizer.step()\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/optimizer.py\", line 140, in step\n",
      "    self.optimizer.step(closure)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/optim/adamw.py\", line 171, in step\n",
      "    adamw(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/optim/adamw.py\", line 321, in adamw\n",
      "    func(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/optim/adamw.py\", line 566, in _multi_tensor_adamw\n",
      "    denom = torch._foreach_add(exp_avg_sq_sqrt, eps)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.18 GiB total capacity; 73.77 GiB already allocated; 54.31 MiB free; 77.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                                  | 0/4222 [01:10<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 941, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 603, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/dccstor/mit_fm/miniconda/envs/open-instruct/bin/python3.10', 'open_instruct/finetune.py', '--model_name_or_path', 'huggyllama/llama-7b', '--tokenizer_name', 'huggyllama/llama-7b', '--train_file', 'data/processed/flanv2_cot_oasst1_dolly.jsonl', '--max_seq_length', '2048', '--lora_rank', '4', '--lora_alpha', '4', '--lora_dropout', '0.05', '--preprocessing_num_workers', '16', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '128', '--learning_rate', '2e-5', '--lr_scheduler_type', 'linear', '--warmup_ratio', '0.03', '--weight_decay', '0.', '--num_train_epochs', '2', '--output_dir', 'results/huggyllama:llama-7b_human_mix', '--with_tracking', '--report_to', 'tensorboard', '--logging_steps', '1']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|▏         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama7b-lora, micro-bsz=1, lora_rank=4: 66gb gpu mem\n",
    "# llama7b-lora-int8, micro-bsz=1, lora_rank=4: 13gb gpu mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51c8d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 1 GPUs, 2 batch size per GPU, 64 gradient accumulation steps.\n",
      "{'job_id': 1809817, 'jbsub_cmd': 'jbsub -queue x86_12h -name ft-trainer -mem 32g -cores 1x32+1 -require a100_80gb -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo \"accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 2 --gradient_accumulation_steps 64 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 1 --num_train_epochs 1 --output_dir results/huggyllama:llama-7b_human_mix-trainer_savebystep --bf16 --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8\"; echo \"======\"; accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 2 --gradient_accumulation_steps 64 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 1 --num_train_epochs 1 --output_dir results/huggyllama:llama-7b_human_mix-trainer_savebystep --bf16 --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8'}\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft-trainer'\n",
    "test_run = False\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 32\n",
    "num_gpus = 1\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 200\n",
    "\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}-trainer\"\n",
    "output_dir += '_savebystep'\n",
    "if num_gpus != 1:\n",
    "    output_dir += f'_ngpus={num_gpus}'\n",
    "if test_run:\n",
    "    output_dir += '_jpt'\n",
    "\n",
    "num_train_epochs = 1\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    {'--multi_gpu' if num_gpus>1 else ''} \n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    open_instruct/finetune_trainer.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --save_strategy {save_strategy} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --save_total_limit 1 \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --bf16 \\\n",
    "    --tf32 True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --report_to tensorboard \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --dataloader_num_workers 8\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5467cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions'), PosixPath('/usr/local/share/zsh/site-functions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1809498.tmpdir/.1689131691.1809498.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}'), PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/12/2023 00:51:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "07/12/2023 00:51:48 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=results/huggyllama:llama-7b_human_mix-trainer_jpt/runs/Jul12_00-51-48_cccxc549,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=results/huggyllama:llama-7b_human_mix-trainer_jpt,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=results/huggyllama:llama-7b_human_mix-trainer_jpt,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 00:51:48 - INFO - datasets.builder - Using custom data configuration default-247ebf1b4910b0d3\n",
      "07/12/2023 00:51:48 - INFO - datasets.info - Loading Dataset Infos from /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "07/12/2023 00:51:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "07/12/2023 00:51:48 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "07/12/2023 00:51:48 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "07/12/2023 00:51:48 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 267.61it/s]\n",
      "[INFO|configuration_utils.py:669] 2023-07-12 00:51:48,344 >> loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-07-12 00:51:48,344 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 00:51:48,369 >> loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 00:51:48,369 >> loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 00:51:48,369 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 00:51:48,369 >> loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-12 00:51:48,369 >> loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2278] 2023-07-12 00:51:48,448 >> The device_map was not initialized.Setting device_map to {'':torch.cuda.current_device()}.If you want to use the model for inference, please set device_map ='auto' \n",
      "[INFO|modeling_utils.py:2578] 2023-07-12 00:51:48,448 >> loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-07-12 00:51:48,448 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:577] 2023-07-12 00:51:48,449 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2690] 2023-07-12 00:51:48,601 >> Detected 8-bit loading: activating 8-bit loading for this model\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:30<00:00, 15.04s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-07-12 00:52:18,874 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-07-12 00:52:18,875 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:539] 2023-07-12 00:52:18,901 >> loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-07-12 00:52:18,902 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 00:52:19,204 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 00:52:19,204 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 00:52:19,204 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-12 00:52:19,204 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/12/2023 00:52:20 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00000_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00001_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00002_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00003_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00004_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00005_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00006_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00007_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00008_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00009_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00010_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00011_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00012_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00013_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00014_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00015_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 00:52:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_*_of_00016.arrow\n",
      "07/12/2023 00:52:31 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "[INFO|trainer.py:399] 2023-07-12 00:52:31,435 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:407] 2023-07-12 00:52:31,435 >> The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n",
      "[INFO|trainer.py:776] 2023-07-12 00:52:31,686 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: id, messages, dataset. If id, messages, dataset are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1786] 2023-07-12 00:52:31,704 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2023-07-12 00:52:31,704 >>   Num examples = 270,679\n",
      "[INFO|trainer.py:1788] 2023-07-12 00:52:31,704 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1789] 2023-07-12 00:52:31,704 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1790] 2023-07-12 00:52:31,704 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1791] 2023-07-12 00:52:31,704 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1792] 2023-07-12 00:52:31,704 >>   Total optimization steps = 2,114\n",
      "[INFO|trainer.py:1793] 2023-07-12 00:52:31,707 >>   Number of trainable parameters = 4,194,304\n",
      "  0%|                                                  | 0/2114 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-07-12 00:52:32,418 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,419 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,419 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,419 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,419 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,419 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,420 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-12 00:52:32,431 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:295] 2023-07-12 00:52:32,522 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 2.1241, 'learning_rate': 3.125e-07, 'epoch': 0.0}                      \n",
      "{'loss': 1.9169, 'learning_rate': 6.25e-07, 'epoch': 0.0}                       \n",
      "{'loss': 2.3817, 'learning_rate': 9.375000000000001e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.3186, 'learning_rate': 1.25e-06, 'epoch': 0.0}                       \n",
      "  0%|                                       | 4/2114 [06:27<55:26:57, 94.61s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune_trainer.py\", line 518, in <module>\n",
      "    main()\n",
      "  File \"/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune_trainer.py\", line 488, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py\", line 2759, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py\", line 2784, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 553, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 541, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/peft/peft_model.py\", line 676, in forward\n",
      "    return self.base_model(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 570, in forward\n",
      "    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 566, in custom_forward\n",
      "    return module(*inputs, output_attentions, None)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 292, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = old_forward(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 195, in forward\n",
      "    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/peft/tuners/lora.py\", line 545, in forward\n",
      "    result = super().forward(x)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 242, in forward\n",
      "    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 488, in matmul\n",
      "    return MatMul8bitLt.apply(A, B, out, bias, state)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 303, in forward\n",
      "    CA, CAt, SCA, SCAt, coo_tensorA = F.double_quant(A.to(torch.float16), threshold=state.threshold)\n",
      "  File \"/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1644, in double_quant\n",
      "    lib.cdouble_rowcol_quant(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --load_in_8bit --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy epoch --save_total_limit 1 --num_train_epochs 1 --output_dir results/huggyllama:llama-7b_human_mix-trainer_jpt --bf16 --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "97c5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo \"{cmd}\"\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f9a46f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if any(x in model_name_or_path for x in ['small', 'base', 'medium', 'large']):\n",
    "    cpu_mem = 2\n",
    "elif any(x in model_name_or_path for x in ['3b']):\n",
    "    cpu_mem = 15\n",
    "elif any(x in model_name_or_path for x in ['7b', '11b', 'xl', 'xxl']):\n",
    "    cpu_mem = 64\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path results/baselines/huggyllama/llama-7b --save_dir results/baselines/huggyllama/llama-7b/eval/gsm --eval_batch_size 5 --n_shot 8\n"
     ]
    }
   ],
   "source": [
    "job_name = 'eval.gsm'\n",
    "test_run = False\n",
    "queue = 'x86_1h'\n",
    "num_cpus = 10\n",
    "cpu_mem = 64\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "models += ['huggyllama/llama-7b'] # , 'mosaicml/mpt-7b'\n",
    "\n",
    "models = [os.path.join('results/baselines', x) for x in models]\n",
    "\n",
    "info = {}\n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/gsm'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    python -m eval.gsm.run_eval \\\n",
    "        --data_dir data/eval/gsm/ \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --save_dir {save_dir} \\\n",
    "        --eval_batch_size 5 \\\n",
    "        --n_shot 8\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job_ccc(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        queue=queue,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "    )\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    if not test_run:\n",
    "        info[model_name_or_path] = out['job_id']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "info['results/baselines/t5-small'] = 1763441\n",
    "info['results/baselines/t5-base'] = 1763442\n",
    "info['results/baselines/t5-large'] = 1763443\n",
    "info['results/baselines/t5-3b'] = 1764783\n",
    "info['results/baselines/t5-11b'] = 1763445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "271cdee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>job_id</th>\n",
       "      <th>cpu_time</th>\n",
       "      <th>avg_mem</th>\n",
       "      <th>max_mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>1763441</td>\n",
       "      <td>139.93</td>\n",
       "      <td>0.491738</td>\n",
       "      <td>0.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>1763442</td>\n",
       "      <td>258.60</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.787109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>1763443</td>\n",
       "      <td>76.73</td>\n",
       "      <td>0.957783</td>\n",
       "      <td>1.317383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>1764783</td>\n",
       "      <td>751.98</td>\n",
       "      <td>6.689150</td>\n",
       "      <td>11.693359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>1763445</td>\n",
       "      <td>8.53</td>\n",
       "      <td>5.601807</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name   job_id  cpu_time   avg_mem    max_mem\n",
       "0  t5-small  1763441    139.93  0.491738   0.597656\n",
       "1   t5-base  1763442    258.60  0.729512   0.787109\n",
       "2  t5-large  1763443     76.73  0.957783   1.317383\n",
       "3     t5-3b  1764783    751.98  6.689150  11.693359\n",
       "4    t5-11b  1763445      8.53  5.601807  10.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = []\n",
    "for k, job_id in info.items():\n",
    "    logfile_path = f'{job_id}.out'\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    data.append((k.split('/')[-1], job_id, out['cpu_time'], out['avg_mem'], out['max_mem']))\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['name', 'job_id', 'cpu_time', 'avg_mem', 'max_mem'])\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
