{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'ppc64le', 'cluster': 'dcs'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info\n",
    "\n",
    "info = get_host_info()\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##\n",
    "##\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     conda_env='open-instruct',\n",
    "#     cwd=os.path.dirname(os.getcwd()),\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir\n",
    "# )\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '00:33:12'\n",
    "n = 15\n",
    "# total = 1515; nnodes = 1\n",
    "# total = 2083; nnodes = 1\n",
    "total = 1587; nnodes = 1\n",
    "# total = 1041; nnodes = 1\n",
    "# total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_data_points = 200000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up checkpoints `optimizer.bin` to save disk space. \n",
    "# (e.g., 7b model, ~8*7=56GB for storing gradient/momentum in `optimizer.bin`)\n",
    "\n",
    "import glob, os\n",
    "\n",
    "def cleanup_checkpoints(save_dir, test_run=False):\n",
    "\n",
    "    checkpoints = glob.glob(os.path.join(save_dir, 'checkpoint-*'))\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "    checkpoints = checkpoints[:-1]\n",
    "    \n",
    "    if not checkpoints: return\n",
    "\n",
    "    for ckpt_path in checkpoints:\n",
    "        optimizer_bin_path = os.path.join(ckpt_path, 'optimizer.bin')\n",
    "        if os.path.isfile(optimizer_bin_path):\n",
    "            print(optimizer_bin_path)\n",
    "            if not test_run:\n",
    "                os.remove(optimizer_bin_path)\n",
    "        \n",
    "        \n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "exp_dirs = [\n",
    "    '../results/ft1',\n",
    "    '../results/ft2',\n",
    "    '../results/oi3',\n",
    "    '../results/oi4',\n",
    "    '../results/oi4_perf_cross_time',\n",
    "    '../results/oi4_tulu_v1_human_mix',\n",
    "    '../results/oi4_flanv2_prune_with_hmv1_model',\n",
    "    '../results/oi4_flan_v2_vary_subsetsize',\n",
    "]\n",
    "\n",
    "print('Remove extra files (e.g., optimizer.bin) for non-latest checkpoints:')\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    for run_name in os.listdir(exp_dir):\n",
    "        save_dir = os.path.join(exp_dir, run_name)\n",
    "        if os.path.islink(save_dir): continue\n",
    "        cleanup_checkpoints(save_dir, test_run=test_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19aebd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md=llama-7b scoring_fn=dppmap_k=vmf_gamma=auto1000_kmd=llama7br256p4096_kemb=grad+rp+loraB\t->\tmd_expand=llama-7b+lora:r=256:a=4096+proj=4096\n",
      "md=llama-7b scoring_fn=dppmap_k=vmf_gamma=auto1000_kmd=llama7br512p4096_kemb=grad+rp+loraB\t->\tmd_expand=llama-7b+lora:r=512:a=11585+proj=4096\n",
      "md=llama-7b scoring_fn=dppmap_k=vmf_gamma=auto1000_kmd=pythia1br512p4096_kemb=grad+rp+loraB\t->\tmd_expand=pythia-1b+lora:r=512:a=11585+proj=4096\n",
      "Training results/baselines/huggyllama/llama-7b using 6 GPUs, 2 batch size per GPU, 2 gradient accumulation steps, Effective batch size 120\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_stanford_alpaca:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_stanford_alpaca:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp0ngn5dlh', 'job_id': 1271159}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_stanford_alpaca:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_stanford_alpaca:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp4p_k1t0m', 'job_id': 1271160}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_stanford_alpaca:llama-7b\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=oi5_stanford_alpaca:llama-7b --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpm1l_sfwg', 'job_id': 1271161}]\n"
     ]
    }
   ],
   "source": [
    "def compute_mixture_num_samples(mixture, max_train_samples):\n",
    "    s = sum(mixture.values())\n",
    "    mixture = {k: int(max_train_samples*v/s) for k, v in mixture.items()}\n",
    "    return mixture\n",
    "\n",
    "add_hardwarespec_to_dirname = False\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  650 if arch == 'ppc64le' else 64\n",
    "\n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "save_total_limit = 1\n",
    "preprocessing_num_workers = 32\n",
    "evaluation_strategy = 'no' # set do_eval=False\n",
    "eval_steps = save_steps\n",
    "report_to = 'tensorboard wandb'\n",
    "suffix = None\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "# model_name_or_path = 'results/baselines/gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-1.4b'; abbr_model_name = 'pythia-1.4b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-2.8b'; abbr_model_name = 'pythia-2.8b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-6.9b'; abbr_model_name = 'pythia-6.9b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "\n",
    "subsample_mixture_list = []\n",
    "# subsample_mixture_list += [\n",
    "#     {k: max_train_samples} for k in datasets\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     ('humanmix', dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items()))\n",
    "# ] # humanmix mixture.\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(max_train_samples/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {'cot':  0.13568177819252014, 'flan_v2': 0.3957784175872803, \n",
    "#      'dolly': 0.05964866653084755, 'oasst1': 0.4088916480541229}.items())\n",
    "# ] # gpt2-medium_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "# ] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*max_train_samples)) for k, v in\n",
    "#     {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "#         )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_normalized_list = []\n",
    "# subsample_mixture_normalized_list += [('uniform:1200k_doremiv2', # llama-7b_humanmix_uniform:1200k_doremiv2.json\n",
    "#                                        {\"cot\": 0.11419677734375, \"dolly\": 0.1024169921875, \"flan_v2\": 0.204833984375, \"oasst1\": 0.204833984375})]\n",
    "## 10 for trying out datamodels\n",
    "# mixes = [{'cot': 0.37664033529374275,\n",
    "#   'dolly': 0.0874640765523398,\n",
    "#   'flan_v2': 0.39740799933549775,\n",
    "#   'oasst1': 0.1384875888184196},\n",
    "#  {'cot': 0.23064419241874784,\n",
    "#   'dolly': 0.04693354147889885,\n",
    "#   'flan_v2': 0.72121745986295,\n",
    "#   'oasst1': 0.0012048062394032465},\n",
    "#  {'cot': 0.11244721555034376,\n",
    "#   'dolly': 0.21997027355988638,\n",
    "#   'flan_v2': 0.5826671754210359,\n",
    "#   'oasst1': 0.08491533546873392},\n",
    "#  {'cot': 0.27704626812045546,\n",
    "#   'dolly': 0.5712282144637615,\n",
    "#   'flan_v2': 0.024940119654536592,\n",
    "#   'oasst1': 0.12678539776124645},\n",
    "#  {'cot': 0.0024519793352964607,\n",
    "#   'dolly': 0.13274603201304974,\n",
    "#   'flan_v2': 0.012268378167304219,\n",
    "#   'oasst1': 0.8525336104843496},\n",
    "#  {'cot': 0.08065633865016615,\n",
    "#   'dolly': 0.41886215168938545,\n",
    "#   'flan_v2': 0.21723932820070485,\n",
    "#   'oasst1': 0.2832421814597436},\n",
    "#  {'cot': 0.13878643021160036,\n",
    "#   'dolly': 0.05686171157146557,\n",
    "#   'flan_v2': 0.6701353469446995,\n",
    "#   'oasst1': 0.13421651127223455},\n",
    "#  {'cot': 0.2461125374866837,\n",
    "#   'dolly': 0.09774240280444893,\n",
    "#   'flan_v2': 0.13974091986040005,\n",
    "#   'oasst1': 0.5164041398484672},\n",
    "#  {'cot': 0.4069781049152398,\n",
    "#   'dolly': 0.06318759506033228,\n",
    "#   'flan_v2': 0.09504719644992135,\n",
    "#   'oasst1': 0.4347871035745066},\n",
    "#  {'cot': 0.22379693013848484,\n",
    "#   'dolly': 0.30565901275011814,\n",
    "#   'flan_v2': 0.15457716965000887,\n",
    "#   'oasst1': 0.31596688746138824}]\n",
    "\n",
    "# mixes = [\n",
    "#     {'cot': 0.46638974, 'dolly': 0.01456044, 'flan_v2': 0.50886009, 'oasst1': 0.01018973},\n",
    "#     {'cot': 0.39744481, 'dolly': 0.00472114, 'flan_v2': 0.59104177, 'oasst1': 0.00679229},\n",
    "# ]\n",
    "\n",
    "# subsample_mixture_normalized_list += [('', d) for d in mixes]\n",
    "# subsample_mixture_normalized_list += [('humanmix', # humanmix\n",
    "#                                        {'cot': 0.48785105, 'dolly': 0.00732313, 'flan_v2': 0.48785105, 'oasst1': 0.01697478})]\n",
    "# subsample_mixture_normalized_list = [(x[0],  compute_mixture_num_samples(x[1], max_train_samples)) \n",
    "#                                      for x in subsample_mixture_normalized_list]\n",
    "# subsample_mixture_list += subsample_mixture_normalized_list\n",
    "\n",
    "\n",
    "subsample_mixture_list = [('',None)]\n",
    "subsample_inds_file_list = [None]\n",
    "\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "\n",
    "\n",
    "\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subsample_inds_file_abbr_fn(x):\n",
    "    s = os.path.basename(x).split('.pkl')[0]\n",
    "    if s.startswith('inds_'):\n",
    "        scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "        pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "        return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "##  ft1: reproduce open-instruct table with llama7b\n",
    "# job_name = 'ft1'; num_train_epochs_list = [2]\n",
    "# job_name = 'ft1_ep=1'; num_train_epochs_list = [1] # train for 1 epoch (baseline for comparison.)\n",
    "# job_name = 'ft1_ep=2'; num_train_epochs_list = [2]\n",
    "# job_name = 'oi2'; num_train_epochs_list = [2] # 5, 10\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "\n",
    "# train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'\n",
    "\n",
    "# train_file = 'data/processed/lima/lima_data.jsonl'; abbr_train_file = 'lima'\n",
    "# train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# # # train_file = 'data/processed/wpq/cot_flanv2_data.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "# # # train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'hmv1'\n",
    "# train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "\n",
    "\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# train_file = 'data/processed/lima/lima_data.jsonl'; abbr_train_file = 'lima'\n",
    "\n",
    "# # max_train_samples_list = [120]; save_steps = 1; save_total_limit = 100\n",
    "\n",
    "\n",
    "\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "# # train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# # train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# # train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "# # train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# # train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# # train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "# # ft2: test mixture weights\n",
    "# # vary mixture weights\n",
    "# job_name = 'ft2'\n",
    "\n",
    "# # oi3: instruction tuning performance w.r.t. steps.\n",
    "# job_name = 'oi3'\n",
    "\n",
    "# # oi4: data pruning \n",
    "# job_name = 'oi4_flan_v2_vary_subsetsize'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.1, .3, .5]]; num_train_epochs_list = [2]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # oi4_perf_cross_time: perf cross time on flan_v2\n",
    "# job_name = 'oi4_perf_cross_time'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=300_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=1000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# # ## oi4_flanv2_prune_with_hmv1_model\n",
    "# job_name = 'oi4_flanv2_prune_with_hmv1_model'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [int(pct*100000) for pct in [.3]]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b_ft=hmv1/flan_v2/'\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "# ]\n",
    "\n",
    "# ## tulu mix v1.\n",
    "# dataset = 'tulu_v1_human_mix'; train_file = 'data/processed/tulu/tulu_v1_human_mix.jsonl'; abbr_train_file = 'tuluv1hm'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# max_train_samples_list = [30000]; num_train_epochs_list = [3]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b/{dataset}/'\n",
    "# subsample_inds_file_list = [\n",
    "#     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcosp.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'dppmap_k=Kcos1np.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'flan2022_1m'; train_file = 'data/processed/flan2022/flan2022_1m_data.jsonl'; abbr_train_file = 'flan2022_1m'\n",
    "# job_name = f'oi4_{dataset}'\n",
    "# save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# ## full data\n",
    "# # max_train_samples_list = [1000000]; num_train_epochs_list = [1]\n",
    "# # subsample_inds_file_list = ['']\n",
    "# # subset\n",
    "# max_train_samples_list = [200000]; num_train_epochs_list = [1]\n",
    "# subsample_inds_file_list = [\n",
    "# #     os.path.join(data_inds_dir, 'random.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeanscd_nc=3000_decr.pkl'),\n",
    "#     # not that helpful\n",
    "# #     os.path.join(data_inds_dir, 'prob_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_decr.pkl'),\n",
    "#     ## gradnorm outputs\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=l2n_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "#     ## random baselines.\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "#     ## kmeans on grads\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=6000_incr.pkl'),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# ## \n",
    "# dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'\n",
    "# # job_name = f'oi4_{dataset}_ep=3'\n",
    "# job_name = f'oi5_tulu_v1_mix:llama-7b' # re-run to see if transformers upgrade altered eval performance.\n",
    "# # save_steps = 50; save_total_limit = 200\n",
    "# subsample_mixture_list = [('',None)]\n",
    "# data_inds_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256/{dataset}/'\n",
    "# max_train_samples_list = [50000]; num_train_epochs_list = [3]\n",
    "# subsample_inds_file_list = [\n",
    "#     # random baselines\n",
    "# #     os.path.join(data_inds_dir, 'random_s=0.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=1.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'random_s=2.pkl'),\n",
    "# #     # correlated statistics\n",
    "# #     os.path.join(data_inds_dir, 'log_prob_incr.pkl'),\n",
    "#     os.path.join(data_inds_dir, 'log_prob_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'logit_margin_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'el2n_agg=mean_decr.pkl'),\n",
    "# #     # grad norm\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'grad_loraB_l2n_decr.pkl'),  \n",
    "# #     # kmeans\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=1000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=grad+rp+loraB_nc=3000_decr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_incr.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'kmeansl2_emb=text+embedding_nc=3000_decr.pkl'),\n",
    "# # #     # kcos only 50k data\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=grad+rp+loraB_k=Kcos1np.pkl'),\n",
    "# #     os.path.join(data_inds_dir, 'dppmap_emb=text+embedding_k=Kcos1np.pkl'),\n",
    "# ]\n",
    "\n",
    "############\n",
    "# oi5: try curriculum learning / pruning\n",
    "\n",
    "scoring_fn_and_pacing_fn = []\n",
    "\n",
    "# ###### 150k(ep=3) baselines on various baselines\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# M = 150_000; dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'; pacing_fn_list = [f'prune_size={M}_ep=3']\n",
    "# # M = 50_000; dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'; pacing_fn_list = [f'prune_size={M}_ep=5'] \n",
    "\n",
    "\n",
    "# # M = 150_000; dataset = 'wizardlm'; train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'\n",
    "# # M = 150_000; dataset = 'sharegpt'; train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "# # M = 150_000; dataset = 'ultrachat200k'; train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "\n",
    "# scoring_fn_list = []\n",
    "# # scoring_fn_list += ['random_s=0', 'random_s=1']\n",
    "# # scoring_fn_list += ['dppmap_k=vmf_gamma=0.0315_kmd=mpnet']\n",
    "# # scoring_fn_list += ['dppmap_k=rbf_gamma=auto10000_kmd=llama7b_kemb=text+embedding']\n",
    "# scoring_fn_list = ['log_pmi_neg', 'ifd_neg']\n",
    "# # pacing_fn_list = [\n",
    "# # #     f'prune_size={M}_ep=3',\n",
    "# #     f'singlestep_size={M}_startingfrac=0.05',\n",
    "# # #     f'singlestep_size={M}_startingfrac=0.1',\n",
    "# # #     f'singlestep_size={M}_startingfrac=0.2',\n",
    "# #     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "# # ]\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "# ######\n",
    "\n",
    "# ##### codellama on starcoder. 5k subset ep=5\n",
    "# model_name_or_path = hf_models_dir+'codellama/CodeLlama-7b-hf'; abbr_model_name = 'codellama-7b'; max_seq_length = 2048\n",
    "# # M = 25_000; dataset='starcoder_commentinstr'; train_file = 'data/processed/starcoder/starcoder_commentinstr.jsonl'; abbr_train_file = 'starcodercmtinstr'; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 5_000\n",
    "# # M = 25_000; dataset='starcoder_commentinstr_cleaned'; train_file = 'data/processed/starcoder/starcoder_commentinstr_cleaned.jsonl'; abbr_train_file = 'starcodercmtinstrcleaned'; pacing_fn_list = [f'prune_size={M}_ep=5'] ; subset_size = 5_000\n",
    "# # M = 50_000; dataset='starcoder_commentinstrv2'; train_file = 'data/processed/starcoder/starcoder_commentinstrv2.jsonl'; abbr_train_file = 'starcodercmtinstrv2'; pacing_fn_list = [f'prune_size={M}_ep=5'] ; subset_size = 10_000\n",
    "# M = 50_000; dataset='starcoder_commentinstrv5'; train_file = 'data/processed/starcoder/starcoder_commentinstrv5.jsonl'; abbr_train_file = 'starcodercmtinstrv5'; pacing_fn_list = [f'prune_size={M}_ep=5'] ; subset_size = 10_000\n",
    "# # M = 60_000; dataset='starcoder_commentinstrv5'; train_file = 'data/processed/starcoder/starcoder_commentinstrv5.jsonl'; abbr_train_file = 'starcodercmtinstrv5'; pacing_fn_list = [f'prune_size={M}_ep=3'] ; subset_size = 20_000\n",
    "\n",
    "# scoring_fn_list = []\n",
    "# scoring_fn_list += ['log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n', 'ifd_neg', 'log_pmi_neg']\n",
    "\n",
    "# # scoring_fn_list += ['log_pmi_neg']\n",
    "# # scoring_fn_list += ['random_s=0', 'random_s=1']\n",
    "# # scoring_fn_list += ['dedup_md=mpnet_emb=text+embedding']\n",
    "\n",
    "# # scoring_fn_list += [\n",
    "# #     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd=mpnet',\n",
    "# #     f'dppmap_k=rbf_gamma=auto{subset_size}_kmd=llama7b_kemb=text+embedding',\n",
    "# #     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd=codellama7b_kemb=grad+rp+loraB',\n",
    "# # ]\n",
    "\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "# #####\n",
    "\n",
    "# ##### \n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# # M = 150_000; dataset = 'tulu_v1_mix'; train_file = 'data/processed/tulu/tulu_v1_mix.jsonl'; abbr_train_file = 'tuluv1m'; pacing_fn_list = [f'prune_size={M}_ep=3']\n",
    "# # M = 200_000; dataset = 'wizardlm'; train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'; pacing_fn_list = [f'prune_size={M}_ep=2']\n",
    "# # M = 100_000; dataset = 'wizardlm'; train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'; pacing_fn_list = [f'prune_size={M}_ep=2'] # 6hr gpu ok.\n",
    "# M = 50_000; dataset = 'wizardlm'; train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'; pacing_fn_list = [f'prune_size={M}_ep=5'] # 6hr gpu ok.\n",
    "# # M = 10_000; dataset = 'wizardlm'; train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'; pacing_fn_list = [f'prune_size={M}_ep=10'] # 6hr gpu ok.\n",
    "\n",
    "\n",
    "# scoring_fn_list = []\n",
    "# # scoring_fn_list += ['log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n'] # 'numtoks_input_neg'\n",
    "# # scoring_fn_list += ['ifd', 'ifd_neg', 'log_pmi', 'log_pmi_neg']\n",
    "# # scoring_fn_list += ['dedup_md=mpnet_emb=text+embedding']\n",
    "# # scoring_fn_list += ['semdedup_cl=kmeansfaisscd_md=mpnet_dist=cd_emb=text+embedding_nc=200',\n",
    "# # #                     'semdedup_cl=kmeansfaisscd_md=bge_dist=cd_emb=text+embedding_nc=200'\n",
    "# #                    ]\n",
    "# # scoring_fn_list = ['semdedup_cl=kmeansfaisscd_md=mpnet_dist=cd_emb=text+embedding_nc=200',\n",
    "# #                    'semdedup_cl=kmeansfaisscd_md=bge_dist=cd_emb=text+embedding_nc=200',\n",
    "# #                    'semdedup_cl=kmeansfaisscd_md=llama7b_dist=cd_emb=text+embedding_nc=200',\n",
    "# #                    'semdedup_cl=kmeansfaisscd_md=llama7b_dist=cd_emb=grad+rp+loraB_nc=200',]\n",
    "# # scoring_fn_list += ['random_s=0', 'random_s=1']\n",
    "# scoring_fn_list = [ \n",
    "#     # ->1k\n",
    "# #     'dppmap:k=lin:kmd=mpnet',\n",
    "# #     'dppmap_k=vmf_gamma=auto1000_kmd=bge'\n",
    "# #     'dppmap_k=vmf_gamma=0.008_kmd=mpnet',\n",
    "# #     'dppmap_theta=0.5_k=vmf_gamma=0.01_kmd=mpnet_q=log+pmi_qmd=llama7b',\n",
    "# #     'dppmap_theta=0.3_k=vmf_gamma=0.01_kmd=mpnet_q=log+pmi_qmd=llama7b',\n",
    "# #     'dppmap_k=rbf_gamma=0.0000007_kmd=llama7b_kemb=grad+rp+loraB',\n",
    "# #     'dppmap_k=rbf_gamma=3.5e-8_kmd=llama7b_kemb=text+embedding',\n",
    "# #     'dppmap_k=vmf_gamma=auto1000_kmd=llama7b_kemb=text+embedding',\n",
    "# #     'dppmap_k=vmf_gamma=auto1000_kmd=llama7b_kemb=grad+rp+loraB',\n",
    "#     # ->10k\n",
    "# #     'dppmap_k=vmf_gamma=auto10000_kmd=bge'\n",
    "# #     'dppmap_theta=0.5_k=vmf_gamma=0.043_kmd=mpnet_q=log+pmi_qmd=llama7b',\n",
    "# #     'dppmap_theta=0.7_k=vmf_gamma=0.043_kmd=mpnet_q=log+pmi_qmd=llama7b',\n",
    "# #     'dppmap_theta=0.3_k=vmf_gamma=0.04_kmd=mpnet_q=ifd_qmd=llama7b',\n",
    "# #     'dppmap_theta=0.3_k=vmf_gamma=0.043_kmd=mpnet_q=ifd+neg_qmd=llama7b',\n",
    "# #     'dppmap_k=rbf_gamma=7.5e-06_kmd=llama7b_kemb=text+embedding',\n",
    "# #     'dppmap_k=rbf_gamma=0.001_kmd=llama7b_kemb=grad+rp+loraB',\n",
    "# #     'dppmap_k=vmf_gamma=auto10000_kmd=llama7b_kemb=text+embedding',\n",
    "# #     'dppmap_k=vmf_gamma=auto10000_kmd=llama7b_kemb=grad+rp+loraB',\n",
    "#     'dppmap_theta=0.1_k=rbf_gamma=auto10000_kmd=llama7b_kemb=text+embedding_q=log+pmi_qmd=llama7b',\n",
    "#     'dppmap_theta=0.05_k=rbf_gamma=auto10000_kmd=llama7b_kemb=text+embedding_q=log+pmi_qmd=llama7b',\n",
    "# #     'dppmap_theta=0.3_k=rbf_gamma=auto10000_kmd=llama7b_kemb=text+embedding_q=log+pmi_qmd=llama7b',\n",
    "# #     'dppmap_theta=0.5_k=rbf_gamma=auto10000_kmd=llama7b_kemb=text+embedding_q=log+pmi_qmd=llama7b',\n",
    "# ]\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "# ####\n",
    "\n",
    "##### sharegptv2, open_orca_slim\n",
    "# model_name_or_path = hf_models_dir+'NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048 # 4096\n",
    "# model_name_or_path = hf_models_dir+'NousResearch/Llama-2-13b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048 # 4096\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "\n",
    "# M = 60_000; dataset = 'wizardlm'; train_file = 'data/processed/wizardlm/wizardlm_data.jsonl'; abbr_train_file = 'wizardlm'; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 60_000; dataset = 'stanford_alpaca'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 60_000; dataset = 'oasst1'; train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 10_000; dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "# M = 10_000; dataset = 'oasst1'; train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "# M = 10_000; dataset = 'flan_v2'; train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "M = 10_000; dataset = 'stanford_alpaca'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "# M = 50_000; dataset = 'sharegptv2'; train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "# M = 10_000; dataset = 'sharegptv2'; train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "# M = 50_000; dataset = 'open_orca_slim'; train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "# M = 50_000; dataset = 'tulu_v2'; train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2'; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "\n",
    "scoring_fn_list = []\n",
    "\n",
    "# scoring_fn_list += ['random_s=0'] # , 'random_s=1'\n",
    "# scoring_fn_list += ['random_s=1']\n",
    "# scoring_fn_list += ['log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n', 'ifd_neg', 'log_pmi_neg']\n",
    "\n",
    "# scoring_fn_list += [\n",
    "#     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd=mpnet',\n",
    "#     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd=llama7b_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=auto{subset_size}_kmd=llama7b_kemb=text+embedding',\n",
    "#     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd=llama7b_kemb=grad+rp+loraB',\n",
    "#     f'dppmap_k=rbf_gamma=auto{subset_size}_kmd=llama7b_kemb=grad+rp+loraB',\n",
    "# ]\n",
    "scoring_fn_list += [\n",
    "    f'dppmap_k=vmf_gamma=auto{subset_size}_kmd={kmd}_kemb=grad+rp+loraB'\n",
    "    for kmd in ['llama7br256p4096', 'llama7br512p4096', 'pythia1br512p4096']\n",
    "]\n",
    "scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "##### \n",
    "\n",
    "\n",
    "\n",
    "# ### mistral-7b on ultrachat15/200k\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# # M =  50_000; dataset = 'ultrachat200k'; train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'; abbr_train_file = 'ultrachat200k'\n",
    "# M = 100_000; dataset = 'ultrachat15';   train_file = 'data/processed/ultrachat/ultrachat15_data.jsonl'; abbr_train_file = 'ultrachat15'; preprocessing_num_workers = 64\n",
    "# # M = 400_000; dataset = 'ultrachat15';   train_file = 'data/processed/ultrachat/ultrachat15_data.jsonl'; abbr_train_file = 'ultrachat15'; preprocessing_num_workers = 64\n",
    "\n",
    "# # scoring_fn_list = ['log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n', 'el2n_agg=mean']\n",
    "# # scoring_fn_list = ['numtoks_input_neg']\n",
    "# # scoring_fn_list = ['log_prob_neg', 'el2n_agg=mean', 'logit_margin_neg', 'grad_loraB_l2n',] #  'kmeansl2_emb=text+embedding_nc=3000_incr'\n",
    "# # scoring_fn_list = ['rhov1_log_prob', 'rhov1_log_prob_neg']\n",
    "# # scoring_fn_list = ['numtoks_input_neg', 'numtoks_output_neg', 'numtoks_total_neg']\n",
    "# # scoring_fn_list = [\n",
    "# #     'semdedup_cl=kmeansfaisscd_md=mpnet_dist=cd_emb=text+embedding_nc=200',\n",
    "# #                    'semdedup_cl=kmeansfaisscd_md=mistral7b_dist=cd_emb=text+embedding_nc=200',\n",
    "# #                    'semdedup_cl=kmeansfaisscd_md=mistral7b_dist=cd_emb=grad+rp+loraB_nc=200',\n",
    "# #                   ]\n",
    "# # scoring_fn_list = ['dppmapbd_nc=200_k=lin_kmd=mpnet']\n",
    "# # scoring_fn_list =[f'dppmapbd_nc=200_k=vmf_gamma={gamma}_kmd=mpnet' for gamma in [.3, 3.]]\n",
    "# # scoring_fn_list = ['dppmapbd_nc=200_k=vmf_gamma=0.000035_kmd=mpnet']\n",
    "# scoring_fn_list = ['dppmapbd_nc=200_k=vmf_gamma=1.0_kmd=mpnet'] \n",
    "# pacing_fn_list = [f'prune_size={M}_ep=2']\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "# ##### \n",
    "\n",
    "evaluation_strategy = 'steps' if 'ultrachat' in dataset else 'no'\n",
    "job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "dataloader_sampler = 'SequentialSampler'\n",
    "\n",
    "## random baselines # 'random_s=0', 'random_s=1'\n",
    "# scoring_fn_list = ['random_s=0']; pacing_fn_list = [f'prune_size={M}_ep=2']\n",
    "# scoring_fn_and_pacing_fn += list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "subsample_inds_file_list = []\n",
    "for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "    from note_pruning import get_final_model_name\n",
    "    md_expand = get_final_model_name(abbr_model_name, scoring_fn)\n",
    "    print(f'md={abbr_model_name} scoring_fn={scoring_fn}\\t->\\tmd_expand={md_expand}')\n",
    "    data_inds_dir = (f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/'\n",
    "                     f'{md_expand}/{dataset}/')\n",
    "    p = os.path.join(data_inds_dir, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "    if not os.path.isfile(p):\n",
    "        raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "    subsample_inds_file_list.append(p)\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "# #### sharegptv2 & open_orca_slim baselines\n",
    "# job_name = 'oi2'; num_train_epochs_list = [2] # 5, 10\n",
    "# # model_name_or_path = hf_models_dir+'NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# # train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'\n",
    "# # train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; \n",
    "# # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "\n",
    "# # train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2'; max_train_samples_list=[100_000]\n",
    "# ####\n",
    "\n",
    "\n",
    "# ##### code instruction tuning\n",
    "# model_name_or_path = hf_models_dir+'codellama/CodeLlama-7b-hf'; abbr_model_name = 'codellama-7b'; max_seq_length = 2048\n",
    "\n",
    "# # job_name = 'oi6_starcoder_ep=5'; # 5k train for 5 epochs\n",
    "# # num_train_epochs_list = [5]\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstr.jsonl'; abbr_train_file = 'starcodercmtinstr'\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstr_cleaned.jsonl'; abbr_train_file = 'starcodercmtinstrcleaned'\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstrv2.jsonl'; abbr_train_file = 'starcodercmtinstrv2'; max_train_samples_list=[5000]\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstrv2_flppl.jsonl'; abbr_train_file = 'starcodercmtinstrv2_flppl'; max_train_samples_list=[5000]\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstrv4_rge5.jsonl'; abbr_train_file = 'starcodercmtinstrv4_rge5'; max_train_samples_list=[5000]\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstrv4.jsonl'; abbr_train_file = 'starcodercmtinstrv4'; max_train_samples_list=[5000]\n",
    "\n",
    "# job_name = 'oi2'; \n",
    "# num_train_epochs_list = [2]\n",
    "# # train_file = 'data/processed/starcoder/starcoder_commentinstrv2.jsonl'; abbr_train_file = 'starcodercmtinstrv2'\n",
    "# train_file = 'data/processed/starcoder/starcoder_commentinstrv5.jsonl'; abbr_train_file = 'starcodercmtinstrv5'\n",
    "\n",
    "# ##### \n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b on 100k. data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12 # llama-7b on 100k. data\n",
    "# nodes = 10; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b on 100k. data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 24 # llama-7b on 100k. data\n",
    "\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 18 # llama-7b on 400k data\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 30 # llama-7b on 600k data\n",
    "\n",
    "# nodes = 1; num_gpus = 1; gpu_type = 'v100'; job_duration = 6  # gpt2\n",
    "# nodes = 2; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-1.4b\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-2.8b|6.9b\n",
    "\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "\n",
    "per_device_train_batch_size = 2; total_batch_size = 128 # 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "\n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16' # ; mixed_precision = None\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = True\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_mixture_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     mix_name_and_subsample_mixture,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "    mix_name, subsample_mixture = mix_name_and_subsample_mixture\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "        \n",
    "    if job_name == 'ft2':\n",
    "        if subsample_mixture is not None:\n",
    "            assert(abbr_train_file=='all')\n",
    "            output_dirname += \\\n",
    "                '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "            \n",
    "    if any(job_name == y for y in ['oi2']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "            \n",
    "    if job_name == 'oi3':\n",
    "        output_dirname += '_'+mix_name\n",
    "        \n",
    "#     if job_name.startswith('oi4'):\n",
    "    if subsample_inds_file:\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            '_dtype='+torch_dtype+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)\n",
    "    if suffix:\n",
    "        output_dirname += suffix\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    wandb_run_name = output_dir.replace('results/', '')\n",
    "    \n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers={preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type={lr_scheduler_type} \\\n",
    "        --warmup_ratio={warmup_ratio} \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy={evaluation_strategy} \\\n",
    "        {'--eval_steps='+str(eval_steps) if eval_steps else ''} \\\n",
    "        {'--report_to '+str(report_to) if report_to else ''} \\\n",
    "        --run_name {wandb_run_name} \\\n",
    "        --logging_strategy=steps \\\n",
    "        --logging_first_step \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        --ddp_timeout=7200 \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --use_flash_attn False \\\n",
    "        --low_cpu_mem_usage \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache # if delete a dataset and need to refresh cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfdbfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama-7b+lora:r=256:a=4096+proj=4096'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# if 'md=mpnet' in scoring_fn: md = 'all-mpnet-base-v2'\n",
    "# elif 'md=bge' in scoring_fn: md = 'bge-large-en-v1.5'\n",
    "# elif 'md=llama7b' in scoring_fn: md = 'llama-7b+lora:r=256:a=256'\n",
    "# elif 'md=llama2:7b' in scoring_fn: md = 'llama2-7b+lora:r=256:a=256'\n",
    "# elif 'md=codellama7b' in scoring_fn: md = 'codellama-7b+lora:r=256:a=256'\n",
    "# elif 'md=mistral7b' in scoring_fn: md = 'mistral-7b+lora:r=256:a=256'\n",
    "# else: raise ValueError(f'md not found in {scoring_fn}')\n",
    "# data_inds_dir = (f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/curriculum/{md}/{dataset}/')\n",
    "\n",
    "\n",
    "from note_pruning import get_final_model_name\n",
    "md_expand = get_final_model_name(None, scoring_fn)\n",
    "md_expand\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_all_symlinks(directory, verbose=False):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files + dirs:\n",
    "            path = os.path.join(root, name)\n",
    "            if os.path.islink(path):\n",
    "                os.unlink(path)\n",
    "                if verbose:\n",
    "                    print(f\"Removed symlink: {path}\")\n",
    "                \n",
    "import uuid\n",
    "\n",
    "def create_unique_symlinks(file_paths, verbose=False):\n",
    "    \"\"\"Create symlinks for each `file` in `files` in the same directory, with a unique name. \"\"\"\n",
    "    dirs = [os.path.dirname(x) for x in file_paths]\n",
    "\n",
    "    symlink_path_dict = {}\n",
    "    for directory, path in zip(dirs, file_paths):\n",
    "        if os.path.isdir(path):\n",
    "            symlink_name = f\"symlink_{str(uuid.uuid4())[:8]}\"  # Generate a unique symlink name\n",
    "            symlink_path = os.path.join(directory, symlink_name)\n",
    "            try:\n",
    "                os.symlink(os.path.abspath(path), symlink_path)\n",
    "                if verbose:\n",
    "                    print(f\"Created symlink: {symlink_path} -> {path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Failed to create symlink: {path}. Error: {e}\")\n",
    "            symlink_path_dict.update({path: symlink_path})\n",
    "    return symlink_path_dict\n",
    "\n",
    "\n",
    "def get_chat_formatting_function(model_name_or_path):\n",
    "    if any(model_name_or_path.endswith(x) for x in [\n",
    "        'NousResearch/Llama-2-7b-chat-hf',\n",
    "        'codellama/CodeLlama-7b-Instruct-hf',\n",
    "    ]):\n",
    "        chat_formatting_function = 'eval.templates.create_prompt_with_llama2_chat_format'\n",
    "    elif any(model_name_or_path.endswith(x) for x in [\n",
    "        'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "        'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "        'HuggingFaceH4/zephyr-7b-alpha',\n",
    "        'HuggingFaceH4/zephyr-7b-beta',\n",
    "    ]):\n",
    "        chat_formatting_function = 'eval.templates.create_prompt_with_zephyr_chat_format'\n",
    "    elif any(model_name_or_path.endswith(x) for x in [\n",
    "        'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "    ]):\n",
    "        chat_formatting_function = 'eval.templates.create_prompt_with_mistral_instruct_chat_format'\n",
    "    else:\n",
    "        chat_formatting_function = 'eval.templates.create_prompt_with_tulu_chat_format'\n",
    "    return chat_formatting_function\n",
    "\n",
    "\n",
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    model_name_or_path = model_name_or_path.lower()\n",
    "    if any(x in model_name_or_path for x in ['gpt2-medium', 'pythia-160m']):\n",
    "        return 50, 1\n",
    "    if any(x in model_name_or_path for x in ['gpt-xl']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp']):\n",
    "            return 16, 1\n",
    "        else:\n",
    "            return 32, 1\n",
    "    if any(x in model_name_or_path for x in ['llama', 'mistral', 'zephyr', 'pythia-1.4b', 'pythia-2.8b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 5, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    if any(x in model_name_or_path for x in ['pythia-6.9b', 'dolly-v2-7b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5', 'mmlu_s=0', 'tydiqa_s=1_gp', 'alpacafarm']):\n",
    "            return 4, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    return 10, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "499d6f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alpacafarm_ann=chatgpt_chatfmt', 'results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=random:s=1_pace=prune:size=10000:ep=10')\n",
      "('alpacafarm_ann=chatgpt_chatfmt', 'results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=random:s=0_pace=prune:size=10000:ep=10')\n",
      "('alpacafarm_ann=chatgpt_chatfmt', 'results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=dppmap:k=rbf:gamma=auto1000:kmd=llama7b:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('alpacafarm_ann=chatgpt_chatfmt', 'results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=dppmap:k=vmf:gamma=auto1000:kmd=llama7b:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "#cmds:  4 \n",
      "\n",
      "python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=random:s=1_pace=prune:size=10000:ep=10\" --max_new_tokens 2048 --save_dir \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=random:s=1_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=chatgpt_chatfmt\" --eval_batch_size 5 --annotators_config chatgpt --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.alpacafarm_ann=chatgpt_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=random:s=0_pace=prune:size=10000:ep=10\" --max_new_tokens 2048 --save_dir \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=random:s=0_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=chatgpt_chatfmt\" --eval_batch_size 5 --annotators_config chatgpt --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.alpacafarm_ann=chatgpt_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=dppmap:k=rbf:gamma=auto1000:kmd=llama7b:kemb=text+embedding_pace=prune:size=10000:ep=10\" --max_new_tokens 2048 --save_dir \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=dppmap:k=rbf:gamma=auto1000:kmd=llama7b:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=chatgpt_chatfmt\" --eval_batch_size 5 --annotators_config chatgpt --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.alpacafarm_ann=chatgpt_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=dppmap:k=vmf:gamma=auto1000:kmd=llama7b:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --max_new_tokens 2048 --save_dir \"results/oi5_flan_v2:llama-7b/llama-7b_flan_v2_score=dppmap:k=vmf:gamma=auto1000:kmd=llama7b:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=chatgpt_chatfmt\" --eval_batch_size 5 --annotators_config chatgpt --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.alpacafarm_ann=chatgpt_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# exp_dir = ''\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "\n",
    "\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "use_slow_tokenizer = True\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0', # \n",
    "    'mmlu_s=5', # ~1hr\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "#     'alpacafarm_ann=gpt35:turbo:1106',\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "task_names_alpacafarm = ['alpacafarm_ann=chatgpt_chatfmt']\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "# # ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "# #     'gpt2',\n",
    "# #     'gpt2-medium',\n",
    "# #     'huggyllama/llama-7b', \n",
    "# #     'mistralai/Mistral-7B-v0.1',\n",
    "# #     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "# #     'NousResearch/Llama-2-7b-hf',\n",
    "# #     'NousResearch/Llama-2-7b-chat-hf',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "# #     'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "# #     'HuggingFaceH4/zephyr-7b-alpha',\n",
    "# #     'HuggingFaceH4/zephyr-7b-beta',\n",
    "# #     'EleutherAI/pythia-1.4b',\n",
    "# #     'EleutherAI/pythia-2.8b',\n",
    "# #     'EleutherAI/pythia-6.9b',\n",
    "# #     'databricks/dolly-v2-7b',\n",
    "# #     'codellama/CodeLlama-7b-hf',\n",
    "# #     'codellama/CodeLlama-7b-Python-hf',\n",
    "# #     'codellama/CodeLlama-7b-Instruct-hf',\n",
    "# ]]\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# # ## baseline re-eval after merge upstream/main\n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b',\n",
    "# ]]\n",
    "# subdir_path_list += ['results/ft1/llama-7b_humanmix']\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1\n",
    "# exp_dir = 'results/ft1'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## ft1_ep=1\n",
    "# # exp_dir = 'results/ft1_ep=1'\n",
    "# # exp_dir = 'results/ft1_ep=2'\n",
    "# exp_dir = 'results/oi2'\n",
    "# subdir_filter_fn = lambda x: 'sharegptv2' in x\n",
    "# task_names = task_names_chatfmt\n",
    "# # task_names = task_names+task_names_chatfmt\n",
    "# # task_names = task_names_alpacafarm\n",
    "\n",
    "\n",
    "# ## ft2\n",
    "# exp_dir = 'results/ft2/'\n",
    "# create_symlinks = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['llama-7b'])\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# ## llama-7b time-series 400k, 600k\n",
    "# exp_dir = 'results/oi3/'\n",
    "# include_checkpoints = True\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['400k', '600k']) # , '600k'\n",
    "# task_names = task_names+task_names_chatfmt\n",
    "\n",
    "# oi4 include checkpoints!\n",
    "# exp_dir = 'results/oi4_perf_cross_time/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_human_mix/'\n",
    "# exp_dir = 'results/oi4_flanv2_prune_with_hmv1_model/'\n",
    "# exp_dir = 'results/oi4_flan2022_1m/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix/'\n",
    "# exp_dir = 'results/oi4_tulu_v1_mix_ep=3/'\n",
    "# include_checkpoints = False\n",
    "# subdir_filter_fn = lambda x: any(y in x for y in ['random']) #$ ['log_prob_decr', 'el2n_agg=mean_incr', 'logit_margin_decr', 'grad_loraB']\n",
    "# task_names = task_names_chatfmt\n",
    "# task_names = task_names_alpacafarm\n",
    "\n",
    "# # oi4 without checkpoint \n",
    "# # exp_dir = 'results/oi4/'\n",
    "# exp_dir = 'results/oi4_flan_v2_vary_subsetsize/'\n",
    "# task_names = task_names_chatfmt\n",
    "\n",
    "# # oi5\n",
    "# exp_dir = 'results/oi5_tulu_v1_mix:llama-7b/'\n",
    "# exp_dir = 'results/oi5_ultrachat:mistral-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat200k:mistral-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat15:mistral-7b'\n",
    "# exp_dir = 'results/oi5_wizardlm:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegptv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_tulu_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_open_orca_slim:llama-7b'\n",
    "# exp_dir = 'results/oi5_stanford_alpaca:llama-7b'\n",
    "exp_dir = 'results/oi5_flan_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_dolly:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b'\n",
    "# exp_dir = 'results/oi6_starcoder_ep=5'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstr:codellama-7b'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstrv2:codellama-7b'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstrv4:codellama-7b'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstrv5:codellama-7b'\n",
    "# subdir_filter_fn = lambda x: '' in x\n",
    "# task_names = task_names + task_names_chatfmt\n",
    "task_names = task_names_alpacafarm\n",
    "# task_names = ['humaneval', 'humaneval_chatfmt']\n",
    "# task_names = ['alpacafarm_ann=gpt35:turbo:1106_chatfmt']\n",
    "\n",
    "\n",
    "# ### code\n",
    "# exp_dir = 'results/oi2'\n",
    "# subdir_filter_fn = lambda x: 'starcoder' in x\n",
    "# task_names = ['humaneval', 'humaneval_chatfmt']\n",
    "# ### \n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "\n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    chat_formatting_function = get_chat_formatting_function(model_name_or_path)\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    if 'gpt2' in ft_args_model_name_or_path:\n",
    "        tydiqa_max_context_length = 400 # max ctx len without exceeding max_seq_len\n",
    "    else:\n",
    "        tydiqa_max_context_length = 512\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 512 \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 3 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length {tydiqa_max_context_length} \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314', 'gpt35_turbo_1106']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --max_new_tokens 2048 \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29757046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 12:24:38,507] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading data and model...\n",
      "WARNING:datasets.builder:Found cached dataset alpaca_eval (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/tatsu-lab___alpaca_eval/alpaca_eval/1.0.0/ee1877b2c22e5daf024c7a22ef948f29e8e16cd28568b91bf82818b7608e04e4)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 529.92it/s]\n",
      "Selecting data subsets:  [654, 114, 25, 759, 281, 250, 228, 142, 754, 104]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:15<00:00,  5.07s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating Completions:   0%|                            | 0/10 [00:00<?, ?it/s]/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Generating Completions: 100%|███████████████████| 10/10 [00:17<00:00,  1.75s/it]\n",
      "INFO:root:Evaluating the llama-7b_tuluv1m-greedy-long outputs.\n",
      "INFO:root:Creating the annotator from `chatgpt`.\n",
      "WARNING:root:Saving_path is given but not 'auto', make sure that it's different for different seeds.\n",
      "WARNING:root:The length of outputs before and after merge are not the same. We have len(outputs_1)==805, len(outputs_2)==10, and len(df_annotated)==10. This means that there are missing examples or duplicates. We are taking a SQL inner join.\n",
      "Annotation chunk:   0%|                                   | 0/1 [00:00<?, ?it/s]INFO:root:Annotating 10 examples with chatgpt\n",
      "INFO:root:Using `openai_completions` on 10 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'temperature': 0}. num_procs=5\n",
      "\n",
      "prompt_batches:   0%|                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "prompt_batches:  10%|██▊                         | 1/10 [00:00<00:04,  1.92it/s]\u001b[A\n",
      "prompt_batches:  60%|████████████████▊           | 6/10 [00:00<00:00,  7.50it/s]\u001b[A\n",
      "prompt_batches: 100%|███████████████████████████| 10/10 [00:01<00:00,  9.72it/s]\u001b[A\n",
      "INFO:root:Completed 10 examples in 1.2 seconds.\n",
      "INFO:root:Saving all annotations to results/ft1_ep=2/llama-7b_tuluv1m/eval/alpacafarm_ann=chatgpt/alpaca_eval_annotator_cache.json.\n",
      "Annotation chunk: 100%|███████████████████████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "INFO:root:Saving all results to results/ft1_ep=2/llama-7b_tuluv1m/eval/alpacafarm_ann=chatgpt\n",
      "Price (per-example / total) = 0.0010 / 0.01\n",
      "Time  (per-example / total) = 0.1155 / 1.16\n",
      "                          model  win_rate  standard_error  n_wins  n_wins_base  n_draws  n_total       mode  avg_length  avg_output_tok_length  price\n",
      "0  llama-7b_tuluv1m-greedy-long     10.00           10.00       1            9        0       10  community         184                  41.00   0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!CUDA_VISIBLE_DEVICES=3 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path \"results/ft1_ep=2/llama-7b_tuluv1m\" --max_new_tokens 2048 --save_dir \"results/ft1_ep=2/llama-7b_tuluv1m/eval/alpacafarm_ann=chatgpt\" --eval_batch_size 5 --annotators_config chatgpt --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --max_num_examples 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4392e9ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/pd.py:123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  col_contains_substr = df[col].str.contains(substr, regex=True)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/pd.py:123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  col_contains_substr = df[col].str.contains(substr, regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ba4ae_row0_col0, #T_ba4ae_row0_col1, #T_ba4ae_row1_col0, #T_ba4ae_row1_col1, #T_ba4ae_row2_col0, #T_ba4ae_row2_col1, #T_ba4ae_row3_col0, #T_ba4ae_row3_col1, #T_ba4ae_row4_col0, #T_ba4ae_row4_col1, #T_ba4ae_row5_col0, #T_ba4ae_row5_col1, #T_ba4ae_row6_col0, #T_ba4ae_row6_col1, #T_ba4ae_row7_col0, #T_ba4ae_row7_col1, #T_ba4ae_row8_col0, #T_ba4ae_row8_col1, #T_ba4ae_row9_col0, #T_ba4ae_row9_col1, #T_ba4ae_row10_col0, #T_ba4ae_row10_col1, #T_ba4ae_row11_col0, #T_ba4ae_row11_col1, #T_ba4ae_row12_col0, #T_ba4ae_row12_col1, #T_ba4ae_row13_col0, #T_ba4ae_row13_col1, #T_ba4ae_row14_col0, #T_ba4ae_row14_col1, #T_ba4ae_row15_col0, #T_ba4ae_row15_col1, #T_ba4ae_row16_col0, #T_ba4ae_row16_col1, #T_ba4ae_row17_col0, #T_ba4ae_row17_col1, #T_ba4ae_row18_col0, #T_ba4ae_row18_col1, #T_ba4ae_row19_col0, #T_ba4ae_row19_col1, #T_ba4ae_row20_col0, #T_ba4ae_row20_col1, #T_ba4ae_row21_col0, #T_ba4ae_row21_col1, #T_ba4ae_row22_col0, #T_ba4ae_row22_col1, #T_ba4ae_row23_col0, #T_ba4ae_row23_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ba4ae_row0_col2, #T_ba4ae_row0_col10, #T_ba4ae_row0_col11, #T_ba4ae_row0_col13, #T_ba4ae_row0_col14, #T_ba4ae_row2_col4, #T_ba4ae_row2_col6, #T_ba4ae_row3_col7, #T_ba4ae_row5_col8, #T_ba4ae_row12_col5, #T_ba4ae_row14_col3, #T_ba4ae_row14_col9, #T_ba4ae_row17_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row0_col3, #T_ba4ae_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row0_col4, #T_ba4ae_row3_col4, #T_ba4ae_row7_col4, #T_ba4ae_row20_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row0_col5, #T_ba4ae_row1_col2, #T_ba4ae_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row0_col6, #T_ba4ae_row0_col7, #T_ba4ae_row10_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row0_col8, #T_ba4ae_row4_col8, #T_ba4ae_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e57058;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row0_col9, #T_ba4ae_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cd423b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row0_col12, #T_ba4ae_row6_col9, #T_ba4ae_row6_col14, #T_ba4ae_row20_col2, #T_ba4ae_row21_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row1_col4, #T_ba4ae_row9_col4, #T_ba4ae_row18_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row1_col6, #T_ba4ae_row3_col6, #T_ba4ae_row11_col3, #T_ba4ae_row14_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ed8366;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row1_col10, #T_ba4ae_row7_col11, #T_ba4ae_row9_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row1_col11, #T_ba4ae_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row1_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row1_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col3, #T_ba4ae_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col5, #T_ba4ae_row4_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col7, #T_ba4ae_row6_col8, #T_ba4ae_row11_col7, #T_ba4ae_row11_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row2_col8, #T_ba4ae_row9_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col9, #T_ba4ae_row5_col10, #T_ba4ae_row10_col4, #T_ba4ae_row14_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row2_col10, #T_ba4ae_row7_col10, #T_ba4ae_row13_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f18d6f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #cc403a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row2_col14, #T_ba4ae_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row3_col2, #T_ba4ae_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row3_col8, #T_ba4ae_row6_col3, #T_ba4ae_row19_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b6cefa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row3_col12, #T_ba4ae_row9_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row3_col13, #T_ba4ae_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row3_col14, #T_ba4ae_row8_col13, #T_ba4ae_row10_col3, #T_ba4ae_row16_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row4_col2, #T_ba4ae_row7_col8, #T_ba4ae_row14_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row4_col4, #T_ba4ae_row4_col7, #T_ba4ae_row9_col2, #T_ba4ae_row9_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row4_col5, #T_ba4ae_row13_col7, #T_ba4ae_row17_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row4_col6, #T_ba4ae_row9_col8, #T_ba4ae_row12_col2, #T_ba4ae_row16_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row4_col9, #T_ba4ae_row17_col13, #T_ba4ae_row19_col7, #T_ba4ae_row20_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row4_col10, #T_ba4ae_row9_col7, #T_ba4ae_row11_col2, #T_ba4ae_row18_col3, #T_ba4ae_row19_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row4_col11, #T_ba4ae_row6_col13, #T_ba4ae_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row4_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row5_col3, #T_ba4ae_row14_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row5_col4, #T_ba4ae_row5_col7, #T_ba4ae_row8_col7, #T_ba4ae_row8_col10, #T_ba4ae_row17_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row5_col9, #T_ba4ae_row6_col7, #T_ba4ae_row20_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row5_col11, #T_ba4ae_row19_col8, #T_ba4ae_row22_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row5_col13, #T_ba4ae_row5_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f39577;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row6_col4, #T_ba4ae_row7_col9, #T_ba4ae_row12_col4, #T_ba4ae_row18_col9, #T_ba4ae_row21_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row6_col5, #T_ba4ae_row10_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row6_col10, #T_ba4ae_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row6_col11, #T_ba4ae_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f49a7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row6_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row7_col5, #T_ba4ae_row15_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #eed0c0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row7_col6, #T_ba4ae_row21_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row7_col7, #T_ba4ae_row21_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row7_col12, #T_ba4ae_row17_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row7_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f08a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row7_col14, #T_ba4ae_row9_col14, #T_ba4ae_row10_col7, #T_ba4ae_row16_col13, #T_ba4ae_row17_col3, #T_ba4ae_row22_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row8_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row8_col4, #T_ba4ae_row9_col6, #T_ba4ae_row13_col11, #T_ba4ae_row19_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d2dbe8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row8_col6, #T_ba4ae_row21_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row8_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row8_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row8_col12, #T_ba4ae_row13_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row8_col14, #T_ba4ae_row12_col3, #T_ba4ae_row15_col2, #T_ba4ae_row15_col13, #T_ba4ae_row18_col7, #T_ba4ae_row20_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row9_col5, #T_ba4ae_row12_col7, #T_ba4ae_row12_col13, #T_ba4ae_row15_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row9_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row9_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row10_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b497;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row10_col6, #T_ba4ae_row17_col6, #T_ba4ae_row22_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row10_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row10_col10, #T_ba4ae_row11_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row10_col11, #T_ba4ae_row15_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #e1dad6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row10_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row10_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col4, #T_ba4ae_row13_col4, #T_ba4ae_row16_col2, #T_ba4ae_row17_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col5, #T_ba4ae_row18_col13, #T_ba4ae_row23_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row11_col11, #T_ba4ae_row12_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row11_col14, #T_ba4ae_row19_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row12_col6, #T_ba4ae_row21_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dadce0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row12_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row12_col10, #T_ba4ae_row13_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row12_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row12_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #c5d6f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row13_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row13_col3, #T_ba4ae_row17_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row13_col5, #T_ba4ae_row16_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row13_col6, #T_ba4ae_row13_col13, #T_ba4ae_row15_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row13_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4961d2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row14_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row14_col4, #T_ba4ae_row15_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row14_col5, #T_ba4ae_row15_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row14_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #d5dbe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row14_col11, #T_ba4ae_row14_col12, #T_ba4ae_row14_col14, #T_ba4ae_row19_col11, #T_ba4ae_row19_col12, #T_ba4ae_row19_col14, #T_ba4ae_row21_col11, #T_ba4ae_row21_col12, #T_ba4ae_row21_col14, #T_ba4ae_row22_col11, #T_ba4ae_row22_col12, #T_ba4ae_row22_col14, #T_ba4ae_row23_col11, #T_ba4ae_row23_col12, #T_ba4ae_row23_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row15_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row15_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row15_col6, #T_ba4ae_row18_col4, #T_ba4ae_row23_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e7d7ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row15_col9, #T_ba4ae_row20_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row15_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row16_col4, #T_ba4ae_row22_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row16_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row16_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #df634e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row16_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row16_col10, #T_ba4ae_row19_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row16_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row16_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row16_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row17_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row17_col7, #T_ba4ae_row22_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row17_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row17_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row18_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row18_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row18_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row18_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #c83836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row18_col10, #T_ba4ae_row20_col4, #T_ba4ae_row20_col5, #T_ba4ae_row20_col11, #T_ba4ae_row20_col12, #T_ba4ae_row20_col14, #T_ba4ae_row22_col9, #T_ba4ae_row23_col2, #T_ba4ae_row23_col3, #T_ba4ae_row23_col6, #T_ba4ae_row23_col7, #T_ba4ae_row23_col8, #T_ba4ae_row23_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row18_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #c1d4f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row18_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #b3cdfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row19_col3, #T_ba4ae_row22_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row19_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row19_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row20_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #6384eb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row20_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row21_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row21_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row21_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row21_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row22_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row22_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #a2c1ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row22_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ba4ae_row23_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ba4ae_row23_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ba4ae\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ba4ae_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_ba4ae_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_ba4ae_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_ba4ae_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_ba4ae_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_ba4ae_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_ba4ae_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_ba4ae_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_ba4ae_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_ba4ae_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_ba4ae_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_ba4ae_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR*</th>\n",
       "      <th id=\"T_ba4ae_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_ba4ae_level0_col13\" class=\"col_heading level0 col13\" >Average</th>\n",
       "      <th id=\"T_ba4ae_level0_col14\" class=\"col_heading level0 col14\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ba4ae_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat15_score=grad:loraB:l2n_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row0_col1\" class=\"data row0 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row0_col2\" class=\"data row0 col2\" >60.3</td>\n",
       "      <td id=\"T_ba4ae_row0_col3\" class=\"data row0 col3\" >62.1</td>\n",
       "      <td id=\"T_ba4ae_row0_col4\" class=\"data row0 col4\" >10.4</td>\n",
       "      <td id=\"T_ba4ae_row0_col5\" class=\"data row0 col5\" >47.6</td>\n",
       "      <td id=\"T_ba4ae_row0_col6\" class=\"data row0 col6\" >42.1</td>\n",
       "      <td id=\"T_ba4ae_row0_col7\" class=\"data row0 col7\" >46.8</td>\n",
       "      <td id=\"T_ba4ae_row0_col8\" class=\"data row0 col8\" >14.1</td>\n",
       "      <td id=\"T_ba4ae_row0_col9\" class=\"data row0 col9\" >56.7</td>\n",
       "      <td id=\"T_ba4ae_row0_col10\" class=\"data row0 col10\" >40.9</td>\n",
       "      <td id=\"T_ba4ae_row0_col11\" class=\"data row0 col11\" >59.9</td>\n",
       "      <td id=\"T_ba4ae_row0_col12\" class=\"data row0 col12\" >266.0</td>\n",
       "      <td id=\"T_ba4ae_row0_col13\" class=\"data row0 col13\" >44.1</td>\n",
       "      <td id=\"T_ba4ae_row0_col14\" class=\"data row0 col14\" >-4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ba4ae_row1_col0\" class=\"data row1 col0\" >mistral-7b_ultrachat15_score=dppmapbd:nc=200:k=vmf:gamma=0.3:kmd=mpnet_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row1_col1\" class=\"data row1 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row1_col2\" class=\"data row1 col2\" >59.1</td>\n",
       "      <td id=\"T_ba4ae_row1_col3\" class=\"data row1 col3\" >62.2</td>\n",
       "      <td id=\"T_ba4ae_row1_col4\" class=\"data row1 col4\" >11.4</td>\n",
       "      <td id=\"T_ba4ae_row1_col5\" class=\"data row1 col5\" >46.6</td>\n",
       "      <td id=\"T_ba4ae_row1_col6\" class=\"data row1 col6\" >42.6</td>\n",
       "      <td id=\"T_ba4ae_row1_col7\" class=\"data row1 col7\" >48.4</td>\n",
       "      <td id=\"T_ba4ae_row1_col8\" class=\"data row1 col8\" >14.7</td>\n",
       "      <td id=\"T_ba4ae_row1_col9\" class=\"data row1 col9\" >52.8</td>\n",
       "      <td id=\"T_ba4ae_row1_col10\" class=\"data row1 col10\" >37.8</td>\n",
       "      <td id=\"T_ba4ae_row1_col11\" class=\"data row1 col11\" >59.3</td>\n",
       "      <td id=\"T_ba4ae_row1_col12\" class=\"data row1 col12\" >238.8</td>\n",
       "      <td id=\"T_ba4ae_row1_col13\" class=\"data row1 col13\" >43.5</td>\n",
       "      <td id=\"T_ba4ae_row1_col14\" class=\"data row1 col14\" >-5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ba4ae_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat15_score=random:s=0_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row2_col1\" class=\"data row2 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row2_col2\" class=\"data row2 col2\" >60.2</td>\n",
       "      <td id=\"T_ba4ae_row2_col3\" class=\"data row2 col3\" >62.0</td>\n",
       "      <td id=\"T_ba4ae_row2_col4\" class=\"data row2 col4\" >12.2</td>\n",
       "      <td id=\"T_ba4ae_row2_col5\" class=\"data row2 col5\" >47.2</td>\n",
       "      <td id=\"T_ba4ae_row2_col6\" class=\"data row2 col6\" >43.7</td>\n",
       "      <td id=\"T_ba4ae_row2_col7\" class=\"data row2 col7\" >46.6</td>\n",
       "      <td id=\"T_ba4ae_row2_col8\" class=\"data row2 col8\" >13.6</td>\n",
       "      <td id=\"T_ba4ae_row2_col9\" class=\"data row2 col9\" >52.9</td>\n",
       "      <td id=\"T_ba4ae_row2_col10\" class=\"data row2 col10\" >36.0</td>\n",
       "      <td id=\"T_ba4ae_row2_col11\" class=\"data row2 col11\" >59.5</td>\n",
       "      <td id=\"T_ba4ae_row2_col12\" class=\"data row2 col12\" >219.4</td>\n",
       "      <td id=\"T_ba4ae_row2_col13\" class=\"data row2 col13\" >43.4</td>\n",
       "      <td id=\"T_ba4ae_row2_col14\" class=\"data row2 col14\" >-6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ba4ae_row3_col0\" class=\"data row3 col0\" >mistral-7b_ultrachat15_score=dppmapbd:nc=200:k=vmf:gamma=3.0:kmd=mpnet_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row3_col1\" class=\"data row3 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row3_col2\" class=\"data row3 col2\" >59.0</td>\n",
       "      <td id=\"T_ba4ae_row3_col3\" class=\"data row3 col3\" >61.7</td>\n",
       "      <td id=\"T_ba4ae_row3_col4\" class=\"data row3 col4\" >10.4</td>\n",
       "      <td id=\"T_ba4ae_row3_col5\" class=\"data row3 col5\" >47.6</td>\n",
       "      <td id=\"T_ba4ae_row3_col6\" class=\"data row3 col6\" >42.6</td>\n",
       "      <td id=\"T_ba4ae_row3_col7\" class=\"data row3 col7\" >49.5</td>\n",
       "      <td id=\"T_ba4ae_row3_col8\" class=\"data row3 col8\" >13.5</td>\n",
       "      <td id=\"T_ba4ae_row3_col9\" class=\"data row3 col9\" >48.6</td>\n",
       "      <td id=\"T_ba4ae_row3_col10\" class=\"data row3 col10\" >36.6</td>\n",
       "      <td id=\"T_ba4ae_row3_col11\" class=\"data row3 col11\" >58.0</td>\n",
       "      <td id=\"T_ba4ae_row3_col12\" class=\"data row3 col12\" >242.2</td>\n",
       "      <td id=\"T_ba4ae_row3_col13\" class=\"data row3 col13\" >42.7</td>\n",
       "      <td id=\"T_ba4ae_row3_col14\" class=\"data row3 col14\" >-8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ba4ae_row4_col0\" class=\"data row4 col0\" >score=random:s=(\\d)_pace=prune:size=100000_avg (N=3)</td>\n",
       "      <td id=\"T_ba4ae_row4_col1\" class=\"data row4 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row4_col2\" class=\"data row4 col2\" >59.5</td>\n",
       "      <td id=\"T_ba4ae_row4_col3\" class=\"data row4 col3\" >61.6</td>\n",
       "      <td id=\"T_ba4ae_row4_col4\" class=\"data row4 col4\" >10.9</td>\n",
       "      <td id=\"T_ba4ae_row4_col5\" class=\"data row4 col5\" >46.3</td>\n",
       "      <td id=\"T_ba4ae_row4_col6\" class=\"data row4 col6\" >41.7</td>\n",
       "      <td id=\"T_ba4ae_row4_col7\" class=\"data row4 col7\" >46.9</td>\n",
       "      <td id=\"T_ba4ae_row4_col8\" class=\"data row4 col8\" >14.1</td>\n",
       "      <td id=\"T_ba4ae_row4_col9\" class=\"data row4 col9\" >52.1</td>\n",
       "      <td id=\"T_ba4ae_row4_col10\" class=\"data row4 col10\" >35.0</td>\n",
       "      <td id=\"T_ba4ae_row4_col11\" class=\"data row4 col11\" >59.0</td>\n",
       "      <td id=\"T_ba4ae_row4_col12\" class=\"data row4 col12\" >236.0</td>\n",
       "      <td id=\"T_ba4ae_row4_col13\" class=\"data row4 col13\" >42.7</td>\n",
       "      <td id=\"T_ba4ae_row4_col14\" class=\"data row4 col14\" >-7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_ba4ae_row5_col0\" class=\"data row5 col0\" >mistral-7b_ultrachat15_score=random:s=2_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row5_col1\" class=\"data row5 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row5_col2\" class=\"data row5 col2\" >59.2</td>\n",
       "      <td id=\"T_ba4ae_row5_col3\" class=\"data row5 col3\" >61.7</td>\n",
       "      <td id=\"T_ba4ae_row5_col4\" class=\"data row5 col4\" >11.0</td>\n",
       "      <td id=\"T_ba4ae_row5_col5\" class=\"data row5 col5\" >48.2</td>\n",
       "      <td id=\"T_ba4ae_row5_col6\" class=\"data row5 col6\" >41.3</td>\n",
       "      <td id=\"T_ba4ae_row5_col7\" class=\"data row5 col7\" >47.0</td>\n",
       "      <td id=\"T_ba4ae_row5_col8\" class=\"data row5 col8\" >15.1</td>\n",
       "      <td id=\"T_ba4ae_row5_col9\" class=\"data row5 col9\" >51.8</td>\n",
       "      <td id=\"T_ba4ae_row5_col10\" class=\"data row5 col10\" >33.5</td>\n",
       "      <td id=\"T_ba4ae_row5_col11\" class=\"data row5 col11\" >57.8</td>\n",
       "      <td id=\"T_ba4ae_row5_col12\" class=\"data row5 col12\" >243.3</td>\n",
       "      <td id=\"T_ba4ae_row5_col13\" class=\"data row5 col13\" >42.7</td>\n",
       "      <td id=\"T_ba4ae_row5_col14\" class=\"data row5 col14\" >-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_ba4ae_row6_col0\" class=\"data row6 col0\" >mistral-7b_ultrachat15_score=dppmapbd:nc=200:k=vmf:gamma=0.000035:kmd=mpnet_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row6_col1\" class=\"data row6 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row6_col2\" class=\"data row6 col2\" >58.6</td>\n",
       "      <td id=\"T_ba4ae_row6_col3\" class=\"data row6 col3\" >60.6</td>\n",
       "      <td id=\"T_ba4ae_row6_col4\" class=\"data row6 col4\" >9.8</td>\n",
       "      <td id=\"T_ba4ae_row6_col5\" class=\"data row6 col5\" >46.4</td>\n",
       "      <td id=\"T_ba4ae_row6_col6\" class=\"data row6 col6\" >40.7</td>\n",
       "      <td id=\"T_ba4ae_row6_col7\" class=\"data row6 col7\" >45.5</td>\n",
       "      <td id=\"T_ba4ae_row6_col8\" class=\"data row6 col8\" >13.1</td>\n",
       "      <td id=\"T_ba4ae_row6_col9\" class=\"data row6 col9\" >51.3</td>\n",
       "      <td id=\"T_ba4ae_row6_col10\" class=\"data row6 col10\" >40.2</td>\n",
       "      <td id=\"T_ba4ae_row6_col11\" class=\"data row6 col11\" >58.6</td>\n",
       "      <td id=\"T_ba4ae_row6_col12\" class=\"data row6 col12\" >232.5</td>\n",
       "      <td id=\"T_ba4ae_row6_col13\" class=\"data row6 col13\" >42.5</td>\n",
       "      <td id=\"T_ba4ae_row6_col14\" class=\"data row6 col14\" >-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_ba4ae_row7_col0\" class=\"data row7 col0\" >mistral-7b_ultrachat15_score=log:prob:neg_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row7_col1\" class=\"data row7 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row7_col2\" class=\"data row7 col2\" >58.1</td>\n",
       "      <td id=\"T_ba4ae_row7_col3\" class=\"data row7 col3\" >60.2</td>\n",
       "      <td id=\"T_ba4ae_row7_col4\" class=\"data row7 col4\" >10.4</td>\n",
       "      <td id=\"T_ba4ae_row7_col5\" class=\"data row7 col5\" >43.8</td>\n",
       "      <td id=\"T_ba4ae_row7_col6\" class=\"data row7 col6\" >42.7</td>\n",
       "      <td id=\"T_ba4ae_row7_col7\" class=\"data row7 col7\" >45.9</td>\n",
       "      <td id=\"T_ba4ae_row7_col8\" class=\"data row7 col8\" >14.3</td>\n",
       "      <td id=\"T_ba4ae_row7_col9\" class=\"data row7 col9\" >50.5</td>\n",
       "      <td id=\"T_ba4ae_row7_col10\" class=\"data row7 col10\" >36.0</td>\n",
       "      <td id=\"T_ba4ae_row7_col11\" class=\"data row7 col11\" >59.2</td>\n",
       "      <td id=\"T_ba4ae_row7_col12\" class=\"data row7 col12\" >242.7</td>\n",
       "      <td id=\"T_ba4ae_row7_col13\" class=\"data row7 col13\" >42.1</td>\n",
       "      <td id=\"T_ba4ae_row7_col14\" class=\"data row7 col14\" >-10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_ba4ae_row8_col0\" class=\"data row8 col0\" >mistral-7b_ultrachat15_score=random:s=1_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row8_col1\" class=\"data row8 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row8_col2\" class=\"data row8 col2\" >59.1</td>\n",
       "      <td id=\"T_ba4ae_row8_col3\" class=\"data row8 col3\" >61.2</td>\n",
       "      <td id=\"T_ba4ae_row8_col4\" class=\"data row8 col4\" >9.6</td>\n",
       "      <td id=\"T_ba4ae_row8_col5\" class=\"data row8 col5\" >43.6</td>\n",
       "      <td id=\"T_ba4ae_row8_col6\" class=\"data row8 col6\" >40.2</td>\n",
       "      <td id=\"T_ba4ae_row8_col7\" class=\"data row8 col7\" >47.0</td>\n",
       "      <td id=\"T_ba4ae_row8_col8\" class=\"data row8 col8\" >13.4</td>\n",
       "      <td id=\"T_ba4ae_row8_col9\" class=\"data row8 col9\" >51.7</td>\n",
       "      <td id=\"T_ba4ae_row8_col10\" class=\"data row8 col10\" >35.4</td>\n",
       "      <td id=\"T_ba4ae_row8_col11\" class=\"data row8 col11\" >59.6</td>\n",
       "      <td id=\"T_ba4ae_row8_col12\" class=\"data row8 col12\" >245.3</td>\n",
       "      <td id=\"T_ba4ae_row8_col13\" class=\"data row8 col13\" >42.1</td>\n",
       "      <td id=\"T_ba4ae_row8_col14\" class=\"data row8 col14\" >-10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_ba4ae_row9_col0\" class=\"data row9 col0\" >mistral-7b_ultrachat15_score=dppmapbd:nc=200:k=vmf:gamma=1.0:kmd=mpnet_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row9_col1\" class=\"data row9 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row9_col2\" class=\"data row9 col2\" >58.5</td>\n",
       "      <td id=\"T_ba4ae_row9_col3\" class=\"data row9 col3\" >60.3</td>\n",
       "      <td id=\"T_ba4ae_row9_col4\" class=\"data row9 col4\" >11.4</td>\n",
       "      <td id=\"T_ba4ae_row9_col5\" class=\"data row9 col5\" >45.6</td>\n",
       "      <td id=\"T_ba4ae_row9_col6\" class=\"data row9 col6\" >40.6</td>\n",
       "      <td id=\"T_ba4ae_row9_col7\" class=\"data row9 col7\" >46.9</td>\n",
       "      <td id=\"T_ba4ae_row9_col8\" class=\"data row9 col8\" >12.8</td>\n",
       "      <td id=\"T_ba4ae_row9_col9\" class=\"data row9 col9\" >47.8</td>\n",
       "      <td id=\"T_ba4ae_row9_col10\" class=\"data row9 col10\" >37.8</td>\n",
       "      <td id=\"T_ba4ae_row9_col11\" class=\"data row9 col11\" >58.3</td>\n",
       "      <td id=\"T_ba4ae_row9_col12\" class=\"data row9 col12\" >255.3</td>\n",
       "      <td id=\"T_ba4ae_row9_col13\" class=\"data row9 col13\" >42.0</td>\n",
       "      <td id=\"T_ba4ae_row9_col14\" class=\"data row9 col14\" >-10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_ba4ae_row10_col0\" class=\"data row10 col0\" >mistral-7b_ultrachat15_score=semdedup:cl=kmeansfaisscd:md=mpnet:dist=cd:emb=text+embedding:nc=200_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row10_col1\" class=\"data row10 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row10_col2\" class=\"data row10 col2\" >58.3</td>\n",
       "      <td id=\"T_ba4ae_row10_col3\" class=\"data row10 col3\" >60.7</td>\n",
       "      <td id=\"T_ba4ae_row10_col4\" class=\"data row10 col4\" >10.6</td>\n",
       "      <td id=\"T_ba4ae_row10_col5\" class=\"data row10 col5\" >45.4</td>\n",
       "      <td id=\"T_ba4ae_row10_col6\" class=\"data row10 col6\" >39.7</td>\n",
       "      <td id=\"T_ba4ae_row10_col7\" class=\"data row10 col7\" >45.7</td>\n",
       "      <td id=\"T_ba4ae_row10_col8\" class=\"data row10 col8\" >14.9</td>\n",
       "      <td id=\"T_ba4ae_row10_col9\" class=\"data row10 col9\" >45.4</td>\n",
       "      <td id=\"T_ba4ae_row10_col10\" class=\"data row10 col10\" >39.0</td>\n",
       "      <td id=\"T_ba4ae_row10_col11\" class=\"data row10 col11\" >57.3</td>\n",
       "      <td id=\"T_ba4ae_row10_col12\" class=\"data row10 col12\" >241.1</td>\n",
       "      <td id=\"T_ba4ae_row10_col13\" class=\"data row10 col13\" >41.7</td>\n",
       "      <td id=\"T_ba4ae_row10_col14\" class=\"data row10 col14\" >-12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_ba4ae_row11_col0\" class=\"data row11 col0\" >mistral-7b_ultrachat15_score=el2n:agg=mean_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row11_col1\" class=\"data row11 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row11_col2\" class=\"data row11 col2\" >58.4</td>\n",
       "      <td id=\"T_ba4ae_row11_col3\" class=\"data row11 col3\" >60.9</td>\n",
       "      <td id=\"T_ba4ae_row11_col4\" class=\"data row11 col4\" >10.2</td>\n",
       "      <td id=\"T_ba4ae_row11_col5\" class=\"data row11 col5\" >43.2</td>\n",
       "      <td id=\"T_ba4ae_row11_col6\" class=\"data row11 col6\" >40.4</td>\n",
       "      <td id=\"T_ba4ae_row11_col7\" class=\"data row11 col7\" >46.6</td>\n",
       "      <td id=\"T_ba4ae_row11_col8\" class=\"data row11 col8\" >13.1</td>\n",
       "      <td id=\"T_ba4ae_row11_col9\" class=\"data row11 col9\" >45.0</td>\n",
       "      <td id=\"T_ba4ae_row11_col10\" class=\"data row11 col10\" >39.0</td>\n",
       "      <td id=\"T_ba4ae_row11_col11\" class=\"data row11 col11\" >57.3</td>\n",
       "      <td id=\"T_ba4ae_row11_col12\" class=\"data row11 col12\" >253.0</td>\n",
       "      <td id=\"T_ba4ae_row11_col13\" class=\"data row11 col13\" >41.4</td>\n",
       "      <td id=\"T_ba4ae_row11_col14\" class=\"data row11 col14\" >-13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_ba4ae_row12_col0\" class=\"data row12 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_ba4ae_row12_col1\" class=\"data row12 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row12_col2\" class=\"data row12 col2\" >57.8</td>\n",
       "      <td id=\"T_ba4ae_row12_col3\" class=\"data row12 col3\" >59.5</td>\n",
       "      <td id=\"T_ba4ae_row12_col4\" class=\"data row12 col4\" >9.8</td>\n",
       "      <td id=\"T_ba4ae_row12_col5\" class=\"data row12 col5\" >50.4</td>\n",
       "      <td id=\"T_ba4ae_row12_col6\" class=\"data row12 col6\" >40.8</td>\n",
       "      <td id=\"T_ba4ae_row12_col7\" class=\"data row12 col7\" >46.5</td>\n",
       "      <td id=\"T_ba4ae_row12_col8\" class=\"data row12 col8\" >14.9</td>\n",
       "      <td id=\"T_ba4ae_row12_col9\" class=\"data row12 col9\" >46.6</td>\n",
       "      <td id=\"T_ba4ae_row12_col10\" class=\"data row12 col10\" >28.7</td>\n",
       "      <td id=\"T_ba4ae_row12_col11\" class=\"data row12 col11\" >57.3</td>\n",
       "      <td id=\"T_ba4ae_row12_col12\" class=\"data row12 col12\" >219.3</td>\n",
       "      <td id=\"T_ba4ae_row12_col13\" class=\"data row12 col13\" >41.2</td>\n",
       "      <td id=\"T_ba4ae_row12_col14\" class=\"data row12 col14\" >-14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_ba4ae_row13_col0\" class=\"data row13 col0\" >mistral-7b_ultrachat15_score=dppmapbd:nc=200:k=lin:kmd=mpnet_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row13_col1\" class=\"data row13 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row13_col2\" class=\"data row13 col2\" >58.2</td>\n",
       "      <td id=\"T_ba4ae_row13_col3\" class=\"data row13 col3\" >61.0</td>\n",
       "      <td id=\"T_ba4ae_row13_col4\" class=\"data row13 col4\" >10.2</td>\n",
       "      <td id=\"T_ba4ae_row13_col5\" class=\"data row13 col5\" >44.2</td>\n",
       "      <td id=\"T_ba4ae_row13_col6\" class=\"data row13 col6\" >41.9</td>\n",
       "      <td id=\"T_ba4ae_row13_col7\" class=\"data row13 col7\" >46.9</td>\n",
       "      <td id=\"T_ba4ae_row13_col8\" class=\"data row13 col8\" >11.3</td>\n",
       "      <td id=\"T_ba4ae_row13_col9\" class=\"data row13 col9\" >43.9</td>\n",
       "      <td id=\"T_ba4ae_row13_col10\" class=\"data row13 col10\" >36.0</td>\n",
       "      <td id=\"T_ba4ae_row13_col11\" class=\"data row13 col11\" >57.0</td>\n",
       "      <td id=\"T_ba4ae_row13_col12\" class=\"data row13 col12\" >245.1</td>\n",
       "      <td id=\"T_ba4ae_row13_col13\" class=\"data row13 col13\" >41.1</td>\n",
       "      <td id=\"T_ba4ae_row13_col14\" class=\"data row13 col14\" >-13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_ba4ae_row14_col0\" class=\"data row14 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_ba4ae_row14_col1\" class=\"data row14 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row14_col2\" class=\"data row14 col2\" >60.2</td>\n",
       "      <td id=\"T_ba4ae_row14_col3\" class=\"data row14 col3\" >62.6</td>\n",
       "      <td id=\"T_ba4ae_row14_col4\" class=\"data row14 col4\" >11.6</td>\n",
       "      <td id=\"T_ba4ae_row14_col5\" class=\"data row14 col5\" >42.0</td>\n",
       "      <td id=\"T_ba4ae_row14_col6\" class=\"data row14 col6\" >42.6</td>\n",
       "      <td id=\"T_ba4ae_row14_col7\" class=\"data row14 col7\" >48.3</td>\n",
       "      <td id=\"T_ba4ae_row14_col8\" class=\"data row14 col8\" >14.4</td>\n",
       "      <td id=\"T_ba4ae_row14_col9\" class=\"data row14 col9\" >57.8</td>\n",
       "      <td id=\"T_ba4ae_row14_col10\" class=\"data row14 col10\" >29.3</td>\n",
       "      <td id=\"T_ba4ae_row14_col11\" class=\"data row14 col11\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row14_col12\" class=\"data row14 col12\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row14_col13\" class=\"data row14 col13\" >41.0</td>\n",
       "      <td id=\"T_ba4ae_row14_col14\" class=\"data row14 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_ba4ae_row15_col0\" class=\"data row15 col0\" >mistral-7b_ultrachat15_score=semdedup:cl=kmeansfaisscd:md=mistral7b:dist=cd:emb=text+embedding:nc=200_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row15_col1\" class=\"data row15 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row15_col2\" class=\"data row15 col2\" >57.8</td>\n",
       "      <td id=\"T_ba4ae_row15_col3\" class=\"data row15 col3\" >59.7</td>\n",
       "      <td id=\"T_ba4ae_row15_col4\" class=\"data row15 col4\" >8.8</td>\n",
       "      <td id=\"T_ba4ae_row15_col5\" class=\"data row15 col5\" >40.8</td>\n",
       "      <td id=\"T_ba4ae_row15_col6\" class=\"data row15 col6\" >41.1</td>\n",
       "      <td id=\"T_ba4ae_row15_col7\" class=\"data row15 col7\" >44.7</td>\n",
       "      <td id=\"T_ba4ae_row15_col8\" class=\"data row15 col8\" >14.3</td>\n",
       "      <td id=\"T_ba4ae_row15_col9\" class=\"data row15 col9\" >49.2</td>\n",
       "      <td id=\"T_ba4ae_row15_col10\" class=\"data row15 col10\" >34.1</td>\n",
       "      <td id=\"T_ba4ae_row15_col11\" class=\"data row15 col11\" >57.6</td>\n",
       "      <td id=\"T_ba4ae_row15_col12\" class=\"data row15 col12\" >256.9</td>\n",
       "      <td id=\"T_ba4ae_row15_col13\" class=\"data row15 col13\" >40.8</td>\n",
       "      <td id=\"T_ba4ae_row15_col14\" class=\"data row15 col14\" >-15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_ba4ae_row16_col0\" class=\"data row16 col0\" >mistral-7b_ultrachat200k_aftersplitlongconv_ep=2</td>\n",
       "      <td id=\"T_ba4ae_row16_col1\" class=\"data row16 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row16_col2\" class=\"data row16 col2\" >57.3</td>\n",
       "      <td id=\"T_ba4ae_row16_col3\" class=\"data row16 col3\" >59.5</td>\n",
       "      <td id=\"T_ba4ae_row16_col4\" class=\"data row16 col4\" >8.6</td>\n",
       "      <td id=\"T_ba4ae_row16_col5\" class=\"data row16 col5\" >47.0</td>\n",
       "      <td id=\"T_ba4ae_row16_col6\" class=\"data row16 col6\" >39.4</td>\n",
       "      <td id=\"T_ba4ae_row16_col7\" class=\"data row16 col7\" >45.6</td>\n",
       "      <td id=\"T_ba4ae_row16_col8\" class=\"data row16 col8\" >14.2</td>\n",
       "      <td id=\"T_ba4ae_row16_col9\" class=\"data row16 col9\" >48.4</td>\n",
       "      <td id=\"T_ba4ae_row16_col10\" class=\"data row16 col10\" >27.4</td>\n",
       "      <td id=\"T_ba4ae_row16_col11\" class=\"data row16 col11\" >58.2</td>\n",
       "      <td id=\"T_ba4ae_row16_col12\" class=\"data row16 col12\" >231.3</td>\n",
       "      <td id=\"T_ba4ae_row16_col13\" class=\"data row16 col13\" >40.6</td>\n",
       "      <td id=\"T_ba4ae_row16_col14\" class=\"data row16 col14\" >-17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_ba4ae_row17_col0\" class=\"data row17 col0\" >mistral-7b_ultrachat15_score=numtoks:input:neg_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row17_col1\" class=\"data row17 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row17_col2\" class=\"data row17 col2\" >58.5</td>\n",
       "      <td id=\"T_ba4ae_row17_col3\" class=\"data row17 col3\" >59.3</td>\n",
       "      <td id=\"T_ba4ae_row17_col4\" class=\"data row17 col4\" >10.2</td>\n",
       "      <td id=\"T_ba4ae_row17_col5\" class=\"data row17 col5\" >40.2</td>\n",
       "      <td id=\"T_ba4ae_row17_col6\" class=\"data row17 col6\" >39.7</td>\n",
       "      <td id=\"T_ba4ae_row17_col7\" class=\"data row17 col7\" >43.5</td>\n",
       "      <td id=\"T_ba4ae_row17_col8\" class=\"data row17 col8\" >13.9</td>\n",
       "      <td id=\"T_ba4ae_row17_col9\" class=\"data row17 col9\" >46.4</td>\n",
       "      <td id=\"T_ba4ae_row17_col10\" class=\"data row17 col10\" >35.4</td>\n",
       "      <td id=\"T_ba4ae_row17_col11\" class=\"data row17 col11\" >57.7</td>\n",
       "      <td id=\"T_ba4ae_row17_col12\" class=\"data row17 col12\" >311.0</td>\n",
       "      <td id=\"T_ba4ae_row17_col13\" class=\"data row17 col13\" >40.5</td>\n",
       "      <td id=\"T_ba4ae_row17_col14\" class=\"data row17 col14\" >-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_ba4ae_row18_col0\" class=\"data row18 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_ba4ae_row18_col1\" class=\"data row18 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row18_col2\" class=\"data row18 col2\" >57.6</td>\n",
       "      <td id=\"T_ba4ae_row18_col3\" class=\"data row18 col3\" >60.3</td>\n",
       "      <td id=\"T_ba4ae_row18_col4\" class=\"data row18 col4\" >10.0</td>\n",
       "      <td id=\"T_ba4ae_row18_col5\" class=\"data row18 col5\" >45.0</td>\n",
       "      <td id=\"T_ba4ae_row18_col6\" class=\"data row18 col6\" >38.2</td>\n",
       "      <td id=\"T_ba4ae_row18_col7\" class=\"data row18 col7\" >46.0</td>\n",
       "      <td id=\"T_ba4ae_row18_col8\" class=\"data row18 col8\" >14.8</td>\n",
       "      <td id=\"T_ba4ae_row18_col9\" class=\"data row18 col9\" >50.5</td>\n",
       "      <td id=\"T_ba4ae_row18_col10\" class=\"data row18 col10\" >18.9</td>\n",
       "      <td id=\"T_ba4ae_row18_col11\" class=\"data row18 col11\" >56.7</td>\n",
       "      <td id=\"T_ba4ae_row18_col12\" class=\"data row18 col12\" >294.1</td>\n",
       "      <td id=\"T_ba4ae_row18_col13\" class=\"data row18 col13\" >39.8</td>\n",
       "      <td id=\"T_ba4ae_row18_col14\" class=\"data row18 col14\" >-15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_ba4ae_row19_col0\" class=\"data row19 col0\" >mistral-7b_sft-alpha</td>\n",
       "      <td id=\"T_ba4ae_row19_col1\" class=\"data row19 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row19_col2\" class=\"data row19 col2\" >58.7</td>\n",
       "      <td id=\"T_ba4ae_row19_col3\" class=\"data row19 col3\" >60.8</td>\n",
       "      <td id=\"T_ba4ae_row19_col4\" class=\"data row19 col4\" >9.6</td>\n",
       "      <td id=\"T_ba4ae_row19_col5\" class=\"data row19 col5\" >46.2</td>\n",
       "      <td id=\"T_ba4ae_row19_col6\" class=\"data row19 col6\" >38.5</td>\n",
       "      <td id=\"T_ba4ae_row19_col7\" class=\"data row19 col7\" >45.6</td>\n",
       "      <td id=\"T_ba4ae_row19_col8\" class=\"data row19 col8\" >12.4</td>\n",
       "      <td id=\"T_ba4ae_row19_col9\" class=\"data row19 col9\" >54.3</td>\n",
       "      <td id=\"T_ba4ae_row19_col10\" class=\"data row19 col10\" >27.4</td>\n",
       "      <td id=\"T_ba4ae_row19_col11\" class=\"data row19 col11\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row19_col12\" class=\"data row19 col12\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row19_col13\" class=\"data row19 col13\" >39.3</td>\n",
       "      <td id=\"T_ba4ae_row19_col14\" class=\"data row19 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_ba4ae_row20_col0\" class=\"data row20 col0\" >mistral-7b_ultrachat15_score=semdedup:cl=kmeansfaisscd:md=mistral7b:dist=cd:emb=grad+rp+loraB:nc=200_pace=prune:size=100000:ep=2</td>\n",
       "      <td id=\"T_ba4ae_row20_col1\" class=\"data row20 col1\" >100000</td>\n",
       "      <td id=\"T_ba4ae_row20_col2\" class=\"data row20 col2\" >57.1</td>\n",
       "      <td id=\"T_ba4ae_row20_col3\" class=\"data row20 col3\" >59.3</td>\n",
       "      <td id=\"T_ba4ae_row20_col4\" class=\"data row20 col4\" >7.4</td>\n",
       "      <td id=\"T_ba4ae_row20_col5\" class=\"data row20 col5\" >34.8</td>\n",
       "      <td id=\"T_ba4ae_row20_col6\" class=\"data row20 col6\" >38.8</td>\n",
       "      <td id=\"T_ba4ae_row20_col7\" class=\"data row20 col7\" >46.0</td>\n",
       "      <td id=\"T_ba4ae_row20_col8\" class=\"data row20 col8\" >12.4</td>\n",
       "      <td id=\"T_ba4ae_row20_col9\" class=\"data row20 col9\" >43.6</td>\n",
       "      <td id=\"T_ba4ae_row20_col10\" class=\"data row20 col10\" >32.3</td>\n",
       "      <td id=\"T_ba4ae_row20_col11\" class=\"data row20 col11\" >54.5</td>\n",
       "      <td id=\"T_ba4ae_row20_col12\" class=\"data row20 col12\" >209.9</td>\n",
       "      <td id=\"T_ba4ae_row20_col13\" class=\"data row20 col13\" >38.6</td>\n",
       "      <td id=\"T_ba4ae_row20_col14\" class=\"data row20 col14\" >-21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_ba4ae_row21_col0\" class=\"data row21 col0\" >mistral-7b-sft-alpha+dpo</td>\n",
       "      <td id=\"T_ba4ae_row21_col1\" class=\"data row21 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row21_col2\" class=\"data row21 col2\" >59.0</td>\n",
       "      <td id=\"T_ba4ae_row21_col3\" class=\"data row21 col3\" >60.1</td>\n",
       "      <td id=\"T_ba4ae_row21_col4\" class=\"data row21 col4\" >7.8</td>\n",
       "      <td id=\"T_ba4ae_row21_col5\" class=\"data row21 col5\" >42.6</td>\n",
       "      <td id=\"T_ba4ae_row21_col6\" class=\"data row21 col6\" >40.8</td>\n",
       "      <td id=\"T_ba4ae_row21_col7\" class=\"data row21 col7\" >45.1</td>\n",
       "      <td id=\"T_ba4ae_row21_col8\" class=\"data row21 col8\" >12.2</td>\n",
       "      <td id=\"T_ba4ae_row21_col9\" class=\"data row21 col9\" >44.1</td>\n",
       "      <td id=\"T_ba4ae_row21_col10\" class=\"data row21 col10\" >32.9</td>\n",
       "      <td id=\"T_ba4ae_row21_col11\" class=\"data row21 col11\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row21_col12\" class=\"data row21 col12\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row21_col13\" class=\"data row21 col13\" >38.3</td>\n",
       "      <td id=\"T_ba4ae_row21_col14\" class=\"data row21 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_ba4ae_row22_col0\" class=\"data row22 col0\" >mistral-7b-sft-beta+dpo</td>\n",
       "      <td id=\"T_ba4ae_row22_col1\" class=\"data row22 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row22_col2\" class=\"data row22 col2\" >58.8</td>\n",
       "      <td id=\"T_ba4ae_row22_col3\" class=\"data row22 col3\" >59.8</td>\n",
       "      <td id=\"T_ba4ae_row22_col4\" class=\"data row22 col4\" >8.6</td>\n",
       "      <td id=\"T_ba4ae_row22_col5\" class=\"data row22 col5\" >39.4</td>\n",
       "      <td id=\"T_ba4ae_row22_col6\" class=\"data row22 col6\" >39.8</td>\n",
       "      <td id=\"T_ba4ae_row22_col7\" class=\"data row22 col7\" >45.7</td>\n",
       "      <td id=\"T_ba4ae_row22_col8\" class=\"data row22 col8\" >12.4</td>\n",
       "      <td id=\"T_ba4ae_row22_col9\" class=\"data row22 col9\" >43.2</td>\n",
       "      <td id=\"T_ba4ae_row22_col10\" class=\"data row22 col10\" >27.6</td>\n",
       "      <td id=\"T_ba4ae_row22_col11\" class=\"data row22 col11\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row22_col12\" class=\"data row22 col12\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row22_col13\" class=\"data row22 col13\" >37.3</td>\n",
       "      <td id=\"T_ba4ae_row22_col14\" class=\"data row22 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ba4ae_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_ba4ae_row23_col0\" class=\"data row23 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_ba4ae_row23_col1\" class=\"data row23 col1\" >None</td>\n",
       "      <td id=\"T_ba4ae_row23_col2\" class=\"data row23 col2\" >53.2</td>\n",
       "      <td id=\"T_ba4ae_row23_col3\" class=\"data row23 col3\" >53.7</td>\n",
       "      <td id=\"T_ba4ae_row23_col4\" class=\"data row23 col4\" >10.0</td>\n",
       "      <td id=\"T_ba4ae_row23_col5\" class=\"data row23 col5\" >36.4</td>\n",
       "      <td id=\"T_ba4ae_row23_col6\" class=\"data row23 col6\" >38.1</td>\n",
       "      <td id=\"T_ba4ae_row23_col7\" class=\"data row23 col7\" >39.5</td>\n",
       "      <td id=\"T_ba4ae_row23_col8\" class=\"data row23 col8\" >8.3</td>\n",
       "      <td id=\"T_ba4ae_row23_col9\" class=\"data row23 col9\" >43.5</td>\n",
       "      <td id=\"T_ba4ae_row23_col10\" class=\"data row23 col10\" >30.7</td>\n",
       "      <td id=\"T_ba4ae_row23_col11\" class=\"data row23 col11\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row23_col12\" class=\"data row23 col12\" >nan</td>\n",
       "      <td id=\"T_ba4ae_row23_col13\" class=\"data row23 col13\" >34.8</td>\n",
       "      <td id=\"T_ba4ae_row23_col14\" class=\"data row23 col14\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffc7605f820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/pd.py:123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  col_contains_substr = df[col].str.contains(substr, regex=True)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/pd.py:123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  col_contains_substr = df[col].str.contains(substr, regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1d0bc_row0_col0, #T_1d0bc_row0_col1, #T_1d0bc_row1_col0, #T_1d0bc_row1_col1, #T_1d0bc_row2_col0, #T_1d0bc_row2_col1, #T_1d0bc_row3_col0, #T_1d0bc_row3_col1, #T_1d0bc_row4_col0, #T_1d0bc_row4_col1, #T_1d0bc_row5_col0, #T_1d0bc_row5_col1, #T_1d0bc_row6_col0, #T_1d0bc_row6_col1, #T_1d0bc_row7_col0, #T_1d0bc_row7_col1, #T_1d0bc_row8_col0, #T_1d0bc_row8_col1, #T_1d0bc_row9_col0, #T_1d0bc_row9_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1d0bc_row0_col2, #T_1d0bc_row0_col6, #T_1d0bc_row1_col2, #T_1d0bc_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row0_col3, #T_1d0bc_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row0_col4, #T_1d0bc_row0_col7, #T_1d0bc_row0_col10, #T_1d0bc_row0_col11, #T_1d0bc_row0_col13, #T_1d0bc_row0_col14, #T_1d0bc_row1_col4, #T_1d0bc_row1_col7, #T_1d0bc_row1_col10, #T_1d0bc_row1_col11, #T_1d0bc_row1_col13, #T_1d0bc_row1_col14, #T_1d0bc_row2_col5, #T_1d0bc_row2_col8, #T_1d0bc_row3_col2, #T_1d0bc_row3_col3, #T_1d0bc_row3_col6, #T_1d0bc_row3_col9, #T_1d0bc_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row0_col5, #T_1d0bc_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e8765c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row0_col8, #T_1d0bc_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row0_col9, #T_1d0bc_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row0_col12, #T_1d0bc_row1_col12, #T_1d0bc_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col2, #T_1d0bc_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col6, #T_1d0bc_row7_col6, #T_1d0bc_row7_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col7, #T_1d0bc_row6_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row2_col10, #T_1d0bc_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row2_col12, #T_1d0bc_row4_col14, #T_1d0bc_row5_col10, #T_1d0bc_row5_col11, #T_1d0bc_row7_col4, #T_1d0bc_row8_col9, #T_1d0bc_row9_col2, #T_1d0bc_row9_col3, #T_1d0bc_row9_col5, #T_1d0bc_row9_col6, #T_1d0bc_row9_col7, #T_1d0bc_row9_col8, #T_1d0bc_row9_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row3_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row3_col7, #T_1d0bc_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row3_col11, #T_1d0bc_row3_col12, #T_1d0bc_row3_col14, #T_1d0bc_row6_col11, #T_1d0bc_row6_col12, #T_1d0bc_row6_col14, #T_1d0bc_row7_col11, #T_1d0bc_row7_col12, #T_1d0bc_row7_col14, #T_1d0bc_row8_col11, #T_1d0bc_row8_col12, #T_1d0bc_row8_col14, #T_1d0bc_row9_col11, #T_1d0bc_row9_col12, #T_1d0bc_row9_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row3_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col3, #T_1d0bc_row4_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col4, #T_1d0bc_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row4_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col10, #T_1d0bc_row6_col10, #T_1d0bc_row6_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row4_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b093;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row5_col4, #T_1d0bc_row5_col9, #T_1d0bc_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row5_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row5_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #6b8df0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row6_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row6_col8, #T_1d0bc_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row7_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row7_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #da5a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row7_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row8_col3, #T_1d0bc_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row8_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row8_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1d0bc_row9_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1d0bc_row9_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1d0bc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1d0bc_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_1d0bc_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_1d0bc_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_1d0bc_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_1d0bc_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_1d0bc_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_1d0bc_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_1d0bc_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_1d0bc_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_1d0bc_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_1d0bc_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_1d0bc_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR*</th>\n",
       "      <th id=\"T_1d0bc_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_1d0bc_level0_col13\" class=\"col_heading level0 col13\" >Average</th>\n",
       "      <th id=\"T_1d0bc_level0_col14\" class=\"col_heading level0 col14\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1d0bc_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat15_score=random:s=0_pace=prune:size=200000:ep=2</td>\n",
       "      <td id=\"T_1d0bc_row0_col1\" class=\"data row0 col1\" >200000</td>\n",
       "      <td id=\"T_1d0bc_row0_col2\" class=\"data row0 col2\" >59.8</td>\n",
       "      <td id=\"T_1d0bc_row0_col3\" class=\"data row0 col3\" >61.9</td>\n",
       "      <td id=\"T_1d0bc_row0_col4\" class=\"data row0 col4\" >12.2</td>\n",
       "      <td id=\"T_1d0bc_row0_col5\" class=\"data row0 col5\" >48.0</td>\n",
       "      <td id=\"T_1d0bc_row0_col6\" class=\"data row0 col6\" >42.3</td>\n",
       "      <td id=\"T_1d0bc_row0_col7\" class=\"data row0 col7\" >48.6</td>\n",
       "      <td id=\"T_1d0bc_row0_col8\" class=\"data row0 col8\" >14.2</td>\n",
       "      <td id=\"T_1d0bc_row0_col9\" class=\"data row0 col9\" >50.7</td>\n",
       "      <td id=\"T_1d0bc_row0_col10\" class=\"data row0 col10\" >34.8</td>\n",
       "      <td id=\"T_1d0bc_row0_col11\" class=\"data row0 col11\" >59.0</td>\n",
       "      <td id=\"T_1d0bc_row0_col12\" class=\"data row0 col12\" >249.8</td>\n",
       "      <td id=\"T_1d0bc_row0_col13\" class=\"data row0 col13\" >43.1</td>\n",
       "      <td id=\"T_1d0bc_row0_col14\" class=\"data row0 col14\" >-5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1d0bc_row1_col0\" class=\"data row1 col0\" >score=random:s=(\\d)_pace=prune:size=200000_avg (N=1)</td>\n",
       "      <td id=\"T_1d0bc_row1_col1\" class=\"data row1 col1\" >200000</td>\n",
       "      <td id=\"T_1d0bc_row1_col2\" class=\"data row1 col2\" >59.8</td>\n",
       "      <td id=\"T_1d0bc_row1_col3\" class=\"data row1 col3\" >61.9</td>\n",
       "      <td id=\"T_1d0bc_row1_col4\" class=\"data row1 col4\" >12.2</td>\n",
       "      <td id=\"T_1d0bc_row1_col5\" class=\"data row1 col5\" >48.0</td>\n",
       "      <td id=\"T_1d0bc_row1_col6\" class=\"data row1 col6\" >42.3</td>\n",
       "      <td id=\"T_1d0bc_row1_col7\" class=\"data row1 col7\" >48.6</td>\n",
       "      <td id=\"T_1d0bc_row1_col8\" class=\"data row1 col8\" >14.2</td>\n",
       "      <td id=\"T_1d0bc_row1_col9\" class=\"data row1 col9\" >50.7</td>\n",
       "      <td id=\"T_1d0bc_row1_col10\" class=\"data row1 col10\" >34.8</td>\n",
       "      <td id=\"T_1d0bc_row1_col11\" class=\"data row1 col11\" >59.0</td>\n",
       "      <td id=\"T_1d0bc_row1_col12\" class=\"data row1 col12\" >249.8</td>\n",
       "      <td id=\"T_1d0bc_row1_col13\" class=\"data row1 col13\" >43.1</td>\n",
       "      <td id=\"T_1d0bc_row1_col14\" class=\"data row1 col14\" >-5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1d0bc_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_1d0bc_row2_col1\" class=\"data row2 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row2_col2\" class=\"data row2 col2\" >57.8</td>\n",
       "      <td id=\"T_1d0bc_row2_col3\" class=\"data row2 col3\" >59.5</td>\n",
       "      <td id=\"T_1d0bc_row2_col4\" class=\"data row2 col4\" >9.8</td>\n",
       "      <td id=\"T_1d0bc_row2_col5\" class=\"data row2 col5\" >50.4</td>\n",
       "      <td id=\"T_1d0bc_row2_col6\" class=\"data row2 col6\" >40.8</td>\n",
       "      <td id=\"T_1d0bc_row2_col7\" class=\"data row2 col7\" >46.5</td>\n",
       "      <td id=\"T_1d0bc_row2_col8\" class=\"data row2 col8\" >14.9</td>\n",
       "      <td id=\"T_1d0bc_row2_col9\" class=\"data row2 col9\" >46.6</td>\n",
       "      <td id=\"T_1d0bc_row2_col10\" class=\"data row2 col10\" >28.7</td>\n",
       "      <td id=\"T_1d0bc_row2_col11\" class=\"data row2 col11\" >57.3</td>\n",
       "      <td id=\"T_1d0bc_row2_col12\" class=\"data row2 col12\" >219.3</td>\n",
       "      <td id=\"T_1d0bc_row2_col13\" class=\"data row2 col13\" >41.2</td>\n",
       "      <td id=\"T_1d0bc_row2_col14\" class=\"data row2 col14\" >-14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1d0bc_row3_col0\" class=\"data row3 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_1d0bc_row3_col1\" class=\"data row3 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row3_col2\" class=\"data row3 col2\" >60.2</td>\n",
       "      <td id=\"T_1d0bc_row3_col3\" class=\"data row3 col3\" >62.6</td>\n",
       "      <td id=\"T_1d0bc_row3_col4\" class=\"data row3 col4\" >11.6</td>\n",
       "      <td id=\"T_1d0bc_row3_col5\" class=\"data row3 col5\" >42.0</td>\n",
       "      <td id=\"T_1d0bc_row3_col6\" class=\"data row3 col6\" >42.6</td>\n",
       "      <td id=\"T_1d0bc_row3_col7\" class=\"data row3 col7\" >48.3</td>\n",
       "      <td id=\"T_1d0bc_row3_col8\" class=\"data row3 col8\" >14.4</td>\n",
       "      <td id=\"T_1d0bc_row3_col9\" class=\"data row3 col9\" >57.8</td>\n",
       "      <td id=\"T_1d0bc_row3_col10\" class=\"data row3 col10\" >29.3</td>\n",
       "      <td id=\"T_1d0bc_row3_col11\" class=\"data row3 col11\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row3_col12\" class=\"data row3 col12\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row3_col13\" class=\"data row3 col13\" >41.0</td>\n",
       "      <td id=\"T_1d0bc_row3_col14\" class=\"data row3 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1d0bc_row4_col0\" class=\"data row4 col0\" >mistral-7b_ultrachat200k_aftersplitlongconv_ep=2</td>\n",
       "      <td id=\"T_1d0bc_row4_col1\" class=\"data row4 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row4_col2\" class=\"data row4 col2\" >57.3</td>\n",
       "      <td id=\"T_1d0bc_row4_col3\" class=\"data row4 col3\" >59.5</td>\n",
       "      <td id=\"T_1d0bc_row4_col4\" class=\"data row4 col4\" >8.6</td>\n",
       "      <td id=\"T_1d0bc_row4_col5\" class=\"data row4 col5\" >47.0</td>\n",
       "      <td id=\"T_1d0bc_row4_col6\" class=\"data row4 col6\" >39.4</td>\n",
       "      <td id=\"T_1d0bc_row4_col7\" class=\"data row4 col7\" >45.6</td>\n",
       "      <td id=\"T_1d0bc_row4_col8\" class=\"data row4 col8\" >14.2</td>\n",
       "      <td id=\"T_1d0bc_row4_col9\" class=\"data row4 col9\" >48.4</td>\n",
       "      <td id=\"T_1d0bc_row4_col10\" class=\"data row4 col10\" >27.4</td>\n",
       "      <td id=\"T_1d0bc_row4_col11\" class=\"data row4 col11\" >58.2</td>\n",
       "      <td id=\"T_1d0bc_row4_col12\" class=\"data row4 col12\" >231.3</td>\n",
       "      <td id=\"T_1d0bc_row4_col13\" class=\"data row4 col13\" >40.6</td>\n",
       "      <td id=\"T_1d0bc_row4_col14\" class=\"data row4 col14\" >-17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1d0bc_row5_col0\" class=\"data row5 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_1d0bc_row5_col1\" class=\"data row5 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row5_col2\" class=\"data row5 col2\" >57.6</td>\n",
       "      <td id=\"T_1d0bc_row5_col3\" class=\"data row5 col3\" >60.3</td>\n",
       "      <td id=\"T_1d0bc_row5_col4\" class=\"data row5 col4\" >10.0</td>\n",
       "      <td id=\"T_1d0bc_row5_col5\" class=\"data row5 col5\" >45.0</td>\n",
       "      <td id=\"T_1d0bc_row5_col6\" class=\"data row5 col6\" >38.2</td>\n",
       "      <td id=\"T_1d0bc_row5_col7\" class=\"data row5 col7\" >46.0</td>\n",
       "      <td id=\"T_1d0bc_row5_col8\" class=\"data row5 col8\" >14.8</td>\n",
       "      <td id=\"T_1d0bc_row5_col9\" class=\"data row5 col9\" >50.5</td>\n",
       "      <td id=\"T_1d0bc_row5_col10\" class=\"data row5 col10\" >18.9</td>\n",
       "      <td id=\"T_1d0bc_row5_col11\" class=\"data row5 col11\" >56.7</td>\n",
       "      <td id=\"T_1d0bc_row5_col12\" class=\"data row5 col12\" >294.1</td>\n",
       "      <td id=\"T_1d0bc_row5_col13\" class=\"data row5 col13\" >39.8</td>\n",
       "      <td id=\"T_1d0bc_row5_col14\" class=\"data row5 col14\" >-15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1d0bc_row6_col0\" class=\"data row6 col0\" >mistral-7b_sft-alpha</td>\n",
       "      <td id=\"T_1d0bc_row6_col1\" class=\"data row6 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row6_col2\" class=\"data row6 col2\" >58.7</td>\n",
       "      <td id=\"T_1d0bc_row6_col3\" class=\"data row6 col3\" >60.8</td>\n",
       "      <td id=\"T_1d0bc_row6_col4\" class=\"data row6 col4\" >9.6</td>\n",
       "      <td id=\"T_1d0bc_row6_col5\" class=\"data row6 col5\" >46.2</td>\n",
       "      <td id=\"T_1d0bc_row6_col6\" class=\"data row6 col6\" >38.5</td>\n",
       "      <td id=\"T_1d0bc_row6_col7\" class=\"data row6 col7\" >45.6</td>\n",
       "      <td id=\"T_1d0bc_row6_col8\" class=\"data row6 col8\" >12.4</td>\n",
       "      <td id=\"T_1d0bc_row6_col9\" class=\"data row6 col9\" >54.3</td>\n",
       "      <td id=\"T_1d0bc_row6_col10\" class=\"data row6 col10\" >27.4</td>\n",
       "      <td id=\"T_1d0bc_row6_col11\" class=\"data row6 col11\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row6_col12\" class=\"data row6 col12\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row6_col13\" class=\"data row6 col13\" >39.3</td>\n",
       "      <td id=\"T_1d0bc_row6_col14\" class=\"data row6 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1d0bc_row7_col0\" class=\"data row7 col0\" >mistral-7b-sft-alpha+dpo</td>\n",
       "      <td id=\"T_1d0bc_row7_col1\" class=\"data row7 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row7_col2\" class=\"data row7 col2\" >59.0</td>\n",
       "      <td id=\"T_1d0bc_row7_col3\" class=\"data row7 col3\" >60.1</td>\n",
       "      <td id=\"T_1d0bc_row7_col4\" class=\"data row7 col4\" >7.8</td>\n",
       "      <td id=\"T_1d0bc_row7_col5\" class=\"data row7 col5\" >42.6</td>\n",
       "      <td id=\"T_1d0bc_row7_col6\" class=\"data row7 col6\" >40.8</td>\n",
       "      <td id=\"T_1d0bc_row7_col7\" class=\"data row7 col7\" >45.1</td>\n",
       "      <td id=\"T_1d0bc_row7_col8\" class=\"data row7 col8\" >12.2</td>\n",
       "      <td id=\"T_1d0bc_row7_col9\" class=\"data row7 col9\" >44.1</td>\n",
       "      <td id=\"T_1d0bc_row7_col10\" class=\"data row7 col10\" >32.9</td>\n",
       "      <td id=\"T_1d0bc_row7_col11\" class=\"data row7 col11\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row7_col12\" class=\"data row7 col12\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row7_col13\" class=\"data row7 col13\" >38.3</td>\n",
       "      <td id=\"T_1d0bc_row7_col14\" class=\"data row7 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1d0bc_row8_col0\" class=\"data row8 col0\" >mistral-7b-sft-beta+dpo</td>\n",
       "      <td id=\"T_1d0bc_row8_col1\" class=\"data row8 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row8_col2\" class=\"data row8 col2\" >58.8</td>\n",
       "      <td id=\"T_1d0bc_row8_col3\" class=\"data row8 col3\" >59.8</td>\n",
       "      <td id=\"T_1d0bc_row8_col4\" class=\"data row8 col4\" >8.6</td>\n",
       "      <td id=\"T_1d0bc_row8_col5\" class=\"data row8 col5\" >39.4</td>\n",
       "      <td id=\"T_1d0bc_row8_col6\" class=\"data row8 col6\" >39.8</td>\n",
       "      <td id=\"T_1d0bc_row8_col7\" class=\"data row8 col7\" >45.7</td>\n",
       "      <td id=\"T_1d0bc_row8_col8\" class=\"data row8 col8\" >12.4</td>\n",
       "      <td id=\"T_1d0bc_row8_col9\" class=\"data row8 col9\" >43.2</td>\n",
       "      <td id=\"T_1d0bc_row8_col10\" class=\"data row8 col10\" >27.6</td>\n",
       "      <td id=\"T_1d0bc_row8_col11\" class=\"data row8 col11\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row8_col12\" class=\"data row8 col12\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row8_col13\" class=\"data row8 col13\" >37.3</td>\n",
       "      <td id=\"T_1d0bc_row8_col14\" class=\"data row8 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d0bc_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1d0bc_row9_col0\" class=\"data row9 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_1d0bc_row9_col1\" class=\"data row9 col1\" >None</td>\n",
       "      <td id=\"T_1d0bc_row9_col2\" class=\"data row9 col2\" >53.2</td>\n",
       "      <td id=\"T_1d0bc_row9_col3\" class=\"data row9 col3\" >53.7</td>\n",
       "      <td id=\"T_1d0bc_row9_col4\" class=\"data row9 col4\" >10.0</td>\n",
       "      <td id=\"T_1d0bc_row9_col5\" class=\"data row9 col5\" >36.4</td>\n",
       "      <td id=\"T_1d0bc_row9_col6\" class=\"data row9 col6\" >38.1</td>\n",
       "      <td id=\"T_1d0bc_row9_col7\" class=\"data row9 col7\" >39.5</td>\n",
       "      <td id=\"T_1d0bc_row9_col8\" class=\"data row9 col8\" >8.3</td>\n",
       "      <td id=\"T_1d0bc_row9_col9\" class=\"data row9 col9\" >43.5</td>\n",
       "      <td id=\"T_1d0bc_row9_col10\" class=\"data row9 col10\" >30.7</td>\n",
       "      <td id=\"T_1d0bc_row9_col11\" class=\"data row9 col11\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row9_col12\" class=\"data row9 col12\" >nan</td>\n",
       "      <td id=\"T_1d0bc_row9_col13\" class=\"data row9 col13\" >34.8</td>\n",
       "      <td id=\"T_1d0bc_row9_col14\" class=\"data row9 col14\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffc766530a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/pd.py:123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  col_contains_substr = df[col].str.contains(substr, regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a30f6_row0_col0, #T_a30f6_row0_col1, #T_a30f6_row1_col0, #T_a30f6_row1_col1, #T_a30f6_row2_col0, #T_a30f6_row2_col1, #T_a30f6_row3_col0, #T_a30f6_row3_col1, #T_a30f6_row4_col0, #T_a30f6_row4_col1, #T_a30f6_row5_col0, #T_a30f6_row5_col1, #T_a30f6_row6_col0, #T_a30f6_row6_col1, #T_a30f6_row7_col0, #T_a30f6_row7_col1, #T_a30f6_row8_col0, #T_a30f6_row8_col1, #T_a30f6_row9_col0, #T_a30f6_row9_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a30f6_row0_col2, #T_a30f6_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row0_col3, #T_a30f6_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row0_col4, #T_a30f6_row1_col4, #T_a30f6_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row0_col5, #T_a30f6_row1_col5, #T_a30f6_row8_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row0_col6, #T_a30f6_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row0_col7, #T_a30f6_row1_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row0_col8, #T_a30f6_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row0_col9, #T_a30f6_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row0_col10, #T_a30f6_row0_col13, #T_a30f6_row0_col14, #T_a30f6_row1_col10, #T_a30f6_row1_col13, #T_a30f6_row1_col14, #T_a30f6_row2_col5, #T_a30f6_row2_col8, #T_a30f6_row3_col2, #T_a30f6_row3_col3, #T_a30f6_row3_col4, #T_a30f6_row3_col6, #T_a30f6_row3_col7, #T_a30f6_row3_col9, #T_a30f6_row4_col11, #T_a30f6_row5_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row0_col11, #T_a30f6_row1_col11, #T_a30f6_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row0_col12, #T_a30f6_row1_col12, #T_a30f6_row4_col14, #T_a30f6_row5_col10, #T_a30f6_row5_col11, #T_a30f6_row7_col4, #T_a30f6_row8_col9, #T_a30f6_row9_col2, #T_a30f6_row9_col3, #T_a30f6_row9_col5, #T_a30f6_row9_col6, #T_a30f6_row9_col7, #T_a30f6_row9_col8, #T_a30f6_row9_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row2_col6, #T_a30f6_row7_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row2_col7, #T_a30f6_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row2_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row2_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row3_col11, #T_a30f6_row3_col12, #T_a30f6_row3_col14, #T_a30f6_row6_col11, #T_a30f6_row6_col12, #T_a30f6_row6_col14, #T_a30f6_row7_col11, #T_a30f6_row7_col12, #T_a30f6_row7_col14, #T_a30f6_row8_col11, #T_a30f6_row8_col12, #T_a30f6_row8_col14, #T_a30f6_row9_col11, #T_a30f6_row9_col12, #T_a30f6_row9_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row3_col13, #T_a30f6_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row4_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col4, #T_a30f6_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col7, #T_a30f6_row8_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row4_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col10, #T_a30f6_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row5_col4, #T_a30f6_row9_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row5_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row5_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f59c7d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row5_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row6_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row6_col8, #T_a30f6_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row6_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row6_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row7_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row7_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row7_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row7_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row7_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row7_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e1dad6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row8_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row8_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row8_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a30f6_row9_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a30f6_row9_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f29072;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a30f6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a30f6_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_a30f6_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_a30f6_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_a30f6_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_a30f6_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_a30f6_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_a30f6_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_a30f6_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_a30f6_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_a30f6_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_a30f6_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_a30f6_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR*</th>\n",
       "      <th id=\"T_a30f6_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_a30f6_level0_col13\" class=\"col_heading level0 col13\" >Average</th>\n",
       "      <th id=\"T_a30f6_level0_col14\" class=\"col_heading level0 col14\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a30f6_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat15_score=random:s=0_pace=prune:size=400000:ep=2</td>\n",
       "      <td id=\"T_a30f6_row0_col1\" class=\"data row0 col1\" >400000</td>\n",
       "      <td id=\"T_a30f6_row0_col2\" class=\"data row0 col2\" >59.1</td>\n",
       "      <td id=\"T_a30f6_row0_col3\" class=\"data row0 col3\" >60.8</td>\n",
       "      <td id=\"T_a30f6_row0_col4\" class=\"data row0 col4\" >10.6</td>\n",
       "      <td id=\"T_a30f6_row0_col5\" class=\"data row0 col5\" >44.4</td>\n",
       "      <td id=\"T_a30f6_row0_col6\" class=\"data row0 col6\" >40.7</td>\n",
       "      <td id=\"T_a30f6_row0_col7\" class=\"data row0 col7\" >46.6</td>\n",
       "      <td id=\"T_a30f6_row0_col8\" class=\"data row0 col8\" >12.0</td>\n",
       "      <td id=\"T_a30f6_row0_col9\" class=\"data row0 col9\" >49.0</td>\n",
       "      <td id=\"T_a30f6_row0_col10\" class=\"data row0 col10\" >34.1</td>\n",
       "      <td id=\"T_a30f6_row0_col11\" class=\"data row0 col11\" >57.9</td>\n",
       "      <td id=\"T_a30f6_row0_col12\" class=\"data row0 col12\" >210.2</td>\n",
       "      <td id=\"T_a30f6_row0_col13\" class=\"data row0 col13\" >41.5</td>\n",
       "      <td id=\"T_a30f6_row0_col14\" class=\"data row0 col14\" >-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a30f6_row1_col0\" class=\"data row1 col0\" >score=random:s=(\\d)_pace=prune:size=400000_avg (N=1)</td>\n",
       "      <td id=\"T_a30f6_row1_col1\" class=\"data row1 col1\" >400000</td>\n",
       "      <td id=\"T_a30f6_row1_col2\" class=\"data row1 col2\" >59.1</td>\n",
       "      <td id=\"T_a30f6_row1_col3\" class=\"data row1 col3\" >60.8</td>\n",
       "      <td id=\"T_a30f6_row1_col4\" class=\"data row1 col4\" >10.6</td>\n",
       "      <td id=\"T_a30f6_row1_col5\" class=\"data row1 col5\" >44.4</td>\n",
       "      <td id=\"T_a30f6_row1_col6\" class=\"data row1 col6\" >40.7</td>\n",
       "      <td id=\"T_a30f6_row1_col7\" class=\"data row1 col7\" >46.6</td>\n",
       "      <td id=\"T_a30f6_row1_col8\" class=\"data row1 col8\" >12.0</td>\n",
       "      <td id=\"T_a30f6_row1_col9\" class=\"data row1 col9\" >49.0</td>\n",
       "      <td id=\"T_a30f6_row1_col10\" class=\"data row1 col10\" >34.1</td>\n",
       "      <td id=\"T_a30f6_row1_col11\" class=\"data row1 col11\" >57.9</td>\n",
       "      <td id=\"T_a30f6_row1_col12\" class=\"data row1 col12\" >210.2</td>\n",
       "      <td id=\"T_a30f6_row1_col13\" class=\"data row1 col13\" >41.5</td>\n",
       "      <td id=\"T_a30f6_row1_col14\" class=\"data row1 col14\" >-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a30f6_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_a30f6_row2_col1\" class=\"data row2 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row2_col2\" class=\"data row2 col2\" >57.8</td>\n",
       "      <td id=\"T_a30f6_row2_col3\" class=\"data row2 col3\" >59.5</td>\n",
       "      <td id=\"T_a30f6_row2_col4\" class=\"data row2 col4\" >9.8</td>\n",
       "      <td id=\"T_a30f6_row2_col5\" class=\"data row2 col5\" >50.4</td>\n",
       "      <td id=\"T_a30f6_row2_col6\" class=\"data row2 col6\" >40.8</td>\n",
       "      <td id=\"T_a30f6_row2_col7\" class=\"data row2 col7\" >46.5</td>\n",
       "      <td id=\"T_a30f6_row2_col8\" class=\"data row2 col8\" >14.9</td>\n",
       "      <td id=\"T_a30f6_row2_col9\" class=\"data row2 col9\" >46.6</td>\n",
       "      <td id=\"T_a30f6_row2_col10\" class=\"data row2 col10\" >28.7</td>\n",
       "      <td id=\"T_a30f6_row2_col11\" class=\"data row2 col11\" >57.3</td>\n",
       "      <td id=\"T_a30f6_row2_col12\" class=\"data row2 col12\" >219.3</td>\n",
       "      <td id=\"T_a30f6_row2_col13\" class=\"data row2 col13\" >41.2</td>\n",
       "      <td id=\"T_a30f6_row2_col14\" class=\"data row2 col14\" >-14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a30f6_row3_col0\" class=\"data row3 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_a30f6_row3_col1\" class=\"data row3 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row3_col2\" class=\"data row3 col2\" >60.2</td>\n",
       "      <td id=\"T_a30f6_row3_col3\" class=\"data row3 col3\" >62.6</td>\n",
       "      <td id=\"T_a30f6_row3_col4\" class=\"data row3 col4\" >11.6</td>\n",
       "      <td id=\"T_a30f6_row3_col5\" class=\"data row3 col5\" >42.0</td>\n",
       "      <td id=\"T_a30f6_row3_col6\" class=\"data row3 col6\" >42.6</td>\n",
       "      <td id=\"T_a30f6_row3_col7\" class=\"data row3 col7\" >48.3</td>\n",
       "      <td id=\"T_a30f6_row3_col8\" class=\"data row3 col8\" >14.4</td>\n",
       "      <td id=\"T_a30f6_row3_col9\" class=\"data row3 col9\" >57.8</td>\n",
       "      <td id=\"T_a30f6_row3_col10\" class=\"data row3 col10\" >29.3</td>\n",
       "      <td id=\"T_a30f6_row3_col11\" class=\"data row3 col11\" >nan</td>\n",
       "      <td id=\"T_a30f6_row3_col12\" class=\"data row3 col12\" >nan</td>\n",
       "      <td id=\"T_a30f6_row3_col13\" class=\"data row3 col13\" >41.0</td>\n",
       "      <td id=\"T_a30f6_row3_col14\" class=\"data row3 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a30f6_row4_col0\" class=\"data row4 col0\" >mistral-7b_ultrachat200k_aftersplitlongconv_ep=2</td>\n",
       "      <td id=\"T_a30f6_row4_col1\" class=\"data row4 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row4_col2\" class=\"data row4 col2\" >57.3</td>\n",
       "      <td id=\"T_a30f6_row4_col3\" class=\"data row4 col3\" >59.5</td>\n",
       "      <td id=\"T_a30f6_row4_col4\" class=\"data row4 col4\" >8.6</td>\n",
       "      <td id=\"T_a30f6_row4_col5\" class=\"data row4 col5\" >47.0</td>\n",
       "      <td id=\"T_a30f6_row4_col6\" class=\"data row4 col6\" >39.4</td>\n",
       "      <td id=\"T_a30f6_row4_col7\" class=\"data row4 col7\" >45.6</td>\n",
       "      <td id=\"T_a30f6_row4_col8\" class=\"data row4 col8\" >14.2</td>\n",
       "      <td id=\"T_a30f6_row4_col9\" class=\"data row4 col9\" >48.4</td>\n",
       "      <td id=\"T_a30f6_row4_col10\" class=\"data row4 col10\" >27.4</td>\n",
       "      <td id=\"T_a30f6_row4_col11\" class=\"data row4 col11\" >58.2</td>\n",
       "      <td id=\"T_a30f6_row4_col12\" class=\"data row4 col12\" >231.3</td>\n",
       "      <td id=\"T_a30f6_row4_col13\" class=\"data row4 col13\" >40.6</td>\n",
       "      <td id=\"T_a30f6_row4_col14\" class=\"data row4 col14\" >-17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a30f6_row5_col0\" class=\"data row5 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_a30f6_row5_col1\" class=\"data row5 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row5_col2\" class=\"data row5 col2\" >57.6</td>\n",
       "      <td id=\"T_a30f6_row5_col3\" class=\"data row5 col3\" >60.3</td>\n",
       "      <td id=\"T_a30f6_row5_col4\" class=\"data row5 col4\" >10.0</td>\n",
       "      <td id=\"T_a30f6_row5_col5\" class=\"data row5 col5\" >45.0</td>\n",
       "      <td id=\"T_a30f6_row5_col6\" class=\"data row5 col6\" >38.2</td>\n",
       "      <td id=\"T_a30f6_row5_col7\" class=\"data row5 col7\" >46.0</td>\n",
       "      <td id=\"T_a30f6_row5_col8\" class=\"data row5 col8\" >14.8</td>\n",
       "      <td id=\"T_a30f6_row5_col9\" class=\"data row5 col9\" >50.5</td>\n",
       "      <td id=\"T_a30f6_row5_col10\" class=\"data row5 col10\" >18.9</td>\n",
       "      <td id=\"T_a30f6_row5_col11\" class=\"data row5 col11\" >56.7</td>\n",
       "      <td id=\"T_a30f6_row5_col12\" class=\"data row5 col12\" >294.1</td>\n",
       "      <td id=\"T_a30f6_row5_col13\" class=\"data row5 col13\" >39.8</td>\n",
       "      <td id=\"T_a30f6_row5_col14\" class=\"data row5 col14\" >-15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a30f6_row6_col0\" class=\"data row6 col0\" >mistral-7b_sft-alpha</td>\n",
       "      <td id=\"T_a30f6_row6_col1\" class=\"data row6 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row6_col2\" class=\"data row6 col2\" >58.7</td>\n",
       "      <td id=\"T_a30f6_row6_col3\" class=\"data row6 col3\" >60.8</td>\n",
       "      <td id=\"T_a30f6_row6_col4\" class=\"data row6 col4\" >9.6</td>\n",
       "      <td id=\"T_a30f6_row6_col5\" class=\"data row6 col5\" >46.2</td>\n",
       "      <td id=\"T_a30f6_row6_col6\" class=\"data row6 col6\" >38.5</td>\n",
       "      <td id=\"T_a30f6_row6_col7\" class=\"data row6 col7\" >45.6</td>\n",
       "      <td id=\"T_a30f6_row6_col8\" class=\"data row6 col8\" >12.4</td>\n",
       "      <td id=\"T_a30f6_row6_col9\" class=\"data row6 col9\" >54.3</td>\n",
       "      <td id=\"T_a30f6_row6_col10\" class=\"data row6 col10\" >27.4</td>\n",
       "      <td id=\"T_a30f6_row6_col11\" class=\"data row6 col11\" >nan</td>\n",
       "      <td id=\"T_a30f6_row6_col12\" class=\"data row6 col12\" >nan</td>\n",
       "      <td id=\"T_a30f6_row6_col13\" class=\"data row6 col13\" >39.3</td>\n",
       "      <td id=\"T_a30f6_row6_col14\" class=\"data row6 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a30f6_row7_col0\" class=\"data row7 col0\" >mistral-7b-sft-alpha+dpo</td>\n",
       "      <td id=\"T_a30f6_row7_col1\" class=\"data row7 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row7_col2\" class=\"data row7 col2\" >59.0</td>\n",
       "      <td id=\"T_a30f6_row7_col3\" class=\"data row7 col3\" >60.1</td>\n",
       "      <td id=\"T_a30f6_row7_col4\" class=\"data row7 col4\" >7.8</td>\n",
       "      <td id=\"T_a30f6_row7_col5\" class=\"data row7 col5\" >42.6</td>\n",
       "      <td id=\"T_a30f6_row7_col6\" class=\"data row7 col6\" >40.8</td>\n",
       "      <td id=\"T_a30f6_row7_col7\" class=\"data row7 col7\" >45.1</td>\n",
       "      <td id=\"T_a30f6_row7_col8\" class=\"data row7 col8\" >12.2</td>\n",
       "      <td id=\"T_a30f6_row7_col9\" class=\"data row7 col9\" >44.1</td>\n",
       "      <td id=\"T_a30f6_row7_col10\" class=\"data row7 col10\" >32.9</td>\n",
       "      <td id=\"T_a30f6_row7_col11\" class=\"data row7 col11\" >nan</td>\n",
       "      <td id=\"T_a30f6_row7_col12\" class=\"data row7 col12\" >nan</td>\n",
       "      <td id=\"T_a30f6_row7_col13\" class=\"data row7 col13\" >38.3</td>\n",
       "      <td id=\"T_a30f6_row7_col14\" class=\"data row7 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a30f6_row8_col0\" class=\"data row8 col0\" >mistral-7b-sft-beta+dpo</td>\n",
       "      <td id=\"T_a30f6_row8_col1\" class=\"data row8 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row8_col2\" class=\"data row8 col2\" >58.8</td>\n",
       "      <td id=\"T_a30f6_row8_col3\" class=\"data row8 col3\" >59.8</td>\n",
       "      <td id=\"T_a30f6_row8_col4\" class=\"data row8 col4\" >8.6</td>\n",
       "      <td id=\"T_a30f6_row8_col5\" class=\"data row8 col5\" >39.4</td>\n",
       "      <td id=\"T_a30f6_row8_col6\" class=\"data row8 col6\" >39.8</td>\n",
       "      <td id=\"T_a30f6_row8_col7\" class=\"data row8 col7\" >45.7</td>\n",
       "      <td id=\"T_a30f6_row8_col8\" class=\"data row8 col8\" >12.4</td>\n",
       "      <td id=\"T_a30f6_row8_col9\" class=\"data row8 col9\" >43.2</td>\n",
       "      <td id=\"T_a30f6_row8_col10\" class=\"data row8 col10\" >27.6</td>\n",
       "      <td id=\"T_a30f6_row8_col11\" class=\"data row8 col11\" >nan</td>\n",
       "      <td id=\"T_a30f6_row8_col12\" class=\"data row8 col12\" >nan</td>\n",
       "      <td id=\"T_a30f6_row8_col13\" class=\"data row8 col13\" >37.3</td>\n",
       "      <td id=\"T_a30f6_row8_col14\" class=\"data row8 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a30f6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a30f6_row9_col0\" class=\"data row9 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_a30f6_row9_col1\" class=\"data row9 col1\" >None</td>\n",
       "      <td id=\"T_a30f6_row9_col2\" class=\"data row9 col2\" >53.2</td>\n",
       "      <td id=\"T_a30f6_row9_col3\" class=\"data row9 col3\" >53.7</td>\n",
       "      <td id=\"T_a30f6_row9_col4\" class=\"data row9 col4\" >10.0</td>\n",
       "      <td id=\"T_a30f6_row9_col5\" class=\"data row9 col5\" >36.4</td>\n",
       "      <td id=\"T_a30f6_row9_col6\" class=\"data row9 col6\" >38.1</td>\n",
       "      <td id=\"T_a30f6_row9_col7\" class=\"data row9 col7\" >39.5</td>\n",
       "      <td id=\"T_a30f6_row9_col8\" class=\"data row9 col8\" >8.3</td>\n",
       "      <td id=\"T_a30f6_row9_col9\" class=\"data row9 col9\" >43.5</td>\n",
       "      <td id=\"T_a30f6_row9_col10\" class=\"data row9 col10\" >30.7</td>\n",
       "      <td id=\"T_a30f6_row9_col11\" class=\"data row9 col11\" >nan</td>\n",
       "      <td id=\"T_a30f6_row9_col12\" class=\"data row9 col12\" >nan</td>\n",
       "      <td id=\"T_a30f6_row9_col13\" class=\"data row9 col13\" >34.8</td>\n",
       "      <td id=\"T_a30f6_row9_col14\" class=\"data row9 col14\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffc75eefe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/pd.py:123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  col_contains_substr = df[col].str.contains(substr, regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_55165_row0_col0, #T_55165_row0_col1, #T_55165_row1_col0, #T_55165_row1_col1, #T_55165_row2_col0, #T_55165_row2_col1, #T_55165_row3_col0, #T_55165_row3_col1, #T_55165_row4_col0, #T_55165_row4_col1, #T_55165_row5_col0, #T_55165_row5_col1, #T_55165_row6_col0, #T_55165_row6_col1, #T_55165_row7_col0, #T_55165_row7_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_55165_row0_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b99e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row0_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row0_col5, #T_55165_row0_col8, #T_55165_row0_col13, #T_55165_row0_col14, #T_55165_row1_col2, #T_55165_row1_col3, #T_55165_row1_col4, #T_55165_row1_col6, #T_55165_row1_col7, #T_55165_row1_col9, #T_55165_row2_col11, #T_55165_row3_col12, #T_55165_row5_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row0_col6, #T_55165_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row0_col7, #T_55165_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row0_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row0_col10, #T_55165_row4_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ad90;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row0_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row0_col12, #T_55165_row2_col14, #T_55165_row3_col10, #T_55165_row3_col11, #T_55165_row5_col4, #T_55165_row6_col9, #T_55165_row7_col2, #T_55165_row7_col3, #T_55165_row7_col5, #T_55165_row7_col6, #T_55165_row7_col7, #T_55165_row7_col8, #T_55165_row7_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row1_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row1_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row1_col11, #T_55165_row1_col12, #T_55165_row1_col14, #T_55165_row4_col11, #T_55165_row4_col12, #T_55165_row4_col14, #T_55165_row5_col11, #T_55165_row5_col12, #T_55165_row5_col14, #T_55165_row6_col11, #T_55165_row6_col12, #T_55165_row6_col14, #T_55165_row7_col11, #T_55165_row7_col12, #T_55165_row7_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row1_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col4, #T_55165_row6_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row2_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f39778;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col7, #T_55165_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d85646;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col10, #T_55165_row4_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row2_col12 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row2_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row3_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row3_col4, #T_55165_row7_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row3_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row3_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row3_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row3_col14 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row4_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row4_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row4_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #5a78e4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row4_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row4_col8, #T_55165_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row4_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row5_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row5_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row5_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c2aa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row5_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row5_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row6_col2 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row6_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #bcd2f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row6_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c5ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row6_col13 {\n",
       "  text-align: left;\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_55165_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_55165_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e57058;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_55165\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_55165_level0_col0\" class=\"col_heading level0 col0\" >run_name</th>\n",
       "      <th id=\"T_55165_level0_col1\" class=\"col_heading level0 col1\" >total_train_samples</th>\n",
       "      <th id=\"T_55165_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_55165_level0_col3\" class=\"col_heading level0 col3\" >MMLU/5-shot</th>\n",
       "      <th id=\"T_55165_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_55165_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_55165_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_55165_level0_col7\" class=\"col_heading level0 col7\" >BBH/CoT</th>\n",
       "      <th id=\"T_55165_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_55165_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_55165_level0_col10\" class=\"col_heading level0 col10\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_55165_level0_col11\" class=\"col_heading level0 col11\" >AlpacaFarm/WR*</th>\n",
       "      <th id=\"T_55165_level0_col12\" class=\"col_heading level0 col12\" >AlpacaFarm/Len</th>\n",
       "      <th id=\"T_55165_level0_col13\" class=\"col_heading level0 col13\" >Average</th>\n",
       "      <th id=\"T_55165_level0_col14\" class=\"col_heading level0 col14\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_55165_row0_col0\" class=\"data row0 col0\" >mistral-7b_ultrachat200k_beforesplitlongconv_ep=2</td>\n",
       "      <td id=\"T_55165_row0_col1\" class=\"data row0 col1\" >None</td>\n",
       "      <td id=\"T_55165_row0_col2\" class=\"data row0 col2\" >57.8</td>\n",
       "      <td id=\"T_55165_row0_col3\" class=\"data row0 col3\" >59.5</td>\n",
       "      <td id=\"T_55165_row0_col4\" class=\"data row0 col4\" >9.8</td>\n",
       "      <td id=\"T_55165_row0_col5\" class=\"data row0 col5\" >50.4</td>\n",
       "      <td id=\"T_55165_row0_col6\" class=\"data row0 col6\" >40.8</td>\n",
       "      <td id=\"T_55165_row0_col7\" class=\"data row0 col7\" >46.5</td>\n",
       "      <td id=\"T_55165_row0_col8\" class=\"data row0 col8\" >14.9</td>\n",
       "      <td id=\"T_55165_row0_col9\" class=\"data row0 col9\" >46.6</td>\n",
       "      <td id=\"T_55165_row0_col10\" class=\"data row0 col10\" >28.7</td>\n",
       "      <td id=\"T_55165_row0_col11\" class=\"data row0 col11\" >57.3</td>\n",
       "      <td id=\"T_55165_row0_col12\" class=\"data row0 col12\" >219.3</td>\n",
       "      <td id=\"T_55165_row0_col13\" class=\"data row0 col13\" >41.2</td>\n",
       "      <td id=\"T_55165_row0_col14\" class=\"data row0 col14\" >-14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_55165_row1_col0\" class=\"data row1 col0\" >mistral-7b</td>\n",
       "      <td id=\"T_55165_row1_col1\" class=\"data row1 col1\" >None</td>\n",
       "      <td id=\"T_55165_row1_col2\" class=\"data row1 col2\" >60.2</td>\n",
       "      <td id=\"T_55165_row1_col3\" class=\"data row1 col3\" >62.6</td>\n",
       "      <td id=\"T_55165_row1_col4\" class=\"data row1 col4\" >11.6</td>\n",
       "      <td id=\"T_55165_row1_col5\" class=\"data row1 col5\" >42.0</td>\n",
       "      <td id=\"T_55165_row1_col6\" class=\"data row1 col6\" >42.6</td>\n",
       "      <td id=\"T_55165_row1_col7\" class=\"data row1 col7\" >48.3</td>\n",
       "      <td id=\"T_55165_row1_col8\" class=\"data row1 col8\" >14.4</td>\n",
       "      <td id=\"T_55165_row1_col9\" class=\"data row1 col9\" >57.8</td>\n",
       "      <td id=\"T_55165_row1_col10\" class=\"data row1 col10\" >29.3</td>\n",
       "      <td id=\"T_55165_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "      <td id=\"T_55165_row1_col12\" class=\"data row1 col12\" >nan</td>\n",
       "      <td id=\"T_55165_row1_col13\" class=\"data row1 col13\" >41.0</td>\n",
       "      <td id=\"T_55165_row1_col14\" class=\"data row1 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_55165_row2_col0\" class=\"data row2 col0\" >mistral-7b_ultrachat200k_aftersplitlongconv_ep=2</td>\n",
       "      <td id=\"T_55165_row2_col1\" class=\"data row2 col1\" >None</td>\n",
       "      <td id=\"T_55165_row2_col2\" class=\"data row2 col2\" >57.3</td>\n",
       "      <td id=\"T_55165_row2_col3\" class=\"data row2 col3\" >59.5</td>\n",
       "      <td id=\"T_55165_row2_col4\" class=\"data row2 col4\" >8.6</td>\n",
       "      <td id=\"T_55165_row2_col5\" class=\"data row2 col5\" >47.0</td>\n",
       "      <td id=\"T_55165_row2_col6\" class=\"data row2 col6\" >39.4</td>\n",
       "      <td id=\"T_55165_row2_col7\" class=\"data row2 col7\" >45.6</td>\n",
       "      <td id=\"T_55165_row2_col8\" class=\"data row2 col8\" >14.2</td>\n",
       "      <td id=\"T_55165_row2_col9\" class=\"data row2 col9\" >48.4</td>\n",
       "      <td id=\"T_55165_row2_col10\" class=\"data row2 col10\" >27.4</td>\n",
       "      <td id=\"T_55165_row2_col11\" class=\"data row2 col11\" >58.2</td>\n",
       "      <td id=\"T_55165_row2_col12\" class=\"data row2 col12\" >231.3</td>\n",
       "      <td id=\"T_55165_row2_col13\" class=\"data row2 col13\" >40.6</td>\n",
       "      <td id=\"T_55165_row2_col14\" class=\"data row2 col14\" >-17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_55165_row3_col0\" class=\"data row3 col0\" >mistral-7b-sft-beta</td>\n",
       "      <td id=\"T_55165_row3_col1\" class=\"data row3 col1\" >None</td>\n",
       "      <td id=\"T_55165_row3_col2\" class=\"data row3 col2\" >57.6</td>\n",
       "      <td id=\"T_55165_row3_col3\" class=\"data row3 col3\" >60.3</td>\n",
       "      <td id=\"T_55165_row3_col4\" class=\"data row3 col4\" >10.0</td>\n",
       "      <td id=\"T_55165_row3_col5\" class=\"data row3 col5\" >45.0</td>\n",
       "      <td id=\"T_55165_row3_col6\" class=\"data row3 col6\" >38.2</td>\n",
       "      <td id=\"T_55165_row3_col7\" class=\"data row3 col7\" >46.0</td>\n",
       "      <td id=\"T_55165_row3_col8\" class=\"data row3 col8\" >14.8</td>\n",
       "      <td id=\"T_55165_row3_col9\" class=\"data row3 col9\" >50.5</td>\n",
       "      <td id=\"T_55165_row3_col10\" class=\"data row3 col10\" >18.9</td>\n",
       "      <td id=\"T_55165_row3_col11\" class=\"data row3 col11\" >56.7</td>\n",
       "      <td id=\"T_55165_row3_col12\" class=\"data row3 col12\" >294.1</td>\n",
       "      <td id=\"T_55165_row3_col13\" class=\"data row3 col13\" >39.8</td>\n",
       "      <td id=\"T_55165_row3_col14\" class=\"data row3 col14\" >-15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_55165_row4_col0\" class=\"data row4 col0\" >mistral-7b_sft-alpha</td>\n",
       "      <td id=\"T_55165_row4_col1\" class=\"data row4 col1\" >None</td>\n",
       "      <td id=\"T_55165_row4_col2\" class=\"data row4 col2\" >58.7</td>\n",
       "      <td id=\"T_55165_row4_col3\" class=\"data row4 col3\" >60.8</td>\n",
       "      <td id=\"T_55165_row4_col4\" class=\"data row4 col4\" >9.6</td>\n",
       "      <td id=\"T_55165_row4_col5\" class=\"data row4 col5\" >46.2</td>\n",
       "      <td id=\"T_55165_row4_col6\" class=\"data row4 col6\" >38.5</td>\n",
       "      <td id=\"T_55165_row4_col7\" class=\"data row4 col7\" >45.6</td>\n",
       "      <td id=\"T_55165_row4_col8\" class=\"data row4 col8\" >12.4</td>\n",
       "      <td id=\"T_55165_row4_col9\" class=\"data row4 col9\" >54.3</td>\n",
       "      <td id=\"T_55165_row4_col10\" class=\"data row4 col10\" >27.4</td>\n",
       "      <td id=\"T_55165_row4_col11\" class=\"data row4 col11\" >nan</td>\n",
       "      <td id=\"T_55165_row4_col12\" class=\"data row4 col12\" >nan</td>\n",
       "      <td id=\"T_55165_row4_col13\" class=\"data row4 col13\" >39.3</td>\n",
       "      <td id=\"T_55165_row4_col14\" class=\"data row4 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_55165_row5_col0\" class=\"data row5 col0\" >mistral-7b-sft-alpha+dpo</td>\n",
       "      <td id=\"T_55165_row5_col1\" class=\"data row5 col1\" >None</td>\n",
       "      <td id=\"T_55165_row5_col2\" class=\"data row5 col2\" >59.0</td>\n",
       "      <td id=\"T_55165_row5_col3\" class=\"data row5 col3\" >60.1</td>\n",
       "      <td id=\"T_55165_row5_col4\" class=\"data row5 col4\" >7.8</td>\n",
       "      <td id=\"T_55165_row5_col5\" class=\"data row5 col5\" >42.6</td>\n",
       "      <td id=\"T_55165_row5_col6\" class=\"data row5 col6\" >40.8</td>\n",
       "      <td id=\"T_55165_row5_col7\" class=\"data row5 col7\" >45.1</td>\n",
       "      <td id=\"T_55165_row5_col8\" class=\"data row5 col8\" >12.2</td>\n",
       "      <td id=\"T_55165_row5_col9\" class=\"data row5 col9\" >44.1</td>\n",
       "      <td id=\"T_55165_row5_col10\" class=\"data row5 col10\" >32.9</td>\n",
       "      <td id=\"T_55165_row5_col11\" class=\"data row5 col11\" >nan</td>\n",
       "      <td id=\"T_55165_row5_col12\" class=\"data row5 col12\" >nan</td>\n",
       "      <td id=\"T_55165_row5_col13\" class=\"data row5 col13\" >38.3</td>\n",
       "      <td id=\"T_55165_row5_col14\" class=\"data row5 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_55165_row6_col0\" class=\"data row6 col0\" >mistral-7b-sft-beta+dpo</td>\n",
       "      <td id=\"T_55165_row6_col1\" class=\"data row6 col1\" >None</td>\n",
       "      <td id=\"T_55165_row6_col2\" class=\"data row6 col2\" >58.8</td>\n",
       "      <td id=\"T_55165_row6_col3\" class=\"data row6 col3\" >59.8</td>\n",
       "      <td id=\"T_55165_row6_col4\" class=\"data row6 col4\" >8.6</td>\n",
       "      <td id=\"T_55165_row6_col5\" class=\"data row6 col5\" >39.4</td>\n",
       "      <td id=\"T_55165_row6_col6\" class=\"data row6 col6\" >39.8</td>\n",
       "      <td id=\"T_55165_row6_col7\" class=\"data row6 col7\" >45.7</td>\n",
       "      <td id=\"T_55165_row6_col8\" class=\"data row6 col8\" >12.4</td>\n",
       "      <td id=\"T_55165_row6_col9\" class=\"data row6 col9\" >43.2</td>\n",
       "      <td id=\"T_55165_row6_col10\" class=\"data row6 col10\" >27.6</td>\n",
       "      <td id=\"T_55165_row6_col11\" class=\"data row6 col11\" >nan</td>\n",
       "      <td id=\"T_55165_row6_col12\" class=\"data row6 col12\" >nan</td>\n",
       "      <td id=\"T_55165_row6_col13\" class=\"data row6 col13\" >37.3</td>\n",
       "      <td id=\"T_55165_row6_col14\" class=\"data row6 col14\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55165_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_55165_row7_col0\" class=\"data row7 col0\" >mistral-7b-Instruct</td>\n",
       "      <td id=\"T_55165_row7_col1\" class=\"data row7 col1\" >None</td>\n",
       "      <td id=\"T_55165_row7_col2\" class=\"data row7 col2\" >53.2</td>\n",
       "      <td id=\"T_55165_row7_col3\" class=\"data row7 col3\" >53.7</td>\n",
       "      <td id=\"T_55165_row7_col4\" class=\"data row7 col4\" >10.0</td>\n",
       "      <td id=\"T_55165_row7_col5\" class=\"data row7 col5\" >36.4</td>\n",
       "      <td id=\"T_55165_row7_col6\" class=\"data row7 col6\" >38.1</td>\n",
       "      <td id=\"T_55165_row7_col7\" class=\"data row7 col7\" >39.5</td>\n",
       "      <td id=\"T_55165_row7_col8\" class=\"data row7 col8\" >8.3</td>\n",
       "      <td id=\"T_55165_row7_col9\" class=\"data row7 col9\" >43.5</td>\n",
       "      <td id=\"T_55165_row7_col10\" class=\"data row7 col10\" >30.7</td>\n",
       "      <td id=\"T_55165_row7_col11\" class=\"data row7 col11\" >nan</td>\n",
       "      <td id=\"T_55165_row7_col12\" class=\"data row7 col12\" >nan</td>\n",
       "      <td id=\"T_55165_row7_col13\" class=\"data row7 col13\" >34.8</td>\n",
       "      <td id=\"T_55165_row7_col14\" class=\"data row7 col14\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffc76652aa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# sort_rows = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b_12.13update_before', '../results/baselines/huggyllama/llama-7b_12.13update_before/'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "# #     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "# #     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "# #     # llama2\n",
    "#     ('llama2-7b_12.13update_before', '../results/baselines/NousResearch/Llama-2-7b-hf_12.13update_before/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b-chat', '../results/baselines/NousResearch/Llama-2-7b-chat-hf/'),\n",
    "# #     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "# #     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "# #     # mistral\n",
    "# #     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct-v0.1_12.13update_before', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1_12.13update_before'),\n",
    "#     ('mistral-7b-Instruct-v0.1', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     # zephyr\n",
    "#     ('zephyr-7b-beta_12.13update_before', '../results/baselines/HuggingFaceH4/zephyr-7b-beta_12.13update_before'),\n",
    "#     ('zephyr-7b-beta', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "\n",
    "# # baselines\n",
    "# save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b+lima_ep=2', '../results/ft1_ep=2/llama-7b_lima/'),\n",
    "# #     ('mistral-7b+lima_ep=2', '../results/ft1_ep=2/mistral-7b_lima/'), \n",
    "# ]\n",
    "# exp_dir = '../results/oi2/'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# # save_dirs = [\n",
    "# #     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "# #     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# # ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'tuluv1m' in x]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# # exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# # exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "###### ultrachat\n",
    "save_dirs = [\n",
    "    # baselines \n",
    "    ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "    ('mistral-7b_ultrachat200k_aftersplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k'),\n",
    "    ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "    \n",
    "    ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "    ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "    ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "    ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-alpha'),\n",
    "    ('mistral-7b-sft-beta+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "]\n",
    "# exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# exp_dir = '../results/oi5_ultrachat200k:mistral-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "exp_dir = '../results/oi5_ultrachat15:mistral-7b'\n",
    "save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "#####\n",
    "\n",
    "\n",
    "# #####\n",
    "# dataset = 'stanford_alpaca'\n",
    "# dataset = 'open_orca_slim'\n",
    "# dataset = 'sharegptv2'\n",
    "# dataset = 'wizardlm'\n",
    "# dataset = 'tulu_v1_mix'\n",
    "# dataset = 'tulu_v2'\n",
    "# dataset = 'flan_v2'\n",
    "# dataset = 'oasst1'\n",
    "# dataset = 'dolly'\n",
    "\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b'),\n",
    "# #              ('llama-7b_lima_ep=5', '../results/oi2/llama-7b_lima_ep=5/'),\n",
    "# #              ('llama-7b_lima_ep=10', '../results/oi2/llama-7b_lima_ep=10/'),\n",
    "#             ]\n",
    "# if dataset == 'tulu_v2':\n",
    "#     save_dirs += [('llama-7b_tulu_v2:100k_ep=2', '../results/oi2/llama-7b_tulu_v2:100k_ep=2'),]\n",
    "# elif dataset == 'open_orca_slim':\n",
    "#     save_dirs += [('llama-7b_openorcaslim:100k_ep=2', '../results/oi2/llama-7b_openorcaslim:100k_ep=2'),]\n",
    "# elif dataset == 'sharegptv2':\n",
    "#     save_dirs += [\n",
    "#         ('llama-7b_sharegptv2_ep=2', '../results/oi2/llama-7b_sharegptv2_ep=2'),\n",
    "#         ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),]\n",
    "# elif dataset == 'tulu_v1_mix':\n",
    "#     save_dirs += [\n",
    "#         ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#         # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "#         # ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "#     ]\n",
    "# else:\n",
    "#     save_dirs += [(f'llama-7b_{dataset}_ep=2', f'../results/oi2/llama-7b_{dataset}_ep=2'),]\n",
    "\n",
    "# exp_dir = f'../results/oi5_{dataset}:llama-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'dppmapbd' not in x and 'semdedup' not in x]\n",
    "# #####\n",
    "\n",
    "\n",
    "# #####\n",
    "# save_dirs = [\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('codellama-7b', '../results/baselines/codellama/CodeLlama-7b-hf/'),\n",
    "#     ('codellama-7b-instruct', '../results/baselines/codellama/CodeLlama-7b-Python-hf/'),\n",
    "#     ('codellama-7b-python', '../results/baselines/codellama/CodeLlama-7b-Instruct-hf/'),\n",
    "# ]\n",
    "\n",
    "# exp_dir = '../results/oi2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'starcoder' in x]\n",
    "# exp_dir = '../results/oi6_starcoder_ep=5'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstr:codellama-7b'\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstrv2:codellama-7b'\n",
    "# exp_dir = '../results/oi5_starcoder_commentinstrv5:codellama-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "###### \n",
    "\n",
    "from llm.evaluate import detect_oom_evals\n",
    "oom_eval_paths = detect_oom_evals([x for l in [glob.glob(os.path.join(x[1], 'eval/*/*.out')) for x in save_dirs] for x in l])\n",
    "if oom_eval_paths: print(oom_eval_paths)\n",
    "    \n",
    "cols_avg_blacklist = ['AlpacaFarm/Len']\n",
    "\n",
    "chat_fmt = False\n",
    "chat_fmt = True\n",
    "# chat_fmt = 'both'\n",
    "# chat_fmt = 'auto' # base model no chatfmt, tuned model with chatfmt\n",
    "chat_fmt = 'mix'  # non-alpacaeval no chatfmt, alpacaeval chatfmt\n",
    "ft_args_fields = [\n",
    "    'run_name',\n",
    "    'model_args.model_name_or_path',\n",
    "    'data_args.subsample_mixture',\n",
    "    'data_args.max_train_samples',\n",
    "]\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/Direct', 'BBH/Direct', 'TydiQA/CB', 'Codex-Eval/Pass@1']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] #  'ToxiGen/Acc'\n",
    "cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*', 'AlpacaFarm/Len'] \n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*'] \n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR', 'AlpacaFarm/ΔWR', 'AlpacaFarm/Len'] \n",
    "\n",
    "\n",
    "# cols = [f'BBH {x}' for x in ['reasoning', 'nlu', 'knowledge', 'multilingual']]; cols = [x+'/Direct' for x in cols] + [x+'/CoT' for x in cols]\n",
    "# cols = [f'MMLU {x}' for x in ['STEM', 'humanities', 'social sciences', 'other']]; cols = [x+'/0-shot' for x in cols] + [x+'/5-shot' for x in cols]\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR', 'AlpacaFarm/Rep', 'AlpacaFarm/WR*'] #  'ToxiGen/Acc'\n",
    "#     cols = ['AlpacaFarm/WR', 'AlpacaFarm/Rep', 'AlpacaFarm/WR*']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR'] #  entire, without tydiqa, which has high variance\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', ] #  entire, without tydiqa, which has high variance\n",
    "if 'open_orca_slim' in exp_dir:\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'BBH/Direct', 'BBH/CoT']\n",
    "    cols = ['MMLU/0-shot', 'BBH/Direct']\n",
    "if 'starcoder' in exp_dir:\n",
    "    cols = ['Codex-Eval/Pass@1']\n",
    "    chat_fmt = 'both'\n",
    "    ft_args_fields += ['data_args.train_file']\n",
    "print(f'chat_fmt={chat_fmt}')\n",
    "df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "\n",
    "cols = [x for x in cols if x in df.columns]\n",
    "df = df[ft_args_fields + cols]\n",
    "if chat_fmt == 'both':\n",
    "    for col_lvl2 in ['', 'chatfmt']:\n",
    "        df[('Average', col_lvl2)] = df[list(set(df.columns) & set([(x, col_lvl2) for x in list(set(cols)-set(cols_avg_blacklist))]))].mean(axis=1)\n",
    "else:\n",
    "    df['Average'] = df[list(set(cols)-set(cols_avg_blacklist))].mean(axis=1)\n",
    "if sort_rows:\n",
    "    df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "    sort_value_col, sort_value_col_ascending = ('Average', '') if chat_fmt=='both' else 'Average', False\n",
    "#     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "    df = df.sort_values(by=sort_value_col, ascending=sort_value_col_ascending)\n",
    "df = df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "def compute_total_train_samples(x):\n",
    "    match = re.search(r'size=(\\d+)', x['run_name' if chat_fmt!='both' else ('run_name', '')])\n",
    "    total_train_samples = match.group(1) if match else None\n",
    "    return total_train_samples\n",
    "df.insert(1, 'total_train_samples' if chat_fmt!='both' else ('total_train_samples', ''), df.apply(compute_total_train_samples, axis=1))\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral', 'zephyr']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x.lower())]\n",
    "        if not len(dfc): continue\n",
    "        from rosemary import pd_average_col_contains_substr\n",
    "        Ns = sorted(np.unique([int(x) for x in df['total_train_samples'].to_numpy() if x]).tolist())\n",
    "        for N in Ns+[None]:\n",
    "            dfc = df.copy()\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(lambda x: int(x) == N if x else True)]\n",
    "            col_runname = 'run_name' if chat_fmt != 'both' else ('run_name', '')\n",
    "            substitute = False\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, '_random_', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=10000:ep=10', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=50000:ep=5', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=150000:ep=3', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=150000:ep=1', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=100000', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=200000', substitute=substitute)\n",
    "            dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=(\\d)_pace=prune:size=400000', substitute=substitute)\n",
    "            #     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "            col = ('Average', '') if chat_fmt == 'both' else 'Average'\n",
    "        #     col = 'AlpacaFarm/WR'\n",
    "        #     col = 'MMLU/0-shot'|\n",
    "        #     col = 'GSM/CoT'\n",
    "        #     col = 'BBH/Direct'\n",
    "        #     col = 'TydiQA/GP'\n",
    "            dfc = dfc.sort_values(by=[col], ascending=False)\n",
    "            dfc = dfc.drop(columns=['model_args.model_name_or_path', 'data_args.subsample_mixture', 'data_args.max_train_samples'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            display(dfc\n",
    "                    .style\n",
    "                    .set_properties(**{'text-align': 'left'})\n",
    "                    .background_gradient(cmap ='coolwarm')\n",
    "                    .applymap(lambda x: 'text-decoration: underline;' \\\n",
    "                              if x in dfc[list(set(dfc.columns) & set([(x, '') for x in cols]))+[col] if chat_fmt=='both' else cols+[col]].values.flatten() and chat_fmt=='both' else '')\n",
    "                    .format(precision=1))\n",
    "    \n",
    "# llama-7b_tulu_v1_mix(paper)\n",
    "# MMLU/0-shot, MMLU/5-shot, GSM/Direct, GSM/CoT, BBH/Direct, BBH/CoT, TydiQA/GP, TydiQA/CB, CodexEval/Pass@1, AlpacaEval(vs.Davinci-003)\n",
    "# 44.8       , 47.1       , 7.0       , 25.0   , 38.5      , 38.5   , 43.5,    , 8.0      , 18.6,           , 48.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "358cd711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "with open('gen_cmds_run_cmds.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'][0]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcb043d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid chat_fmt: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_decr', 'checkpoint-*'))]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m save_dirs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(x), x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(exp_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama-7b_flan_v2:30k_kmeansl2_nc=3000_incr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint-*\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_eval_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_fmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_args_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_args_fields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m dfc \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# add base model performance\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/src/llm/evaluate.py:207\u001b[0m, in \u001b[0;36mget_eval_results\u001b[0;34m(save_dirs, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)): \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     r \u001b[38;5;241m=\u001b[39m EvalResults(save_dir, model_name)\n\u001b[0;32m--> 207\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m    209\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/src/llm/evaluate.py:181\u001b[0m, in \u001b[0;36mEvalResults.get_result_df\u001b[0;34m(self, chat_fmt, ft_args_fields, return_original_df, use_normalized_preferred_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_chatfmt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid chat_fmt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_fmt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ft_args_fields \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ft_args_fields[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid chat_fmt: None"
     ]
    }
   ],
   "source": [
    "# ylabel = 'llama-7b:600k'\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "ylabel = 'llama-7b_flan_v2:30k'\n",
    "exp_dir = '../results/oi4_perf_cross_time'\n",
    "# exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_decr', 'checkpoint-*'))]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "save_dirs += [(os.path.basename(x), x) for x in glob.glob(os.path.join(exp_dir, 'llama-7b_flan_v2:30k_kmeansl2_nc=3000_incr', 'checkpoint-*'))]\n",
    "df = get_eval_results(save_dirs, chat_fmt=None, ft_args_fields=ft_args_fields)\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "# add base model performance\n",
    "dfc.loc[dfc['model_args.model_name_or_path']=='huggyllama/llama-7b', 'model_args.model_name_or_path'] = 'checkpoint-0'\n",
    "# get steps \n",
    "dfc.insert(0, 'steps', dfc['model_args.model_name_or_path'].apply(lambda x: int(x.split('-')[-1])))\n",
    "dfc = dfc.sort_values('steps')\n",
    "\n",
    "\n",
    "y_labels_list = [\n",
    "    ['MMLU/0-shot',\n",
    "     'MMLU/0-shot_chatfmt',\n",
    "     'MMLU/5-shot',\n",
    "     'MMLU/5-shot_chatfmt',\n",
    "    ],\n",
    "    ['GSM/Direct',\n",
    "     'GSM/Direct_chatfmt',\n",
    "     'GSM/CoT', \n",
    "     'GSM/CoT_chatfmt', \n",
    "    ],\n",
    "    ['BBH/Direct',\n",
    "     'BBH/Direct_chatfmt',\n",
    "     'BBH/CoT',\n",
    "     'BBH/CoT_chatfmt',\n",
    "    ],\n",
    "    ['TydiQA/CB',\n",
    "     'TydiQA/CB_chatfmt',\n",
    "     'TydiQA/GP',\n",
    "     'TydiQA/GP_chatfmt',\n",
    "    ],\n",
    "    ['Codex-Eval/Pass@1',\n",
    "     'Codex-Eval/Pass@1_chatfmt'],\n",
    "    ['MMLU/0-shot',\n",
    "     'GSM/CoT',\n",
    "     'BBH/CoT',],\n",
    "]\n",
    "\n",
    "N = len(y_labels_list)\n",
    "\n",
    "fig, axs = plt.subplots(1,N,figsize=(5*N,5))\n",
    "\n",
    "axs[0].set_ylabel(ylabel, fontsize=20)\n",
    "\n",
    "for axi, y_labels in enumerate(y_labels_list):\n",
    "    ax = axs[axi]\n",
    "\n",
    "    x = dfc['steps']\n",
    "    y_list = []\n",
    "    for y_label in y_labels:\n",
    "        if y_label not in dfc.columns: continue\n",
    "        y = dfc[y_label].to_numpy()\n",
    "        y_list.append(y)\n",
    "        ax.plot(x, y, label=y_label)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_ylim(0, 55)\n",
    "    \n",
    "    \n",
    "# for y_label in ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1']:\n",
    "    \n",
    "#     for chat_fmt in ['', 'chatfmt']:\n",
    "#         col = '_'.join([y_label, chat_fmt]) if chat_fmt else y_label\n",
    "#         y = dfc[col].to_numpy()\n",
    "#         print(f'{col}\\t{y.mean():.2f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_labels = [\n",
    "    'Answer:\\n<|assistant|>\\nThe answer is:',\n",
    "    'Answer:\\n<|assistant|>\\n',\n",
    "    '<|assistant|>\\nAnswer:',\n",
    "    '<|assistant|>\\nThe answer is:',\n",
    "]\n",
    "x_labels = [f'v{i+1}:\\n{x}' for i,x in enumerate(x_labels)]\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc = df.filter(regex='_v|run')\n",
    "\n",
    "runs = dfc['run_name'].to_list()[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for axi, task in enumerate(['MMLU/0-shot', 'MMLU/5-shot']):\n",
    "\n",
    "    ax = axs[axi]\n",
    "    cols = [f'{task}_v{x}' for x in [1, 2, 3, 4]]\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    width = .25\n",
    "    multiplier = 0\n",
    "\n",
    "    for run in runs:\n",
    "        offset = width*multiplier\n",
    "        y = dfc[dfc['run_name']==run][cols].to_numpy().squeeze()\n",
    "        rects = ax.bar(x+offset, y, width, label=run)\n",
    "        ax.bar_label(rects, padding=3, fmt='{:.2f}')\n",
    "        multiplier += 1\n",
    "\n",
    "    ax.set_title(task)\n",
    "    ax.set_xticks(x+width)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(0, 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6ba4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_data_points = 200000 # 10000, 50000, 100000, 200000\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "        )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6323+40966+81933+81933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'results/ft1'\n",
    "\n",
    "# d = {\n",
    "#     'bbh_s=0': 'bbh_s=3',\n",
    "#     'gsm': 'gsm_s=8_cot',\n",
    "#     'mmlu': 'mmlu_s=0',\n",
    "#     'tydiqa_cb': 'tydiqa_s=1_cb',\n",
    "#     'tydiqa_gp': 'tydiqa_s=1_gp',\n",
    "# }\n",
    "\n",
    "# d.update({k+'_chatfmt': v+'_chatfmt' for k,v in d.items()})\n",
    "\n",
    "# for subdir in os.listdir(exp_dir):    \n",
    "#     for task_name_src, task_name_tgt in d.items():\n",
    "#         path_src = os.path.join(exp_dir, subdir, 'eval', task_name_src)\n",
    "#         path_tgt = os.path.join(exp_dir, subdir, 'eval', task_name_tgt)\n",
    "#         if os.path.isdir(path_src):\n",
    "# #             os.rename(path_src, path_tgt)\n",
    "#             print(path_src)\n",
    "#             print(path_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27138820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc = df.copy()\n",
    "dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "    lambda d: sum(list(d.values())) if d else 200000))\n",
    "# dfc[dfc['total_train_samples'].apply(\n",
    "#     lambda x: total_train_samples-500<x<total_train_samples+500)]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dată în istoria Uniunii Europene, la drepturile persoanelor care aparţin acestor minorităţi şi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
