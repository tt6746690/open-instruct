{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 20 16:18:54 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\r\n",
      "| N/A   29C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from llm.submit import multiline_to_singleline, submit_job_ccc, get_run_statistics\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf4f2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.submit import multiline_to_singleline\n",
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2281747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aimos job requires allocating at least 1 gpu. setting `num_gpus=1`.\n",
      "\n",
      "sbatch     --job-name=wpq-job     --partition=el8     --nodes=1     --ntasks=1     --cpus-per-task=1     --mem=3GB     --gres=gpu:1     --time=6:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shell_scripts = \\\n",
    "\"\"\"\n",
    "#!/bin/bash\n",
    "source ~/.profile\n",
    "conda activate wpq-llm\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/\n",
    "jupyter notebook --ip=0.0.0.0 --no-browser --port=8001\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "shell_scripts = \"\"\"\n",
    "echo hi\n",
    "echo foo\n",
    "\"\"\"\n",
    "job_name='wpq-job'\n",
    "partition='el8'\n",
    "num_cpus=1\n",
    "cpu_mem=3\n",
    "num_gpus=0\n",
    "gpu_type=None\n",
    "log_dir=None\n",
    "log_filename='%J.out'\n",
    "test_run=False\n",
    "job_duration=None\n",
    "shell_scripts_modification_fn=None\n",
    "\n",
    "if log_dir is None:\n",
    "    log_dir = os.getcwd()\n",
    "if not shell_scripts.strip(' \\n').startswith('#!'):\n",
    "    shell_scripts = '#!/bin/bash\\n'+shell_scripts\n",
    "    \n",
    "if partition != 'el8':\n",
    "    raise ValueError(f'only `el8` partition allowed, got {partition}.')\n",
    "if gpu_type is not None and gpu_type != 'v100':\n",
    "    raise ValueError(f'aimos has v100 gpu only, {gpu_type} not supported.')\n",
    "if num_gpus <= 0:\n",
    "    num_gpus = 1\n",
    "    print('aimos job requires allocating at least 1 gpu. setting `num_gpus=1`.')\n",
    "\n",
    "    \n",
    "std_out_dir = os.path.join(log_dir, log_filename)\n",
    "\n",
    "jbsub_cmd = f\"\"\"\n",
    "sbatch \\\n",
    "    --job-name={job_name} \\\n",
    "    --partition={partition} \\\n",
    "    --nodes=1 \\\n",
    "    --ntasks=1 \\\n",
    "    --cpus-per-task={num_cpus} \\\n",
    "    --mem={cpu_mem}GB \\\n",
    "    --gres=gpu:{num_gpus} \\\n",
    "    --time=6:00:00\n",
    "\"\"\"\n",
    "print(jbsub_cmd)\n",
    "jbsub_cmd = multiline_to_singleline(jbsub_cmd)\n",
    "jbsub_cmd = shlex.split(jbsub_cmd)\n",
    "\n",
    "\n",
    "\n",
    "# shell_scripts_cmd = shell_scripts.strip().split('\\n')\n",
    "# shell_scripts_cmd = [x for x in shell_scripts_cmd if x != '']\n",
    "# shell_scripts_cmd = '; '.join(shell_scripts_cmd)\n",
    "# shell_scripts_cmd = [f\"bash -c '{shell_scripts_cmd}'\"]\n",
    "\n",
    "# args = jbsub_cmd + shell_scripts_cmd\n",
    "# ' '.join(args)\n",
    "\n",
    "import tempfile\n",
    "temp_file = tempfile.NamedTemporaryFile(delete=False, dir='.', mode='w')\n",
    "temp_file.write(shell_scripts)\n",
    "temp_file.flush()\n",
    "temp_file_path = temp_file.name\n",
    "os.chmod(temp_file_path, 0o755)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86d6765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --job-name=wpq-job --partition=el8 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=3GB --gres=gpu:1 --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpw292uvcf\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(jbsub_cmd+[temp_file_path]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b097b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 681221\r\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=wpq-job --partition=el8 --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=3GB --gres=gpu:1 --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpw292uvcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "648b57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "import os\n",
    "os.remove(temp_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}*.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}*.out\" \"{save_dir}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir)\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|▏         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "job_name = 'ft-trainer'\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "job_duration = None\n",
    "shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 32\n",
    "cpu_mem = 64\n",
    "num_gpus = 1; require = 'a100_80gb'\n",
    "\n",
    "overwrite_output_dir = True\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "\n",
    "num_train_epochs = 2\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "\n",
    "fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"\n",
    "if 'gpt2' in model_name_or_path: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in model_name_or_path: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 128 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "gradient_checkpointing = True\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "abbr_model_name = model_name_or_path.replace('/', ':')\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "if test_run:\n",
    "    output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "exe = 'python' if num_gpus == 1 else f\"torchrun --nproc_per_node={num_gpus} --master_port=10001\"\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}{exe}\n",
    "    open_instruct/finetune_trainer.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    {'--lora_rank '+str(lora_rank) if use_lora else ''}\n",
    "    {'--lora_alpha '+str(lora_alpha) if use_lora else ''}\n",
    "    {'--lora_dropout '+str(lora_dropout) if use_lora else ''}\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --save_strategy {save_strategy} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --save_total_limit 1 \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    {'--fsdp \"'+fsdp+'\"' if fsdp else ''}\n",
    "    {'--fsdp_transformer_layer_cls_to_wrap \"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "        if fsdp else ''}\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "    --bf16 True \\\n",
    "    --tf32 True \\\n",
    "    --report_to tensorboard \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    --output_dir \"{output_dir}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    "    job_duration=job_duration,\n",
    "    shell_scripts_modification_fn=shell_scripts_modification_fn,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30877747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!cd .. && python open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 256 --lora_alpha 256 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 1 --num_train_epochs 2 --bf16 True --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'mmlu'\n",
    "task_name = 'gsm'\n",
    "task_name = 'bbh_s=0'\n",
    "task_name = 'bbh_s=3'\n",
    "# task_name = 'humaneval'\n",
    "job_name = f'eval.{task_name}'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "use_chat_format = True\n",
    "eval_final_model = True\n",
    "\n",
    "batch_size = 10\n",
    "if task_name == 'gsm':\n",
    "    queue = 'x86_1h' # 10min for n=200\n",
    "if task_name == 'bbh_s=0':\n",
    "    queue = 'x86_1h'\n",
    "if task_name == 'bbh_s=3':\n",
    "    queue = 'x86_12h'\n",
    "    batch_size = 5 # for longer prompts.\n",
    "if task_name == 'mmlu':\n",
    "    queue = 'x86_1h'\n",
    "    batch_size = 10\n",
    "if task_name == 'humaneval':\n",
    "    queue = 'x86_1h' # pass@1: 10min, pass@10: 100min\n",
    "    batch_size = 10\n",
    "    \n",
    "num_cpus = 10\n",
    "cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "models += [os.path.join('results/baselines', x) for x in [\n",
    "    'huggyllama/llama-7b',  # , 'mosaicml/mpt-7b'\n",
    "]]\n",
    "\n",
    "# models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "# models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "# models = [x for l in models for x in l]\n",
    "\n",
    "models += [x if eval_final_model else get_last_checkpoint(x) for x in [\n",
    "#     'results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix',\n",
    "    'results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix',\n",
    "#     'results/huggyllama:llama-7b_humanmix',\n",
    "]]\n",
    "\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    if use_chat_format:\n",
    "        save_dir += '_chatfmt'\n",
    "    \n",
    "    if task_name == 'mmlu':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain 0 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name == 'gsm':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 200 \\\n",
    "            --n_shot 8 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('bbh'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_cot' if 's=0' in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "            {'--load_in_8bit' if load_in_8bit else ''}\n",
    "        \"\"\"\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job_ccc(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        queue=queue,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "    )\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir, run_name=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        self.run_name = run_name if run_name else self.save_dir\n",
    "        \n",
    "    def get_result_df(self):\n",
    "\n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "        print(task_names)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "\n",
    "            columns = [f'{task_name}/{k}' for k in metrics.keys()]\n",
    "            # columns = pd.MultiIndex.from_tuples(columns)\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        mapper = {\n",
    "            'mmlu/average_acc': 'MMLU/0-shot',\n",
    "            'mmlu_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'gsm/exact_match': 'GSM/CoT',\n",
    "            'gsm_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            'bbh_s=0/average_exact_match': 'BBH/Direct',\n",
    "            'bbh_s=0_chatfmt/average_exact_match': 'BBH/Direct_chatfmt',\n",
    "            'bbh_s=3/average_exact_match': 'BBH/CoT', \n",
    "            'bbh_s=3_chatfmt/average_exact_match': 'BBH/CoT_chatfmt',\n",
    "            'humaneval/pass@1': 'Codex-Eval/Pass@1',\n",
    "            'humaneval_chatfmt/pass@1': 'Codex-Eval/Pass@1_chatfmt',\n",
    "        }\n",
    "        cols = [col for col in mapper.keys() if col in df]\n",
    "        for col in cols:\n",
    "            df[col] = df[col]*100\n",
    "        df = df[cols]\n",
    "        df = df.rename(columns=mapper)\n",
    "        df.insert(0, 'Model', [self.run_name])\n",
    "        return df\n",
    "\n",
    "# get_last_checkpoint(v)\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "    ('llama-7b+humanmix', '../results/huggyllama:llama-7b_humanmix'),\n",
    "    ('llama-7b+lora(r=8,a=8)', '../results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix'),\n",
    "    ('llama-7b+lora(r=128,a=128)', '../results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix'),\n",
    "]\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model_name, save_dir in save_dirs:\n",
    "    r = EvalResults(save_dir, model_name)\n",
    "    df = r.get_result_df()\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    dfc = df.copy()\n",
    "    cols = [x.split('_') for x in df.columns]\n",
    "    cols = [x+[''] if len(x)==1 else x for x in cols]\n",
    "    dfc.columns = pd.MultiIndex.from_tuples(cols)\n",
    "    display(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efef1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
