{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 18 10:24:00 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:C7:00.0 Off |                    0 |\r\n",
      "| N/A   24C    P0    53W / 400W |      0MiB / 40960MiB |      0%   E. Process |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from llm.submit import multiline_to_singleline, submit_job_ccc, get_run_statistics\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5607ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction tune human-mix on 1 a100_40g:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>size</th>\n",
       "      <th>mixed-precision</th>\n",
       "      <th>deepspeed</th>\n",
       "      <th>gpu mem (GB)</th>\n",
       "      <th>cpu mem (GB)</th>\n",
       "      <th>per-epoch time (hr)</th>\n",
       "      <th>per-iter time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-Large</td>\n",
       "      <td>0.774</td>\n",
       "      <td>bf16</td>\n",
       "      <td>no</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   size mixed-precision deepspeed  gpu mem (GB)  cpu mem (GB)  \\\n",
       "0  gpt2-Large  0.774            bf16        no            36             7   \n",
       "\n",
       "   per-epoch time (hr)  per-iter time (s)  \n",
       "0                  9.5                  9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem usage (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "\n",
    "# 55k data points, batch_size=128\n",
    "data_oasst1 = [\n",
    "    ('gpt2', 0.124, 'bf16', 'no', 10, None, None),\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 2.5, 11),\n",
    "    # incorporate deep speed is costly!\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'stage 3 no offloading', 40, 6, 25),\n",
    "    # 1 a100_40g: without offloading OOM on `.backward()`, runs fine with offloading.\n",
    "    ('gpt2-xl', 1.5, 'bf16', 'stage 3 with offloading', 40, 13, 55),\n",
    "    # 4 v100_32g: without offloading.\n",
    "]\n",
    "\n",
    "df_oasst1 = pd.DataFrame(data_oasst1, columns=cols)\n",
    "\n",
    "cols = ['model', 'size', 'mixed-precision', 'deepspeed', \n",
    "        'gpu mem (GB)', 'cpu mem (GB)', 'per-epoch time (hr)', 'per-iter time (s)']\n",
    "data = [\n",
    "    ('gpt2-Large', 0.774, 'bf16', 'no', 36, 7, 9.5, 9),\n",
    "]\n",
    "\n",
    "print('instruction tune human-mix on 1 a100_40g:')\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b984427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}*.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}*.out\" \"{save_dir}\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 2 GPUs, 2 batch size per GPU, 32 gradient accumulation steps.\n",
      "\n",
      "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 2 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 2 --gradient_accumulation_steps 32 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir jpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft'\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "queue = 'x86_12h' # 'x86_12h'\n",
    "num_cpus = 20\n",
    "num_gpus = 1\n",
    "cpu_mem = 32\n",
    "require = 'a100_80gb'\n",
    "\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "if test_run:\n",
    "    output_dir = 'jpt_' + output_dir\n",
    "\n",
    "use_deepspeed = False\n",
    "# deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "use_lora = True\n",
    "lora_rank = 4\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "#     --use_slow_tokenizer \\\n",
    "# do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "#     --use_flash_attn \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines 1 \\\n",
    "    --num_processes {num_gpus} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''}\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "    open_instruct/finetune.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    --lora_rank {lora_rank} \\\n",
    "    --lora_alpha {lora_alpha} \\\n",
    "    --lora_dropout {lora_dropout} \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "    --logging_steps 1\n",
    "\"\"\"\n",
    "\n",
    "# things to test to see its effects on (1) eval perf (2) runtime.\n",
    "#\n",
    "# - int8\n",
    "# - mixed_precision bf16 or no\n",
    "# - with/without LoRA\n",
    "# - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# - batch size\n",
    "# - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions'), PosixPath('/usr/local/share/zsh/site-functions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1832148.tmpdir/.1689173487.1832148.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/12/2023 12:59:09 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "07/12/2023 12:59:09 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 292.59it/s]\n",
      "loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:50<00:00, 25.41s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Assigning <s> to the bos_token key of the tokenizer\n",
      "Assigning </s> to the eos_token key of the tokenizer\n",
      "Assigning <unk> to the unk_token key of the tokenizer\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/12/2023 13:00:49 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 2097152 || all params: 6740520960 || trainable%: 0.03111261002591705\n",
      "GPU memory occupied: 837 MB.\n",
      "07/12/2023 13:01:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1038c98439ac66d6_*_of_00016.arrow\n",
      "07/12/2023 13:01:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-88ed2eb4a77829ae.arrow\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 134049 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29925,  1745,   895,\n",
      "        29901,   376,  1576,  7306,   347,   310,   385, 14890, 18881,  3815,\n",
      "          338,  1641, 15569,   373,   408,  3196,  5144,   310,   670,  3815,\n",
      "         6505,  1213,    13, 29933,  1463,   373,   445,  5188,   895, 29892,\n",
      "          508,   591, 17668,   393,   278, 20051,   376, 29909,  7306,   347,\n",
      "         3907, 27401,  1213,   338,  1565, 29973,    13,  5856, 29901,    13,\n",
      "        29899,  4874,    13, 29899,   372,   338,   451,  1950,   304,  2649,\n",
      "           13, 29899,   694,  2567, 29892,  1235, 29915, 29879,   367, 16232,\n",
      "          408,  1950, 29889,  3834,  7291,   937, 29901,    13, 29966, 29989,\n",
      "          465, 22137, 29989, 29958,    13, 29909,  7306,   347,  2609,   367,\n",
      "         3907,  4078,   565,   540,   338,  1641, 15569,   373, 29889,  1105,\n",
      "        29892,   278,  1234,   338,   694, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100, 29909,  7306,   347,  2609,   367,\n",
      "         3907,  4078,   565,   540,   338,  1641, 15569,   373, 29889,  1105,\n",
      "        29892,   278,  1234,   338,   694, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 180142 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  5618,   338,   278,\n",
      "        11564,   310,  1554,  1641,   619,  1717,   304,  1554,  1683, 29973,\n",
      "           13,  5856, 29901,    13, 29899,  2313,   824,    13, 29899, 20820,\n",
      "           13, 29899,  1539,   481,  2015,    13, 29899,  2313,  5632, 16976,\n",
      "           13, 29899, 12814,    13, 12024, 29915, 29879,  4505,   372,  5232,\n",
      "        29889,  1281,   509,   579,   338,   263,  2106,   310,  1641, 19492,\n",
      "        11687,  1422,   515,  1554,  1683,   297,  3623,   486,   481,  4490,\n",
      "          470,  3802, 15477, 29889,  1281,   509,   579,   338,   278, 11564,\n",
      "          310,  1554,  1641,   619,  1717,   304,  1554,  1683,  3045,  1105,\n",
      "          278,  1234,   338, 12814, 29889,    13,    13, 11921,   297,   596,\n",
      "        19546, 12713,   508,   366,  3013,   366,  2343, 19531, 29973,    13,\n",
      "         5856, 29901,    13, 29899,  3762,    13, 29899,  2919,  3271,    13,\n",
      "        29899,  3699,    13, 29899,  3056,  1153,   384,    13, 29899,  4694,\n",
      "          300,    13, 12024, 29915, 29879,  4505,   372,  5232, 29889, 12252,\n",
      "        19531,   508,   367,  8126,   472,  3056,  1153,   384, 29889,  1670,\n",
      "         1795,   367,  3056,  1153,   384,   338,   297,   596, 19546, 12713,\n",
      "         3045,  1105,   278,  1234,   338,  3056,  1153,   384, 29889,    13,\n",
      "           13,  1576, 13013,   471,  6820, 18872,   304, 13175, 29892,   825,\n",
      "          892,   278, 13175, 16743,   411, 29973,    13,  5856, 29901,    13,\n",
      "        29899, 12045,  5622,    13, 29899,   437,   290,    13, 29899, 19912,\n",
      "         4045,    13, 29899,  2898,  3527,    13, 29899,  8444,  2264,    13,\n",
      "        12024, 29915, 29879,  4505,   372,  5232, 29889, 21882, 25700,  2367,\n",
      "        18872,   304,  6460,   322,   817, 29891, 13175, 29889,  3929,   272,\n",
      "          322,   817, 29891,  5304,  1090,  2898,  3527,  3045,  1105,   278,\n",
      "         1234,   338,  2898,  3527, 29889,    13,    13,  1576, 12736,   368,\n",
      "          471,  1925,   964,   263,   260,  4003, 29892,   988,   471,   372,\n",
      "         3216, 29973,    13,  5856, 29901,    13, 29899,  1016,  8842,    13,\n",
      "        29899,  1591,    13, 29899,  1236,   273,   329,   541,   357,    13,\n",
      "        29899, 14631,    13, 29899,   337,  1341,  4087,  1061,    13, 29966,\n",
      "        29989,   465, 22137, 29989, 29958,    13, 12024, 29915, 29879,  4505,\n",
      "          372,  5232, 29889,  3872,  8842,   338,   263,  2319,   285,  1255,\n",
      "          274,  1296,   310,  7901,  6302,   287,   270,   820, 29892, 12234,\n",
      "          297,   278,  8267,   310,   263,  8287,   470,  9228, 29889,   450,\n",
      "        12736,   368,   471,  1925,   964,   263,   260,  4003, 29936,   372,\n",
      "          471,  3216,   297,  1016,  8842,  3045,  1105,   278,  1234,   338,\n",
      "         1016,  8842, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100, 12024, 29915, 29879,  4505,\n",
      "          372,  5232, 29889,  3872,  8842,   338,   263,  2319,   285,  1255,\n",
      "          274,  1296,   310,  7901,  6302,   287,   270,   820, 29892, 12234,\n",
      "          297,   278,  8267,   310,   263,  8287,   470,  9228, 29889,   450,\n",
      "        12736,   368,   471,  1925,   964,   263,   260,  4003, 29936,   372,\n",
      "          471,  3216,   297,  1016,  8842,  3045,  1105,   278,  1234,   338,\n",
      "         1016,  8842, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n",
      "07/12/2023 13:01:05 - INFO - __main__ - Sample 26934 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  4300,  9632,   304,\n",
      "         5176, 29901,    13,    13, 29930,   830,   790,  6742,   363, 16905,\n",
      "         9590, 29889,    13, 29966, 29989,   465, 22137, 29989, 29958,    13,\n",
      "        29930,   405,  6223,   585,   260,  3055,   479,  1671,  1153, 14125,\n",
      "        13698, 29889,     2]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "        29930,   405,  6223,   585,   260,  3055,   479,  1671,  1153, 14125,\n",
      "        13698, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/12/2023 13:01:07 - INFO - __main__ - ***** Running training *****\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) =  Num examples = 270152\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Num Epochs = 2\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/12/2023 13:01:07 - INFO - __main__ -  128\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Gradient Accumulation steps = 128\n",
      "07/12/2023 13:01:07 - INFO - __main__ -   Total optimization steps = 4222\n",
      "  0%|                                                  | 0/4222 [00:00<?, ?it/s]before train loop:\n",
      "GPU memory occupied: 14747 MB.\n",
      "torch.cuda.memory_allocated():  13552361472\n",
      "model.device: cuda:0\n",
      "model.dtype: torch.bfloat16\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                       | 1/4222 [00:31<37:07:52, 31.67s/it]07/12/2023 13:01:39 - INFO - __main__ -   Step: 1, LR: 1.5873015873015874e-07, Loss: 2.1495587825775146\n",
      "GPU memory occupied: 64167 MB.\n",
      "torch.cuda.memory_allocated():  13856768512\n",
      "  0%|                                       | 2/4222 [01:01<35:51:27, 30.59s/it]07/12/2023 13:02:09 - INFO - __main__ -   Step: 2, LR: 3.174603174603175e-07, Loss: 2.2048227787017822\n",
      "GPU memory occupied: 64941 MB.\n",
      "torch.cuda.memory_allocated():  13600457216\n",
      "  0%|                                       | 3/4222 [01:25<32:11:57, 27.48s/it]07/12/2023 13:02:33 - INFO - __main__ -   Step: 3, LR: 4.7619047619047623e-07, Loss: 2.066133737564087\n",
      "GPU memory occupied: 64941 MB.\n",
      "torch.cuda.memory_allocated():  13647059968\n",
      "  0%|                                       | 4/4222 [01:50<30:56:53, 26.41s/it]07/12/2023 13:02:57 - INFO - __main__ -   Step: 4, LR: 6.34920634920635e-07, Loss: 2.149609088897705\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13616460800\n",
      "  0%|                                       | 5/4222 [02:12<29:27:35, 25.15s/it]07/12/2023 13:03:20 - INFO - __main__ -   Step: 5, LR: 7.936507936507937e-07, Loss: 2.120454788208008\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13791988224\n",
      "  0%|                                       | 6/4222 [02:41<30:40:37, 26.19s/it]07/12/2023 13:03:49 - INFO - __main__ -   Step: 6, LR: 9.523809523809525e-07, Loss: 1.988757848739624\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13606858752\n",
      "  0%|                                       | 7/4222 [03:04<29:43:35, 25.39s/it]07/12/2023 13:04:12 - INFO - __main__ -   Step: 7, LR: 1.111111111111111e-06, Loss: 2.1727051734924316\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13607242752\n",
      "  0%|                                       | 8/4222 [03:26<28:09:31, 24.06s/it]07/12/2023 13:04:34 - INFO - __main__ -   Step: 8, LR: 1.26984126984127e-06, Loss: 2.1439826488494873\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13618892800\n",
      "  0%|                                       | 9/4222 [03:47<27:16:21, 23.30s/it]07/12/2023 13:04:55 - INFO - __main__ -   Step: 9, LR: 1.4285714285714286e-06, Loss: 2.0475826263427734\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13606730752\n",
      "  0%|                                      | 10/4222 [04:09<26:37:32, 22.76s/it]07/12/2023 13:05:17 - INFO - __main__ -   Step: 10, LR: 1.5873015873015873e-06, Loss: 2.175341844558716\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13640785920\n",
      "  0%|                                      | 11/4222 [04:31<26:17:33, 22.48s/it]07/12/2023 13:05:39 - INFO - __main__ -   Step: 11, LR: 1.746031746031746e-06, Loss: 2.0671279430389404\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13609802752\n",
      "  0%|                                      | 12/4222 [04:51<25:24:28, 21.73s/it]07/12/2023 13:05:59 - INFO - __main__ -   Step: 12, LR: 1.904761904761905e-06, Loss: 1.9760054349899292\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13613132800\n",
      "  0%|                                      | 13/4222 [05:15<26:19:37, 22.52s/it]07/12/2023 13:06:23 - INFO - __main__ -   Step: 13, LR: 2.0634920634920634e-06, Loss: 2.154667377471924\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13631056384\n",
      "  0%|▏                                     | 14/4222 [05:36<25:46:28, 22.05s/it]07/12/2023 13:06:44 - INFO - __main__ -   Step: 14, LR: 2.222222222222222e-06, Loss: 2.0096664428710938\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13611347456\n",
      "  0%|▏                                     | 15/4222 [05:57<25:20:09, 21.68s/it]07/12/2023 13:07:05 - INFO - __main__ -   Step: 15, LR: 2.380952380952381e-06, Loss: 1.9334514141082764\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13617740800\n",
      "  0%|▏                                     | 16/4222 [06:16<24:27:22, 20.93s/it]07/12/2023 13:07:24 - INFO - __main__ -   Step: 16, LR: 2.53968253968254e-06, Loss: 2.1369714736938477\n",
      "GPU memory occupied: 64947 MB.\n",
      "torch.cuda.memory_allocated():  13622862336\n",
      "  0%|▏                                     | 17/4222 [06:40<25:29:42, 21.83s/it]07/12/2023 13:07:48 - INFO - __main__ -   Step: 17, LR: 2.6984126984126986e-06, Loss: 2.1751840114593506\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13624782336\n",
      "  0%|▏                                     | 18/4222 [07:00<25:00:18, 21.41s/it]07/12/2023 13:08:08 - INFO - __main__ -   Step: 18, LR: 2.8571428571428573e-06, Loss: 2.0311388969421387\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13606090752\n",
      "  0%|▏                                     | 19/4222 [07:21<24:51:09, 21.29s/it]07/12/2023 13:08:29 - INFO - __main__ -   Step: 19, LR: 3.015873015873016e-06, Loss: 1.9443316459655762\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607370752\n",
      "  0%|▏                                     | 20/4222 [07:41<24:15:36, 20.78s/it]07/12/2023 13:08:49 - INFO - __main__ -   Step: 20, LR: 3.1746031746031746e-06, Loss: 2.0267834663391113\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13599817216\n",
      "  0%|▏                                     | 21/4222 [08:04<24:57:26, 21.39s/it]07/12/2023 13:09:12 - INFO - __main__ -   Step: 21, LR: 3.3333333333333333e-06, Loss: 2.293339490890503\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13608522752\n",
      "  1%|▏                                     | 22/4222 [08:26<25:18:40, 21.70s/it]07/12/2023 13:09:34 - INFO - __main__ -   Step: 22, LR: 3.492063492063492e-06, Loss: 2.053500175476074\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13696606720\n",
      "  1%|▏                                     | 23/4222 [08:48<25:24:58, 21.79s/it]07/12/2023 13:09:56 - INFO - __main__ -   Step: 23, LR: 3.6507936507936507e-06, Loss: 2.0365335941314697\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13631952384\n",
      "  1%|▏                                     | 24/4222 [09:08<24:48:56, 21.28s/it]07/12/2023 13:10:16 - INFO - __main__ -   Step: 24, LR: 3.80952380952381e-06, Loss: 2.125044584274292\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13643729920\n",
      "  1%|▏                                     | 25/4222 [09:33<25:56:59, 22.26s/it]07/12/2023 13:10:41 - INFO - __main__ -   Step: 25, LR: 3.968253968253968e-06, Loss: 2.2882473468780518\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13639505920\n",
      "  1%|▏                                     | 26/4222 [09:54<25:39:15, 22.01s/it]07/12/2023 13:11:02 - INFO - __main__ -   Step: 26, LR: 4.126984126984127e-06, Loss: 2.098587989807129\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13633360384\n",
      "  1%|▏                                     | 27/4222 [10:15<25:03:45, 21.51s/it]07/12/2023 13:11:22 - INFO - __main__ -   Step: 27, LR: 4.2857142857142855e-06, Loss: 2.2885560989379883\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13599177216\n",
      "  1%|▎                                     | 28/4222 [10:39<26:05:23, 22.39s/it]07/12/2023 13:11:47 - INFO - __main__ -   Step: 28, LR: 4.444444444444444e-06, Loss: 1.9634195566177368\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13646931968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▎                                     | 29/4222 [11:00<25:30:06, 21.90s/it]07/12/2023 13:12:08 - INFO - __main__ -   Step: 29, LR: 4.603174603174604e-06, Loss: 2.1846425533294678\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13609034752\n",
      "  1%|▎                                     | 30/4222 [11:24<26:26:01, 22.70s/it]07/12/2023 13:12:32 - INFO - __main__ -   Step: 30, LR: 4.761904761904762e-06, Loss: 2.024883270263672\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13641553920\n",
      "  1%|▎                                     | 31/4222 [11:49<27:12:57, 23.38s/it]07/12/2023 13:12:57 - INFO - __main__ -   Step: 31, LR: 4.920634920634921e-06, Loss: 2.1247711181640625\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13728229888\n",
      "  1%|▎                                     | 32/4222 [12:10<26:22:46, 22.67s/it]07/12/2023 13:13:18 - INFO - __main__ -   Step: 32, LR: 5.07936507936508e-06, Loss: 2.2556421756744385\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13748969472\n",
      "  1%|▎                                     | 33/4222 [12:32<26:05:45, 22.43s/it]07/12/2023 13:13:40 - INFO - __main__ -   Step: 33, LR: 5.2380952380952384e-06, Loss: 2.040525197982788\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13721059840\n",
      "  1%|▎                                     | 34/4222 [12:56<26:30:54, 22.79s/it]07/12/2023 13:14:04 - INFO - __main__ -   Step: 34, LR: 5.396825396825397e-06, Loss: 1.9864712953567505\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13655893504\n",
      "  1%|▎                                     | 35/4222 [13:20<26:50:51, 23.08s/it]07/12/2023 13:14:28 - INFO - __main__ -   Step: 35, LR: 5.555555555555557e-06, Loss: 2.0973119735717773\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13807221760\n",
      "  1%|▎                                     | 36/4222 [13:41<26:15:48, 22.59s/it]07/12/2023 13:14:49 - INFO - __main__ -   Step: 36, LR: 5.7142857142857145e-06, Loss: 1.9493706226348877\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13621966336\n",
      "  1%|▎                                     | 37/4222 [14:03<26:08:18, 22.48s/it]07/12/2023 13:15:11 - INFO - __main__ -   Step: 37, LR: 5.873015873015874e-06, Loss: 2.276886463165283\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13666007552\n",
      "  1%|▎                                     | 38/4222 [14:26<26:20:50, 22.67s/it]07/12/2023 13:15:34 - INFO - __main__ -   Step: 38, LR: 6.031746031746032e-06, Loss: 2.124443292617798\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607626752\n",
      "  1%|▎                                     | 39/4222 [14:50<26:49:37, 23.09s/it]07/12/2023 13:15:58 - INFO - __main__ -   Step: 39, LR: 6.1904761904761914e-06, Loss: 2.173035144805908\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13665239552\n",
      "  1%|▎                                     | 40/4222 [15:13<26:40:05, 22.96s/it]07/12/2023 13:16:21 - INFO - __main__ -   Step: 40, LR: 6.349206349206349e-06, Loss: 2.138033628463745\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13613388800\n",
      "  1%|▎                                     | 41/4222 [15:34<25:58:14, 22.36s/it]07/12/2023 13:16:42 - INFO - __main__ -   Step: 41, LR: 6.507936507936509e-06, Loss: 2.203645706176758\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13607626752\n",
      "  1%|▍                                     | 42/4222 [15:56<25:45:23, 22.18s/it]07/12/2023 13:17:04 - INFO - __main__ -   Step: 42, LR: 6.666666666666667e-06, Loss: 2.1471872329711914\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13617868800\n",
      "  1%|▍                                     | 43/4222 [16:15<24:49:39, 21.39s/it]07/12/2023 13:17:23 - INFO - __main__ -   Step: 43, LR: 6.825396825396826e-06, Loss: 2.0088717937469482\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13643089920\n",
      "  1%|▍                                     | 44/4222 [16:39<25:42:27, 22.15s/it]07/12/2023 13:17:47 - INFO - __main__ -   Step: 44, LR: 6.984126984126984e-06, Loss: 2.1421897411346436\n",
      "GPU memory occupied: 65457 MB.\n",
      "torch.cuda.memory_allocated():  13605578752\n",
      "  1%|▍                                     | 45/4222 [17:02<26:03:43, 22.46s/it]07/12/2023 13:18:10 - INFO - __main__ -   Step: 45, LR: 7.1428571428571436e-06, Loss: 2.163747549057007\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13642193920\n",
      "  1%|▍                                     | 46/4222 [17:27<26:37:44, 22.96s/it]07/12/2023 13:18:35 - INFO - __main__ -   Step: 46, LR: 7.301587301587301e-06, Loss: 1.9889566898345947\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13806709760\n",
      "  1%|▍                                     | 47/4222 [17:49<26:25:59, 22.79s/it]07/12/2023 13:18:57 - INFO - __main__ -   Step: 47, LR: 7.460317460317461e-06, Loss: 2.114711046218872\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13665111552\n",
      "  1%|▍                                     | 48/4222 [18:14<27:03:40, 23.34s/it]07/12/2023 13:19:22 - INFO - __main__ -   Step: 48, LR: 7.61904761904762e-06, Loss: 1.8832544088363647\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13607882752\n",
      "  1%|▍                                     | 49/4222 [18:43<29:03:37, 25.07s/it]07/12/2023 13:19:51 - INFO - __main__ -   Step: 49, LR: 7.77777777777778e-06, Loss: 2.012969732284546\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13758701056\n",
      "  1%|▍                                     | 50/4222 [19:06<28:23:18, 24.50s/it]07/12/2023 13:20:14 - INFO - __main__ -   Step: 50, LR: 7.936507936507936e-06, Loss: 1.881207823753357\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13611852800\n",
      "  1%|▍                                     | 51/4222 [19:29<27:54:22, 24.09s/it]07/12/2023 13:20:37 - INFO - __main__ -   Step: 51, LR: 8.095238095238097e-06, Loss: 1.9218477010726929\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13642321920\n",
      "  1%|▍                                     | 52/4222 [19:55<28:30:00, 24.60s/it]07/12/2023 13:21:03 - INFO - __main__ -   Step: 52, LR: 8.253968253968254e-06, Loss: 2.3106133937835693\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13809271808\n",
      "  1%|▍                                     | 53/4222 [20:20<28:34:53, 24.68s/it]07/12/2023 13:21:28 - INFO - __main__ -   Step: 53, LR: 8.412698412698414e-06, Loss: 1.8654088973999023\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13638353920\n",
      "  1%|▍                                     | 54/4222 [20:44<28:25:02, 24.54s/it]07/12/2023 13:21:52 - INFO - __main__ -   Step: 54, LR: 8.571428571428571e-06, Loss: 2.1221580505371094\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13630800384\n",
      "  1%|▍                                     | 55/4222 [21:07<28:02:57, 24.23s/it]07/12/2023 13:22:15 - INFO - __main__ -   Step: 55, LR: 8.730158730158731e-06, Loss: 2.2923853397369385\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13611347456\n",
      "  1%|▌                                     | 56/4222 [21:30<27:21:38, 23.64s/it]07/12/2023 13:22:38 - INFO - __main__ -   Step: 56, LR: 8.888888888888888e-06, Loss: 2.0254714488983154\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13610058752\n",
      "  1%|▌                                     | 57/4222 [21:51<26:32:53, 22.95s/it]07/12/2023 13:22:59 - INFO - __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.0491456985473633\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13601737216\n",
      "  1%|▌                                     | 58/4222 [22:12<26:02:31, 22.51s/it]07/12/2023 13:23:20 - INFO - __main__ -   Step: 58, LR: 9.206349206349207e-06, Loss: 2.109394073486328\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13667543552\n",
      "  1%|▌                                     | 59/4222 [22:34<25:46:58, 22.30s/it]07/12/2023 13:23:42 - INFO - __main__ -   Step: 59, LR: 9.365079365079366e-06, Loss: 2.172915458679199\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13666903552\n",
      "  1%|▌                                     | 60/4222 [22:58<26:08:58, 22.62s/it]07/12/2023 13:24:06 - INFO - __main__ -   Step: 60, LR: 9.523809523809525e-06, Loss: 2.035609483718872\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13603402752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▌                                     | 61/4222 [23:21<26:16:36, 22.73s/it]07/12/2023 13:24:29 - INFO - __main__ -   Step: 61, LR: 9.682539682539683e-06, Loss: 1.951716661453247\n",
      "GPU memory occupied: 65969 MB.\n",
      "torch.cuda.memory_allocated():  13657301504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|▏         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "!cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training huggyllama/llama-7b using 2 GPUs, 1 batch size per GPU, 64 gradient accumulation steps.\n",
      "[{'jbsub_cmd': 'jbsub -queue x86_6h -name ft-trainer -mem 64g -cores 1x32+2 -require a100_80gb -out /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/scripts/%J.out bash -c \\'export OPENAI_API_KEY=$(cat ~/.openai_api_key); export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"; source /dccstor/mit_fm/miniconda/bin/activate open-instruct; cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/; echo \"Running on $(hostname)\"; echo \"======\"; echo torchrun --nproc_per_node=2 --master_port=10001 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 64 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 100 --save_total_limit 1 --num_train_epochs 2 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap \"LlamaDecoderLayer\" --gradient_checkpointing --bf16 True --tf32 True --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/huggyllama:llama-7b_humanmix\"; echo \"======\"; g; torchrun --nproc_per_node=2 --master_port=10001 open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 64 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 100 --save_total_limit 1 --num_train_epochs 2 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap \"LlamaDecoderLayer\" --gradient_checkpointing --bf16 True --tf32 True --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/huggyllama:llama-7b_humanmix\"\\'', 'job_id': 1951639}]\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft-trainer'\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "job_duration = None\n",
    "shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "queue = 'x86_6h' # 'x86_12h'\n",
    "num_cpus = 32\n",
    "cpu_mem = 64\n",
    "num_gpus = 2; require = 'a100_80gb'\n",
    "\n",
    "overwrite_output_dir = False\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "\n",
    "model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "\n",
    "num_train_epochs = 2\n",
    "batch_size_per_gpu = 1\n",
    "total_batch_size = 128\n",
    "mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "\n",
    "fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"\n",
    "if 'gpt2' in model_name_or_path: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in model_name_or_path: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 128 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank\n",
    "lora_dropout = 0.05\n",
    "gradient_checkpointing = True\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "abbr_model_name = model_name_or_path.replace('/', ':')\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "if test_run:\n",
    "    output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "exe = 'python' if num_gpus == 1 else f\"torchrun --nproc_per_node={num_gpus} --master_port=10001\"\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}{exe}\n",
    "    open_instruct/finetune_trainer.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    {'--lora_rank '+str(lora_rank) if use_lora else ''}\n",
    "    {'--lora_alpha '+str(lora_alpha) if use_lora else ''}\n",
    "    {'--lora_dropout '+str(lora_dropout) if use_lora else ''}\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --save_strategy {save_strategy} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --save_total_limit 1 \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    {'--fsdp \"'+fsdp+'\"' if fsdp else ''}\n",
    "    {'--fsdp_transformer_layer_cls_to_wrap \"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "        if fsdp else ''}\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "    --bf16 True \\\n",
    "    --tf32 True \\\n",
    "    --report_to tensorboard \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    --output_dir \"{output_dir}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job_ccc(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    queue=queue,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    require=require,\n",
    "    num_gpus=num_gpus,\n",
    "    test_run=test_run,\n",
    "    job_duration=job_duration,\n",
    "    shell_scripts_modification_fn=shell_scripts_modification_fn,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30877747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/usr/local/share/zsh/site-functions'), PosixPath('/u/wpq/.oh-my-zsh/completions')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/dev/pts/541')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1886966.tmpdir/.1689346154.1886966.acct')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}'), PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n",
      "07/14/2023 14:10:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "07/14/2023 14:10:12 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix/runs/Jul14_14-10-12_cccxc552,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=200,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14/2023 14:10:13 - INFO - datasets.builder - Using custom data configuration default-247ebf1b4910b0d3\n",
      "07/14/2023 14:10:13 - INFO - datasets.info - Loading Dataset Infos from /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "07/14/2023 14:10:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "07/14/2023 14:10:13 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "07/14/2023 14:10:13 - WARNING - datasets.builder - Found cached dataset json (/dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "07/14/2023 14:10:13 - INFO - datasets.info - Loading Dataset info from /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.29s/it]\n",
      "[INFO|configuration_utils.py:669] 2023-07-14 14:10:24,678 >> loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-07-14 14:10:24,684 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,725 >> loading file tokenizer.model from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,725 >> loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,726 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,726 >> loading file special_tokens_map.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-14 14:10:24,726 >> loading file tokenizer_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2578] 2023-07-14 14:10:25,375 >> loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1173] 2023-07-14 14:10:25,381 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:577] 2023-07-14 14:10:25,381 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:56<00:00, 58.13s/it]\n",
      "[INFO|modeling_utils.py:3295] 2023-07-14 14:13:08,348 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-07-14 14:13:08,348 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:539] 2023-07-14 14:13:08,395 >> loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-07-14 14:13:08,396 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:921] 2023-07-14 14:13:08,396 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "07/14/2023 14:13:15 - INFO - __main__ - Initializing LORA model...\n",
      "trainable params: 268435456 || all params: 7006859264 || trainable%: 3.8310382139280827\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #0 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00000_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #1 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00001_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #2 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00002_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #3 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00003_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #4 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00004_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #5 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00005_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #6 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00006_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #7 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00007_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #8 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00008_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #9 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00009_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #10 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00010_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #11 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00011_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #12 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00012_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #13 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00013_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #14 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00014_of_00016.arrow\n",
      "07/14/2023 14:13:40 - INFO - datasets.arrow_dataset - Process #15 will write at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_00015_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/14/2023 14:13:52 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc0eb8d201835f20_*_of_00016.arrow\n",
      "07/14/2023 14:13:52 - INFO - datasets.arrow_dataset - Concatenating 16 shards\n",
      "[INFO|trainer.py:776] 2023-07-14 14:14:10,575 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: dataset, messages, id. If dataset, messages, id are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1786] 2023-07-14 14:14:10,604 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2023-07-14 14:14:10,604 >>   Num examples = 270,679\n",
      "[INFO|trainer.py:1788] 2023-07-14 14:14:10,604 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1789] 2023-07-14 14:14:10,604 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1790] 2023-07-14 14:14:10,605 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1791] 2023-07-14 14:14:10,605 >>   Gradient Accumulation steps = 128\n",
      "[INFO|trainer.py:1792] 2023-07-14 14:14:10,605 >>   Total optimization steps = 4,228\n",
      "[INFO|trainer.py:1793] 2023-07-14 14:14:10,606 >>   Number of trainable parameters = 268,435,456\n",
      "  0%|                                                  | 0/4228 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-07-14 14:14:11,413 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,454 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,467 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,481 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,510 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,522 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,553 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-07-14 14:14:11,592 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 2.1112, 'learning_rate': 1.5748031496062994e-07, 'epoch': 0.0}         \n",
      "{'loss': 1.9091, 'learning_rate': 3.149606299212599e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.3673, 'learning_rate': 4.724409448818898e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.3104, 'learning_rate': 6.299212598425198e-07, 'epoch': 0.0}          \n",
      "{'loss': 1.9968, 'learning_rate': 7.874015748031496e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.2831, 'learning_rate': 9.448818897637796e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.1034, 'learning_rate': 1.1023622047244096e-06, 'epoch': 0.0}         \n",
      "{'loss': 3.4951, 'learning_rate': 1.2598425196850396e-06, 'epoch': 0.0}         \n",
      "{'loss': 3.7316, 'learning_rate': 1.4173228346456693e-06, 'epoch': 0.0}         \n",
      "{'loss': 2.0593, 'learning_rate': 1.5748031496062992e-06, 'epoch': 0.0}         \n",
      "{'loss': 1.9925, 'learning_rate': 1.7322834645669292e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.2873, 'learning_rate': 1.8897637795275591e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.0844, 'learning_rate': 2.0472440944881893e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.2608, 'learning_rate': 2.2047244094488192e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.9228, 'learning_rate': 2.362204724409449e-06, 'epoch': 0.01}         \n",
      "{'loss': 4.0822, 'learning_rate': 2.519685039370079e-06, 'epoch': 0.01}         \n",
      "{'loss': 4.6403, 'learning_rate': 2.677165354330709e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.5807, 'learning_rate': 2.8346456692913386e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.0202, 'learning_rate': 2.992125984251969e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.544, 'learning_rate': 3.1496062992125985e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.1635, 'learning_rate': 3.307086614173229e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.9579, 'learning_rate': 3.4645669291338583e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.9001, 'learning_rate': 3.6220472440944887e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.9951, 'learning_rate': 3.7795275590551182e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.8361, 'learning_rate': 3.937007874015748e-06, 'epoch': 0.01}         \n",
      "{'loss': 3.3521, 'learning_rate': 4.0944881889763785e-06, 'epoch': 0.01}        \n",
      "{'loss': 1.8555, 'learning_rate': 4.251968503937008e-06, 'epoch': 0.01}         \n",
      "{'loss': 2.0398, 'learning_rate': 4.4094488188976384e-06, 'epoch': 0.01}        \n",
      "{'loss': 2.5598, 'learning_rate': 4.566929133858268e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7405, 'learning_rate': 4.724409448818898e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.8333, 'learning_rate': 4.881889763779528e-06, 'epoch': 0.01}         \n",
      "{'loss': 1.7384, 'learning_rate': 5.039370078740158e-06, 'epoch': 0.02}         \n",
      "{'loss': 1.8449, 'learning_rate': 5.196850393700788e-06, 'epoch': 0.02}         \n",
      "  1%|▎                                     | 33/4228 [12:43<27:08:04, 23.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd .. && python open_instruct/finetune_trainer.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --use_fast_tokenizer True --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 256 --lora_alpha 256 --lora_dropout 0.05 --do_train --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --evaluation_strategy \"no\" --logging_steps 1 --save_strategy steps --save_steps 200 --save_total_limit 1 --num_train_epochs 2 --bf16 True --tf32 True --overwrite_output_dir --report_to tensorboard --torch_dtype bfloat16 --dataloader_num_workers 8 --output_dir \"results/jpt_huggyllama:llama-7b+lora(r=256,a=256)_humanmix\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c5a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_scripts_template = \"\"\"\n",
    "export OPENAI_API_KEY=$(cat ~/.openai_api_key)\n",
    "export HF_HOME=\"/dccstor/mit_fm/wpq/hf_cache/\"\n",
    "\n",
    "source /dccstor/mit_fm/miniconda/bin/activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo \"Running on $(hostname)\"\n",
    "echo \"======\"\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "\n",
    "{cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/${{LSB_JOBID}}.out\" ] || mv \"{log_dir}/${{LSB_JOBID}}.out\" \"{save_dir}\"\n",
    "\"\"\"\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix/checkpoint-4200\" --save_dir \"results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix/checkpoint-4200/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix/checkpoint-4200\" --save_dir \"results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix/checkpoint-4200/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/huggyllama:llama-7b_humanmix/checkpoint-4100_copy\" --save_dir \"results/huggyllama:llama-7b_humanmix/checkpoint-4100_copy/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n"
     ]
    }
   ],
   "source": [
    "# task_name = 'mmlu'\n",
    "# task_name = 'gsm'\n",
    "# task_name = 'bbh_s=0'\n",
    "# task_name = 'bbh_s=3'\n",
    "# task_name = 'humaneval'\n",
    "job_name = f'eval.{task_name}'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "use_chat_format = True\n",
    "\n",
    "batch_size = 10\n",
    "if task_name == 'gsm':\n",
    "    queue = 'x86_1h' # 10min for n=200\n",
    "if task_name == 'bbh_s=0':\n",
    "    queue = 'x86_1h'\n",
    "if task_name == 'bbh_s=3':\n",
    "    queue = 'x86_12h'\n",
    "    batch_size = 5 # for longer prompts.\n",
    "if task_name == 'mmlu':\n",
    "    queue = 'x86_1h'\n",
    "    batch_size = 10\n",
    "if task_name == 'humaneval':\n",
    "    queue = 'x86_1h' # pass@1: 10min, pass@10: 100min\n",
    "    batch_size = 10\n",
    "    \n",
    "num_cpus = 10\n",
    "cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "models += [os.path.join('results/baselines', x) for x in [\n",
    "    'huggyllama/llama-7b',  # , 'mosaicml/mpt-7b'\n",
    "]]\n",
    "\n",
    "# models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "# models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "# models = [x for l in models for x in l]\n",
    "models += [get_last_checkpoint(x) for x in [\n",
    "#     'results/huggyllama:llama-7b_human_mix-trainer_savebystep',\n",
    "    'results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix',\n",
    "    'results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix',\n",
    "]]\n",
    "models += [\n",
    "    'results/huggyllama:llama-7b_humanmix/checkpoint-4100_copy',\n",
    "]\n",
    "\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for model_name_or_path in models:\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    if use_chat_format:\n",
    "        save_dir += '_chatfmt'\n",
    "    \n",
    "    if task_name == 'mmlu':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain 0 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name == 'gsm':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 200 \\\n",
    "            --n_shot 8 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('bbh'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_cot' if 's=0' in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "            {'--load_in_8bit' if load_in_8bit else ''}\n",
    "        \"\"\"\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job_ccc(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        queue=queue,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "    )\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'bbh_s=3', 'gsm_chatfmt', 'humaneval', 'mmlu_chatfmt', 'gsm', 'mmlu', 'humaneval_chatfmt']\n",
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'bbh_s=3', 'gsm_chatfmt', 'humaneval', 'mmlu_chatfmt', 'gsm', 'mmlu', 'humaneval_chatfmt']\n",
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'bbh_s=3', 'gsm_chatfmt', 'humaneval', 'mmlu_chatfmt', 'gsm', 'mmlu', 'humaneval_chatfmt']\n",
      "['bbh_s=0', 'bbh_s=0_chatfmt', 'bbh_s=3', 'gsm_chatfmt', 'humaneval', 'mmlu_chatfmt', 'gsm', 'mmlu', 'humaneval_chatfmt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MMLU/0-shot</th>\n",
       "      <th colspan=\"2\" halign=\"left\">GSM/CoT</th>\n",
       "      <th colspan=\"2\" halign=\"left\">BBH/Direct</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>31.9</td>\n",
       "      <td>32.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>31.7</td>\n",
       "      <td>32.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+humanmix</td>\n",
       "      <td>43.4</td>\n",
       "      <td>45.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>36.7</td>\n",
       "      <td>10.4</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=8,a=8)</td>\n",
       "      <td>36.7</td>\n",
       "      <td>36.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32.5</td>\n",
       "      <td>32.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=128,a=128)</td>\n",
       "      <td>42.9</td>\n",
       "      <td>43.4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.2</td>\n",
       "      <td>35.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model MMLU/0-shot         GSM/CoT         BBH/Direct  \\\n",
       "                                          chatfmt         chatfmt              \n",
       "0                    llama-7b        31.9    32.4    11.0    11.5       31.7   \n",
       "0           llama-7b+humanmix        43.4    45.3    23.5    26.0       36.3   \n",
       "0      llama-7b+lora(r=8,a=8)        36.7    36.6    10.5    12.0       32.5   \n",
       "0  llama-7b+lora(r=128,a=128)        42.9    43.4    10.5    14.0       34.2   \n",
       "\n",
       "          Codex-Eval/Pass@1          \n",
       "  chatfmt                   chatfmt  \n",
       "0    32.6              14.0     6.1  \n",
       "0    36.7              10.4     6.1  \n",
       "0    32.6              14.0    13.4  \n",
       "0    35.4              11.0    14.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir, run_name=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        self.run_name = run_name if run_name else self.save_dir\n",
    "        \n",
    "    def get_result_df(self):\n",
    "\n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "        print(task_names)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "\n",
    "            columns = [f'{task_name}/{k}' for k in metrics.keys()]\n",
    "            # columns = pd.MultiIndex.from_tuples(columns)\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        mapper = {\n",
    "            'mmlu/average_acc': 'MMLU/0-shot',\n",
    "            'mmlu_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'gsm/exact_match': 'GSM/CoT',\n",
    "            'gsm_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            'bbh_s=0/average_exact_match': 'BBH/Direct',\n",
    "            'bbh_s=0_chatfmt/average_exact_match': 'BBH/Direct_chatfmt',\n",
    "#             'bbh_s=3/average_exact_match': 'BBH/CoT', \n",
    "#             'bbh_s=3_chatfmt/average_exact_match': 'BBH/CoT_chatfmt',\n",
    "            'humaneval/pass@1': 'Codex-Eval/Pass@1',\n",
    "            'humaneval_chatfmt/pass@1': 'Codex-Eval/Pass@1_chatfmt',\n",
    "        }\n",
    "        cols = [col for col in mapper.keys() if col in df]\n",
    "        for col in cols:\n",
    "            df[col] = df[col]*100\n",
    "        df = df[cols]\n",
    "        df = df.rename(columns=mapper)\n",
    "        df.insert(0, 'Model', [self.run_name])\n",
    "        return df\n",
    "\n",
    "\n",
    "save_dirs = [\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "    ('llama-7b+humanmix', '../results/huggyllama:llama-7b_humanmix/checkpoint-4100_copy'),\n",
    "#     ('llama-7b+humanmix (3/4 epochs)', '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400'),\n",
    "] + [\n",
    "    (k, get_last_checkpoint(v)) for k, v in [\n",
    "        ('llama-7b+lora(r=8,a=8)', '../results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix'),\n",
    "        ('llama-7b+lora(r=128,a=128)', '../results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix'),\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for model_name, save_dir in save_dirs:\n",
    "    r = EvalResults(save_dir, model_name)\n",
    "    df = r.get_result_df()\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    dfc = df.copy()\n",
    "    cols = [x.split('_') for x in df.columns]\n",
    "    cols = [x+[''] if len(x)==1 else x for x in cols]\n",
    "    dfc.columns = pd.MultiIndex.from_tuples(cols)\n",
    "    display(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9efef1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MMLU/0-shot</th>\n",
       "      <th colspan=\"2\" halign=\"left\">GSM/CoT</th>\n",
       "      <th colspan=\"2\" halign=\"left\">BBH/Direct</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>31.868680</td>\n",
       "      <td>32.395670</td>\n",
       "      <td>10.993177</td>\n",
       "      <td>11.5</td>\n",
       "      <td>31.667109</td>\n",
       "      <td>32.608905</td>\n",
       "      <td>14.024390</td>\n",
       "      <td>6.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+humanmix</td>\n",
       "      <td>43.369890</td>\n",
       "      <td>45.278450</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>36.270949</td>\n",
       "      <td>36.653997</td>\n",
       "      <td>10.365854</td>\n",
       "      <td>6.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=8,a=8)</td>\n",
       "      <td>36.746902</td>\n",
       "      <td>36.554622</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32.545342</td>\n",
       "      <td>32.579733</td>\n",
       "      <td>14.024390</td>\n",
       "      <td>13.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=128,a=128)</td>\n",
       "      <td>42.942601</td>\n",
       "      <td>43.426862</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.217147</td>\n",
       "      <td>35.410440</td>\n",
       "      <td>10.975610</td>\n",
       "      <td>14.024390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model MMLU/0-shot               GSM/CoT          \\\n",
       "                                             chatfmt            chatfmt   \n",
       "0                    llama-7b   31.868680  32.395670  10.993177    11.5   \n",
       "0           llama-7b+humanmix   43.369890  45.278450  23.500000    26.0   \n",
       "0      llama-7b+lora(r=8,a=8)   36.746902  36.554622  10.500000    12.0   \n",
       "0  llama-7b+lora(r=128,a=128)   42.942601  43.426862  10.500000    14.0   \n",
       "\n",
       "  BBH/Direct            Codex-Eval/Pass@1             \n",
       "                chatfmt                      chatfmt  \n",
       "0  31.667109  32.608905         14.024390   6.097561  \n",
       "0  36.270949  36.653997         10.365854   6.097561  \n",
       "0  32.545342  32.579733         14.024390  13.414634  \n",
       "0  34.217147  35.410440         10.975610  14.024390  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU/0-shot_chatfmt</th>\n",
       "      <th>GSM/CoT_chatfmt</th>\n",
       "      <th>BBH/Direct_chatfmt</th>\n",
       "      <th>Codex-Eval/Pass@1_chatfmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+humanmix</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=8,a=8)</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=128,a=128)</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  MMLU/0-shot_chatfmt  GSM/CoT_chatfmt  \\\n",
       "0                    llama-7b                0.324            0.115   \n",
       "0           llama-7b+humanmix                0.453            0.260   \n",
       "0      llama-7b+lora(r=8,a=8)                0.366            0.120   \n",
       "0  llama-7b+lora(r=128,a=128)                0.434            0.140   \n",
       "\n",
       "   BBH/Direct_chatfmt  Codex-Eval/Pass@1_chatfmt  \n",
       "0               0.326                      0.061  \n",
       "0               0.367                      0.061  \n",
       "0               0.326                      0.134  \n",
       "0               0.354                      0.140  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>BBH/CoT</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+humanmix</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=8,a=8)</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+lora(r=128,a=128)</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  MMLU/0-shot  GSM/CoT  BBH/Direct  BBH/CoT  \\\n",
       "0                    llama-7b        0.319    0.110       0.317    0.335   \n",
       "0           llama-7b+humanmix        0.434    0.235       0.363    0.387   \n",
       "0      llama-7b+lora(r=8,a=8)        0.367    0.105       0.325    0.354   \n",
       "0  llama-7b+lora(r=128,a=128)        0.429    0.105       0.342    0.360   \n",
       "\n",
       "   Codex-Eval/Pass@1  \n",
       "0              0.140  \n",
       "0              0.104  \n",
       "0              0.140  \n",
       "0              0.110  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cpu_time (hr)</th>\n",
       "      <th>avg_mem</th>\n",
       "      <th>max_mem</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>0.038869</td>\n",
       "      <td>0.491738</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>0.071833</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.018196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.957783</td>\n",
       "      <td>1.317383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>0.208883</td>\n",
       "      <td>6.689150</td>\n",
       "      <td>11.693359</td>\n",
       "      <td>0.013647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>0.458394</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>33.018555</td>\n",
       "      <td>0.009098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huggyllama/llama-7b</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.639141</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.109932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  cpu_time (hr)   avg_mem    max_mem  exact_match\n",
       "0             t5-small       0.038869  0.491738   0.597656     0.014405\n",
       "1              t5-base       0.071833  0.729512   0.787109     0.018196\n",
       "2             t5-large       0.021314  0.957783   1.317383     0.000000\n",
       "3                t5-3b       0.208883  6.689150  11.693359     0.013647\n",
       "4               t5-11b       0.458394  0.872705  33.018555     0.009098\n",
       "5  huggyllama/llama-7b       0.790400  0.639141   0.710938     0.109932"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th>model</th>\n",
       "      <th>mmlu</th>\n",
       "      <th>gsm</th>\n",
       "      <th>bbh_s=0</th>\n",
       "      <th>bbh_s=3</th>\n",
       "      <th>humaneval</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "      <th>average_acc</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>average_exact_match</th>\n",
       "      <th>average_exact_match</th>\n",
       "      <th>pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/baselines/huggyllama/llama-7b/</td>\n",
       "      <td>0.318687</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.316671</td>\n",
       "      <td>0.334654</td>\n",
       "      <td>0.140244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.100834</td>\n",
       "      <td>0.332591</td>\n",
       "      <td>0.343629</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix/checkpoint-4200</td>\n",
       "      <td>0.429426</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.342171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task_name                                                                        model  \\\n",
       "metrics                                                                                  \n",
       "0                                            ../results/baselines/huggyllama/llama-7b/   \n",
       "0          ../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400   \n",
       "0            ../results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix/checkpoint-4200   \n",
       "\n",
       "task_name        mmlu         gsm             bbh_s=0             bbh_s=3  \\\n",
       "metrics   average_acc exact_match average_exact_match average_exact_match   \n",
       "0            0.318687    0.109932            0.316671            0.334654   \n",
       "0            0.333001    0.100834            0.332591            0.343629   \n",
       "0            0.429426    0.105000            0.342171                 NaN   \n",
       "\n",
       "task_name humaneval  \n",
       "metrics      pass@1  \n",
       "0          0.140244  \n",
       "0               NaN  \n",
       "0          0.109756  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
