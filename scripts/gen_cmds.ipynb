{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ppc64le', 'dcs')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "    get_run_statistics)\n",
    "import pandas as pd\n",
    "import json\n",
    "import platform\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import socket\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "arch = platform.uname().processor\n",
    "hostname = socket.gethostname()\n",
    "cluster = 'ccc' if hostname.startswith('ccc') else ('dcs' if hostname.startswith('dcs') else 'npl')\n",
    "arch, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir)\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|‚ñè         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "# !cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"Running on $SLURM_JOB_NODELIST\"\n",
      "echo \"======\"\n",
      "\n",
      "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
      "master_port=10002\n",
      "RDZV_ENDPOINT=$master_addr:$master_port\n",
      "\n",
      "source ~/.profile\n",
      "conda activate open-instruct\n",
      "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
      "\n",
      "set -e\n",
      "set -x\n",
      "echo \"======\"\n",
      "srun {cmd}\n",
      "\n",
      "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=256), micro-bsz=1,  grad-ckpt,  a100_80gb, 37s/it, 43hrs# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n",
    "\n",
    "# aimos\n",
    "# shared: humanmix, max_sequence_length=2048. 1 node = 6x v100_32gb\n",
    "\n",
    "# take-aways: \n",
    "#   (1) fsdp (v4.28.1, v4.32.0.dev0 are pretty similar in terms of speed. don't use v4.31.0)\n",
    "#   (2) micro-bsz=1->2 seems to be the best here. leads to ~30% reduction in runtime.\n",
    "#   (3) increasing #nodes almost linear reduction in time, e.g., Use 4x nodes cost 30% time (25% if linear.)\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=none, nodes=1, 74s/it, 89hrs, loss=0\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 72s/it, 86hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=none, nodes=1, 52s/it, 66hrs, loss=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 49s/it, 61hrs\n",
    "# - resume from lastest checkpt (trained 4.28.1, resume 4.32.0.dev0), did\n",
    "#   did not found `optimizer.bin` \n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# # nodes > 1\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=4, 14s/it, 18hrs\n",
    "\n",
    "# take-away: \n",
    "#   (1) torch_dtype=float16 gives loss=0. setting torch_dtype=float32 solves the issue but takes more memory and compute\n",
    "#   (2) mbsz=2 oom for nodes=1 but works fine with nodes=2. more nodes -> potentially larger mbsz.\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, 139s/it, 166hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=2, 33s/it, 41hrs, loss!=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 17s/it, 21hrs\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 18s/it, 21hrs\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=5, 4s/it, 21.7hrs\n",
    "\n",
    "\n",
    "#\n",
    "# deepspeed\n",
    "# shared: llama7b, deepspeed, grad-ckpt\n",
    "# take-aways\n",
    "#   (1) with deepspeed, using mixed-precision gives x50% speed improvement\n",
    "#   (2) no loss overflow issues. deepspeed has good mixed-precision integration, loss_scaler handles it.\n",
    "# \n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 161s/it, 192hrs, loss ok, loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=none, nodes=1, 259s/it, 309hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=fp16, nodes=1, 169s/it, 202hrs, loss ok. loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=none, nodes=1, 258s/it, 307hrs, loss ok.\n",
    "#\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 108s/it, 135hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3,offload), float32, mp=fp16, nodes=1,  96s/it, 120hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3), float32, mp=fp16, nodes=1, 123s/it, 147hrs, loss ok.\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=1, 66s/it, 83hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=1, oom\n",
    "#\n",
    "# nodes>1\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=2, error with downloading config.json\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=4, \n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=4, 16s/it, 21hrs\n",
    "# llama7b, micro-bsz=6, deepspeed(s=3), float32, mp=fp16, nodes=4,  3s/it, 17hrs\n",
    "\n",
    "\n",
    "# 1 node, 1 v100_32gb, no-grad-ckpt, nodes=1\n",
    "#\n",
    "# gpt2, micro-bsz=1, float32, mp=16, 7s/it, 8.6hrs\n",
    "# gpt2, micro-bsz=2, float32, mp=16, 5s/it, 5.6hrs\n",
    "# gpt2, micro-bsz=4, float32, mp=16, 5s/it, 6.3hrs\n",
    "# gpt2, micro-bsz=8, float32, mp=16, 6s/it, 6.6hrs\n",
    "# gpt2, micro-bsz=16, float32, mp=16, oom\n",
    "# gpt2-medium, micro-bsz=2, float32, mp=16, 14s/it, 17.0hrs\n",
    "# gpt2-medium, micro-bsz=2, float32, mp=16, 10s/it, 12.2hrs\n",
    "# gpt2-medium, micro-bsz=4, float32, mp=16, oom\n",
    "# gpt2-medium, micro-bsz=8, float32, mp=16, oom\n",
    "# \n",
    "\n",
    "\n",
    "shell_scripts_template_slurm = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template_lsf = \"\"\"\n",
    "echo \"Running on $LSB_DJOB_HOSTFILE\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\")\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$LSB_JOBID*.out\" ] && mv {log_dir}/$LSB_JOBID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf\n",
    "\n",
    "print(shell_scripts_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86305f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133s/it, 58.5hrs\n"
     ]
    }
   ],
   "source": [
    "t = '00:33:12'\n",
    "n = 15\n",
    "# total = 1515; nnodes = 1\n",
    "# total = 2083; nnodes = 1\n",
    "total = 1587; nnodes = 1\n",
    "# total = 1041; nnodes = 1\n",
    "# total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a669464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot\n",
      "sharegpt\n",
      "dolly\n",
      "gpt4_alpaca\n",
      "flan_v2\n",
      "super_ni\n",
      "stanford_alpaca\n",
      "baize\n",
      "code_alpaca\n",
      "self_instruct\n",
      "unnatural_instructions\n",
      "oasst1\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "p = '../data/processed/'\n",
    "for x in os.listdir(p):\n",
    "    y = os.path.join(p, x)\n",
    "    if os.path.isdir(y):\n",
    "        d = os.path.join(y, os.listdir(y)[0])\n",
    "    else:\n",
    "        continue\n",
    "    d = d[3:]\n",
    "    if 'shuffled' in d:\n",
    "        continue\n",
    "#     print(f\"train_file = '{d}'; abbr_train_file = '{x}'\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'baize': 100000},\n",
       " {'code_alpaca': 100000},\n",
       " {'cot': 100000},\n",
       " {'dolly': 100000},\n",
       " {'gpt4_alpaca': 100000},\n",
       " {'oasst1': 100000},\n",
       " {'self_instruct': 100000},\n",
       " {'sharegpt': 100000},\n",
       " {'stanford_alpaca': 100000},\n",
       " {'super_ni': 100000},\n",
       " {'unnatural_instructions': 100000},\n",
       " {'cot': 50000, 'flan_v2': 50000, 'dolly': 50000, 'oasst1': 50000},\n",
       " {'cot': 97570, 'flan_v2': 97570, 'dolly': 1464, 'oasst1': 3394},\n",
       " {'baize': 16666,\n",
       "  'code_alpaca': 16666,\n",
       "  'cot': 16666,\n",
       "  'dolly': 16666,\n",
       "  'flan_v2': 16666,\n",
       "  'gpt4_alpaca': 16666,\n",
       "  'oasst1': 16666,\n",
       "  'self_instruct': 16666,\n",
       "  'sharegpt': 16666,\n",
       "  'stanford_alpaca': 16666,\n",
       "  'super_ni': 16666,\n",
       "  'unnatural_instructions': 16666},\n",
       " {'cot': 15356, 'flan_v2': 182740, 'dolly': 894, 'oasst1': 1814},\n",
       " {'cot': 22540, 'flan_v2': 174520, 'dolly': 2790, 'oasst1': 278}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_data_points = 200000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e1e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/lhoestq___csv/lhoestq--demo1-ea767574a693953b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e7086944fd42699ab47483d1d50b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['7bd227d9-afc9-11e6-aba1-c4b301cdf627',\n",
       " '7bd227d9-afc9-11e6-aba1-c4b301cdf627']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lhoestq/demo1\")\n",
    "dataset['train'].select([0,0])['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results/baselines/huggyllama/llama-7b using 6 GPUs, 2 batch size per GPU, 2 gradient accumulation steps.\n",
      "\n",
      "!cd .. && TORCHELASTIC_ERROR_FILE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/all.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=16 --per_device_train_batch_size=2 --gradient_accumulation_steps=2 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --optim=adamw_hf --weight_decay=0. --evaluation_strategy=\"no\" --logging_steps=1 --save_strategy=steps --save_steps=50 --save_total_limit=100 --num_train_epochs=1 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --gradient_checkpointing --report_to=tensorboard --torch_dtype=float32 --dataloader_num_workers=8 --fp16=True --overwrite_output_dir --subsample_mixture=\"{'cot':48785,'flan_v2':48785,'dolly':732,'oasst1':1697}\" --output_dir=\"results/oi3/jpt_llama-7b_all:100k\"\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi3\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": true,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ft1: reproduce open-instruct table with llama7b\n",
    "job_name = 'ft1'\n",
    "\n",
    "# ft2: test mixture weights\n",
    "# vary mixture weights\n",
    "job_name = 'ft2'\n",
    "\n",
    "# oi3: instruction tuning performance w.r.t. steps.\n",
    "job_name = 'oi3'\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "# job_duration = 6 if arch == 'ppc64le' else 12\n",
    "# job_duration = 6\n",
    "# shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  512 if arch == 'ppc64le' else 64\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6 # llama-7b\n",
    "# nodes = 1; num_gpus = 1; gpu_type = 'v100'; job_duration = 6  # gpt2\n",
    "# nodes = 2; num_gpus = 6; gpu_type = 'v100'; job_duration = 6  # pythia-1.4b\n",
    "\n",
    "debug_mode = test_run\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "save_strategy = 'steps'\n",
    "save_steps = 50 # 100\n",
    "save_total_limit = 100 # 1\n",
    "\n",
    "\n",
    "hf_models_dir = 'results/baselines/'\n",
    "# model_name_or_path = 'results/baselines/gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-1.4b'; abbr_model_name = 'pythia-1.4b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "# subsample_mixture = {'flan_v2': 100000}\n",
    "# subsample_mixture = dict(sorted(subsample_mixture.items()))\n",
    "\n",
    "total_data_points = 100000 # 10000, 50000, 100000, 200000\n",
    "subsample_mixture_list = []\n",
    "# subsample_mixture_list += [\n",
    "#     {k: total_data_points} for k in datasets\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "# ]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {'cot':  0.13568177819252014, 'flan_v2': 0.3957784175872803, \n",
    "#      'dolly': 0.05964866653084755, 'oasst1': 0.4088916480541229}.items())\n",
    "# ] # gpt2-medium_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "# ] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "# ] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "#         )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "         \n",
    "# subsample_mixture_list = [dict(sorted(d.items())) for d in subsample_mixture_list]\n",
    "\n",
    "\n",
    "# subsample_mixture_list = [None]\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "# train_file = 'data/processed/dolly_oasst1.jsonl'; abbr_train_file = 'dolly:oasst1'\n",
    "# train_file = 'data/processed/cot_flanv2.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "\n",
    "# train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "\n",
    "# train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "\n",
    "# train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "# train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "num_train_epochs = 1\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128 # 128\n",
    "gradient_acc_steps = int(total_batch_size/(num_gpus*nodes)/batch_size_per_gpu)\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = True\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_mixture_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (subsample_mixture,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}:{int(total_data_points/1000)}k\"\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "        \n",
    "    if job_name == 'ft2':\n",
    "        if subsample_mixture is not None:\n",
    "            assert(abbr_train_file=='all')\n",
    "            output_dirname += \\\n",
    "                '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "            \n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "    #         ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "    #         ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "    #         '_mbsz='+str(batch_size_per_gpu)+\\\n",
    "    #         '_dtype='+torch_dtype+\\\n",
    "    #         ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "    #         '_seqlen='+str(max_seq_length)+\\\n",
    "    #         '_nodes='+str(nodes)\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers=16 \\\n",
    "        --per_device_train_batch_size={batch_size_per_gpu} \\\n",
    "        --gradient_accumulation_steps={gradient_acc_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type=linear \\\n",
    "        --warmup_ratio=0.03 \\\n",
    "        --optim={optimizer} \\\n",
    "        --weight_decay=0. \\\n",
    "        --evaluation_strategy=\"no\" \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --report_to=tensorboard \\\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be16cfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\n",
      "  entrypoint       : open_instruct/finetune_trainer.py\n",
      "  min_nodes        : 1\n",
      "  max_nodes        : 1\n",
      "  nproc_per_node   : 6\n",
      "  run_id           : none\n",
      "  rdzv_backend     : static\n",
      "  rdzv_endpoint    : 127.0.0.1:10002\n",
      "  rdzv_configs     : {'rank': 0, 'timeout': 900}\n",
      "  max_restarts     : 0\n",
      "  monitor_interval : 5\n",
      "  log_dir          : None\n",
      "  metrics_cfg      : {}\n",
      "\n",
      "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3.10\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
      "[I socket.cpp:566] [c10d] The server socket has started to listen on [::]:10002.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42066.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42068.\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
      "  restart_count=0\n",
      "  master_addr=127.0.0.1\n",
      "  master_port=10002\n",
      "  group_rank=0\n",
      "  group_world_size=1\n",
      "  local_ranks=[0, 1, 2, 3, 4, 5]\n",
      "  role_ranks=[0, 1, 2, 3, 4, 5]\n",
      "  global_ranks=[0, 1, 2, 3, 4, 5]\n",
      "  role_world_sizes=[6, 6, 6, 6, 6, 6]\n",
      "  global_world_sizes=[6, 6, 6, 6, 6, 6]\n",
      "\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
      "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz/attempt_0/0/error.json\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz/attempt_0/1/error.json\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz/attempt_0/2/error.json\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz/attempt_0/3/error.json\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz/attempt_0/4/error.json\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_i9b_fdqu/none_dd6qmsmz/attempt_0/5/error.json\n",
      "[2023-08-09 21:54:09,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 21:54:09,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 21:54:09,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 21:54:09,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 21:54:09,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-08-09 21:54:09,717] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42084.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42082.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42100.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42078.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42098.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42080.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42102.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42104.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42106.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42108.\n",
      "[I ProcessGroupNCCL.cpp:672] [Rank 2] ProcessGroupNCCL initialized with following options:\n",
      "NCCL_ASYNC_ERROR_HANDLING: 1\n",
      "NCCL_DESYNC_DEBUG: 0\n",
      "NCCL_BLOCKING_WAIT: 0\n",
      "TIMEOUT(ms): 1800000\n",
      "USE_HIGH_PRIORITY_STREAM: 0\n",
      "[I ProcessGroupNCCL.cpp:672] [Rank 0] ProcessGroupNCCL initialized with following options:\n",
      "NCCL_ASYNC_ERROR_HANDLING: 1\n",
      "NCCL_DESYNC_DEBUG: 0\n",
      "NCCL_BLOCKING_WAIT: 0\n",
      "TIMEOUT(ms): 1800000\n",
      "USE_HIGH_PRIORITY_STREAM: 0\n",
      "[I ProcessGroupNCCL.cpp:672] [Rank 4] ProcessGroupNCCL initialized with following options:\n",
      "NCCL_ASYNC_ERROR_HANDLING: 1\n",
      "NCCL_DESYNC_DEBUG: 0\n",
      "NCCL_BLOCKING_WAIT: 0\n",
      "TIMEOUT(ms): 1800000\n",
      "USE_HIGH_PRIORITY_STREAM: 0\n",
      "[I ProcessGroupNCCL.cpp:672] [Rank 5] ProcessGroupNCCL initialized with following options:\n",
      "NCCL_ASYNC_ERROR_HANDLING: 1\n",
      "NCCL_DESYNC_DEBUG: 0\n",
      "NCCL_BLOCKING_WAIT: 0\n",
      "TIMEOUT(ms): 1800000\n",
      "USE_HIGH_PRIORITY_STREAM: 0\n",
      "[I ProcessGroupNCCL.cpp:672] [Rank 3] ProcessGroupNCCL initialized with following options:\n",
      "NCCL_ASYNC_ERROR_HANDLING: 1\n",
      "NCCL_DESYNC_DEBUG: 0\n",
      "NCCL_BLOCKING_WAIT: 0\n",
      "TIMEOUT(ms): 1800000\n",
      "USE_HIGH_PRIORITY_STREAM: 0\n",
      "[I ProcessGroupNCCL.cpp:850] [Rank 0] NCCL watchdog thread started!\n",
      "[I ProcessGroupNCCL.cpp:850] [Rank 4] NCCL watchdog thread started!\n",
      "[I ProcessGroupNCCL.cpp:850] [Rank 3] NCCL watchdog thread started!\n",
      "[I ProcessGroupNCCL.cpp:850] [Rank 5] NCCL watchdog thread started!\n",
      "[I ProcessGroupNCCL.cpp:850] [Rank 2] NCCL watchdog thread started!\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42110.\n",
      "[I socket.cpp:787] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:42112.\n",
      "[I ProcessGroupNCCL.cpp:672] [Rank 1] ProcessGroupNCCL initialized with following options:\n",
      "NCCL_ASYNC_ERROR_HANDLING: 1\n",
      "NCCL_DESYNC_DEBUG: 0\n",
      "NCCL_BLOCKING_WAIT: 0\n",
      "TIMEOUT(ms): 1800000\n",
      "USE_HIGH_PRIORITY_STREAM: 0\n",
      "[I ProcessGroupNCCL.cpp:850] [Rank 1] NCCL watchdog thread started!\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "subsample mixture:\n",
      "{'cot': 97570, 'dolly': 1464, 'flan_v2': 97570, 'oasst1': 3394}\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "subsample mixture:\n",
      "{'cot': 97570, 'dolly': 1464, 'flan_v2': 97570, 'oasst1': 3394}\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "subsample mixture:\n",
      "{'cot': 97570, 'dolly': 1464, 'flan_v2': 97570, 'oasst1': 3394}\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "subsample mixture:\n",
      "{'cot': 97570, 'dolly': 1464, 'flan_v2': 97570, 'oasst1': 3394}\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "subsample mixture:\n",
      "{'cot': 97570, 'dolly': 1464, 'flan_v2': 97570, 'oasst1': 3394}\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1519: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "subsample mixture:\n",
      "{'cot': 97570, 'dolly': 1464, 'flan_v2': 97570, 'oasst1': 3394}\n",
      "Saving args dict to results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394.args.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I ProcessGroupNCCL.cpp:2466] Rank 1 using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
      "[I ProcessGroupNCCL.cpp:2466] Rank 2 using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
      "[I ProcessGroupNCCL.cpp:2466] Rank 3 using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
      "[I ProcessGroupNCCL.cpp:2466] Rank 5 using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
      "[I ProcessGroupNCCL.cpp:2466] Rank 0 using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
      "[I ProcessGroupNCCL.cpp:2466] Rank 4 using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
      "dcs012:1131205:1131205 [0] NCCL INFO Bootstrap : Using enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131205:1131205 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "dcs012:1131205:1131205 [0] NCCL INFO cudaDriverVersion 11020\n",
      "NCCL version 2.14.3+cuda11.2\n",
      "dcs012:1131208:1131208 [3] NCCL INFO cudaDriverVersion 11020\n",
      "dcs012:1131207:1131207 [2] NCCL INFO cudaDriverVersion 11020\n",
      "dcs012:1131210:1131210 [5] NCCL INFO cudaDriverVersion 11020\n",
      "dcs012:1131209:1131209 [4] NCCL INFO cudaDriverVersion 11020\n",
      "dcs012:1131206:1131206 [1] NCCL INFO cudaDriverVersion 11020\n",
      "dcs012:1131205:1131370 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Using network IB\n",
      "dcs012:1131208:1131208 [3] NCCL INFO Bootstrap : Using enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131208:1131208 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "dcs012:1131207:1131207 [2] NCCL INFO Bootstrap : Using enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131207:1131207 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "dcs012:1131210:1131210 [5] NCCL INFO Bootstrap : Using enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131210:1131210 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "dcs012:1131209:1131209 [4] NCCL INFO Bootstrap : Using enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131206:1131206 [1] NCCL INFO Bootstrap : Using enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131209:1131209 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "dcs012:1131206:1131206 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "dcs012:1131208:1131375 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Using network IB\n",
      "dcs012:1131207:1131376 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Using network IB\n",
      "dcs012:1131210:1131377 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Using network IB\n",
      "dcs012:1131209:1131379 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Using network IB\n",
      "dcs012:1131206:1131380 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB enP5p1s0f0:172.31.236.12<0>\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Using network IB\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Setting affinity for GPU 4 to ffffff,ffffffff,ffff0000,00000000,00000000\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff,ffffffff\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffffffff,ffffffff\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Setting affinity for GPU 3 to ffffff,ffffffff,ffff0000,00000000,00000000\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Setting affinity for GPU 5 to ffffff,ffffffff,ffff0000,00000000,00000000\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->2 [2] 2/-1/-1->1->0 [3] -1/-1/-1->1->2\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 4/-1/-1->5->3 [2] -1/-1/-1->5->4 [3] 4/-1/-1->5->3\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 5/-1/-1->3->-1 [2] 4/-1/-1->3->2 [3] 5/-1/-1->3->-1\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 0/-1/-1->4->5 [2] 5/-1/-1->4->3 [3] 0/-1/-1->4->5\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->0 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->0\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 2/-1/-1->0->4 [2] 1/-1/-1->0->-1 [3] 2/-1/-1->0->4\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Channel 00/0 : 1[405000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 00/0 : 3[3503000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 00/0 : 5[3505000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 00/0 : 4[3504000] -> 5[3505000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 00/0 : 0[404000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 00/0 : 2[406000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Channel 01/0 : 1[405000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 01/0 : 3[3503000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 01/0 : 5[3505000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 01/0 : 4[3504000] -> 5[3505000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 01/0 : 0[404000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 01/0 : 2[406000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Channel 02/0 : 1[405000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 02/0 : 3[3503000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 02/0 : 4[3504000] -> 5[3505000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 02/0 : 5[3505000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 02/0 : 0[404000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 02/0 : 2[406000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Channel 03/0 : 1[405000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 03/0 : 4[3504000] -> 5[3505000] via P2P/IPC\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 03/0 : 3[3503000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 03/0 : 5[3505000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 03/0 : 0[404000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 03/0 : 2[406000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Connected all rings\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Connected all rings\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Connected all rings\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 01/0 : 3[3503000] -> 5[3505000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Connected all rings\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Connected all rings\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 01/0 : 4[3504000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Connected all rings\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 03/0 : 3[3503000] -> 5[3505000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 03/0 : 4[3504000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 01/0 : 0[404000] -> 2[406000] via P2P/IPC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 03/0 : 0[404000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 01/0 : 5[3505000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Channel 00/0 : 1[405000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 03/0 : 5[3505000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Channel 02/0 : 1[405000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 01/0 : 2[406000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 03/0 : 2[406000] -> 0[404000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 00/0 : 5[3505000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 01/0 : 5[3505000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 01/0 : 0[404000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 02/0 : 5[3505000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 00/0 : 3[3503000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Channel 03/0 : 0[404000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Channel 03/0 : 5[3505000] -> 4[3504000] via P2P/IPC\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Channel 02/0 : 3[3503000] -> 2[406000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 00/0 : 2[406000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 01/0 : 2[406000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 00/0 : 4[3504000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 02/0 : 2[406000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Channel 02/0 : 4[3504000] -> 3[3503000] via P2P/IPC\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Channel 03/0 : 2[406000] -> 1[405000] via P2P/IPC\n",
      "dcs012:1131205:1131370 [0] NCCL INFO Connected all trees\n",
      "dcs012:1131205:1131370 [0] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512\n",
      "dcs012:1131205:1131370 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "dcs012:1131210:1131377 [5] NCCL INFO Connected all trees\n",
      "dcs012:1131210:1131377 [5] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512\n",
      "dcs012:1131210:1131377 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "dcs012:1131208:1131375 [3] NCCL INFO Connected all trees\n",
      "dcs012:1131208:1131375 [3] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512\n",
      "dcs012:1131208:1131375 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "dcs012:1131209:1131379 [4] NCCL INFO Connected all trees\n",
      "dcs012:1131209:1131379 [4] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512\n",
      "dcs012:1131209:1131379 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "dcs012:1131206:1131380 [1] NCCL INFO Connected all trees\n",
      "dcs012:1131206:1131380 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512\n",
      "dcs012:1131206:1131380 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "dcs012:1131207:1131376 [2] NCCL INFO Connected all trees\n",
      "dcs012:1131207:1131376 [2] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512\n",
      "dcs012:1131207:1131376 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\n",
      "dcs012:1131205:1131370 [0] NCCL INFO comm 0x15fefa2c0 rank 0 nranks 6 cudaDev 0 busId 404000 - Init COMPLETE\n",
      "[I ProcessGroupNCCL.cpp:1288] NCCL_DEBUG: INFO\n",
      "dcs012:1131207:1131376 [2] NCCL INFO comm 0x17eedaf50 rank 2 nranks 6 cudaDev 2 busId 406000 - Init COMPLETE\n",
      "dcs012:1131206:1131380 [1] NCCL INFO comm 0x15e32f630 rank 1 nranks 6 cudaDev 1 busId 405000 - Init COMPLETE\n",
      "dcs012:1131210:1131377 [5] NCCL INFO comm 0x1a4b44e40 rank 5 nranks 6 cudaDev 5 busId 3505000 - Init COMPLETE\n",
      "dcs012:1131208:1131375 [3] NCCL INFO comm 0x19395ff30 rank 3 nranks 6 cudaDev 3 busId 3503000 - Init COMPLETE\n",
      "dcs012:1131209:1131379 [4] NCCL INFO comm 0x17960e8e0 rank 4 nranks 6 cudaDev 4 busId 3504000 - Init COMPLETE\n",
      "08/09/2023 21:54:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "Saving args dict to results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394.args.json\n",
      "Saving args dict to results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394.args.json\n",
      "Saving args dict to results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394.args.json\n",
      "08/09/2023 21:54:19 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'fsdp_transformer_layer_cls_to_wrap': ['GPTNeoXLayer'], 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=GPTNeoXLayer,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=10,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394/runs/Aug09_21-54-17_dcs012,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Saving args dict to results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394.args.json\n",
      "Saving args dict to results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394.args.json\n",
      "08/09/2023 21:54:19 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/09/2023 21:54:19 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/09/2023 21:54:19 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/09/2023 21:54:19 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "08/09/2023 21:54:19 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09/2023 21:54:20 - INFO - datasets.builder - Using custom data configuration default-501e2cfa496eaf7a\n",
      "08/09/2023 21:54:20 - INFO - datasets.info - Loading Dataset Infos from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "08/09/2023 21:54:20 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 21:54:20 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 21:54:20 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 21:54:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "08/09/2023 21:54:20 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/09/2023 21:54:20 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "08/09/2023 21:54:20 - INFO - datasets.info - Loading Dataset info from /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 21:54:20 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]08/09/2023 21:54:20 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.14s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it]\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-09 21:54:21,294 >> loading configuration file results/baselines/EleutherAI/pythia-1.4b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-09 21:54:21,298 >> Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"results/baselines/EleutherAI/pythia-1.4b\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 21:54:21,300 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 21:54:21,300 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 21:54:21,300 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 21:54:21,300 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 21:54:21,300 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-09 21:54:21,300 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2604] 2023-08-09 21:54:21,898 >> loading weights file results/baselines/EleutherAI/pythia-1.4b/model.safetensors\n",
      "[INFO|modeling_utils.py:1176] 2023-08-09 21:54:24,418 >> Instantiating GPTNeoXForCausalLM model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:603] 2023-08-09 21:54:24,419 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3333] 2023-08-09 21:54:47,744 >> All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3341] 2023-08-09 21:54:47,745 >> All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-1.4b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "[INFO|modeling_utils.py:2953] 2023-08-09 21:54:47,751 >> Generation config file not found, using a generation config created from the model config.\n",
      "[INFO|tokenization_utils_base.py:921] 2023-08-09 21:54:47,762 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #0 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00000_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #1 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00001_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #2 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00002_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #3 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00003_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #4 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00004_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #5 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00005_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #6 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00006_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #7 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00007_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #8 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00008_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #9 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00009_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #10 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00010_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #11 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00011_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #12 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00012_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #13 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00013_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #14 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00014_of_00016.arrow\n",
      "08/09/2023 21:54:47 - INFO - datasets.arrow_dataset - Process #15 will write at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00015_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Spawning 16 processes\n",
      "Tokenizing and reformatting instruction data (num_proc=16):   0%| | 0/1013410 [008/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00000_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00001_of_00016.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=16):   0%| | 43/1013410 [08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00002_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00003_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00004_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00005_of_00016.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=16):   0%| | 184/1013410 08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00006_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00007_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00008_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00009_of_00016.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=16):   0%| | 358/1013410 08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00013_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00010_of_00016.arrow\n",
      "Tokenizing and reformatting instruction data (num_proc=16):   0%| | 1027/101341008/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00011_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00012_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00014_of_00016.arrow\n",
      "08/09/2023 21:54:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_00015_of_00016.arrow\n",
      "08/09/2023 22:19:31 - INFO - datasets.arrow_dataset - Concatenating 16 shards   \n",
      "08/09/2023 22:19:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_*_of_00016.arrow\n",
      "08/09/2023 22:19:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_*_of_00016.arrow\n",
      "08/09/2023 22:19:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_*_of_00016.arrow\n",
      "08/09/2023 22:19:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_*_of_00016.arrow\n",
      "08/09/2023 22:19:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-501e2cfa496eaf7a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a98b47517cbad2b7_*_of_00016.arrow\n",
      "[INFO|trainer.py:750] 2023-08-09 22:20:13,463 >> The following columns in the training set don't have a corresponding argument in `GPTNeoXForCausalLM.forward` and have been ignored: dataset, id, messages. If dataset, id, messages are not expected by `GPTNeoXForCausalLM.forward`,  you can safely ignore this message.\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "08/09/2023 22:20:14 - WARNING - accelerate.accelerator - FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer\n",
      "08/09/2023 22:20:14 - WARNING - accelerate.accelerator - FSDP Warning: When using FSDP, several parameter groups will be conflated into a single one due to nested module wrapping and parameter flattening.\n",
      "[INFO|trainer.py:1682] 2023-08-09 22:20:14,391 >> ***** Running training *****\n",
      "[INFO|trainer.py:1683] 2023-08-09 22:20:14,391 >>   Num examples = 199,998\n",
      "[INFO|trainer.py:1684] 2023-08-09 22:20:14,391 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1685] 2023-08-09 22:20:14,391 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1688] 2023-08-09 22:20:14,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 120\n",
      "[INFO|trainer.py:1689] 2023-08-09 22:20:14,391 >>   Gradient Accumulation steps = 10\n",
      "[INFO|trainer.py:1690] 2023-08-09 22:20:14,391 >>   Total optimization steps = 1,666\n",
      "[INFO|trainer.py:1691] 2023-08-09 22:20:14,392 >>   Number of trainable parameters = 235,774,651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1666 [00:00<?, ?it/s]/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,593 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,593 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,593 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,593 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,593 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,594 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,595 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,653 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,800 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,801 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,801 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,801 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,805 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,805 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,829 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,831 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,992 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,992 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,992 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,993 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,993 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,994 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,996 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:14,996 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,039 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,039 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,039 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,039 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,039 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,039 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,040 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,040 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,040 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,040 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,041 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,041 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,041 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,042 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,043 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,044 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,056 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,056 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,056 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,057 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,058 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,058 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,058 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:280] 2023-08-09 22:20:15,059 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1498, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}         \n",
      "{'loss': 2.4323, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}         \n",
      "{'loss': 2.3211, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}          \n",
      "{'loss': 2.2403, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}         \n",
      "{'loss': 2.1903, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}         \n",
      "  0%|                                        | 5/1666 [01:35<8:43:29, 18.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd .. && TORCHELASTIC_ERROR_FILE=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --nproc_per_node=6 --master_port=10002 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/EleutherAI/pythia-1.4b --tokenizer_name=results/baselines/EleutherAI/pythia-1.4b --use_fast_tokenizer=True --train_file=data/processed/all.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=16 --per_device_train_batch_size=2 --gradient_accumulation_steps=10 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --optim=adamw_hf --weight_decay=0. --evaluation_strategy=\"no\" --logging_steps=1 --save_strategy=steps --save_steps=100 --save_total_limit=1 --num_train_epochs=1 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"GPTNeoXLayer\" --gradient_checkpointing --report_to=tensorboard --torch_dtype=float32 --dataloader_num_workers=8 --fp16=True --overwrite_output_dir --subsample_mixture=\"{'cot':97570,'dolly':1464,'flan_v2':97570,'oasst1':3394}\" --output_dir=\"results/ft2/jpt_pythia-1.4b_all:200k_mix=cot:97570,dolly:1464,flan_v2:97570,oasst1:3394\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_all_symlinks(directory, verbose=False):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files + dirs:\n",
    "            path = os.path.join(root, name)\n",
    "            if os.path.islink(path):\n",
    "                os.unlink(path)\n",
    "                if verbose:\n",
    "                    print(f\"Removed symlink: {path}\")\n",
    "                \n",
    "import uuid\n",
    "\n",
    "def create_unique_symlinks(file_paths, verbose=False):\n",
    "    \"\"\"Create symlinks for each `file` in `files` in the same directory, with a unique name. \"\"\"\n",
    "    dirs = [os.path.dirname(x) for x in file_paths]\n",
    "\n",
    "    symlink_path_dict = {}\n",
    "    for directory, path in zip(dirs, file_paths):\n",
    "        if os.path.isdir(path):\n",
    "            symlink_name = f\"symlink_{str(uuid.uuid4())[:8]}\"  # Generate a unique symlink name\n",
    "            symlink_path = os.path.join(directory, symlink_name)\n",
    "            try:\n",
    "                os.symlink(os.path.abspath(path), symlink_path)\n",
    "                if verbose:\n",
    "                    print(f\"Created symlink: {symlink_path} -> {path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Failed to create symlink: {path}. Error: {e}\")\n",
    "            symlink_path_dict.update({path: symlink_path})\n",
    "    return symlink_path_dict\n",
    "\n",
    "\n",
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    if any(x in model_name_or_path for x in ['gpt2-medium', 'pythia-160m']):\n",
    "        return 50, 1\n",
    "    if any(x in model_name_or_path for x in ['gpt-xl', 'pythia-1.4b']):\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'tydiqa_s=1_gp']):\n",
    "            return 16, 1\n",
    "        else:\n",
    "            return 32, 1\n",
    "    if 'llama' in model_name_or_path:\n",
    "        if any(x in task_name for x in ['bbh_s=3', 'mmlu_s=5']):\n",
    "            return 5, 1\n",
    "        else:\n",
    "            return 10, 1\n",
    "    return batch_size, job_duration\n",
    "\n",
    "def finished_training(path):\n",
    "    return os.path.isfile(os.path.join(path, 'config.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9b68375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/mmlu_s=5_chatfmt_v4\" --eval_batch_size 5 --ntrain 5 --use_chat_format --chat_format_version 4\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/ft1/llama-7b_humanmix\" --save_dir \"results/ft1/llama-7b_humanmix/eval/mmlu_s=5_chatfmt_v4\" --eval_batch_size 5 --ntrain 5 --use_chat_format --chat_format_version 4\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "task_name = 'mmlu'\n",
    "task_name = 'gsm'\n",
    "task_name = 'bbh_s=0'\n",
    "task_name = 'bbh_s=3'\n",
    "task_name = 'humaneval'\n",
    "task_name = 'tydiqa_cb'\n",
    "task_name = 'tydiqa_gp'\n",
    "job_name = f'eval.{task_name}'\n",
    "job_name = 'eval'\n",
    "\n",
    "test_run = 1\n",
    "eval_rest = 1\n",
    "test_run = bool(test_run)\n",
    "eval_test = bool(eval_rest)\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "models = []\n",
    "models += [os.path.join('results/baselines', x) for x in [\n",
    "    'huggyllama/llama-7b', \n",
    "#     'NousResearch/Llama-2-7b-hf',\n",
    "#     'gpt2',\n",
    "#     'gpt2-medium',\n",
    "#     'EleutherAI/pythia-1.4b',\n",
    "]]\n",
    "\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    ## 'mmlu_s=5', # some exceed max_seq_len error.\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    ## 'bbh_s=3_cot', # runtime too long.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb',\n",
    "    'tydiqa_s=1_gp',\n",
    "]\n",
    "task_names = [\n",
    "#     'mmlu_s=0',\n",
    "    'mmlu_s=5',\n",
    "]\n",
    "\n",
    "# `use_chat_format`\n",
    "task_names = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "chat_format_version = 4\n",
    "\n",
    "# create_symlinks = False; exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "\n",
    "\n",
    "# eval_rest = 1; create_symlinks = False; exp_dir = 'results/ft1'\n",
    "# eval_rest = 1; create_symlinks = True; exp_dir = 'results/ft2/'; \n",
    "# include_checkpoints = True; eval_rest = True; create_symlinks = False; exp_dir = 'results/oi3/'; \n",
    "\n",
    "models += [\n",
    "    'results/ft1/llama-7b_humanmix',\n",
    "]\n",
    "eval_rest = False\n",
    "\n",
    "\n",
    "if eval_rest:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    for subdir in os.listdir(exp_dir):\n",
    "        if not any(x in subdir for x in ['llama', 'pythia']):\n",
    "            continue\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "    if create_symlinks:\n",
    "        # create symlink for each directory.\n",
    "        symlink_path_dict = create_unique_symlinks(\n",
    "            list([x[1] for x in task_name_and_model]))\n",
    "        options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "    else:\n",
    "        options_list = task_name_and_model\n",
    "else:\n",
    "    options_list = itertools.product(\n",
    "        task_names,\n",
    "        models,\n",
    "    )\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    if 'gpt2' in ft_args_model_name_or_path:\n",
    "        tydiqa_max_context_length = 400 # max ctx len without exceeding max_seq_len\n",
    "    else:\n",
    "        tydiqa_max_context_length = 512\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if chat_format_version:\n",
    "        save_dir += '_v'+str(chat_format_version)\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            {'--chat_format_version '+str(chat_format_version) if chat_format_version else ''} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 200 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length {tydiqa_max_context_length} \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b16c708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chat_format_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7dba3_row0_col0, #T_7dba3_row0_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_7dba3_row0_col2, #T_7dba3_row0_col3, #T_7dba3_row0_col4, #T_7dba3_row0_col5, #T_7dba3_row0_col6, #T_7dba3_row0_col7, #T_7dba3_row0_col8, #T_7dba3_row0_col9, #T_7dba3_row0_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7dba3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7dba3_level0_col0\" class=\"col_heading level0 col0\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_7dba3_level0_col1\" class=\"col_heading level0 col1\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_7dba3_level0_col2\" class=\"col_heading level0 col2\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_7dba3_level0_col3\" class=\"col_heading level0 col3\" >GSM/Direct</th>\n",
       "      <th id=\"T_7dba3_level0_col4\" class=\"col_heading level0 col4\" >GSM/CoT</th>\n",
       "      <th id=\"T_7dba3_level0_col5\" class=\"col_heading level0 col5\" >BBH/Direct</th>\n",
       "      <th id=\"T_7dba3_level0_col6\" class=\"col_heading level0 col6\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_7dba3_level0_col7\" class=\"col_heading level0 col7\" >TydiQA/CB</th>\n",
       "      <th id=\"T_7dba3_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/GP</th>\n",
       "      <th id=\"T_7dba3_level0_col9\" class=\"col_heading level0 col9\" >Average</th>\n",
       "      <th id=\"T_7dba3_level0_col10\" class=\"col_heading level0 col10\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7dba3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7dba3_row0_col0\" class=\"data row0 col0\" >huggyllama/llama-7b</td>\n",
       "      <td id=\"T_7dba3_row0_col1\" class=\"data row0 col1\" >{}</td>\n",
       "      <td id=\"T_7dba3_row0_col2\" class=\"data row0 col2\" >32.46</td>\n",
       "      <td id=\"T_7dba3_row0_col3\" class=\"data row0 col3\" >5.50</td>\n",
       "      <td id=\"T_7dba3_row0_col4\" class=\"data row0 col4\" >11.00</td>\n",
       "      <td id=\"T_7dba3_row0_col5\" class=\"data row0 col5\" >32.97</td>\n",
       "      <td id=\"T_7dba3_row0_col6\" class=\"data row0 col6\" >0.00</td>\n",
       "      <td id=\"T_7dba3_row0_col7\" class=\"data row0 col7\" >10.35</td>\n",
       "      <td id=\"T_7dba3_row0_col8\" class=\"data row0 col8\" >38.56</td>\n",
       "      <td id=\"T_7dba3_row0_col9\" class=\"data row0 col9\" >18.69</td>\n",
       "      <td id=\"T_7dba3_row0_col10\" class=\"data row0 col10\" >-14.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6de512c80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import dict_iterated_getitem, pd_sort_rows_by_avg_ranking\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir, run_name=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        self.run_name = run_name if run_name else self.save_dir\n",
    "        \n",
    "    def get_ft_args(self):\n",
    "        ft_args_path = os.path.join(self.save_dir, 'ft_args.json')\n",
    "        if os.path.isfile(ft_args_path):\n",
    "            with open(ft_args_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {}\n",
    "        \n",
    "    def get_result_df(self, chat_fmt=None, ft_args_fields=None):\n",
    "        if os.path.islink(self.save_dir) or not os.path.isdir(self.eval_dir):\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "            if task_name.startswith('tydiqa'):\n",
    "                metrics['average_f1'] = metrics['average']['f1']\n",
    "            columns = [f'{task_name}/{k}' for k in metrics.keys()]\n",
    "            # columns = pd.MultiIndex.from_tuples(columns)\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        \n",
    "        mapper = {\n",
    "#             'mmlu/average_acc': 'MMLU/0-shot',\n",
    "#             'mmlu_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'mmlu_s=0/average_acc': 'MMLU/0-shot',\n",
    "            'mmlu_s=0_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'mmlu_s=5/average_acc': 'MMLU/5-shot',\n",
    "            'mmlu_s=5_chatfmt/average_acc': 'MMLU/t-shot_chatfmt',\n",
    "            # gsm\n",
    "#             'gsm/exact_match': 'GSM/CoT',\n",
    "#             'gsm_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            'gsm_s=8/exact_match': 'GSM/Direct',\n",
    "            'gsm_s=8_chatfmt/exact_match': 'GSM/Direct_chatfmt',\n",
    "            'gsm_s=8_cot/exact_match': 'GSM/CoT',\n",
    "            'gsm_s=8_cot_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            # bbh\n",
    "            'bbh_s=3/average_exact_match': 'BBH/Direct',\n",
    "            'bbh_s=3_chatfmt/average_exact_match': 'BBH/Direct_chatfmt',\n",
    "#             'bbh_s=3_cot/average_exact_match': 'BBH/CoT',\n",
    "#             'bbh_s=3_cot_chatfmt/average_exact_match': 'BBH/CoT_chatfmt',\n",
    "            # humaneval\n",
    "            'humaneval/pass@1': 'Codex-Eval/Pass@1',\n",
    "            'humaneval_chatfmt/pass@1': 'Codex-Eval/Pass@1_chatfmt',\n",
    "            # tydiqa\n",
    "#             'tydiqa_cb/average_f1': 'TydiQA/CB',\n",
    "#             'tydiqa_cb_chatfmt/average_f1': 'TydiQA/CB_chatfmt',\n",
    "#             'tydiqa_gp/average_f1': 'TydiQA/GP',\n",
    "#             'tydiqa_gp_chatfmt/average_f1': 'TydiQA/GP_chatfmt',\n",
    "            'tydiqa_s=1_cb/average_f1': 'TydiQA/CB',\n",
    "            'tydiqa_s=1_cb_chatfmt/average_f1': 'TydiQA/CB_chatfmt',\n",
    "            'tydiqa_s=1_gp/average_f1': 'TydiQA/GP',\n",
    "            'tydiqa_s=1_gp_chatfmt/average_f1': 'TydiQA/GP_chatfmt',\n",
    "        }\n",
    "        cols = [col for col in mapper.keys() if col in df]\n",
    "        if chat_fmt is not None:\n",
    "            if chat_fmt:\n",
    "                cols = [col for col in cols if 'chatfmt' in col]\n",
    "            else:\n",
    "                cols = [col for col in cols if 'chatfmt' not in col]\n",
    "        \n",
    "        for col in cols:\n",
    "            if 'tydiqa' not in col:\n",
    "                df[col] = df[col]*100\n",
    "        df = df[cols]\n",
    "        df = df.rename(columns=mapper)\n",
    "        if chat_fmt is not None:\n",
    "            df.columns = [x.replace('_chatfmt', '') for x in df.columns]\n",
    "        else:\n",
    "            cols = [x.split('_') for x in df.columns]\n",
    "            cols = [x+[''] if len(x)==1 else x for x in cols]\n",
    "            df.columns = pd.MultiIndex.from_tuples(cols)\n",
    "            \n",
    "        df.insert(len(df.columns), 'Average', df.mean(axis=1))\n",
    "        \n",
    "        # append extra fields to the table.\n",
    "        if ft_args_fields is not None:\n",
    "            ft_args = r.get_ft_args()\n",
    "            for k in ft_args_fields[::-1]:\n",
    "                try:\n",
    "                    v = dict_iterated_getitem(ft_args, k)\n",
    "                except:\n",
    "                    if k == 'model_args.model_name_or_path':\n",
    "                        v = self.run_name\n",
    "                    else:\n",
    "                        v = None\n",
    "                df.insert(0, k, [v])\n",
    "        return df\n",
    "\n",
    "# get_last_checkpoint(v)\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "#     ('gpt2', '../results/baselines/gpt2'),\n",
    "    ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "    ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "    ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "]\n",
    "\n",
    "chat_fmt = True\n",
    "sort_rows = True\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/ft2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "exp_dir = '../results/oi3'\n",
    "save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "save_dirs += [(os.path.basename(x), x) for x in \n",
    "             glob.glob(os.path.join(exp_dir, 'llama-7b_all:100k', 'checkpoint-*'))]\n",
    "\n",
    "\n",
    "save_dirs = [x for x in save_dirs if 'replace' not in x[1]]\n",
    "\n",
    "ft_args_fields = ['model_args.model_name_or_path',\n",
    "                  'data_args.subsample_mixture']\n",
    "\n",
    "dfs = []\n",
    "for model_name, save_dir in save_dirs:\n",
    "    if finished_training(save_dir) and not os.path.islink(save_dir):\n",
    "        r = EvalResults(save_dir, model_name)\n",
    "        df = r.get_result_df(chat_fmt=chat_fmt, ft_args_fields=ft_args_fields)\n",
    "        dfs.append(df)\n",
    "df = pd.concat(dfs, axis=0)\n",
    "if sort_rows:\n",
    "    df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "    sort_value_col, sort_value_col_ascending = 'Average', False\n",
    "    sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "    df = df.sort_values(sort_value_col, ascending=sort_value_col_ascending)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "if exp_dir.endswith('ft2'):\n",
    "    for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x)]\n",
    "        dfc = dfc.reset_index(drop=True)\n",
    "        if len(dfc):\n",
    "            display(dfc\n",
    "                    .style\n",
    "                    .set_properties(**{'text-align': 'left'})\n",
    "                    .background_gradient(cmap ='coolwarm')\n",
    "                    .format(precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231de89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_args.model_name_or_path</th>\n",
       "      <th>data_args.subsample_mixture</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/Direct</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "      <th>TydiQA/CB</th>\n",
       "      <th>TydiQA/GP</th>\n",
       "      <th>Average</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checkpoint-400</td>\n",
       "      <td>None</td>\n",
       "      <td>37.580117</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>34.428543</td>\n",
       "      <td>13.414634</td>\n",
       "      <td>10.630766</td>\n",
       "      <td>46.317736</td>\n",
       "      <td>23.910257</td>\n",
       "      <td>-2.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checkpoint-450</td>\n",
       "      <td>None</td>\n",
       "      <td>41.532545</td>\n",
       "      <td>6.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>34.162596</td>\n",
       "      <td>12.195122</td>\n",
       "      <td>11.560986</td>\n",
       "      <td>44.767313</td>\n",
       "      <td>23.602652</td>\n",
       "      <td>-3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>checkpoint-500</td>\n",
       "      <td>None</td>\n",
       "      <td>39.061387</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>34.183009</td>\n",
       "      <td>13.414634</td>\n",
       "      <td>10.626070</td>\n",
       "      <td>43.882946</td>\n",
       "      <td>23.381150</td>\n",
       "      <td>-3.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checkpoint-300</td>\n",
       "      <td>None</td>\n",
       "      <td>39.958695</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>34.059814</td>\n",
       "      <td>13.414634</td>\n",
       "      <td>10.550496</td>\n",
       "      <td>40.544674</td>\n",
       "      <td>22.932616</td>\n",
       "      <td>-5.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>checkpoint-550</td>\n",
       "      <td>None</td>\n",
       "      <td>38.463182</td>\n",
       "      <td>5.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.210373</td>\n",
       "      <td>11.585366</td>\n",
       "      <td>10.039605</td>\n",
       "      <td>40.939640</td>\n",
       "      <td>22.676881</td>\n",
       "      <td>-6.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>checkpoint-250</td>\n",
       "      <td>None</td>\n",
       "      <td>38.961686</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>33.042837</td>\n",
       "      <td>14.634146</td>\n",
       "      <td>9.972445</td>\n",
       "      <td>42.915849</td>\n",
       "      <td>22.860995</td>\n",
       "      <td>-6.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>checkpoint-600</td>\n",
       "      <td>None</td>\n",
       "      <td>37.864976</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.111300</td>\n",
       "      <td>12.195122</td>\n",
       "      <td>9.469516</td>\n",
       "      <td>42.857361</td>\n",
       "      <td>22.356896</td>\n",
       "      <td>-7.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>checkpoint-200</td>\n",
       "      <td>None</td>\n",
       "      <td>26.342401</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>33.540157</td>\n",
       "      <td>13.414634</td>\n",
       "      <td>9.946306</td>\n",
       "      <td>44.783758</td>\n",
       "      <td>20.932465</td>\n",
       "      <td>-7.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>checkpoint-100</td>\n",
       "      <td>None</td>\n",
       "      <td>36.618715</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>33.968080</td>\n",
       "      <td>13.414634</td>\n",
       "      <td>10.220836</td>\n",
       "      <td>41.853036</td>\n",
       "      <td>22.082186</td>\n",
       "      <td>-8.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>checkpoint-50</td>\n",
       "      <td>None</td>\n",
       "      <td>36.974790</td>\n",
       "      <td>5.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>33.072601</td>\n",
       "      <td>11.585366</td>\n",
       "      <td>10.564826</td>\n",
       "      <td>42.875720</td>\n",
       "      <td>21.867615</td>\n",
       "      <td>-8.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>checkpoint-350</td>\n",
       "      <td>None</td>\n",
       "      <td>28.201111</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.079810</td>\n",
       "      <td>10.975610</td>\n",
       "      <td>9.983932</td>\n",
       "      <td>44.085093</td>\n",
       "      <td>20.617936</td>\n",
       "      <td>-9.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>checkpoint-150</td>\n",
       "      <td>None</td>\n",
       "      <td>33.691782</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.145086</td>\n",
       "      <td>12.804878</td>\n",
       "      <td>8.935932</td>\n",
       "      <td>40.727274</td>\n",
       "      <td>20.972136</td>\n",
       "      <td>-9.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>huggyllama/llama-7b</td>\n",
       "      <td>{}</td>\n",
       "      <td>31.861558</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>31.652294</td>\n",
       "      <td>11.585366</td>\n",
       "      <td>9.505480</td>\n",
       "      <td>40.403106</td>\n",
       "      <td>20.215401</td>\n",
       "      <td>-11.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>checkpoint-800</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>checkpoint-700</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>checkpoint-650</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>checkpoint-750</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_args.model_name_or_path data_args.subsample_mixture  MMLU/0-shot  \\\n",
       "0                 checkpoint-400                        None    37.580117   \n",
       "1                 checkpoint-450                        None    41.532545   \n",
       "2                 checkpoint-500                        None    39.061387   \n",
       "3                 checkpoint-300                        None    39.958695   \n",
       "4                 checkpoint-550                        None    38.463182   \n",
       "5                 checkpoint-250                        None    38.961686   \n",
       "6                 checkpoint-600                        None    37.864976   \n",
       "7                 checkpoint-200                        None    26.342401   \n",
       "8                 checkpoint-100                        None    36.618715   \n",
       "9                  checkpoint-50                        None    36.974790   \n",
       "10                checkpoint-350                        None    28.201111   \n",
       "11                checkpoint-150                        None    33.691782   \n",
       "12           huggyllama/llama-7b                          {}    31.861558   \n",
       "13                checkpoint-800                        None          NaN   \n",
       "14                checkpoint-700                        None          NaN   \n",
       "15                checkpoint-650                        None          NaN   \n",
       "16                checkpoint-750                        None          NaN   \n",
       "\n",
       "    GSM/Direct  GSM/CoT  BBH/Direct  Codex-Eval/Pass@1  TydiQA/CB  TydiQA/GP  \\\n",
       "0          6.0     19.0   34.428543          13.414634  10.630766  46.317736   \n",
       "1          6.5     14.5   34.162596          12.195122  11.560986  44.767313   \n",
       "2          6.0     16.5   34.183009          13.414634  10.626070  43.882946   \n",
       "3          5.0     17.0   34.059814          13.414634  10.550496  40.544674   \n",
       "4          5.5     17.0   35.210373          11.585366  10.039605  40.939640   \n",
       "5          5.0     15.5   33.042837          14.634146   9.972445  42.915849   \n",
       "6          6.0     14.0   34.111300          12.195122   9.469516  42.857361   \n",
       "7          6.0     12.5   33.540157          13.414634   9.946306  44.783758   \n",
       "8          4.0     14.5   33.968080          13.414634  10.220836  41.853036   \n",
       "9          5.5     12.5   33.072601          11.585366  10.564826  42.875720   \n",
       "10         5.0     12.0   34.079810          10.975610   9.983932  44.085093   \n",
       "11         4.5     12.0   34.145086          12.804878   8.935932  40.727274   \n",
       "12         6.0     10.5   31.652294          11.585366   9.505480  40.403106   \n",
       "13         NaN      NaN         NaN                NaN        NaN        NaN   \n",
       "14         NaN      NaN         NaN                NaN        NaN        NaN   \n",
       "15         NaN      NaN         NaN                NaN        NaN        NaN   \n",
       "16         NaN      NaN         NaN                NaN        NaN        NaN   \n",
       "\n",
       "      Average  ranking  \n",
       "0   23.910257  -2.7500  \n",
       "1   23.602652  -3.3750  \n",
       "2   23.381150  -3.6250  \n",
       "3   22.932616  -5.9375  \n",
       "4   22.676881  -6.2500  \n",
       "5   22.860995  -6.5000  \n",
       "6   22.356896  -7.4375  \n",
       "7   20.932465  -7.9375  \n",
       "8   22.082186  -8.0625  \n",
       "9   21.867615  -8.3750  \n",
       "10  20.617936  -9.6875  \n",
       "11  20.972136  -9.9375  \n",
       "12  20.215401 -11.1250  \n",
       "13        NaN      NaN  \n",
       "14        NaN      NaN  \n",
       "15        NaN      NaN  \n",
       "16        NaN      NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfc[dfc['model_args.model_name_or_path']]\n",
    "dfc = df.copy()\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "569d3c13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dfc = df.copy()\n",
    "\n",
    "# add base model performance\n",
    "dfc.loc[dfc['model_args.model_name_or_path']=='huggyllama/llama-7b', 'model_args.model_name_or_path'] = 'checkpoint-0'\n",
    "# get steps \n",
    "dfc.insert(0, 'steps', dfc['model_args.model_name_or_path'].apply(lambda x: int(x.split('-')[-1])))\n",
    "dfc = dfc.sort_values('steps')\n",
    "\n",
    "\n",
    "y_labels = [\n",
    "    'MMLU/0-shot', \n",
    "    'GSM/CoT', \n",
    "    'TydiQA/GP',\n",
    "    'BBH/Direct',\n",
    "    # \n",
    "#     'GSM/Direct', \n",
    "#     'Codex-Eval/Pass@1', \n",
    "#     'TydiQA/CB',\n",
    "]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "x = dfc['steps']\n",
    "for y_label in y_labels:\n",
    "    y = dfc[y_label].to_numpy()\n",
    "    ax.plot(x, y, label=y_label)\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa6ba4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cot': 97570, 'flan_v2': 97570, 'dolly': 1464, 'oasst1': 3394},\n",
       " {'cot': 72119, 'dolly': 439, 'flan_v2': 126074, 'oasst1': 339},\n",
       " {'cot': 45092, 'dolly': 2818, 'flan_v2': 34790, 'oasst1': 118847},\n",
       " {'cot': 17126, 'dolly': 108593, 'flan_v2': 69580, 'oasst1': 2066},\n",
       " {'cot': 6323, 'dolly': 40966, 'flan_v2': 81933, 'oasst1': 81933}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "total_data_points = 200000 # 10000, 50000, 100000, 200000\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.360595703125, \"dolly\": 0.0021991729736328125, \"flan_v2\": 0.63037109375, \"oasst1\": 0.0016956329345703125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv1.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.2254638671875, \"dolly\": 0.01409149169921875, \"flan_v2\": 0.1739501953125, \"oasst1\": 0.59423828125}.items())\n",
    "] # pythia-1.4b humanmix_uniform:200k_doremiv2.json\n",
    "\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.08563232421875, \"dolly\": 0.54296875, \"flan_v2\": 0.347900390625, \"oasst1\": 0.0103302001953125}.items())\n",
    "] # llama-7b_humanmix_uniform:200k_doremiv2.json\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {\"cot\": 0.0316162109375, \"dolly\": 0.204833984375, \"flan_v2\": 0.40966796875, \"oasst1\": 0.40966796875}.items()\n",
    "        )] # llama-7b_humanmix_uniform:600k_doremiv2.json\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b1ec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211155"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6323+40966+81933+81933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3dd3dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'results/ft1'\n",
    "\n",
    "# d = {\n",
    "#     'bbh_s=0': 'bbh_s=3',\n",
    "#     'gsm': 'gsm_s=8_cot',\n",
    "#     'mmlu': 'mmlu_s=0',\n",
    "#     'tydiqa_cb': 'tydiqa_s=1_cb',\n",
    "#     'tydiqa_gp': 'tydiqa_s=1_gp',\n",
    "# }\n",
    "\n",
    "# d.update({k+'_chatfmt': v+'_chatfmt' for k,v in d.items()})\n",
    "\n",
    "# for subdir in os.listdir(exp_dir):    \n",
    "#     for task_name_src, task_name_tgt in d.items():\n",
    "#         path_src = os.path.join(exp_dir, subdir, 'eval', task_name_src)\n",
    "#         path_tgt = os.path.join(exp_dir, subdir, 'eval', task_name_tgt)\n",
    "#         if os.path.isdir(path_src):\n",
    "# #             os.rename(path_src, path_tgt)\n",
    "#             print(path_src)\n",
    "#             print(path_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27138820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_train_samples</th>\n",
       "      <th>model_args.model_name_or_path</th>\n",
       "      <th>data_args.subsample_mixture</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/Direct</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "      <th>TydiQA/CB</th>\n",
       "      <th>TydiQA/GP</th>\n",
       "      <th>Average</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200804</td>\n",
       "      <td>results/baselines/huggyllama/llama-7b</td>\n",
       "      <td>{'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}</td>\n",
       "      <td>44.601909</td>\n",
       "      <td>3.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.147236</td>\n",
       "      <td>1.829268</td>\n",
       "      <td>9.934316</td>\n",
       "      <td>44.361673</td>\n",
       "      <td>22.196343</td>\n",
       "      <td>-2.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200128</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 22540, 'dolly': 2790, 'flan_v2': 174520, 'oasst1': 278}</td>\n",
       "      <td>23.123487</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>29.677548</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.543347</td>\n",
       "      <td>8.898066</td>\n",
       "      <td>9.907458</td>\n",
       "      <td>-9.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'flan_v2': 100000}</td>\n",
       "      <td>23.002421</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.757914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.821590</td>\n",
       "      <td>8.841241</td>\n",
       "      <td>9.989024</td>\n",
       "      <td>-9.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'self_instruct': 100000}</td>\n",
       "      <td>23.557898</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.590479</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>1.410211</td>\n",
       "      <td>6.140904</td>\n",
       "      <td>9.774144</td>\n",
       "      <td>-10.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 50000, 'dolly': 50000, 'flan_v2': 50000, 'oasst1': 50000}</td>\n",
       "      <td>22.938328</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>29.973845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875094</td>\n",
       "      <td>7.962404</td>\n",
       "      <td>9.892810</td>\n",
       "      <td>-11.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200804</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}</td>\n",
       "      <td>23.045150</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>30.422551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.395340</td>\n",
       "      <td>7.712177</td>\n",
       "      <td>9.867888</td>\n",
       "      <td>-11.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 12500, 'dolly': 12500, 'flan_v2': 12500, 'oasst1': 12500}</td>\n",
       "      <td>22.952571</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.284956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.896922</td>\n",
       "      <td>7.082380</td>\n",
       "      <td>9.745261</td>\n",
       "      <td>-11.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>199992</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'baize': 16666, 'code_alpaca': 16666, 'cot': 16666, 'dolly': 16666, 'flan_v2': 16666, 'gpt4_alpaca': 16666, 'oasst1': 16666, 'self_instruct': 16666, 'sharegpt': 16666, 'stanford_alpaca': 16666, 'super_ni': 16666, 'unnatural_instructions': 16666}</td>\n",
       "      <td>23.016664</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.758117</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>1.367148</td>\n",
       "      <td>7.950251</td>\n",
       "      <td>9.830242</td>\n",
       "      <td>-12.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>99999</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 40031, 'dolly': 6009, 'flan_v2': 40031, 'oasst1': 13928}</td>\n",
       "      <td>23.023786</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.827929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.158432</td>\n",
       "      <td>7.918454</td>\n",
       "      <td>9.704086</td>\n",
       "      <td>-12.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'super_ni': 100000}</td>\n",
       "      <td>23.151973</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.258770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.877706</td>\n",
       "      <td>7.116692</td>\n",
       "      <td>9.486449</td>\n",
       "      <td>-13.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>99996</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'baize': 8333, 'code_alpaca': 8333, 'cot': 8333, 'dolly': 8333, 'flan_v2': 8333, 'gpt4_alpaca': 8333, 'oasst1': 8333, 'self_instruct': 8333, 'sharegpt': 8333, 'stanford_alpaca': 8333, 'super_ni': 8333, 'unnatural_instructions': 8333}</td>\n",
       "      <td>22.938328</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.391050</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.773780</td>\n",
       "      <td>5.948287</td>\n",
       "      <td>9.380172</td>\n",
       "      <td>-14.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'unnatural_instructions': 100000}</td>\n",
       "      <td>23.201823</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.127942</td>\n",
       "      <td>1.829268</td>\n",
       "      <td>1.730040</td>\n",
       "      <td>4.095138</td>\n",
       "      <td>9.212030</td>\n",
       "      <td>-14.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9998</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 4878, 'dolly': 73, 'flan_v2': 4878, 'oasst1': 169}</td>\n",
       "      <td>22.945449</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>28.108541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083539</td>\n",
       "      <td>6.646854</td>\n",
       "      <td>9.469198</td>\n",
       "      <td>-15.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 25000, 'dolly': 25000, 'flan_v2': 25000, 'oasst1': 25000}</td>\n",
       "      <td>22.938328</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.980136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.445112</td>\n",
       "      <td>7.088936</td>\n",
       "      <td>9.564645</td>\n",
       "      <td>-15.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49998</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 24392, 'dolly': 366, 'flan_v2': 24392, 'oasst1': 848}</td>\n",
       "      <td>22.895599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.028842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.448538</td>\n",
       "      <td>6.055629</td>\n",
       "      <td>9.489801</td>\n",
       "      <td>-15.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50031</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 5635, 'dolly': 697, 'flan_v2': 43630, 'oasst1': 69}</td>\n",
       "      <td>22.852870</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.429958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.456560</td>\n",
       "      <td>8.441289</td>\n",
       "      <td>9.668668</td>\n",
       "      <td>-15.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10005</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 1127, 'dolly': 139, 'flan_v2': 8726, 'oasst1': 13}</td>\n",
       "      <td>23.159094</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>29.467971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922864</td>\n",
       "      <td>4.975758</td>\n",
       "      <td>9.289384</td>\n",
       "      <td>-15.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 100000}</td>\n",
       "      <td>23.016664</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.558459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.931833</td>\n",
       "      <td>7.495874</td>\n",
       "      <td>9.000404</td>\n",
       "      <td>-16.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'stanford_alpaca': 100000}</td>\n",
       "      <td>22.988178</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>27.052148</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.615986</td>\n",
       "      <td>5.113901</td>\n",
       "      <td>8.982853</td>\n",
       "      <td>-16.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{}</td>\n",
       "      <td>23.721692</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>24.459354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.398436</td>\n",
       "      <td>5.114215</td>\n",
       "      <td>8.599100</td>\n",
       "      <td>-17.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'code_alpaca': 100000}</td>\n",
       "      <td>22.966814</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.876242</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>1.383378</td>\n",
       "      <td>5.525014</td>\n",
       "      <td>8.852994</td>\n",
       "      <td>-17.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10000</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 2500, 'dolly': 2500, 'flan_v2': 2500, 'oasst1': 2500}</td>\n",
       "      <td>22.852870</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.637130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.946636</td>\n",
       "      <td>5.612540</td>\n",
       "      <td>8.435597</td>\n",
       "      <td>-19.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'baize': 100000}</td>\n",
       "      <td>22.924085</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>25.485221</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.329486</td>\n",
       "      <td>4.139038</td>\n",
       "      <td>8.498227</td>\n",
       "      <td>-19.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'sharegpt': 100000}</td>\n",
       "      <td>23.052272</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.323305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.732119</td>\n",
       "      <td>3.702220</td>\n",
       "      <td>8.187131</td>\n",
       "      <td>-19.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'dolly': 100000}</td>\n",
       "      <td>22.810141</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>15.476953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.347582</td>\n",
       "      <td>5.569227</td>\n",
       "      <td>7.171986</td>\n",
       "      <td>-20.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'oasst1': 100000}</td>\n",
       "      <td>22.973935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.000967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.797507</td>\n",
       "      <td>6.326418</td>\n",
       "      <td>8.156975</td>\n",
       "      <td>-20.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'gpt4_alpaca': 100000}</td>\n",
       "      <td>23.052272</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.812740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.571938</td>\n",
       "      <td>4.294121</td>\n",
       "      <td>8.104439</td>\n",
       "      <td>-21.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>200000</td>\n",
       "      <td>huggyllama/llama-7b</td>\n",
       "      <td>{}</td>\n",
       "      <td>32.459764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.268293</td>\n",
       "      <td>9.765382</td>\n",
       "      <td>37.489078</td>\n",
       "      <td>18.996503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    total_train_samples          model_args.model_name_or_path  \\\n",
       "0                200804  results/baselines/huggyllama/llama-7b   \n",
       "1                200128          results/baselines/gpt2-medium   \n",
       "2                100000                            gpt2-medium   \n",
       "3                100000                            gpt2-medium   \n",
       "4                200000                            gpt2-medium   \n",
       "5                200804          results/baselines/gpt2-medium   \n",
       "6                 50000          results/baselines/gpt2-medium   \n",
       "7                199992                            gpt2-medium   \n",
       "8                 99999                            gpt2-medium   \n",
       "9                100000                            gpt2-medium   \n",
       "10                99996                            gpt2-medium   \n",
       "11               100000                            gpt2-medium   \n",
       "12                 9998          results/baselines/gpt2-medium   \n",
       "13               100000                            gpt2-medium   \n",
       "14                49998          results/baselines/gpt2-medium   \n",
       "15                50031          results/baselines/gpt2-medium   \n",
       "16                10005          results/baselines/gpt2-medium   \n",
       "17               100000                            gpt2-medium   \n",
       "18               100000                            gpt2-medium   \n",
       "19               200000                            gpt2-medium   \n",
       "20               100000                            gpt2-medium   \n",
       "21                10000          results/baselines/gpt2-medium   \n",
       "22               100000                            gpt2-medium   \n",
       "23               100000                            gpt2-medium   \n",
       "24               100000                            gpt2-medium   \n",
       "25               100000                            gpt2-medium   \n",
       "26               100000                            gpt2-medium   \n",
       "27               200000                    huggyllama/llama-7b   \n",
       "\n",
       "                                                                                                                                                                                                                               data_args.subsample_mixture  \\\n",
       "0                                                                                                                                                                                          {'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}   \n",
       "1                                                                                                                                                                                          {'cot': 22540, 'dolly': 2790, 'flan_v2': 174520, 'oasst1': 278}   \n",
       "2                                                                                                                                                                                                                                      {'flan_v2': 100000}   \n",
       "3                                                                                                                                                                                                                                {'self_instruct': 100000}   \n",
       "4                                                                                                                                                                                        {'cot': 50000, 'dolly': 50000, 'flan_v2': 50000, 'oasst1': 50000}   \n",
       "5                                                                                                                                                                                          {'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}   \n",
       "6                                                                                                                                                                                        {'cot': 12500, 'dolly': 12500, 'flan_v2': 12500, 'oasst1': 12500}   \n",
       "7   {'baize': 16666, 'code_alpaca': 16666, 'cot': 16666, 'dolly': 16666, 'flan_v2': 16666, 'gpt4_alpaca': 16666, 'oasst1': 16666, 'self_instruct': 16666, 'sharegpt': 16666, 'stanford_alpaca': 16666, 'super_ni': 16666, 'unnatural_instructions': 16666}   \n",
       "8                                                                                                                                                                                         {'cot': 40031, 'dolly': 6009, 'flan_v2': 40031, 'oasst1': 13928}   \n",
       "9                                                                                                                                                                                                                                     {'super_ni': 100000}   \n",
       "10              {'baize': 8333, 'code_alpaca': 8333, 'cot': 8333, 'dolly': 8333, 'flan_v2': 8333, 'gpt4_alpaca': 8333, 'oasst1': 8333, 'self_instruct': 8333, 'sharegpt': 8333, 'stanford_alpaca': 8333, 'super_ni': 8333, 'unnatural_instructions': 8333}   \n",
       "11                                                                                                                                                                                                                      {'unnatural_instructions': 100000}   \n",
       "12                                                                                                                                                                                              {'cot': 4878, 'dolly': 73, 'flan_v2': 4878, 'oasst1': 169}   \n",
       "13                                                                                                                                                                                       {'cot': 25000, 'dolly': 25000, 'flan_v2': 25000, 'oasst1': 25000}   \n",
       "14                                                                                                                                                                                           {'cot': 24392, 'dolly': 366, 'flan_v2': 24392, 'oasst1': 848}   \n",
       "15                                                                                                                                                                                             {'cot': 5635, 'dolly': 697, 'flan_v2': 43630, 'oasst1': 69}   \n",
       "16                                                                                                                                                                                              {'cot': 1127, 'dolly': 139, 'flan_v2': 8726, 'oasst1': 13}   \n",
       "17                                                                                                                                                                                                                                         {'cot': 100000}   \n",
       "18                                                                                                                                                                                                                             {'stanford_alpaca': 100000}   \n",
       "19                                                                                                                                                                                                                                                      {}   \n",
       "20                                                                                                                                                                                                                                 {'code_alpaca': 100000}   \n",
       "21                                                                                                                                                                                           {'cot': 2500, 'dolly': 2500, 'flan_v2': 2500, 'oasst1': 2500}   \n",
       "22                                                                                                                                                                                                                                       {'baize': 100000}   \n",
       "23                                                                                                                                                                                                                                    {'sharegpt': 100000}   \n",
       "24                                                                                                                                                                                                                                       {'dolly': 100000}   \n",
       "25                                                                                                                                                                                                                                      {'oasst1': 100000}   \n",
       "26                                                                                                                                                                                                                                 {'gpt4_alpaca': 100000}   \n",
       "27                                                                                                                                                                                                                                                      {}   \n",
       "\n",
       "    MMLU/0-shot  GSM/Direct  GSM/CoT  BBH/Direct  Codex-Eval/Pass@1  \\\n",
       "0     44.601909         3.5     14.0   37.147236           1.829268   \n",
       "1     23.123487         4.0      1.5   29.677548           0.609756   \n",
       "2     23.002421         3.5      3.0   29.757914           0.000000   \n",
       "3     23.557898         4.0      2.5   29.590479           1.219512   \n",
       "4     22.938328         5.0      1.5   29.973845           0.000000   \n",
       "5     23.045150         3.0      3.5   30.422551           0.000000   \n",
       "6     22.952571         3.0      3.0   30.284956           0.000000   \n",
       "7     23.016664         2.5      3.0   29.758117           1.219512   \n",
       "8     23.023786         2.5      2.5   29.827929           0.000000   \n",
       "9     23.151973         2.5      2.5   29.258770           0.000000   \n",
       "10    22.938328         3.5      0.5   30.391050           0.609756   \n",
       "11    23.201823         3.5      1.0   29.127942           1.829268   \n",
       "12    22.945449         4.0      3.5   28.108541           0.000000   \n",
       "13    22.938328         3.0      2.5   29.980136           0.000000   \n",
       "14    22.895599         3.0      3.0   30.028842           0.000000   \n",
       "15    22.852870         2.5      3.0   29.429958           0.000000   \n",
       "16    23.159094         3.0      3.5   29.467971           0.000000   \n",
       "17    23.016664         3.0      0.0   27.558459           0.000000   \n",
       "18    22.988178         3.0      2.5   27.052148           0.609756   \n",
       "19    23.721692         4.0      1.5   24.459354           0.000000   \n",
       "20    22.966814         3.5      0.5   26.876242           1.219512   \n",
       "21    22.852870         3.0      1.0   24.637130           0.000000   \n",
       "22    22.924085         3.5      1.5   25.485221           0.609756   \n",
       "23    23.052272         0.5      2.0   26.323305           0.000000   \n",
       "24    22.810141         2.5      1.5   15.476953           0.000000   \n",
       "25    22.973935         2.0      0.0   24.000967           0.000000   \n",
       "26    23.052272         1.5      0.5   25.812740           0.000000   \n",
       "27    32.459764         NaN     11.0         NaN           4.268293   \n",
       "\n",
       "    TydiQA/CB  TydiQA/GP    Average  ranking  \n",
       "0    9.934316  44.361673  22.196343  -2.1250  \n",
       "1    1.543347   8.898066   9.907458  -9.2500  \n",
       "2    1.821590   8.841241   9.989024  -9.7500  \n",
       "3    1.410211   6.140904   9.774144 -10.3125  \n",
       "4    1.875094   7.962404   9.892810 -11.0625  \n",
       "5    1.395340   7.712177   9.867888 -11.2500  \n",
       "6    1.896922   7.082380   9.745261 -11.8750  \n",
       "7    1.367148   7.950251   9.830242 -12.0625  \n",
       "8    2.158432   7.918454   9.704086 -12.0625  \n",
       "9    1.877706   7.116692   9.486449 -13.6875  \n",
       "10   1.773780   5.948287   9.380172 -14.1250  \n",
       "11   1.730040   4.095138   9.212030 -14.1875  \n",
       "12   1.083539   6.646854   9.469198 -15.0000  \n",
       "13   1.445112   7.088936   9.564645 -15.0000  \n",
       "14   1.448538   6.055629   9.489801 -15.2500  \n",
       "15   1.456560   8.441289   9.668668 -15.5000  \n",
       "16   0.922864   4.975758   9.289384 -15.8750  \n",
       "17   1.931833   7.495874   9.000404 -16.1250  \n",
       "18   1.615986   5.113901   8.982853 -16.2500  \n",
       "19   1.398436   5.114215   8.599100 -17.0000  \n",
       "20   1.383378   5.525014   8.852994 -17.8125  \n",
       "21   1.946636   5.612540   8.435597 -19.5000  \n",
       "22   1.329486   4.139038   8.498227 -19.7500  \n",
       "23   1.732119   3.702220   8.187131 -19.8750  \n",
       "24   2.347582   5.569227   7.171986 -20.8125  \n",
       "25   1.797507   6.326418   8.156975 -20.8750  \n",
       "26   1.571938   4.294121   8.104439 -21.2500  \n",
       "27   9.765382  37.489078  18.996503      NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfc = df.copy()\n",
    "dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "    lambda d: sum(list(d.values())) if d else 200000))\n",
    "# dfc[dfc['total_train_samples'].apply(\n",
    "#     lambda x: total_train_samples-500<x<total_train_samples+500)]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6ec921210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3618: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3619: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff9c6f3250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6ec19eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>32.459764</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.970313</td>\n",
       "      <td>5.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-7b+dolly</td>\n",
       "      <td>37.231164</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.603476</td>\n",
       "      <td>11.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-7b+oasst1</td>\n",
       "      <td>34.147557</td>\n",
       "      <td>7.5</td>\n",
       "      <td>29.692361</td>\n",
       "      <td>2.439024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-7b+dolly:oasst1</td>\n",
       "      <td>37.900584</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.150571</td>\n",
       "      <td>9.146341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  MMLU/0-shot  GSM/CoT  BBH/Direct  Codex-Eval/Pass@1\n",
       "0                llama-7b    32.459764     11.0   32.970313           5.487805\n",
       "4          llama-7b+dolly    37.231164     13.0   30.603476          11.585366\n",
       "5         llama-7b+oasst1    34.147557      7.5   29.692361           2.439024\n",
       "14  llama-7b+dolly:oasst1    37.900584      7.0   30.150571           9.146341"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima datƒÉ √Æn istoria Uniunii Europene, la drepturile persoanelor care apar≈£in acestor minoritƒÉ≈£i ≈üi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
