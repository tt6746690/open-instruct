{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da1794b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/data-pruning/wpq/github/mitibm2023/external/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': 'x86_64', 'cluster': 'ccc', 'queue': 'alt_7d'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "        get_run_statistics)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "from llm.submit import shell_scripts_template_slurm, shell_scripts_template_lsf, get_host_info, move_lsf_job_summary_to_save_dir\n",
    "from note_pruning_analysis import open_instruct_dir\n",
    "import getpass\n",
    "\n",
    "queue = None if getpass.getuser() == 'wpq' else 'alt_7d'\n",
    "info = get_host_info()\n",
    "info.update({'queue': queue})\n",
    "arch, cluster = info['arch'], info['cluster']\n",
    "print(info)\n",
    "\n",
    "os.environ['TORCHELASTIC_ERROR_FILE'] = os.path.join(os.getcwd(), 'torchelastic_error_file') \n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '../../../../mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##\n",
    "##\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c421d1",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c09b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "{\n",
      "    \"scoring_fn\": \"dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/oi2/llama-7b_sharegptv2_ep=2 using 6 GPUs, 1 batch size per GPU, 1 gradient accumulation steps, 30 effective batch size.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp7pssto1b', 'job_id': 1354503}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2p8zx1bt', 'job_id': 1354504}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"dpo2_ultrafeedback:llama-7b+sharegptv2ep2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 650,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=dpo2_ultrafeedback:llama-7b+sharegptv2ep2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=650GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2_mpillk', 'job_id': 1354505}]\n"
     ]
    }
   ],
   "source": [
    "from llm.submit import shell_scripts_template_slurm\n",
    "debug = False\n",
    "if debug:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'INFO'\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "else:\n",
    "    os.environ['TORCH_CPP_LOG_LEVEL'] = 'WARNING'\n",
    "    os.environ['NCCL_DEBUG'] = ''\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  650 if arch == 'ppc64le' else 64\n",
    "\n",
    "preprocessing_num_workers = 32\n",
    "report_to = 'wandb'\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16'\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float32'\n",
    "gradient_checkpointing = True\n",
    "use_fast_tokenizer = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "resume_from_checkpoint = True # resume from latest checkpoint if exists, otherwise train from scratch\n",
    "num_train_epochs = 2\n",
    "checkpointing_steps = 300 # (50_000 / 32) * 2 / 6 ~= 500 (data size of 50k, bsz=32, ep=2, total save 6 times at most)\n",
    "max_train_steps = None\n",
    "subsample_inds_file_list = [None]\n",
    "dataloader_sampler = 'RandomSampler'\n",
    "overwrite_cache = True\n",
    "\n",
    "\n",
    "# #####\n",
    "# job_name = 'dpo1'\n",
    "\n",
    "# # model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-410m-deduped'; max_seq_length = 2048; abbr_model_name = 'pythia-410m'\n",
    "# # model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "\n",
    "# train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama-7b+sharegptv2ep2'\n",
    "train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; dataset = 'ultrafeedback'\n",
    "\n",
    "\n",
    "# M = 60_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 20_000\n",
    "# M = 50_000; pacing_fn_list = [f'prune_size={M}_ep=5']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "    (10_000, 10),\n",
    "#     (30_000, 3),\n",
    "]]\n",
    "\n",
    "gen_output_md = 'llama7br512p4096'\n",
    "# gen_output_md = 'llama7b+sharegptv2ep2+r512p4096'\n",
    "# gen_output_model_name = 'all-mpnet-base-v2'\n",
    "\n",
    "scoring_fn_list = []\n",
    "scoring_fn_list += ['random_s=0']\n",
    "# scoring_fn_list += ['random_s=1']\n",
    "scoring_fn_list += [ \n",
    "    f'dppmap_k=vmf_gamma=1_kmd={gen_output_md}_kemb=grad+rp+loraB',\n",
    "    f'dppmap_k=rbf_gamma=1e-3_kmd={gen_output_md}_kemb=text+embedding',\n",
    "#     f'dppmap_k=rbf_gamma=1_kmd=mpnet_kemb=text+embedding',\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'dpo2_{dataset}:{abbr_model_name}'\n",
    "    \n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "# nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12\n",
    "# nodes = 2; num_gpus = 1; gpu_type = 'v100'; job_duration = 6; cpu_mem = 100; num_cpus = 32; max_train_steps = 5; checkpointing_steps = 2; report_to = 'tensorboard'\n",
    "\n",
    "    \n",
    "#####\n",
    "\n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs = 1 # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "use_deepspeed = True\n",
    "deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate.conf'\n",
    "\n",
    "per_device_train_batch_size = 1; total_batch_size = 32\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"{effective_batch_size} effective batch size.\")\n",
    "\n",
    "# reference: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n",
    "launcher = f\"\"\"accelerate launch \\\n",
    "    --mixed_precision {mixed_precision} \\\n",
    "    --num_machines {nodes} \\\n",
    "    --num_processes {num_gpus*nodes} \\\n",
    "    {'--use_deepspeed' if use_deepspeed else ''} \\\n",
    "    {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''} \\\n",
    "    {'--main_process_ip $master_addr' if use_deepspeed else ''} \\\n",
    "    {'--main_process_port $master_port' if use_deepspeed else ''} \\\n",
    "    {'--machine_rank $SLURM_PROCID' if use_deepspeed else ''} \\\n",
    "    {'--rdzv_backend c10d' if use_deepspeed and nodes>1 else ''} \\\n",
    "    {'--deepspeed_multinode_launcher standard' if use_deepspeed and nodes>1 else ''} \\\n",
    "\"\"\"\n",
    "\n",
    "cmds = []\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_inds_file_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (subsample_inds_file,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{dataset}\"\n",
    "    if any(job_name == y for y in ['dpo1']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "\n",
    "    if subsample_inds_file:\n",
    "        assert(num_train_epochs==1)\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                s = f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "\n",
    "    if subsample_inds_file is not None:\n",
    "        assert(dataloader_sampler=='SequentialSampler')\n",
    "        assert(num_train_epochs==1)\n",
    "    else:\n",
    "        assert(dataloader_sampler=='RandomSampler')\n",
    "\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join('results', job_name), exist_ok=True)\n",
    "    wandb_run_name = output_dir.replace('results/', '')\n",
    "\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {f'cd .. && CUDA_VISIBLE_DEVICES={os.environ[\"CUDA_VISIBLE_DEVICES\"]} ' if test_run else ''}{launcher}\n",
    "        open_instruct/dpo_tune.py \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --tokenizer_name {model_name_or_path} \\\n",
    "        {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "        --train_file {train_file} \\\n",
    "        --max_seq_length {max_seq_length} \\\n",
    "        {'--subsample_inds_file '+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        --dataloader_sampler {dataloader_sampler} \\\n",
    "        --preprocessing_num_workers {preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "        --learning_rate 5e-7 \\\n",
    "        --lr_scheduler_type linear \\\n",
    "        --warmup_ratio 0.1 \\\n",
    "        --weight_decay 0. \\\n",
    "        --num_train_epochs {num_train_epochs} \\\n",
    "        --with_tracking \\\n",
    "        {'--report_to \"'+str(report_to)+'\"' if report_to else ''} \\\n",
    "        --checkpointing_steps {checkpointing_steps} \\\n",
    "        {'--max_train_steps '+str(max_train_steps) if max_train_steps else ''} \\\n",
    "        {'--resume_from_checkpoint' if resume_from_checkpoint else ''} \\\n",
    "        {'--low_cpu_mem_usage' if not use_deepspeed else ''} \\\n",
    "        {'--overwrite_cache' if overwrite_cache else ''} \\\n",
    "        --logging_steps 1 \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    # if test_run:\n",
    "    #     print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "\n",
    "    if test_run:\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b79b9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_dpo.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce604bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=5\n",
      "+ cd ..\n",
      "+ CUDA_VISIBLE_DEVICES=2,5\n",
      "+ accelerate launch --mixed_precision fp16 --num_machines 1 --num_processes 2 --use_deepspeed --deepspeed_config_file ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py --model_name_or_path results/baselines/huggyllama/llama-7b --tokenizer_name results/baselines/huggyllama/llama-7b --gradient_checkpointing --train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl --max_seq_length 2048 --preprocessing_num_workers 32 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --learning_rate 5e-7 --lr_scheduler_type linear --warmup_ratio 0.1 --weight_decay 0. --num_train_epochs 2 --with_tracking --report_to tensorboard --checkpointing_steps 500 --logging_steps 1 --output_dir results/dpo1/jpt_llama-7b_ultrafeedback\n",
      "[2024-01-08 20:41:08,547] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-01-08 20:41:17,396] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-08 20:41:17,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "[2024-01-08 20:41:23,297] [INFO] [comm.py:662:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2024-01-08 20:41:23,611] [INFO] [comm.py:631:init_distributed] cdb=None\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "01/08/2024 20:41:23 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'auto_cast': True}}\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 251.17it/s]\n",
      "01/08/2024 20:41:25 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-201ebfceee303348/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 252.78it/s]\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:26,493] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 6.74B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.11s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-01-08 20:41:43,556] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.48B parameters\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/baselines/huggyllama/llama-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/baselines/huggyllama/llama-7b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "01/08/2024 20:42:20 - INFO - accelerate.accelerator - Updating DeepSpeed's gradient accumulation steps to 16 from 1.\n",
      "[2024-01-08 20:42:20,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.1+23a11a39, git-hash=23a11a39, git-branch=master\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "01/08/2024 20:42:20 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "[2024-01-08 20:42:20,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2024-01-08 20:42:20,754] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-01-08 20:42:20,779] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-01-08 20:42:20,779] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-01-08 20:42:20,891] [INFO] [utils.py:786:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 14.16 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,892] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.9 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:118:__init__] Reduce bucket size 16777216\n",
      "[2024-01-08 20:42:20,896] [INFO] [stage3.py:119:__init__] Prefetch bucket size 15099494\n",
      "[2024-01-08 20:42:20,995] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:787:see_memory_usage] MA 13.64 GB         Max_MA 13.64 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:20,996] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2024-01-08 20:42:21,139] [INFO] [utils.py:786:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.76 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,141] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:21,240] [INFO] [utils.py:786:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:787:see_memory_usage] MA 13.4 GB         Max_MA 13.4 GB         CA 16.92 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:21,241] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 102.88 GB, percent = 14.8%\n",
      "[2024-01-08 20:42:23,090] [INFO] [utils.py:786:see_memory_usage] After creating fp16 partitions: 4\n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.4 GB         CA 14.6 GB         Max_CA 17 GB \n",
      "[2024-01-08 20:42:23,091] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.99 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,190] [INFO] [utils.py:786:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:787:see_memory_usage] MA 13.39 GB         Max_MA 13.39 GB         CA 14.6 GB         Max_CA 15 GB \n",
      "[2024-01-08 20:42:23,191] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 91.81 GB, percent = 13.2%\n",
      "[2024-01-08 20:42:23,742] [INFO] [utils.py:786:see_memory_usage] After creating fp32 partitions\n",
      "[2024-01-08 20:42:23,743] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 26.61 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,744] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 116.15 GB, percent = 16.7%\n",
      "[2024-01-08 20:42:23,836] [INFO] [utils.py:786:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:787:see_memory_usage] MA 25.94 GB         Max_MA 25.94 GB         CA 29.55 GB         Max_CA 30 GB \n",
      "[2024-01-08 20:42:23,838] [INFO] [utils.py:794:see_memory_usage] CPU Virtual Memory:  used = 117.28 GB, percent = 16.8%\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 1; 31.75 GiB total capacity; 25.93 GiB already allocated; 3.34 GiB free; 27.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 806, in <module>\n",
      "    main()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/dpo_tune.py\", line 646, in main\n",
      "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1198, in prepare\n",
      "    result = self._prepare_deepspeed(*args)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1537, in _prepare_deepspeed\n",
      "    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/__init__.py\", line 171, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 310, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1205, in _configure_optimizer\n",
      "    self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1503, in _configure_zero_optimizer\n",
      "    optimizer = DeepSpeedZeroOptimizer_Stage3(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 324, in __init__\n",
      "    self._setup_for_real_optimizer()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 421, in _setup_for_real_optimizer\n",
      "    self.initialize_optimizer_states()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py\", line 892, in initialize_optimizer_states\n",
      "    gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.74 GiB (GPU 0; 31.75 GiB total capacity; 25.94 GiB already allocated; 3.21 GiB free; 27.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 409110) of binary: /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/python3.10\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/bin/accelerate\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 964, in launch_command\r\n",
      "    deepspeed_launcher(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 687, in deepspeed_launcher\r\n",
      "    distrib_run.run(args)\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "open_instruct/dpo_tune.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "[1]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 1 (local_rank: 1)\r\n",
      "  exitcode  : 1 (pid: 409111)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2024-01-08_20:42:28\r\n",
      "  host      : dcs068.ccni.rpi.edu\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 409110)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_dpo.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19aebd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`\n",
      "{\n",
      "    \"scoring_fn\": \"random_s=0\",\n",
      "    \"gen_output_md\": \"llama7br512p4096\",\n",
      "    \"gen_output_model_name\": \"llama-7b+lora:r=512:a=11585+proj=4096\"\n",
      "}\n",
      "Training results/baselines/huggyllama/llama-7b using 2 GPUs, 2 batch size per GPU, 32 gradient accumulation steps, Effective batch size 128\n",
      "\n",
      "cd .. && TORCHELASTIC_ERROR_FILE=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/error_file TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=10002 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/flan_v2/flan_v2_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=32 --per_device_train_batch_size=2 --gradient_accumulation_steps=32 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=100 --report_to tensorboard wandb --run_name /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi5_flan_v2:llama-7b/jpt_llama-7b_flan_v2_score=random:s=0_pace=prune:size=10000:ep=10 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=100 --save_total_limit=1 --num_train_epochs=1 --ddp_timeout=7200 --fsdp=\"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap=\"LlamaDecoderLayer\" --gradient_checkpointing --torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --overwrite_output_dir --subsample_inds_file=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/curriculum/llama-7b+lora:r=512:a=11585+proj=4096/flan_v2/random_s=0/inds_prune_size=10000_ep=10.pkl --dataloader_sampler SequentialSampler --use_flash_attn True --low_cpu_mem_usage --overwrite_cache --output_dir=\"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi5_flan_v2:llama-7b/jpt_llama-7b_flan_v2_score=random:s=0_pace=prune:size=10000:ep=10\"\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"oi5_flan_v2:llama-7b\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 192,\n",
      "    \"num_gpus\": 2,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": true,\n",
      "    \"queue\": \"x86_6h\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_hardwarespec_to_dirname = False\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100 if getpass.getuser() == 'wpq' else 1_000_000\n",
    "save_total_limit = 1\n",
    "preprocessing_num_workers = 32\n",
    "evaluation_strategy = 'no' # set do_eval=False\n",
    "eval_steps = save_steps\n",
    "report_to = 'tensorboard wandb'\n",
    "suffix = None\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.03\n",
    "dataloader_sampler = None\n",
    "hf_models_dir = 'results/baselines/'\n",
    "subsample_inds_file_list = [None]\n",
    "max_train_samples_list = [None]\n",
    "num_train_epochs_list = [1]\n",
    "scoring_fn_and_pacing_fn = None\n",
    "\n",
    "\n",
    "# ########### sft baselines\n",
    "\n",
    "\n",
    "# job_name = 'oi2'; num_train_epochs_list = [2] \n",
    "# # model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "\n",
    "# # train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# # train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# # train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'; \n",
    "# # train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1';\n",
    "# # train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2'\n",
    "# # train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2'; max_train_samples_list=[100_000]\n",
    "\n",
    "# # train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; max_train_samples_list=[100_000]\n",
    "# # train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2'; max_train_samples_list=[100_000]\n",
    "# ###########\n",
    "\n",
    "\n",
    "############ pruning runs\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'mistralai/Mistral-7B-v0.1'; abbr_model_name = 'mistral-7b'; max_seq_length = 2048; gen_output_md = 'mistral7br512p4096'\n",
    "model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048; gen_output_md = 'llama7br512p4096'\n",
    "\n",
    "\n",
    "dataset = 'flan_v2'; train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2';\n",
    "# dataset = 'dolly'; train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly';\n",
    "# dataset = 'stanford_alpaca'; train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca';\n",
    "# dataset = 'oasst1'; train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'; \n",
    "# dataset = 'wizardlmv2'; train_file = 'data/processed/wizardlm/wizardlmv2_data.jsonl'; abbr_train_file = 'wizardlmv2';\n",
    "# dataset = 'sharegptv2'; train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'; abbr_train_file = 'sharegptv2';\n",
    "# dataset = 'ultrachat200kv2'; train_file = 'data/processed/ultrachat/ultrachat200kv2_train_data.jsonl'; abbr_train_file = 'ultrachat200kv2';\n",
    "\n",
    "# dataset = 'open_orca_slim'; train_file = 'data/processed/open_orca/open_orca_slim_data.jsonl'; abbr_train_file = 'openorcaslim'; \n",
    "# dataset = 'tulu_v2'; train_file = 'data/processed/tulu_v2/tulu_v2_data.jsonl'; abbr_train_file = 'tulu_v2';\n",
    "        \n",
    "# M = 80_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 40_000\n",
    "# M = 40_000; pacing_fn_list = [f'prune_size={M}_ep=2']; subset_size = 20_000\n",
    "# M = 30_000; pacing_fn_list = [f'prune_size={M}_ep=3']; subset_size = 10_000\n",
    "# M = 20_000; pacing_fn_list = [f'prune_size={M}_ep=4']; subset_size = 5_000\n",
    "# M = 10_000; pacing_fn_list = [f'prune_size={M}_ep=10']; subset_size = 1_000\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep={ep}' for M, ep in [\n",
    "        (10_000, 10),\n",
    "        (20_000, 4),\n",
    "        (30_000, 3),\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "scoring_fn_list = [\n",
    "    'random_s=0',\n",
    "#     'random_s=1',\n",
    "#     'log_prob_neg', 'el2n_agg=mean', 'grad_loraB_l2n',\n",
    "#     'ifd_neg', 'log_pmi_neg',\n",
    "#     'numtoks_input_neg', 'numtoks_output_neg', 'numtoks_total_neg',\n",
    "#     f'dppmap_k=vmf_gamma=1_kmd=mpnet', #_kemb=text+embedding',\n",
    "    f'dppmap_k=rbf_gamma=1e-3_kmd=llama7br512p4096_kemb=text+embedding',\n",
    "    f'dppmap_k=vmf_gamma=1_kmd=llama7br512p4096_kemb=grad+rp+loraB',\n",
    "]\n",
    "\n",
    "scoring_fn_list += [ # vary kernel embedding model \n",
    "#     f'dppmap_k=vmf_gamma=auto{subset_size}_kmd={kmd}_kemb=grad+rp+loraB'\n",
    "#     for kmd in ['llama7br256p4096', 'llama7br512p4096', 'pythia1br512p4096']\n",
    "]\n",
    "scoring_fn_and_pacing_fn = list(itertools.product(scoring_fn_list, pacing_fn_list))\n",
    "\n",
    "\n",
    "job_name = f'oi5_{dataset}:{abbr_model_name}'\n",
    "############ \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "debug_mode = test_run\n",
    "\n",
    "if arch == 'x86_64':\n",
    "#     nodes = 1; num_gpus = 1; gpu_type = 'a100_80gb'; job_duration = 6\n",
    "    nodes = 1; num_gpus = 2; gpu_type = 'a100_80gb'; job_duration = 6\n",
    "    num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus)\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_checkpointing = True\n",
    "    mixed_precision = 'bf16'; torch_dtype = 'bfloat16'; use_flash_attn = True\n",
    "else:\n",
    "    nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 6\n",
    "#     nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 12\n",
    "#     nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 18\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_checkpointing = True\n",
    "    mixed_precision = 'fp16'; torch_dtype = 'float32'; use_flash_attn = False\n",
    "    \n",
    "\n",
    "if scoring_fn_and_pacing_fn is not None: # pruning runs. \n",
    "    print('Set up data pruning runs. epochs=1, sampler=SequentialSampler, and `subsampe_inds_file`')\n",
    "    num_train_epochs_list = [1] # offload handling of epochs to `generate_curriculum`\n",
    "    dataloader_sampler = 'SequentialSampler'\n",
    "    subsample_inds_file_list = []\n",
    "    for scoring_fn, pacing_fn in scoring_fn_and_pacing_fn:\n",
    "        from note_pruning import get_final_model_name\n",
    "        from note_pruning_analysis import get_full_model_name, curriculum_dir\n",
    "        gen_output_model_name = get_final_model_name(get_full_model_name(gen_output_md), scoring_fn)\n",
    "        print(json.dumps({'scoring_fn': scoring_fn, 'gen_output_md': gen_output_md, 'gen_output_model_name': gen_output_model_name}, indent=4))\n",
    "        p = os.path.join(curriculum_dir, gen_output_model_name, dataset, scoring_fn, 'inds_'+pacing_fn+'.pkl')\n",
    "        if not os.path.isfile(p):\n",
    "            raise ValueError(f'path={p} does not exists for {scoring_fn}')\n",
    "        subsample_inds_file_list.append(p)\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "\n",
    "total_batch_size = 128\n",
    "gradient_accumulation_steps = round(total_batch_size/(num_gpus*nodes)/per_device_train_batch_size)\n",
    "effective_batch_size = per_device_train_batch_size*nodes*num_gpus*gradient_accumulation_steps\n",
    "\n",
    "optimizer = 'adamw_hf'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\" \n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "elif 'pythia' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPTNeoXLayer'        \n",
    "elif 'mistral' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MistralDecoderLayer'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "# fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 \n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "load_in_8bit = False\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{per_device_train_batch_size} batch size per GPU, \"\n",
    "      f\"{gradient_accumulation_steps} gradient accumulation steps, \"\n",
    "      f\"Effective batch size {effective_batch_size}\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file = os.path.join(open_instruct_dir, 'scripts', 'error_file')\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "if not os.path.isfile(train_file):\n",
    "    print(f'train_file={train_file} does not exists')\n",
    "\n",
    "options_list = itertools.product(\n",
    "    num_train_epochs_list,\n",
    "    subsample_inds_file_list,\n",
    "    max_train_samples_list,\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "output_dirname_list = []\n",
    "for (num_train_epochs,\n",
    "     subsample_inds_file,\n",
    "     max_train_samples,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "    if max_train_samples:\n",
    "        output_dirname += f\":{int(max_train_samples/1000)}k\"\n",
    "            \n",
    "    if any(job_name == y for y in ['oi2']):\n",
    "        output_dirname += f'_ep={num_train_epochs}'\n",
    "        \n",
    "    if subsample_inds_file:\n",
    "        def subsample_inds_file_abbr_fn(x):\n",
    "            s = os.path.basename(x).split('.pkl')[0]\n",
    "            if s.startswith('inds_'):\n",
    "                scoring_fn = os.path.basename(os.path.dirname(x)).replace('_', ':')\n",
    "                pacing_fn = s.split('inds_')[-1].replace('_', ':')\n",
    "                return f'score={scoring_fn}_pace={pacing_fn}'\n",
    "            else:\n",
    "                return s\n",
    "        subsample_inds_file_abbr = subsample_inds_file_abbr_fn(subsample_inds_file)\n",
    "        if subsample_inds_file_abbr:\n",
    "            output_dirname += f'_{subsample_inds_file_abbr}'\n",
    "            \n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "            \n",
    "    if add_hardwarespec_to_dirname:\n",
    "        output_dirname += \\\n",
    "            ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "            ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "            ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "            '_mbsz='+str(per_device_train_batch_size)+\\\n",
    "            '_dtype='+torch_dtype+\\\n",
    "            ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "            '_seqlen='+str(max_seq_length)+\\\n",
    "            '_nodes='+str(nodes)\n",
    "    if suffix:\n",
    "        output_dirname += suffix\n",
    "    output_dir = os.path.join(open_instruct_dir, 'results', job_name, output_dirname)\n",
    "    os.makedirs(os.path.join(open_instruct_dir, 'results', job_name), exist_ok=True)\n",
    "    wandb_run_name = output_dir.replace('results/', '')\n",
    "    \n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    {'cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--max_train_samples='+str(max_train_samples) if max_train_samples else ''} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers={preprocessing_num_workers} \\\n",
    "        --per_device_train_batch_size={per_device_train_batch_size} \\\n",
    "        --gradient_accumulation_steps={gradient_accumulation_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type={lr_scheduler_type} \\\n",
    "        --warmup_ratio={warmup_ratio} \\\n",
    "        --weight_decay=0. \\\n",
    "        --optim={optimizer} \\\n",
    "        --evaluation_strategy={evaluation_strategy} \\\n",
    "        {'--eval_steps='+str(eval_steps) if eval_steps else ''} \\\n",
    "        {'--report_to '+str(report_to) if report_to else ''} \\\n",
    "        --run_name {wandb_run_name} \\\n",
    "        --logging_strategy=steps \\\n",
    "        --logging_first_step \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit={save_total_limit} \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        --ddp_timeout=7200 \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "        {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "        --use_flash_attn {'True' if use_flash_attn else 'False'} \\\n",
    "        --low_cpu_mem_usage \\\n",
    "        --overwrite_cache \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #  --overwrite_cache   # if delete a dataset and need to refresh cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    if arch == 'x86_64':\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        queue=queue,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fc2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_cmds_sft.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0c2894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=\n",
      "+ cd ..\n",
      "+ TORCHELASTIC_ERROR_FILE=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/error_file\n",
      "+ TORCH_CPP_LOG_LEVEL=INFO\n",
      "+ NCCL_DEBUG=INFO\n",
      "+ LOGLEVEL=INFO\n",
      "+ CUDA_VISIBLE_DEVICES=0,1\n",
      "+ torchrun --nproc_per_node=2 --master_port=10002 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/dolly/dolly_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=32 --per_device_train_batch_size=2 --gradient_accumulation_steps=32 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=100 --report_to tensorboard --run_name /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi2/jpt_llama-7b_dolly_ep=2 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=100 --save_total_limit=1 --num_train_epochs=2 --ddp_timeout=7200 '--fsdp=full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap=LlamaDecoderLayer --gradient_checkpointing --torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --overwrite_output_dir --use_flash_attn True --low_cpu_mem_usage --output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/jpt_llama-7b_dolly_ep=2\n",
      "[2024-01-14 16:18:53,802] torch.distributed.run: [WARNING] \n",
      "[2024-01-14 16:18:53,802] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-01-14 16:18:53,802] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-01-14 16:18:53,802] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO] Starting elastic_operator with launch configs:\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   entrypoint       : open_instruct/finetune_trainer.py\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   min_nodes        : 1\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   max_nodes        : 1\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   nproc_per_node   : 2\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   run_id           : none\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   rdzv_backend     : static\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   rdzv_endpoint    : 127.0.0.1:10002\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   rdzv_configs     : {'rank': 0, 'timeout': 900}\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   max_restarts     : 0\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   monitor_interval : 5\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   log_dir          : None\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO]   metrics_cfg      : {}\n",
      "[2024-01-14 16:18:53,803] torch.distributed.launcher.api: [INFO] \n",
      "[2024-01-14 16:18:53,806] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] log directory set to: /tmp/torchelastic_ksd7enii/none_h8abwftr\n",
      "[2024-01-14 16:18:53,806] torch.distributed.elastic.agent.server.api: [INFO] [default] starting workers for entrypoint: python3.10\n",
      "[2024-01-14 16:18:53,806] torch.distributed.elastic.agent.server.api: [INFO] [default] Rendezvous'ing worker group\n",
      "[I socket.cpp:576] [c10d] The server socket has started to listen on [::]:10002.\n",
      "[I socket.cpp:849] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:53412.\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO] [default] Rendezvous complete for workers. Result:\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   restart_count=0\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=127.0.0.1\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   master_port=10002\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   group_rank=0\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   group_world_size=1\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   local_ranks=[0, 1]\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   role_ranks=[0, 1]\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   global_ranks=[0, 1]\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   role_world_sizes=[2, 2]\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO]   global_world_sizes=[2, 2]\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO] \n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.api: [INFO] [default] Starting worker group\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.multiprocessing: [INFO] Setting worker0 reply file to: /tmp/torchelastic_ksd7enii/none_h8abwftr/attempt_0/0/error.json\n",
      "[2024-01-14 16:18:53,809] torch.distributed.elastic.multiprocessing: [INFO] Setting worker1 reply file to: /tmp/torchelastic_ksd7enii/none_h8abwftr/attempt_0/1/error.json\n",
      "[2024-01-14 16:19:02,180] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-14 16:19:02,185] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[I socket.cpp:849] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:55424.\n",
      "[I socket.cpp:849] [c10d] The client socket has connected to [localhost]:10002 on [localhost]:55436.\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1584: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/training_args.py:1584: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "Saving args dict to /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/jpt_llama-7b_dolly_ep=2.args.json\n",
      "Saving args dict to /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/jpt_llama-7b_dolly_ep=2.args.json\n",
      "01/14/2024 16:19:07 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "01/14/2024 16:19:07 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_sampler=RandomSampler,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=7200,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=100.0,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'min_num_params': 0, 'transformer_layer_cls_to_wrap': ['LlamaDecoderLayer'], 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=LlamaDecoderLayer,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=32,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/jpt_llama-7b_dolly_ep=2/runs/Jan14_16-19-07_cccxc501,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/jpt_llama-7b_dolly_ep=2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi2/jpt_llama-7b_dolly_ep=2,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "01/14/2024 16:19:07 - WARNING - __main__ - Process rank: 1, device: cpu, n_gpu: 1distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3ab84c08398a775b\n",
      "01/14/2024 16:19:07 - INFO - datasets.builder - Using custom data configuration default-3ab84c08398a775b\n",
      "Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "01/14/2024 16:19:07 - INFO - datasets.info - Loading Dataset Infos from /dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Generating train split: 14956 examples [00:00, 62021.35 examples/s]\n",
      "Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/dolly/json/default-3ab84c08398a775b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "01/14/2024 16:19:07 - INFO - datasets.builder - Found cached dataset json (/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/dolly/json/default-3ab84c08398a775b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/dolly/json/default-3ab84c08398a775b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "01/14/2024 16:19:07 - INFO - datasets.info - Loading Dataset info from /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/data/processed/dolly/json/default-3ab84c08398a775b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "[INFO|configuration_utils.py:715] 2024-01-14 16:19:07,912 >> loading configuration file results/baselines/huggyllama/llama-7b/config.json\n",
      "[INFO|configuration_utils.py:777] 2024-01-14 16:19:07,917 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/baselines/huggyllama/llama-7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-14 16:19:07,921 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-14 16:19:07,921 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-14 16:19:07,921 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-14 16:19:07,921 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2020] 2024-01-14 16:19:07,921 >> loading file tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3118] 2024-01-14 16:19:08,480 >> loading weights file results/baselines/huggyllama/llama-7b/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1222] 2024-01-14 16:19:08,481 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Traceback (most recent call last):\n",
      "  File \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune_trainer.py\", line 791, in <module>\n",
      "    main()\n",
      "  File \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune_trainer.py\", line 564, in main\n",
      "Traceback (most recent call last):\n",
      "  File \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune_trainer.py\", line 791, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n",
      "    main()\n",
      "  File \"/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/open_instruct/finetune_trainer.py\", line 564, in main\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3233, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3233, in from_pretrained\n",
      "    config = cls._check_and_enable_flash_attn_2(config, torch_dtype=torch_dtype, device_map=device_map)\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1273, in _check_and_enable_flash_attn_2\n",
      "    config = cls._check_and_enable_flash_attn_2(config, torch_dtype=torch_dtype, device_map=device_map)\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1273, in _check_and_enable_flash_attn_2\n",
      "    raise ImportError(\n",
      "ImportError: Flash Attention 2 is not available. Please refer to the documentation of https://github.com/Dao-AILab/flash-attention for installing it. Make sure to have at least the version 2.1.0\n",
      "    raise ImportError(\n",
      "ImportError: Flash Attention 2 is not available. Please refer to the documentation of https://github.com/Dao-AILab/flash-attention for installing it. Make sure to have at least the version 2.1.0\n",
      "[2024-01-14 16:19:18,852] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2233944) of binary: /dccstor/data-pruning/miniconda3/envs/open-instruct/bin/python3.10\n",
      "[2024-01-14 16:19:18,857] torch.distributed.elastic.multiprocessing.errors: [INFO] ('local_rank %s FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html', 0)\n",
      "Traceback (most recent call last):\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\n",
      "    run(args)\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "open_instruct/finetune_trainer.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-01-14_16:19:18\n",
      "  host      : cccxc501.pok.ibm.com\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2233945)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-01-14_16:19:18\n",
      "  host      : cccxc501.pok.ibm.com\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 2233944)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!bash gen_cmds_sft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499d6f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3')\n",
      "('mmlu_s=0', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=5', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_cot', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('humaneval', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_cb', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_gp', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=0_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('mmlu_s=5_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('gsm_s=8_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('bbh_s=3_cot_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('humaneval_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_cb_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "('tydiqa_s=1_gp_chatfmt', 'results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4')\n",
      "#cmds:  144 \n",
      "\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/mmlu_s=5\" --eval_batch_size 10 --ntrain 5 --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/bbh_s=3\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/bbh_s=3_cot\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/humaneval\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --use_vllm --no_context --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_gp\" --eval_batch_size 10 --use_vllm --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/mmlu_s=0_chatfmt\" --eval_batch_size 10 --ntrain 0 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/mmlu_s=5_chatfmt\" --eval_batch_size 10 --ntrain 5 --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=5_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/gsm_s=8_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/gsm_s=8_cot_chatfmt\" --eval_batch_size 10 --max_num_examples 500 --n_shot 8 --max_new_tokens 512 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/bbh_s=3_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --no_cot --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/bbh_s=3_cot_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --n_shot 3 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --max_num_examples_per_task 40 --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3_cot_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/humaneval_chatfmt\" --eval_batch_size 10 --max_new_tokens 512 --eval_pass_at_ks 1 --unbiased_sampling_size_n 3 --temperature 0.1 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_cb_chatfmt\" --eval_batch_size 10 --use_vllm --no_context --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4\" --save_dir \"results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=20000:ep=4/eval/tydiqa_s=1_gp_chatfmt\" --eval_batch_size 10 --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp_chatfmt\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"cpu_mem\": 96,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"a100_80gb\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"alt_7d\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gen_cmds_utils import remove_all_symlinks, create_unique_symlinks, get_chat_formatting_function, get_resource_for_task\n",
    "\n",
    "create_symlinks = False\n",
    "include_checkpoints = False\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "use_slow_tokenizer = True\n",
    "\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    'mmlu_s=5', \n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    'bbh_s=3_cot', # max_datapoints_per_task=40 -> 40min.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb', # 3min\n",
    "    'tydiqa_s=1_gp',\n",
    "    # 'toxigen', # ~1.5hr\n",
    "#     'alpacafarm_ann=gpt35:turbo:1106',\n",
    "    # 'alpacafarm_ann=chatgpt', # ~$1 per eval.\n",
    "]\n",
    "task_names_alpacafarm = ['alpacafarm_ann=chatgpt_chatfmt']\n",
    "task_names_chatfmt = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "# ## baselines eval \n",
    "# subdir_path_list = [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b', \n",
    "#     'mistralai/Mistral-7B-v0.1',\n",
    "#     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "#     'NousResearch/Llama-2-7b-hf',\n",
    "#     'NousResearch/Llama-2-7b-chat-hf',\n",
    "#     'HuggingFaceH4/mistral-7b-sft-alpha',\n",
    "#     'HuggingFaceH4/mistral-7b-sft-beta',\n",
    "#     'HuggingFaceH4/zephyr-7b-alpha',\n",
    "#     'HuggingFaceH4/zephyr-7b-beta',\n",
    "#     'codellama/CodeLlama-7b-hf',\n",
    "#     'codellama/CodeLlama-7b-Python-hf',\n",
    "#     'codellama/CodeLlama-7b-Instruct-hf',\n",
    "# ]]\n",
    "# task_names = task_names + task_names_chatfmt\n",
    "\n",
    "\n",
    "# # oi5\n",
    "# exp_dir = 'results/oi2'\n",
    "# exp_dir = 'results/oi5_tulu_v1_mix:llama-7b/'\n",
    "# exp_dir = 'results/oi5_ultrachat:mistral-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat200k:mistral-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat15:mistral-7b'\n",
    "# exp_dir = 'results/oi5_ultrachat200kv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlm:llama-7b'\n",
    "# exp_dir = 'results/oi5_wizardlmv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_sharegptv2:llama-7b'\n",
    "# exp_dir = 'results/oi5_tulu_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_open_orca_slim:llama-7b'\n",
    "exp_dir = 'results/oi5_stanford_alpaca:llama-7b'\n",
    "# exp_dir = 'results/oi5_flan_v2:llama-7b'\n",
    "# exp_dir = 'results/oi5_dolly:llama-7b'\n",
    "# exp_dir = 'results/oi5_oasst1:llama-7b'\n",
    "# exp_dir = 'results/oi6_starcoder_ep=5'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstr:codellama-7b'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstrv2:codellama-7b'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstrv4:codellama-7b'\n",
    "# exp_dir = 'results/oi5_starcoder_commentinstrv5:codellama-7b'\n",
    "# exp_dir = 'results/dpo1'\n",
    "# exp_dir = 'results/dpo2_ultrafeedback:llama-7b+sharegptv2ep2'\n",
    "subdir_filter_fn = lambda x: 'lama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10' not in x\n",
    "task_names = task_names + task_names_chatfmt\n",
    "# task_names = task_names_alpacafarm\n",
    "# task_names = ['humaneval', 'humaneval_chatfmt']\n",
    "# task_names = ['alpacafarm_ann=gpt35:turbo:1106_chatfmt']\n",
    "\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "\n",
    "num_gpus = 1\n",
    "if arch == 'x86_64': # ccc\n",
    "    gpu_type = 'a100_80gb' # or 'a100_40gb' but might need to change batch size to prevent oom\n",
    "    num_cpus = int(128/8*num_gpus); cpu_mem = int(768/8*num_gpus)\n",
    "    use_vllm = True; torch_dtype = 'bfloat16'\n",
    "else:\n",
    "    gpu_type = 'v100'\n",
    "    num_cpus = int(128/6*num_gpus); cpu_mem = int(512/6*num_gpus)\n",
    "    use_vllm = False; torch_dtype = 'float16'\n",
    "    \n",
    "if len(subdir_path_list)==0:\n",
    "    if create_symlinks:\n",
    "        remove_all_symlinks(exp_dir)\n",
    "    subdir_path_list = []\n",
    "    subdirs = list(os.listdir(exp_dir))\n",
    "    subdirs = filter(subdir_filter_fn, subdirs)\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        if include_checkpoints:\n",
    "            subdir_path_list += glob.glob(os.path.join(subdir_path, 'checkpoint-*'))\n",
    "        if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "            continue\n",
    "        subdir_path_list.append(subdir_path)\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "\n",
    "print('#cmds: ', len(list(task_name_and_model)), '\\n')\n",
    "\n",
    "if create_symlinks:\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = task_name_and_model\n",
    "    \n",
    "    \n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    chat_formatting_function = get_chat_formatting_function(model_name_or_path)\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "            ft_args = json.load(f)\n",
    "        # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "        # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "        if 'model_args' in ft_args:\n",
    "            ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "        else:\n",
    "            ft_args_model_name_or_path = ft_args['model_name_or_path']\n",
    "    except:\n",
    "        ft_args_model_name_or_path = model_name_or_path\n",
    "\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        # open-instruct used 200 examples. use higher amount to get a more accurate number\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 500 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        max_num_examples_per_task = 40\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens {512 if arch=='x86_64' else 256} \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--max_num_examples_per_task '+str(max_num_examples_per_task) if max_num_examples_per_task else ''} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 512 \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 3 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length 512 \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--no_context' if no_context else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('toxigen'):\n",
    "        # max_prompts_per_group=500 (out of 1000) is open-instruct default.\n",
    "        # eval batch size=1 much faster (llama-7b) not sure why.\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.toxigen.run_eval \\\n",
    "            --data_dir data/eval/toxigen \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size 1 \\\n",
    "            --max_prompts_per_group 200 \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('alpacafarm'):\n",
    "        match = re.search(r'ann=([^_]+)', task_name)\n",
    "        annotators_config = match.group(1)\n",
    "        annotators_config = annotators_config.replace(':', '_')\n",
    "        if not annotators_config in ['chatgpt', 'alpaca_eval_gpt4_0314', 'gpt35_turbo_1106']:\n",
    "            raise ValueError('Just support 2 annotators_config.')\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.alpaca_farm.run_eval \\\n",
    "            --reference_path alpaca_eval_data \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --max_new_tokens 2048 \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --annotators_config {annotators_config} \\\n",
    "            {'--use_vllm' if use_vllm else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''} \\\n",
    "            --chat_formatting_function {chat_formatting_function} \\\n",
    "            {'--use_slow_tokenizer' if use_slow_tokenizer else ''} \\\n",
    "            --torch_dtype {torch_dtype} \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.path.dirname(os.getcwd()),\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    if arch == 'x86_64': # ccc\n",
    "        shell_scripts = re.sub('~/.profile', '/dccstor/data-pruning/.profile', shell_scripts)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "        queue=queue,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ac978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7859d6d6",
   "metadata": {},
   "source": [
    "# Visualize Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b033ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./1223352.out does not have `--save_dir` specified. Probably still running.\n",
      "Move ./1223732.out -> /dccstor/data-pruning/results/baselines/huggyllama/llama-7b/eval/gsm_s=8_cot/1223732.out.lsf\n"
     ]
    }
   ],
   "source": [
    "# if job successfully ran, lsf system will generate a summary in log_dir,\n",
    "# call this function to move lsf summary to save_dir if job is successful.\n",
    "move_lsf_job_summary_to_save_dir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4392e9ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_fmt=both\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2068185/2171432402.py:306: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  df = df.drop('train_file', axis=1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 338\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrosemary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pd_average_col_contains_substr\n\u001b[1;32m    337\u001b[0m Ns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(np\u001b[38;5;241m.\u001b[39munique([\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_train_samples\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;28;01mif\u001b[39;00m x])\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m--> 338\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m N \u001b[38;5;129;01min\u001b[39;00m Ns\u001b[38;5;241m+\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "from rosemary import pd_sort_rows_by_avg_ranking\n",
    "from llm.evaluate import EvalResults, get_eval_results\n",
    "\n",
    "\n",
    "\n",
    "exp_dir = ''\n",
    "chat_fmt = None\n",
    "sort_rows = True\n",
    "use_normalized_preferred_metric = False\n",
    "\n",
    "\n",
    "# ## investigate code change / package update effect on eval baselines.\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# use_normalized_preferred_metric = False\n",
    "# sort_rows = False\n",
    "# save_dirs = [\n",
    "#     # llama\n",
    "#     ('llama-7b_12.13update_before', '../results/baselines/huggyllama/llama-7b_12.13update_before/'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('llama-7b_10.30update', '../results/baselines/huggyllama/llama-7b_10.30update/'),\n",
    "# #     ('llama-7b_09.23update', '../results/baselines/huggyllama/llama-7b_09.23update/'),\n",
    "# #     ('llama-7b_09.23update_before', '../results/baselines/huggyllama/llama-7b_09.23update_before/'),\n",
    "# #     # llama2\n",
    "#     ('llama2-7b_12.13update_before', '../results/baselines/NousResearch/Llama-2-7b-hf_12.13update_before/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b-chat', '../results/baselines/NousResearch/Llama-2-7b-chat-hf/'),\n",
    "# #     ('llama2-7b_10.30update', '../results/baselines/NousResearch/Llama-2-7b-hf_10.30update/'),\n",
    "# #     ('llama2-7b_original', '../results/baselines/NousResearch/Llama-2-7b-hf_original/'),\n",
    "# #     # mistral\n",
    "# #     ('mistral-7b_10.16update', '../results/baselines/mistralai/Mistral-7B-v0.1_10.16update/'),\n",
    "#     ('mistral-7b-Instruct-v0.1_12.13update_before', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1_12.13update_before'),\n",
    "#     ('mistral-7b-Instruct-v0.1', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     # zephyr\n",
    "#     ('zephyr-7b-beta_12.13update_before', '../results/baselines/HuggingFaceH4/zephyr-7b-beta_12.13update_before'),\n",
    "#     ('zephyr-7b-beta', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "\n",
    "# # baselines\n",
    "# save_dirs = []\n",
    "# save_dirs += [\n",
    "# #     ('gpt2', '../results/baselines/gpt2'),\n",
    "# #     ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "# #     ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "# #     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "# #     ('pythia-1.4b', '../results/baselines/EleutherAI/pythia-1.4b'),\n",
    "# #     ('pythia-2.8b', '../results/baselines/EleutherAI/pythia-2.8b'),\n",
    "# #     ('pythia-6.9b', '../results/baselines/EleutherAI/pythia-6.9b'),\n",
    "# #     ('dolly-v2-7b', '../results/baselines/databricks/dolly-v2-7b'),\n",
    "#     ('mistral-7b-v0.1', '../results/baselines/mistralai/Mistral-7B-v0.1'),\n",
    "# ]\n",
    "\n",
    "\n",
    "# save_dirs = [\n",
    "#     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b+lima_ep=2', '../results/ft1_ep=2/llama-7b_lima/'),\n",
    "# #     ('mistral-7b+lima_ep=2', '../results/ft1_ep=2/mistral-7b_lima/'), \n",
    "# ]\n",
    "# exp_dir = '../results/oi2/'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "\n",
    "# exp_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# # exp_dir = '../results/ft2'\n",
    "# # exp_dir = '../results/ft1'\n",
    "# exp_dir = '../results/ft1_ep=2'\n",
    "# # save_dirs = [\n",
    "# #     ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "# #     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1/'),\n",
    "# # ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#               [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'tuluv1m' in x]\n",
    "\n",
    "# exp_dir = '../results/oi3'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/')]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in \n",
    "#              glob.glob(os.path.join(exp_dir, 'llama-7b_all:600k_humanmix', 'checkpoint-*'))]\n",
    "\n",
    "# # exp_dir = '../results/oi4'\n",
    "# # exp_dir = '../results/oi4_perf_cross_time'\n",
    "# # exp_dir = '../results/oi4_flanv2_prune_with_hmv1_model'\n",
    "# exp_dir = '../results/oi4_flan_v2_vary_subsetsize'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# exp_dir = '../results/oi4_flan2022_1m'\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "# #              ('llama-7b_flan_v2_ep=2', '../results/ft1/llama-7b_flan_v2'),\n",
    "# #              ('llama-7b_humanmix_ep=2', '../results/ft1/llama-7b_humanmix'),\n",
    "#              ('llama-7b_flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_flan_v2'),\n",
    "#              ('llama-7b_humanmix_ep=1', '../results/ft1_ep=1/llama-7b_hmv1'),\n",
    "# #              ('llama-7b_cot:flan_v2_ep=1', '../results/ft1_ep=1/llama-7b_cot:flanv2'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "\n",
    "# # exp_dir = '../results/oi4_tulu_v1_mix'\n",
    "# exp_dir = '../results/oi4_tulu_v1_mix_ep=3'\n",
    "# use_normalized_preferred_metric = False\n",
    "# save_dirs = [('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#              ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "#             ]\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "\n",
    "# ###### ultrachat\n",
    "# save_dirs = [\n",
    "#     # baselines \n",
    "#     ('mistral-7b', '../results/baselines/mistralai/Mistral-7B-v0.1/'),\n",
    "#     ('mistral-7b_ultrachat200k_aftersplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k'),\n",
    "#     ('mistral-7b_ultrachat200k_beforesplitlongconv_ep=2', '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'),\n",
    "    \n",
    "#     ('mistral-7b-Instruct', '../results/baselines/mistralai/Mistral-7B-Instruct-v0.1'),\n",
    "#     ('mistral-7b_sft-alpha', '../results/baselines/HuggingFaceH4/mistral-7b-sft-alpha'),\n",
    "#     ('mistral-7b-sft-beta', '../results/baselines/HuggingFaceH4/mistral-7b-sft-beta'),\n",
    "#     ('mistral-7b-sft-alpha+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-alpha'),\n",
    "#     ('mistral-7b-sft-beta+dpo', '../results/baselines/HuggingFaceH4/zephyr-7b-beta'),\n",
    "# ]\n",
    "# # exp_dir = '../results/oi5_ultrachat:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_ultrachat200k:mistral-7b'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# exp_dir = '../results/oi5_ultrachat15:mistral-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "#####\n",
    "# dataset = 'stanford_alpaca'\n",
    "# dataset = 'open_orca_slim'\n",
    "# dataset = 'sharegptv2'\n",
    "# dataset = 'ultrachat200kv2'\n",
    "# dataset = 'wizardlm'\n",
    "# dataset = 'wizardlmv2'\n",
    "# dataset = 'tulu_v2'\n",
    "# dataset = 'flan_v2'\n",
    "# dataset = 'oasst1'\n",
    "# dataset = 'dolly'\n",
    "dataset_list = [\n",
    "    'stanford_alpaca', \n",
    "#     'dolly',\n",
    "#     'oasst1', \n",
    "#     'flan_v2', \n",
    "#     'tulu_v2', \n",
    "#     'wizardlmv2', \n",
    "#     'sharegptv2', \n",
    "#     'ultrachat200kv2',\n",
    "]; finetune_type = 'sft'\n",
    "# dataset_list = [\n",
    "#     'ultrafeedback',\n",
    "#     'ultrafeedbackfull',\n",
    "# ]; finetune_type = 'pref'\n",
    "\n",
    "## older\n",
    "# dataset = 'tulu_v1_mix'\n",
    "save_dirs = []\n",
    "save_dirs += [('llama-7b', '../results/baselines/huggyllama/llama-7b'),\n",
    "#              ('llama-7b_lima_ep=5', '../results/oi2/llama-7b_lima_ep=5/'),\n",
    "#              ('llama-7b_lima_ep=10', '../results/oi2/llama-7b_lima_ep=10/'),\n",
    "            ]\n",
    "for dataset in dataset_list:\n",
    "    if dataset == 'tulu_v2':\n",
    "        save_dirs += [('llama-7b_tulu_v2:100k_ep=2', '../results/oi2/llama-7b_tulu_v2:100k_ep=2'),]\n",
    "    elif dataset == 'open_orca_slim':\n",
    "        save_dirs += [('llama-7b_openorcaslim:100k_ep=2', '../results/oi2/llama-7b_openorcaslim:100k_ep=2'),]\n",
    "    elif dataset == 'sharegptv2':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_sharegptv2_ep=2', '../results/oi2/llama-7b_sharegptv2_ep=2'),\n",
    "            ('llama-7b_sharegpt_ep=2', '../results/ft1_ep=2/llama-7b_sharegpt'),]\n",
    "    elif dataset == 'tulu_v1_mix':\n",
    "        save_dirs += [\n",
    "            ('llama-7b_tuluv1_mix_ep=2', '../results/ft1_ep=2/llama-7b_tuluv1m'),\n",
    "            # oi4_tulu_v1_mix_ep=3 models before transformers update.\n",
    "            # ('llama-7b_tuluv1m:50k_log_prob_decr_<10.16update', '../results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'),\n",
    "        ]\n",
    "    elif dataset == 'ultrafeedback':\n",
    "        exp_dir = f'../results/dpo1/'\n",
    "        save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "    else:\n",
    "        save_dirs += [(f'llama-7b_{dataset}_ep=2', f'../results/oi2/llama-7b_{dataset}_ep=2'),]\n",
    "    \n",
    "    if finetune_type == 'sft':\n",
    "        exp_dir = f'../results/oi5_{dataset}:llama-7b'\n",
    "    elif finetune_type == 'pref':\n",
    "        exp_dir = f'../results/dpo2_{dataset}:llama-7b+sharegptv2ep2/'\n",
    "    save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'dppmapbd' not in x and 'semdedup' not in x]\n",
    "    \n",
    "    if finetune_type == 'pref':\n",
    "        sft_model_dataset = 'sharegptv2'\n",
    "        save_dirs += [('llama-7b_sharegptv2_ep=2', f'../results/oi2/llama-7b_{sft_model_dataset}_ep=2')]\n",
    "# ## just compare dppmap grad vs. text\n",
    "# save_dirs = [x for x in save_dirs if 'prune:size=10000:ep=10' in x[1] and (\n",
    "#         'random' in x[1] or \n",
    "#         'dppmap' in x[1]\n",
    "#     )\n",
    "# ]\n",
    "#####\n",
    "\n",
    "\n",
    "# ##### code instructions\n",
    "# save_dirs = [\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('codellama-7b', '../results/baselines/codellama/CodeLlama-7b-hf/'),\n",
    "#     ('codellama-7b-instruct', '../results/baselines/codellama/CodeLlama-7b-Python-hf/'),\n",
    "#     ('codellama-7b-python', '../results/baselines/codellama/CodeLlama-7b-Instruct-hf/'),\n",
    "# ]\n",
    "\n",
    "# exp_dir = '../results/oi2'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)] if 'starcoder' in x]\n",
    "# exp_dir = '../results/oi6_starcoder_ep=5'\n",
    "# # save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstr:codellama-7b'\n",
    "# # exp_dir = '../results/oi5_starcoder_commentinstrv2:codellama-7b'\n",
    "# exp_dir = '../results/oi5_starcoder_commentinstrv5:codellama-7b'\n",
    "# save_dirs += [(os.path.basename(x), x) for x in [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "# #####\n",
    "\n",
    "\n",
    "###### \n",
    "\n",
    "from llm.evaluate import detect_oom_evals\n",
    "oom_eval_paths = detect_oom_evals([x for l in [glob.glob(os.path.join(x[1], 'eval/*/*.out')) for x in save_dirs] for x in l])\n",
    "if oom_eval_paths: print(oom_eval_paths)\n",
    "    \n",
    "cols_avg_blacklist = ['AlpacaFarm/Len']\n",
    "\n",
    "# chat_fmt = False\n",
    "# chat_fmt = True\n",
    "chat_fmt = 'both'\n",
    "# chat_fmt = 'auto' # base model no chatfmt, tuned model with chatfmt\n",
    "# chat_fmt = 'mix'  # non-alpacaeval no chatfmt, alpacaeval chatfmt\n",
    "ft_args_fields = {\n",
    "    'run_name': ('run_name',),\n",
    "    'model_name_or_path': ('model_args.model_name_or_path', 'model_name_or_path'),\n",
    "    'subsample_mixture': ('data_args.subsample_mixture',),\n",
    "    'max_train_samples': ('data_args.max_train_samples', 'max_train_samples'),\n",
    "    'train_file': ('data_args.train_file', 'train_file'),\n",
    "}\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'TydiQA/GP', 'Codex-Eval/Pass@1']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/Direct', 'BBH/Direct', 'TydiQA/CB', 'Codex-Eval/Pass@1']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT']\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "#     cols = ['MMLU/0-shot', 'GSM/CoT', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR']\n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1'] #  'ToxiGen/Acc'\n",
    "cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*', 'AlpacaFarm/Len'] \n",
    "# cols = ['AlpacaFarm/WR', 'AlpacaFarm/ΔWR', 'AlpacaFarm/Len']\n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*'] \n",
    "# cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR', 'AlpacaFarm/ΔWR', 'AlpacaFarm/Len'] \n",
    "\n",
    "\n",
    "# cols = [f'BBH {x}' for x in ['reasoning', 'nlu', 'knowledge', 'multilingual']]; cols = [x+'/Direct' for x in cols] + [x+'/CoT' for x in cols]\n",
    "# cols = [f'MMLU {x}' for x in ['STEM', 'humanities', 'social sciences', 'other']]; cols = [x+'/0-shot' for x in cols] + [x+'/5-shot' for x in cols]\n",
    "\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR', 'AlpacaFarm/Rep', 'AlpacaFarm/WR*'] #  'ToxiGen/Acc'\n",
    "#     cols = ['AlpacaFarm/WR', 'AlpacaFarm/Rep', 'AlpacaFarm/WR*']\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR'] #  entire, without tydiqa, which has high variance\n",
    "#     cols = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'Codex-Eval/Pass@1', ] #  entire, without tydiqa, which has high variance\n",
    "if 'open_orca_slim' in exp_dir:\n",
    "    cols = ['MMLU/0-shot', 'MMLU/5-shot', 'BBH/Direct', 'BBH/CoT']\n",
    "    cols = ['MMLU/0-shot', 'BBH/Direct']\n",
    "if 'starcoder' in exp_dir:\n",
    "    cols = ['Codex-Eval/Pass@1']\n",
    "    chat_fmt = 'both'\n",
    "print(f'chat_fmt={chat_fmt}')\n",
    "df = get_eval_results(save_dirs, chat_fmt=chat_fmt, ft_args_fields=ft_args_fields, use_normalized_preferred_metric=use_normalized_preferred_metric)\n",
    "\n",
    "cols = [x for x in cols if x in df.columns]\n",
    "df = df[list(ft_args_fields.keys()) + cols]\n",
    "if chat_fmt == 'both':\n",
    "    for col_lvl2 in ['', 'chatfmt']:\n",
    "        df[('Average', col_lvl2)] = df[list(set(df.columns) & set([(x, col_lvl2) for x in list(set(cols)-set(cols_avg_blacklist))]))].mean(axis=1)\n",
    "else:\n",
    "    df['Average'] = df[list(set(cols)-set(cols_avg_blacklist))].mean(axis=1)\n",
    "if sort_rows:\n",
    "    df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "    sort_value_col, sort_value_col_ascending = ('Average', 'chatfmt') if chat_fmt=='both' else 'Average', False\n",
    "#     sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "    df = df.sort_values(by=sort_value_col, ascending=sort_value_col_ascending)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def compute_total_train_samples(x):\n",
    "    match = re.search(r'size=(\\d+)', x['run_name' if chat_fmt!='both' else ('run_name', '')])\n",
    "    total_train_samples = match.group(1) if match else None\n",
    "    return total_train_samples\n",
    "df.insert(1, 'total_train_samples' if chat_fmt!='both' else ('total_train_samples', ''), df.apply(compute_total_train_samples, axis=1))\n",
    "def extract_dataset_from_train_file(x):\n",
    "    if x is None: return None\n",
    "    x = x.split('/')[-1].split('.jsonl')[0]\n",
    "    if x.endswith('_data'): x = x[:-5]\n",
    "    if x.endswith('_train'): x = x[:-6]\n",
    "    return x\n",
    "df.insert(1, 'dataset' if chat_fmt!='both' else ('dataset', ''), df['train_file'].apply(extract_dataset_from_train_file))\n",
    "df = df.drop('train_file', axis=1)\n",
    "\n",
    "if any(exp_dir.endswith(x) for x in ['ft2']):\n",
    "#     for model_name_contain in ['gpt2', 'llama', 'pythia-1.4b']:\n",
    "#         for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "    for model_name_contain in ['llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "#         for total_train_samples in [200000, 400000, 600000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-20000<x<total_train_samples+20000)]\n",
    "            dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.drop(columns=['model_name_or_path', 'subsample_mixture'])\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "else:\n",
    "    for model_name_contain in ['llama', 'pythia-1.4b', 'mistral', 'zephyr']:\n",
    "        dfc = df.copy()\n",
    "        dfc = dfc[dfc['model_name_or_path'].apply(\n",
    "            lambda x: model_name_contain in x.lower())]\n",
    "        if not len(dfc): continue\n",
    "        from rosemary import pd_average_col_contains_substr\n",
    "        Ns = sorted(np.unique([int(x) for x in df['total_train_samples'].to_numpy() if x]).tolist())\n",
    "        datasets = sorted(np.unique(df['dataset']).tolist())\n",
    "        for N in Ns+[None]:\n",
    "            for dataset in datasets:\n",
    "                dfc = df.copy()\n",
    "                dfc = dfc[dfc['total_train_samples'].apply(lambda x: int(x) == N if x else True)]\n",
    "                dfc = dfc[dfc['dataset'].apply(lambda x: x == dataset if x else True)]\n",
    "                if not len(dfc): continue\n",
    "                col_runname = 'run_name' if chat_fmt != 'both' else ('run_name', '')\n",
    "                substitute = True\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, '_random_', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=10000:ep=10', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=50000:ep=5', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=3', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=150000:ep=1', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=100000', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=200000', substitute=substitute)\n",
    "                dfc = pd_average_col_contains_substr(dfc, col_runname, 'score=random:s=\\d_pace=prune:size=400000', substitute=substitute)\n",
    "                #     dfc = dfc.sort_values(['ranking'], ascending=False)\n",
    "                col = ('Average', 'chatfmt') if chat_fmt == 'both' else 'Average'\n",
    "            #     col = 'AlpacaFarm/WR'\n",
    "            #     col = 'MMLU/0-shot'|\n",
    "            #     col = 'GSM/CoT'\n",
    "            #     col = 'BBH/Direct'\n",
    "            #     col = 'TydiQA/GP'\n",
    "                dfc = dfc.sort_values(by=[col], ascending=False)\n",
    "                dfc = dfc.drop(columns=['model_name_or_path', 'subsample_mixture', 'max_train_samples', 'dataset'], \n",
    "                               axis=1, level=0 if chat_fmt=='both' else None)\n",
    "                dfc = dfc.reset_index(drop=True)\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .applymap(lambda x: f'max-width: 60ch;', subset=['run_name'])\n",
    "                        .set_table_styles([{'selector': 'td', 'props': [('white-space', 'pre-wrap'), ('word-wrap', 'break-word')]}])\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .applymap(lambda x: 'text-decoration: underline;' \\\n",
    "                                  if x in dfc[list(set(dfc.columns) & set([(x, '') for x in cols]))+[col] if chat_fmt=='both' else cols+[col]].values.flatten() and chat_fmt=='both' else '')\n",
    "                        .format(precision=1))\n",
    "\n",
    "# llama-7b_tulu_v1_mix(paper)\n",
    "# MMLU/0-shot, MMLU/5-shot, GSM/Direct, GSM/CoT, BBH/Direct, BBH/CoT, TydiQA/GP, TydiQA/CB, CodexEval/Pass@1, AlpacaEval(vs.Davinci-003)\n",
    "# 44.8       , 47.1       , 7.0       , 25.0   , 38.5      , 38.5   , 43.5,    , 8.0      , 18.6,           , 48.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deeba0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'dppmap', 'k': 'rbf', 'gamma': 0.001, 'kmd': 'llama7br512p4096', 'kemb': 'text+embedding'}\n",
      "{0: 'dppmap', 'k': 'rbf', 'gamma': 0.001, 'kmd': 'llama7br512p4096', 'kemb': 'text+embedding'}\n",
      "{0: 'dppmap', 'k': 'rbf', 'gamma': 0.001, 'kmd': 'llama7br512p4096', 'kemb': 'text+embedding'}\n",
      "{0: 'dppmap', 'k': 'rbf', 'gamma': 0.001, 'kmd': 'llama7br512p4096', 'kemb': 'text+embedding'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run_name',\n",
       " 'nonchat',\n",
       " 'sort_by',\n",
       " 'total_train_samples',\n",
       " 'model_name_or_path',\n",
       " 'subsample_mixture',\n",
       " 'max_train_samples',\n",
       " 'MMLU/0-shot',\n",
       " 'MMLU/5-shot',\n",
       " 'GSM/Direct',\n",
       " 'GSM/CoT',\n",
       " 'BBH/Direct',\n",
       " 'BBH/CoT',\n",
       " 'TydiQA/CB',\n",
       " 'TydiQA/GP',\n",
       " 'Codex-Eval/Pass@1',\n",
       " 'AlpacaFarm/WR*',\n",
       " 'AlpacaFarm/Len',\n",
       " 'Average',\n",
       " 'ranking']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rosemary import parse_kv_from_string\n",
    "\n",
    "non_chateval_task_names = ['MMLU/0-shot', 'MMLU/5-shot', 'GSM/Direct', 'GSM/CoT', 'BBH/Direct', 'BBH/CoT', 'TydiQA/CB', 'TydiQA/GP', 'Codex-Eval/Pass@1', ]\n",
    "\n",
    "def compute_nonchateval_average_performance(row):\n",
    "    L = [row[k] for k in non_chateval_task_names if k in row]\n",
    "    return np.average(L)\n",
    "\n",
    "def parse_prune_subset_size(row):\n",
    "    run_name = row['run_name']\n",
    "    match = re.search(r'(?<=pace=)([^_]+)', run_name)\n",
    "    if match:\n",
    "        pace = match.group(1).replace(':', '_')\n",
    "        kvs = parse_kv_from_string(pace)\n",
    "        return int(kvs['size'] / kvs['ep'])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_sort_by_from_run_name(row):\n",
    "    run_name = row['run_name']\n",
    "    match = re.search(r'score=([^_]+)', run_name)\n",
    "    if match:\n",
    "        sort_by = match.group(1).replace(':', '_')\n",
    "        kvs = parse_kv_from_string(sort_by)\n",
    "    else:\n",
    "        kvs = {}\n",
    "    return kvs\n",
    "\n",
    "\n",
    "def parse_sort_by_type(row):\n",
    "    d = row['sort_by']\n",
    "    if 'gamma' in d:\n",
    "        if d['gamma'] == 1e-3:\n",
    "            print(d)\n",
    "    if not d:\n",
    "        return None\n",
    "    if d[0] == 'dppmap':\n",
    "        if d['k']=='vmf' and d['kmd']=='mpnet': #and (d['gamma']==1 or d['gamma'] == 'auto1000'):\n",
    "            return 'vmf+text'\n",
    "        elif d['k']=='rbf' and d['kemb']=='text+embedding' and d['kmd'] == 'llama7br512p4096':\n",
    "            return f\"rbf+text_gamma={d['gamma']}\"\n",
    "        elif d['k']=='vmf' and d['kemb']=='grad+rp+loraB' and d['kmd'] == 'llama7br512p4096':\n",
    "            return f\"vmf+grad_gamma={d['gamma']}\"\n",
    "        else:\n",
    "            return None\n",
    "    elif d[0] == 'random':\n",
    "        return 'random'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "dfc.insert(1, 'sort_by' if chat_fmt!='both' else ('sort_by', ''), dfc.apply(parse_sort_by_from_run_name, axis=1))\n",
    "dfc.insert(1, 'sort_by_type' if chat_fmt!='both' else ('sort_by_type', ''), dfc.apply(parse_sort_by_type, axis=1))\n",
    "dfc.insert(1, 'subset_size' if chat_fmt!='both' else ('subset_size', ''), dfc.apply(parse_prune_subset_size, axis=1))\n",
    "dfc.insert(1, 'nonchat' if chat_fmt!='both' else ('nonchat', ''), dfc.apply(compute_nonchateval_average_performance, axis=1))\n",
    "\n",
    "\n",
    "dfc = dfc[dfc['sort_by_type'].notnull()]\n",
    "startswithstrs = ('rbf+text', 'random')\n",
    "startswithstrs = (\n",
    "    'random', \n",
    "#     'rbf+text_gamma=0.001', \n",
    "    'vmf+grad_gamma=1',\n",
    "#     'rbf+text_gamma=auto1000', \n",
    "    'vmf+grad_gamma=auto1000',\n",
    ")\n",
    "dfc = dfc[dfc.apply(lambda x: x['sort_by_type'].startswith(startswithstrs)\n",
    "                   , axis=1)]\n",
    "dfc['subset_size'] = dfc['subset_size'].apply(lambda x: int(x) if x else x)\n",
    "dfc = dfc[dfc['subset_size']<=10_000]\n",
    "\n",
    "\n",
    "D = dfc.set_index(['sort_by_type', 'dataset', 'subset_size']).to_dict()\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class DKey:\n",
    "    sort_by_type: str\n",
    "    dataset: str\n",
    "    subset_size: int\n",
    "def convert_key_to_dataclass(d):\n",
    "    return {DKey(*k): v for k, v in d.items()}\n",
    "D = {k: convert_key_to_dataclass(v) for k, v in D.items()}\n",
    "\n",
    "list(D.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662539cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWoAAAJOCAYAAAAu1D7cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gU1dfA8e+mJ6QR0uiEbiiGFum9ShFp0qsgqEhViq8UC0Wk6g9QkCZFkCZIE6QTpIQmUsUQaugkgfTd+/6xZsmyG5JANpvA+TxPHsjMnZkzs3dnJ2fvnNEopRRCCCGEEEIIIYQQQgghrMbG2gEIIYQQQgghhBBCCCHEq04StUIIIYQQQgghhBBCCGFlkqgVQgghhBBCCCGEEEIIK5NErRBCCCGEEEIIIYQQQliZJGqFEEIIIYQQQgghhBDCyiRRK4QQQgghhBBCCCGEEFYmiVohhBBCCCGEEEIIIYSwMknUCiGEEEIIIYQQQgghhJVJolYIIYQQQgghhBBCCCGsTBK1WWjWrFmUKVMGR0dHNBoNdevWpWfPnmg0Gnbv3m3t8DJVTEwM69evp0+fPpQvXx53d3dy5crF66+/zueff86jR4+sHaJIRWb0ycuXLxv6eEq7d+9Go9HQs2fPF4pRZA2tVsuYMWMoVqwYDg4OVn3tFi1ahEajYdy4cS/1NsXL5+7du8yfP59+/foRFBSEnZ0dGo2Gn3/+2dqhZboiRYqg0WisHUaGZZfPpocPH7J8+XI6d+5MYGAguXLlws3NjTfeeIOZM2eSmJiY6rI6nY4ZM2ZQrlw5nJ2d8fHxoX379pw5c+aZ2/ztt9+oU6cOHh4euLu7U6dOHX777TezbceNG4dGo2HRokUvspupyo7XjhqNhiJFimT5dl+Eufdhatdl2UFoaCjjxo2jVq1a5MuXD0dHRwoWLEjXrl05deqUtcN7Ydnl/PKqOXfuHJMnT6ZBgwYUKlQIR0dH/P39adOmDfv27bN2eEII8UySqM0ia9euZdCgQdy8eZNWrVrRo0cPmjZtau2wLGb58uW8/fbbLFiwAJ1OR9OmTalVqxZhYWGMHTuWKlWqcPv2bWuHKUS2k50u6GfOnMkXX3xBXFwcbdq0oUePHtSsWdPaYQmRraSVnNy/fz99+/Zl3rx5nDx5Eq1Wm4XRWV92ThBltbS+/Pnmm2/o0qULK1euxMXFhZYtWxIcHMzJkycZPHgw9evXJyYmxmQ5pRTvvPMOQ4YM4dq1azRv3pwyZcqwZs0aKleuzKFDh8xub9asWbRs2ZKQkBCqV69O/fr1OXLkCC1btmTWrFmZuevpIteOr56kpCQqV67M+PHjOXfuHBUqVKBVq1Y4OjqybNkyKleuzOrVq60dprAwS3xONGzYkJEjR3L06FFKly5N69at8fHxYd26ddSpU4cZM2Zk2raEECKz2Vk7gFfF+vXrAVi9ejX169c3TM8OyRhLcHBwYMCAAQwZMoQSJUoYpt+8eZPmzZtz/PhxBg8ezPLly60YpRDiWZLPW/v27aNo0aLWDUaIHMrPz4/333+fKlWqUKVKFSZPnsxPP/1k7bBENuTq6sro0aN5//33yZ8/v2H6xYsXadiwIfv37+fLL79kwoQJRsstXLiQ1atXU6JECfbt24efnx8Aa9asoV27dnTp0oVz585hZ/fksv/ChQsMGzYMR0dHdu3aRbVq1QzTq1evzrBhw2jWrJnRNZylZcdrx7Nnz2Jvb59l23sVvfHGG3z22Wc0a9YMGxv9GCKdTseYMWP46quv6N27N3Xr1sXb29vKkYqcJDAwkClTptC2bVscHBwM07///nv69+/P8OHDady4MYGBgVaMUgghzJMRtVnk2rVrAK9MsqN79+7Mnj3b5AI/b968/O9//wP0o4wTEhKsEZ4QIh1etfOWEJZQrVo1/ve//9GzZ0/KlCljSEQI8bSRI0fy1VdfGSVpAUqUKMGkSZMAWLFihclyU6dOBeDrr782JGkB2rZtS6tWrbh06RK//vqr0TIzZ84kKSmJ/v37G5K0ACVLluTTTz8lKSkpy0fVZsdrx9KlS1OsWLEs296rxs7Ojj///JPmzZsbnRttbGz44osvKF26NNHR0WzatMmKUYqc6Pfff6dTp05GSVqA9957j8aNG6PVavnll1+sFJ0QQjyb/LVgYcn1vHbt2gVAQEAAGo0mzRqgJ06c4JNPPqFSpUr4+Pjg6OhI0aJFef/997lx44ZJ+5S3jMTGxjJy5EgKFy6Mo6MjxYsXZ/LkySilnmsfBg4ciEajYe7cuam2KVOmDBqNhgsXLqS5vtdffx2A+Ph47t2791wxiRe3Zs0agoODcXZ2xs/Pj+7du5vtW8nOnDlDly5dyJs3Lw4ODuTPn5/u3btz/vz5F4rjgw8+QKPRMG/ePLPzlVIUK1YMW1tbwsPDX2hb1nb27Fm6detGsWLFcHJywsfHh6CgIAYPHszNmzfp2bMn9erVA2Dx4sWGc8XTt8pu2rSJ3r1789prrxnV8JswYQLx8fEm2015u+2VK1fo3LkzPj4+ODs7U7lyZTZu3GjUPrlOcVhYGIBRHJcvXza0u3r1Ku+9957hXOPr60ubNm04cuSISQwpz1FRUVEMGzaMgIAA7O3tGTx4sKHdqVOnaNGiBR4eHnh4eNCoUSMOHjz4AkddTynFihUr6NixIyVLljTUfgwODmb27NnodLp0rytlHectW7ZQs2ZNXF1dyZ07N23atOHcuXMmy8TFxfHjjz/y1ltvUbRoUZydnfH09KR27drPrFWqlGLZsmU0aNCAPHny4OTkRNGiRencuTMHDhywyP5Z08GDB3nrrbcMn3tFihQx+7n3PMczMTGR77//nuDgYLy9vXFxcaFIkSK0aNHCZJnHjx8zefJkgoKC8PT0xNXVlWLFitG+fXu2bdsGPClTknxeSvk+yWk1Lc1JqwxLeuqZjxs3joCAAAD27NljdIxSrjf5mCUkJPD5559TunRpHB0dad26NWD5909K9+/fZ8CAAeTNmxdHR0fKli3LggULzLbNyLm4bt269OrVC4Dx48cbHYv01HxNvnZ6+r0QFhbGmTNncHZ2pnnz5ibLtWvXDsDkPJ9chzZ5fkrt27c3u0xqlFIMGjQIjUZD7dq1iYyMtPq1Y1xcHE5OTob+l1KLFi3QaDSGz9uUypYti52dHVFRUYD5GrUpXztzPynbP0/fTfne2rZtG/Xq1cPT0xONRsPDhw8BfcmAiRMnUqJECUO//uyzzzKcxE55fXDp0iU6dOiAt7c37u7uNGvWzFDjOCkpiQkTJlCyZEmcnJwoXrw4s2fPNlpXaGgoGo2GqlWrprq9r7/+Go1Gw6effppmbBqNhnLlygGm/T67SOu67mmWOr+A8Wt54cIFOnbsiJ+fHzY2NoY7pAD++usvunTpQv78+XF0dCRfvnz06tXL6PoupUePHjF8+HAKFiyIs7MzgYGBzJo1C6WU2ffH03G0bduWPHnykCtXLmrUqMHmzZuN2qf1OXH79m3s7OzInz9/qtcyq1atQqPR0KVLF7Pzn5ba+VQIIbILKX1gYUFBQfTo0YOtW7dy69Yt2rZti6urKwD+/v6pLjdp0iRWr15N2bJlqVGjBhqNhhMnTjBnzhzWr1/P0aNHyZcvn8lyCQkJNG7cmL///pvg4GBee+019uzZw8iRI4mOjubLL7/M8D506dKF7777jmXLltG/f3+T+SdOnODMmTNUqVKFkiVLprm+f//9FwB7e3u8vLwyHI94cd999x0DBw7E1taWOnXq4O3tzY4dO6hatarh4iWlP/74g5YtWxIbG0vFihWpW7cu586d46effmLdunVs3ryZWrVqPVcs/fv3Z/bs2cybN4++ffuazN+5cyf//vsvTZs2pXDhws+1jezg2LFj1KxZk7i4OIKDgwkODiY6Opp///2XmTNn0rp1a2rWrElERATbtm2jWLFiRvVgg4KCDP/v06cPjx8/pkyZMpQrV46oqCgOHz7Mp59+yh9//MHvv/+Ora2tSQyXL1+mSpUqODk5UbNmTW7dusXBgwdp3bo1W7ZsoXHjxgCG7a5evZrHjx/To0cPwzqSz19//fUX9evX5+7du5QuXZo2bdpw5coV1q1bx8aNG1m+fLnhj/2UYmNjqVOnDuHh4dSpU4eKFSuSO3duAA4dOmSowRgUFETp0qU5ffo0derUeeEyMfHx8XTu3JncuXMTGBhIxYoVuXv3LgcPHuSDDz7g8OHDGX5Azi+//MKcOXOoXLkyLVu25NSpU6xbt46dO3eyZ88eo/fS5cuXeffdd/Hz86N06dIEBwcTERFBSEgI+/bt49y5cyZ1K7VaLR07dmT16tU4OjpSs2ZNvL29DcfZwcGBGjVqWGz/strSpUvp2bMnOp2O6tWrU7BgQY4dO8acOXNYu3Ytu3fvpnTp0sDzHc9u3bqxcuVKvL29qV69Oi4uLly/fp19+/bx6NEjOnbsCOiPe+PGjQkJCaFAgQLUrVsXBwcHrl27xm+//UauXLlo0qQJ/v7+9OjRw+z7RG7R1QsKCqJt27asWbMGPz8/o9r8T9e71ul0tG7dmr1791KnTh3Kly9Pnjx5AMu/f5I9fPiQatWqERkZSXBwMI8ePWLv3r306dMHnU7Hu+++a9Q+I+fipk2bkpSUxIEDB3j99deNzunFixdP81gmXzs9fe148uRJQJ9gNHeLfsWKFY3aJe/nlStXAKhQoYLJMgUKFMDb25vw8HAiIyPx8PBINa6kpCR69+7NTz/9RPPmzfnll19wdna2+rWjk5MTb7zxBnv37uXy5cuGZJJWq2X//v2A/ouh5IQu6B/8d+bMGSpWrIi7u3uq6075Xk/pn3/+4cCBA0afv8/Td5MtX76c+fPnU7lyZZo1a8alS5cM9bA7derE6tWrcXV1pWnTpiilmDZtGsePH3+ugRlhYWEEBwfj6elJnTp1uHjxIlu3biU0NJRTp07Rv39/du7cSbVq1ShatCi7du3igw8+wN7e3nDtVqlSJUqXLs2hQ4e4dOmS2ZHIyWUrOnfunK64Uuv32UF6ruvy5s1raG/J80tK58+fp0qVKuTJk4d69erx4MEDw7lhzZo1dO7cmYSEBCpVqkT16tW5dOkSixYtYuPGjezZs4cyZcoY1hUXF0eDBg04fPgwPj4+tGjRgkePHvHxxx9z6dKlZx6fS5cuERwcjJeXF40bN+bGjRvs27ePFi1asGDBAsN1XVqfE76+vjRs2JBt27axa9cuGjRoYLKt5H6V3kRtdu5XQggBgBJZok6dOgpQYWFhRtN79OihALVr1y6j6X/88Ye6ceOG0TStVqvGjx+vANWrVy+jeWFhYQpQgKpVq5a6c+eOYd6RI0eUnZ2dcnFxUdHR0c8Vf7FixZRGo1GXL182mffxxx8rQM2YMSNd63r33XcVoFq2bPlcsYgXExYWphwdHZWjo6NRv3v8+LFq1KiRoR8lz3v06JHy8/NTgJozZ47RuqZNm6YAVaBAARUXF2e0DUDVqVPHqP2uXbsUoHr06GE0vXr16gpQJ06cMIn3nXfeUYBas2bNC+23tSW/183tx5kzZwzv99SOUUrr1q1Tjx49MpoWFRWlWrRooQC1ePFio3kLFy40vK4DBw5UiYmJhnkzZswwnDeeVrhwYWXuY0Kn06ly5copQI0aNUrpdDrDvF9++UXZ2NgoNzc3FRERYZie8hxVrVo19eDBA6N1arVaVbp0aQWoiRMnGs37v//7P8OyY8eOTfW4PEtiYqJas2aNio+PN5p++/ZtVblyZQWoPXv2GM1LPm5PbzP5tQTUDz/8YJiu0+nUiBEjFKAqVqxotMzdu3fVtm3blFarNZr+77//qiJFiigbGxuTz4cvvvhCAapcuXIm59579+6p/fv3v9D+ZSdXrlxRzs7Oys7OTm3cuNEwXavVqsGDBytAValSxTA9o8czuf9VqVJFxcbGGi0TExOjQkJCDL8nvwffeustk/U/fPhQHT161Ghaau+T1CT3nxUrVqR7GWtI61xk7vrF3LFI7fMgpeT3U/HixdW1a9dM5lv6/ZO8r4Bq27at0fl1/fr1ClCFChUyiet5z8XPcx5r2LCh4Rye0syZMxWg3n77bbPLPXz4UAHKy8vLMO3kyZMKULlz5051e0FBQQpQp06dMkwbO3asAtTChQuVUvr3TvK+du7c2eizRSnrXzuOGTPGKF6l9NfEgCpTpoxJ//3ll18UoIYNG2aYBqjChQunua27d++qokWLKkCtXLnSaHpG+27Kz5iff/7ZZFvLly9XgCpatKjR++Xff/9VBQoUMCybUmrvw5TXB0OHDjXEqdPpVM+ePRWgAgMDVdmyZdXVq1cNy+3YscPssfn8888VoD7//HOTuM+cOaMAFRQUZHoAzdi3b58ClIODg8nfRNlBRq/rsur8AqgPP/xQJSUlGc3/999/lYuLi/Lw8DC5Hli8eLHJ56xST86j1apVU5GRkYbpJ0+eVLlz5zbbB1LG0b17d6PzwsaNG5Wtra3KlSuX0Wua1ufEkiVLFKB69+5tMu/BgwfKwcFBeXt7m5yDzPnnn3+Uo6OjAkw+z4UQIruQRG0WyWii9lny589vdMGt1JMPOBsbG3X+/HmTZVq2bJnh7aT02WefmU2g6HQ6VbBgQWVra6tu3ryZ5no2bdqkNBqNsre3N5uUE5aX/Fr27dvXZN65c+eURqMx6isLFixINZGnlFKVKlUySTpkNFGbfIH44YcfGk2/e/eucnR0VH5+fiohISHjO5uNNGvWTAEmCcqnpSdRm5qLFy8qQLVp08ZoevJFc9GiRU2OY2JiosqdO7eyt7c3SfKlloDauXOnAlRAQIDJHwJKKdWmTRuT80XKRO2RI0dMlvnjjz8UoEqWLGmU+E2OsVChQi+UqH2W7du3G/5ITSmtRG316tVN1pWQkKAKFiyoAKPk37PMmzdPAWrWrFmGafHx8crT01NpNBqzxysjUtu/7CQ5odKtWzeTeXFxcSpfvnwKUAcPHkxzXeaO56FDhxSgBg0alObyK1euVICaPn16umKXRO0uw7QXTdT+8ssvGY4zM94/yfvq7u6u7t27ZzI/+Yupp6/hUpPWuTij57E5c+YoQHl6eqrr168bzfvqq68UoLp06WJ22cTEREOyK9mBAwcUoPLnz5/qNmvUqGFyHkuZqH348KGqXbu2AtQHH3xgct5WyvrXjsmfKyn78DfffGNIpj79Wnz44YcKMPqyKD2J2sTERFWvXj0FqM8++yzd8Znru0o9eW81b97c7HK1atVSgFq2bJnJvO+///65ErXFihUzSXKdOnXKsK6dO3eabKtChQom74tLly4pQJUqVcqk/aeffqoANWXKFLP7lVJkZKQqUaKEAtSIESPSbG8NGb2uy6rzi4+Pj3r8+LHJcoMGDVKA+v77782ut3Xr1gpQoaGhhmnJiX9zn73Jn9upJWpdXV3V/fv3TZZLHoAxYcIEw7S0Pieio6MNSeaUA0OUevI++uCDD8wum1JiYqKqWbOmAtQ777yTZnshhLAWKX2Qjd27d48NGzZw+vRpHj58iFarBfR19u7fv8/9+/dNbv8qUqSI2VvIkqeZq5eUHl26dOGLL75g+fLljBw50jB93759XL16lcaNG6d5+8jZs2fp2rUrSimmTJli9hZ7YXnJt/x16NDBZF6pUqWoUKECx44dM0zbt28fkPrtRF27diU0NJR9+/YZbh3OqA4dOjBkyBCWLl3K119/jbOzMwBLliwhPj6enj175vinLleqVIktW7bQvXt3/u///o/KlSu/0EOFLl68yObNm/nnn394/PgxOp3OcLvjxYsXzS5Tt25dk+NoZ2dH0aJFCQ0N5d69e0a36aUmuU+88847Zm+769atG2vXrmXfvn1G5wvQPxSmcuXKJssk98v27dsbbu1MGWO7du2YNm1amrGl5cSJE/z++++Eh4cTExODUoro6Ggg9eOWGnP93d7enrZt2zJjxgz2799v9JAe0O/n7t27uX79OnFxcSilDOfllNs/evQoDx8+pFKlSmaPV1bsX1Z61nnG0dGR9u3bM3PmTPbt22dU/zC9x7N06dLkypWLhQsXUqZMGdq0aWO4rf5pQUFB2NjYMGXKFPz9/WnevDlubm6ZubvCDI1GQ8uWLZ/ZxtLvn8qVK5u9rb5kyZL89ddf3Lx506Qe4/OcizNiz549hvqvCxYsMCl7lbytp8+bz5KeZZLbmHP79m3q1avH8ePH+eyzz/j888/NtrP2tWP16tVxdHQ0qqO8e/duPD09adeuHQUKFDCZZ2NjY1KWIy0DBw5k165dvP3224wfP95sm/T23ZRatWplMi0xMZFDhw5hY2Njtr5wp06deO+99zIUP+ivD+zsjP8sTH6QqIODA3Xq1DFZplixYhw/ftzofVG0aFGqVq3Kn3/+ybFjxwylNwB+/vlnbGxs0rxW1Gq1dO7cmYsXLxIcHJxq/7K2jF7XZdX5pWHDhri4uJhM3759OwBvvfWW2eVq1qzJ+vXrOXLkCBUrVuTKlStcu3aNAgUKmK073L59+2e+No0bNzaUtkqpU6dOrFy50nDdlx6urq60atWKn3/+mU2bNtGmTRvDvIyUPRg4cCD79++naNGiJjWWhRAiO5FEbTa1YsUK+vXrx6NHj1JtEx0dbfKBX6BAAbNtk+tKmis+nx6lSpWiUqVKhIaG8tdffxmK+6f3w/HatWs0bdqUBw8eMHToUAYNGvRccYgXl1w4v1ChQmbnFypUyChRm9w+tYfjJE9/kYL8Tk5OdO/enRkzZrB69Wq6desGwPz589FoNPTp0+e5151dfPzxx+zfv5+NGzeyceNGPDw8eOONN2jRogU9e/ZMdyJIKcXw4cOZPn16qn9IJyfmnpZZ54cX6ROp9bv09MsXkZCQQM+ePc0+MT1ZasctNanVTDa3/5GRkbRp04adO3ema/tXr14FSPfTxi2xf1kpo30qo8fT3d2defPm0a9fP/r168d7771HqVKlqFevHt27dzf6I7RkyZJMmTKFkSNH0qlTJ2xtbSlbtiwNGzakV69eRvX7RObx9fXF0dHR7DxLv3+SZeQc+SLn4vQ6deoUrVu3JiEhgVmzZvH222+btEn+7Hj8+LHZdSRPT96H9CwDEBMTY7Jcsk8//ZSkpCQGDBjwzESNta8dnZycCA4OZt++fVy+fJlChQqxf/9+ateujY2NDXXq1GH16tXExcXx6NEj/v77bypUqICnp2e6tzF79mzmzp1L+fLl+emnn0yS3xntuymZ+9y7d+8eCQkJhge7Ps3NzQ1PT0/DQ8fSK3/+/CbTcuXKBejreJpLQCbPf/raoUuXLvz5558sW7bMkKj9888/uXTpEvXq1Uv1fZasX79+bNq0iVKlSrFp0yaz+5kdZPS6LqvOL6ldLyU/LCytL0fu3r0LPPm8LViwYIa2kywj10jp0aVLF37++WeWLVtmSNTeuHGDPXv2EBAQYPLF+NM+//xz5s6di5+fH9u2bZPnpAghsrXnH84lLCY8PJyePXsSHx/PjBkzuHjxomFklFLK8EFk7oM7IyMqMir5gjr5AjsxMZHVq1fj7Oxs9o+HZHfv3qVRo0ZcuXKFXr168c0331gsRpG25xl9k572L9r3kh82Mn/+fABCQkI4c+YMdevWpUSJEi+07uzA3d2dnTt3sm/fPj755BNKlSrFH3/8wUcffUSpUqXSfChDspUrVzJt2jTy58/P6tWruX79OgkJCSilDBf5qV3UZ/b54Xn6RPJDW572vP0yvaZNm8aKFSsoW7YsW7Zs4datW4bjdv78eaMYXpS59YwYMYKdO3dSu3Ztdu/ezd27d0lKSkIpxbZt21JdLr3HIyv3z5LS26ee53h26tSJf//9l3nz5tGuXTvu37/PnDlzqFatGp988olR26FDh3Lp0iVmzZrFm2++SXh4OFOnTqV8+fL873//y8Q9zrlSe/r280rt3ACWf/88T/sXORenx6VLl2jSpAkPHz5k3LhxDBw40Gy75GTJtWvXzM5Pnp4yqZL8/wcPHqSarDW3XLI2bdrg4ODATz/9lOaoOGtfOyaPBN29ezcnTpzg4cOH1K1bF9CPIo2Pj+fPP/9k7969KKUM89Jj165dDBo0CB8fHzZs2GBIXKb0vH0XzL8nLPVZ+az1ZXRb77zzDnZ2dvz888+G80R6k/Mff/wxCxYsoGDBgmzfvj1bP5gxo9d1WXV+Se1cqtVq0Wg09OjR45k/T38Zmdl97XnPi02aNMHb25tNmzYRGRkJYOhjafWr//3vf4wdOxYPDw+2bt2argc4CiGENUmiNhvavHkzCQkJfPTRRwwaNIjixYsbbgWHJ0+qzGodO3bE1taW5cuXGy4w7927R6tWrVIdDRgdHU2zZs04d+4cbdq0Yd68eRZNJou0Jd82GR4ebnZ+8pOgn24fFhZmtn3yetJzy/yzlCpVirp167J3717Onz/PvHnzAAxPE34ZaDQaatasyeTJkzl06BA3b96kU6dO3Lx5k9GjR6drHevWrQNgzpw5tG3blnz58hnKGWTVucESfSKj/TKjko/bihUraNq0Kb6+vi983NKKNeUtyuvWrcPW1pYNGzZQp04d8uTJYygbYW77ySNY/vnnn3TFYon9y0oZ7VMZPZ7JfHx8ePfdd1m1ahURERFs2bIFd3d3pkyZwpkzZ4zaFixYkIEDB7Jhwwbu3LnDTz/9hI2NDUOHDs3waLWcKHkUW2p39iSPWs0Kln7/PG9MYJlz8Y0bN2jUqBEREREMGjSIsWPHpto2uRTA6dOnSUxMNJmffIdM+fLlDdM8PT0NCdjjx4+bLHPt2jXu3r1LoUKF8PDwMJnfrFkzfvnlF+Lj43nzzTc5ePBgqvFZ+9oxOfG6e/duQ5mDlInap+eZu8XfnH///Zf27dtjY2PD2rVrUx09+LznqtR4e3vj4OBAREQECQkJJvOjo6Otfn7y8fGhUaNG3Lhxg927d6PValm1ahWOjo60bds21eUmTpzIN998g6+vL9u3b091JGd2khnXdeZY4vxSoEABlFLMmjWLRYsWpfrTunVr4MnnbWrXX2ldl2XkGik97O3tad++PfHx8axZswZ48gVA586dU11u2bJlDBw4EBcXFzZt2kRQUFCGtiuEENYgidps6MGDB4D5W0327t3LrVu3sjokQP+BXa9ePa5cucKBAwfS/HY8Pj6et956i6NHj9KkSRNWrFhhtp6lyFrJtdd++eUXk3kXLlzgxIkTRtNq1aoF6C90zEmentzuRSTXVZs2bRqrVq3Cy8vLqA7Vy8bHx4dx48YB8NdffwFPkiNJSUlml3nW+WHVqlUWiNJU8mu9cuVKQ+3slJYuXWrULj2S++WaNWtMRlskJSUZLsqflyWO28qVK02mpYy1Ro0aRtt3c3Mzm/Qwt/3KlSvj6enJsWPHCA0NTTOW7NAvXsSzzjMJCQmG81Vyu4weT3M0Gg1NmzalefPmgD7RlRo7Ozu6du1KlSpVSEhI4MKFC4Z5ab1nc6rkP9JT7muye/fuGZXIeZbMOD6Wfv88b0yQsfdceo7FgwcPaNKkCWFhYfTq1Yvp06c/M46AgABee+01YmNj2bRpk8n81atXA9CiRQuj6cn9Pnl+Ssnvt6eXSalVq1asWrWKuLg4mjZtyqFDh8y2s/a1Y/Xq1XFwcDAkY3Pnzm1IbhcvXtxQpza5Pm3t2rXTXGd0dDQtW7bk3r17zJ49+5k1bTPjXJWSvb09wcHB6HQ6s5+LP//8c4bXaQkpR1L/8ccf3Lp1i+bNm6daVuKHH35g9OjReHp6sm3bNkqVKpWF0WYec9d1z8MSn+kNGzYEYP369elqX7hwYfLly8e1a9fMvr/NnTtS+v33381+aZBcoinlNVJ6PydS9qsLFy4QGhpKxYoVee2118y237x5s+E5F+vWrTPaphBCZGeSqM2Gkh/8tXTpUqNb0q5fv264Pdxakj8gf/jhBzZs2ICXlxdNmzY1aafVaunUqRO7du2iVq1arF27NtvWmHrV9OrVCwcHB5YsWWJ4gA9AbGwsgwYNMrmdtUOHDvj5+bFv3z5++OEHo3mzZs3iyJEjFChQ4Jm3MKZXmzZt8PHx4YcffiAmJobu3bunWrMwp5k7d67Z0YJbtmwBntxemjzCIPl29aclnx9++OEHo4Tmvn37mDJlSqbGnJq6detSrlw5wsLCGDNmjFEc69evZ+3atbi6utKzZ890r7NevXqULFmSc+fOmdzi+uWXX6Y6MiO9ko/b3LlzjaavXr2aJUuWPNc6Dxw4wIIFCwy/K6UYO3YsV65c4fXXX6d69epG23/48KFJcnf69Ons2rXLZN0ODg4MGTIEpRR9+vQxGb14//59Dhw4YNH9y0p9+vTB2dmZFStWGCWbdDodo0eP5vr161SpUsVQSzajx/P48eOsXbvWZMThgwcPDH+AJr8Hd+3axY4dO0zOheHh4Zw9exaNRmNUazCt92xOFRAQQKFChfjrr7/49ddfDdMfP35M3759iYqKStd6vL29sbe359KlS2a/2EkPS79/njcmyNi5OK2+EhMTw5tvvsnp06fp0KFDukeSDh06FIBPPvmE27dvG6avXbuWDRs2EBAQYBgll2zQoEHY2toyd+5c/vzzT8P0ixcv8tVXX2Fra8tHH330zO22bt2an3/+mZiYGJo0acLRo0fNtrPmtaOzszNVqlQhPDyc7du3G+rTJqtTpw4HDx7k9OnTvP7662nWp9XpdHTu3JkzZ84waNCgNGvoZ7Tvpkfyl9pjxowxekhweHg4X3zxxXOtM7O1bt2aXLlysWbNGhYuXAiknpxfvXo1AwYMwNXVlc2bN+eYEY/pva57Hpa41hs2bBjOzs4MGTKEjRs3msy/f/8+s2fPJjY21jAtua8NGzbMqCbu6dOn+fbbb5+5vUePHjF06FCj5OvmzZv55ZdfcHFxoUePHobp6f2cqF69OkWKFGHXrl1MnToVSL1fHThwwPDAvZUrV9K4ceNnxiuEENmKElmiTp06ClBhYWFG03v06KEAtWvXLsO0+Ph4VaZMGQUof39/1bZtW9W8eXPl4uKiqlevrqpXr26yrrCwMAWoOnXqmN3+2LFjFaAWLlz4QvsRGRmpnJycFKAA1b9/f7PtZsyYYWjz9ttvqx49epj9uXPnzgvFI57P9OnTFaBsbW1VgwYN1DvvvKPy5cunChQooFq0aGHSJ3fs2KGcnZ0VoCpVqqQ6deqkKlSooACVK1cutXfvXqP1p9Yfd+3apQDVo0ePVGP75JNPDH3n9OnTmbjX1vX6668rQAUGBqq2bduqd955RwUFBSlAOTs7q5CQEEPb8uXLK0BVqVJF9ezZU/Xp00f9+uuvSimlzp8/r3LlymVYV8eOHVWtWrWURqNRw4cPV4AqXLiw0bYXLlyoADV27FizsaV2fipcuLBK7WPi1KlTKk+ePApQr732murUqZOqUaOGApSdnZ1atWqVUfu0zlFKKRUSEmLoZxUqVFCdOnVS5cqVU/b29urdd9995j6kZc+ePcrW1taoD1euXFkBhuP2dGypHbfk8/aAAQOURqNRwcHBqlOnTobztpubmzp27JjRMkuXLjX061q1aqlOnTqpwMBAZWNjo4YMGWL2fZGYmKhat26tAOXo6KgaNmyoOnbsqKpXr66cnJyM2j/P/mU3P/30k7K1tVUajUbVrFlTderUSZUqVUoBys/PT509e9bQNqPHc926dQpQHh4eqkGDBqpLly6qefPmyt3d3fA5lSz5/Ojj46OaNm2qunTpoho3bmz47Bs8eLBR3FOnTjXE2LFjR9WnTx81YsQIozZvvPGG4cfb21sBqnjx4oZpAwYMsMxBfUELFiwwfFbUq1dPtWzZUvn5+akSJUqoVq1amXxWpHbOaNmypQJUmTJlVLdu3VSfPn3UggULDPPNnbdSsvT7J63PJnPXas9zLo6NjVW+vr6G92OvXr1Unz591IEDB5RSSg0ePNhwvDt37pzqtdPTtFqtevvttxWgcufOrdq1a6fq1q2rNBqNcnJyMqz/adOmTTOcs5s1a6beeustwzl42rRpJu1Tu5b85ZdflJ2dncqdO7cKDQ01Wc7a146ffvqpYb3Tp083mjdv3jzDvKff20qZ9s29e/caXqOuXbuajW/YsGGG9s/Td831t5R0Op3h9XZzc1OtW7dWb731lsqVK5d68803VaFChUzeh6l9Bqd1ffCs92ZacXbu3Nmw7x4eHiouLs6kza1bt5SDg4MCVLly5VJ9zdetW2d2G9aU3uu6rDq/pPVaKqXUmjVrDO/xUqVKGfpOUFCQ4XV48OCBoX1MTIzhWsLHx0e1b99eNWvWTDk6OqoPP/xQAapEiRJm4+jSpYvy8PBQAQEBqmPHjqpOnTpKo9EoQM2bN88ktrQ+J5KNHj3a0K9sbGzU9evXze6rp6enAlRAQECq/cpcHEIIkR1IojaLZCRRq5RS9+/fVwMGDFBFihRRjo6OqmjRomrEiBHq8ePHZteVVYlapZRq37694QPy6QTd09tL6+fp4yGyzqpVq1SlSpWUo6Oj8vb2Vp07d1bXrl1LtU+ePn1aderUSfn5+Sl7e3uVN29e1bVrV3Xu3DmTdb9Ionbbtm0KUNWrV8+Evcw+NmzYoHr37q3KlCmjPD09lYuLiypZsqTq16+funjxolHbixcvqtatW6s8efIoGxsbkwvvM2fOqJYtWypfX1/l4uKiKlSooH744QellPk/qiyRqFVKqfDwcNW3b19VsGBBZW9vr7y9vVXr1q3VoUOHTNqmJ1GrlFLHjx9XzZo1U25ubsrNzU3Vr19f7d+/P11/gKTl4MGDqn79+ip37tzKzc1NVa9eXa1ZsybDf8CmfI9s3LhRVatWTbm4uCgPDw/11ltvqb///tvs9jdt2qSqVq2q3NzclKenp2rYsKHavXv3M98XWq1WLViwQNWsWVO5u7srJycnFRAQoLp06WKU3H+e/cuODhw4oFq2bKny5Mmj7O3tVaFChdSAAQPUtWvXTNpm5HjevHlTffnll6p+/fqqQIECysHBQfn5+amaNWuqxYsXq8TEREPbixcvqv/7v/9TNWrUUHnz5lUODg4qf/78qlGjRmaTBYmJier//u//VLFixZS9vb3Z92Ban4XZ+bVZuHChKlu2rOGYvfvuu+ru3btmPytSO2fcunVLdevWTfn7+xu+UEj5+qSVqFXKsu+f50mkKJXxc7FSSh05ckQ1atRIeXh4GJIWyddmydtJ68ecpKQkNXXqVFWmTBnl5OSk8uTJo9q0aZPmF54bNmxQtWrVUq6ursrV1VXVrFnT8MXg0551Lbly5Upla2urvLy81PHjx03mW/Pacfv27YZln47t4sWLhnnr1683Wfbp1zG5rzzr5+nXPaN9N60EqFJKJSQkqK+++koVLVpUOTg4qMKFC6uRI0equLg4s+9DayRqN23aZDgmvXv3NtsmOa60fl7ks99S0ntdl1Xnl/ReJ124cEG99957qmjRosrR0VF5eHio1157TfXq1Uv99ttvSqfTGbWPjIxUQ4YMUfnz51cODg6qVKlSaurUqerq1asKUFWrVk01jjNnzqi33npL5c6dWzk7O6tq1aqpjRs3mo0rrc+JZH///behX9SvXz/V/UxPv3rW3yNCCGFNGqVywGOghRCvjH79+jFv3jwWLlyYoVvnhcgqPXv2ZPHixezatStDTwgXQgghhHgZrFy5ko4dO9K/f3/mzJljmL5o0SJ69erF2LFjDfV6hRBCZIzUqBVCZBvh4eEsXboUb29v3nnnHWuHI4QQQgghxCvrxIkTJjXb//rrLz755BMAOnfubI2whBDipWZn7QCEEGLKlCmcOnWK7du3Exsby6RJk3B2drZ2WEIIIYQQQryyOnbsSFRUFOXKlSN37txcvnyZo0ePotVq6d+/P7Vq1bJ2iEII8dKRRO0r6O7duwwfPjxdbUuXLs3IkSMtHJF41W3atIk9e/aQP39+xo8fz8CBA60dksjmhg8fzt27d9PVdtGiRZYNRgghhBDiJTRw4EB+/vlnTpw4wYMHD3BxcaF69er06dOHHj16WDs8IYR4KUmN2lfQ5cuXCQgISFfbOnXqsHv3bssGJIQQGVSkSBHCw8PT1VY+5oQQQgghhBBC5ASSqBVCCCGEEEIIIYQQQggrk4eJCSGEEEIIIYQQQgghhJVJjVozdDodN27cwM3NDY1GY+1wRDoppYiOjiZfvnzY2GTf7yCkf+VMOaF/Sd/KmXJC3wLpXzlVTuhf0rdyLulfwlJyQt8C6V85VU7pX0KIV5Mkas24ceMGBQsWtHYY4jldvXqVAgUKWDuMVEn/ytmyc/+SvpWzZee+BdK/crrs3L+kb+V80r+EpWTnvgXSv3K67N6/hBCvJknUmuHm5gboT9zu7u5WjkakV1RUFAULFjS8ftmV9K+cKSf0L+lbOVNO6Fsg/Sunygn9S/pWziX9S1hKTuhbIP0rp8op/UsI8WrKUYnaiRMnsnbtWs6dO4ezszPVq1dn8uTJlCpVCoDExET+7//+j82bN/Pvv//i4eFBw4YNmTRpEvny5Uv3dpJvW3F3d5cP3Bwou992JP0rZ8vO/Uv6Vs6WnfsWSP/K6bJz/5K+lfNJ/xKWkp37Fkj/yumye/8SQryaclRBlj179vDBBx/w559/sn37dpKSkmjcuDGPHz8GICYmhmPHjvHZZ59x7Ngx1q5dy4ULF2jVqpWVI89iOi2E7YO/Vuv/1WmtHZEQQgghhBBCCCGEEOIZctSI2q1btxr9vnDhQnx9fQkNDaV27dp4eHiwfft2ozbffvstwcHBXLlyhUKFCmVluNZxZgNsHQFRN55Mc88HTSdD4CuWsBZCCCGEEEIIIYQQIofIUYnap0VGRgLg5eX1zDYajQZPT89U28THxxMfH2/4PSoqKtNizFJnNsCq7oAynh51Uz+9wxJJ1lrBS9O/RLYjfUtYkvQvYSnSt4QlSf8SliT9SwghhKXlqNIHKSmlGDp0KDVr1qRs2bJm28TFxTFy5Eg6d+78zJpBEydOxMPDw/CTI5/cqdPqR9I+naSFJ9O2jpQyCFbwUvQvkS1J3xKWJP1LWIr0LWFJ0r+EJUn/EkIIYWkapZS5zF6298EHH7Bp0yb2799PgQIFTOYnJibSvn17rly5wu7du5+ZqDX3zWjBggWJjIzMOUXhw/bB4hZpt+vxGwTUsnw8VhAVFYWHh0e2e91eiv4lsmX/kr71csiOfQukf70ssmP/kr718pD+JSwlO/YtkP71ssiu/UsIISCHlj4YOHAgGzZsYO/evakmaTt06EBYWBg7d+5M8+Tr6OiIo6OjpcLNGo9uZW47kWleiv4lsiXpW8KSpH8JS5G+JSxJ+pewJOlfQgghLC1HJWqVUgwcOJB169axe/duAgICTNokJ2kvXrzIrl27yJMnjxUizWKP78Gplelr6+pn2ViEEEIIIYQQQgghhBAZlqMStR988AHLly/n119/xc3NjYiICAA8PDxwdnYmKSmJdu3acezYMX777Te0Wq2hjZeXFw4ODtYMP/PptHBsMewYD3EP02isAfd8ULh6VkQmhBBCCCGEEEIIIYTIgByVqJ0zZw4AdevWNZq+cOFCevbsybVr19iwYQMAQUFBRm127dplslyOdi0UNg+DG8f1v/uVgzKtYeeX/zVIWXpYo/+n6SSwsc3CIIUQQgghhBBCCCGEEOmRoxK1aT33rEiRImm2yfEe34M/xsOxJYACRw+o/39QuTfY2oF3Sdg6AqJuPFnGPZ8+SRvYymphCyGEEEIIIYQQQgghUpejErWvNJ1Wn5z9YzzEPtBPe70zNBoPrr5P2gW2gtLNITxE/+AwVz99uQMZSSuEEEIIIYQQQgghRLYlidqc4HoobBoON47pf/crC29+A4WrmW9vYwsBtbIuPiGEEEIIIYQQQgghxAuRRG12FnNfP4I2dDH6MgfuUO9TqPKuvsyBEEIIIYQQQgghhBDipSDZvuxIp4PjS2DHuBRlDjpBw/Hg5mfV0IQQQgghhBBCCCGEEJlPErXZzfVjsHm4vtwBgG8ZaP6Nvs6sEEIIIYQQQgghhBDipSSJ2uwi5j7s/AKOLuRJmYPRUKWvlDkQQgghhBBCCCGEEOIlJxlAa9Pp4PhP/5U5uK+fVr4jNPpcyhwIIYQQQgghhBBCCPGKkEStNd04DpuGw/Wj+t99A+HNb6BIDevGJYQQQgghhBBCCCGEyFKSqLWGmPuw80s4ugBQ4OCmL3MQ3Bds7a0dnRDiJaG0WmKOhpJ05w52Pj64VK6ExtbW2mEJIYQQQgghhBDCDEnUZiWdDk4s1Zc5iLmnn1auAzT+Atz8rRqaEOLlEvX779yaMJGkiAjDNDt/f/xGj8K9cWMrRiaEEEIIIYQQQghzJFGbVW6cgE3DnpQ58HkNmn8DRWpaNSwhxMsn6o8/iB79KShlND3p1i2uDxoMM2dIslYIIYQQQgghhMhmJFFrabEP9GUOjvyIvsyB639lDvpJmQMhhEXcmToNp6eStIA+cavRcGvCRNwaNJAyCEIIIYQQQgghRDYiiVpL0eng5HLYPiZFmYP20OgLcM9r3diEEC+1pFu3ILUkrFIkRUQQczSUXG8EZ21gQgghhBBCCCGESJUkai3h5knYNByuHdb/7lMa3vwGAmpZNy4hhPhP0p071g5BCCGEEEIIIYQQKUiiNjPFPoCdX8HRH0Hp9GUO6o6EN/pLmQMhRLZi5+Nj7RCEEEIIIYQQQgiRgiRqM4NOBydX/Ffm4K5+Wtm20PhLcM9n3diEEK8cOz8/uHfP5GFiAGg02Pn54VK5UtYHJoQQQgghhBBCiFRJojY9dFoID4FHt8DVDwpXB5v/6j/ePAWbh8PVQ/rfvUtB828goLb14hVCvNJ8hg0levSnoNEYJ2s1GgD8Ro+SB4kJIYQQQgghhBDZjCRq03JmA2wdAVE3nkxzzwf1x8CNY3Bkvr7MgX2uJ2UO7BysF68Q4pXn3qAB7jNzcWvCRJIiIgzT7fz88Bs9CvfGja0YnRBCCCGEEEIIIcyRRO2znNsMm/sDT90+HHUD1vd/8nuZNvoyBx75szQ8IYRIjXvjxrg1aEDM0VCS7tzBzscHl8qVZCStEEIIIYQQQgiRTUmi9ll2jMEkSZuSjR10XgXFG2RZSOIl9KzSGkK8AI2tLbneCLZ2GEIIIYQQQgghhEgHSdQ+S3QEOGpSn69LAlspcyBeQGqlNZpOhsBW1otLCCGEEEIIIYQQQmQpG2sHkOM9umXtCEROdW4zrOpunKQFiLqpn35mg3XiEkIIIYQQQgghhBBZThK1L8rVz9oRiJwq1dIa/03bOlJfFkEIIYQQQgghhBBCvPSk9MGzuPlDwm3MJ9M0+lvUC1fP6qjSpNUpDofd53Z0HL5uTgQHeGFr84wSDsI6nllaQ0HUdX3t2oBaWRqWEEIIIYQQQoj0UUqRlJSEViuDbIQQpmxtbbGzs0OjSV9eThK1z9Lwc9jcH9BgnKz97+A2nZTtHvq09fRNxm88w83IOMO0vB5OjG0ZSNOyea0YmXgu2bC0hnwRIIQQQgghhBCQkJDAzZs3iYmJsXYoQohszMXFhbx58+LgkPZzriRR+yyl3wTXJWi3juBYwj3u2Nrio9VS0cEb26aTst3DnraevsmApcdMxv9GRMYxYOkx5nStKMnanCabldaQLwKEEEIIIYQQAnQ6HWFhYdja2pIvXz4cHBzSPWJOCPFqUEqRkJDAnTt3CAsLo0SJEtjYPLsKrSRq07AjlwuTCubnVsyTQ+Xn4sfIXC40tGJcT9PqFOM3nkm14qkGGL/xDI0C/WX0Y3bxzNIagKt/tiqtIV8ECCGEEEIIIYReQkICOp2OggUL4uLiYu1whBDZlLOzM/b29oSHh5OQkICTk9Mz28vDxJ5h15VdDN09lFsxxref3465zdDdQ9kRvsNKkZk6HHbfaJTj0xRwMzKOw2H3sy4o8WwNP//vP6kkzrWJcO9SloXzLGl9EQD6LwK0ulSSzkIIIYQQQgjxEkprdJwQQmTkPJGjzigTJ06kSpUquLm54evrS+vWrTl//rxRG6UU48aNI1++fDg7O1O3bl3+/vvv59re9NDpKDOpqeRpkw9PRqvLHgXDb0ennqR9nnYiC5R+EzosAfenRqG6+oNrXoi9BwuawLVQ68SXgnwRILKMTgth++Cv1fp/s8k5VgghhBBCCCGEsLQclajds2cPH3zwAX/++Sfbt28nKSmJxo0b8/jxY0Obr7/+mmnTpvHdd99x5MgR/P39adSoEdHR0Rne3u3Y2wBodIrAcB01/tYRGK5Do1MoFBExERy7fSzT9u9F+Lo9e+h0RtuJLBLYCgafhh6/Qdsf9f8OPQMDDkC+ihB7Hxa3hH/+sGqY5yKi0tVOvggQL+TMBphRFha3gDV99P/OKKufLoQQQgghhMhxevbsSevWra0dhhA5Ro6qUbt161aj3xcuXIivry+hoaHUrl0bpRQzZszg008/pU2bNgAsXrwYPz8/li9fznvvvZfhbQaf19Fzuw7vFHneu26wqJENh0vZcCfmzgvtU2bRABoNqFTuPNcA/h5OBAd4ZWVYIj1sbCGglvG0XHmgx0ZY2RX+3QXL34G350K5dlka2oPHCfxv1z8sCrmcrvbyRYB4bmc2wKrumNRsjrqpn95hSbZ7gKMQQgghhBAvSqtTHA67z+3oOHzd9H+zy3NlhHh15ahE7dMiIyMB8PLSJx/DwsKIiIigcePGhjaOjo7UqVOHkJCQVBO18fHxxMfHG36PitKPHqx0UcewTaYnSK9oGLZWx9Q24NPEJ9P253mtPXaNEWtOPTNJCzC2ZaCc8K0gtf6VJkdX6LwS1vWHv9fCmnch5j680c9CkT4Rm6BlwYEw5u6+RHR8EgAOthoStOY7mXwRYB1p9S2tTsux28e4E3MHHxcfKvpWxNbGNqvDTJtOC1tHYP7Bev89DnHrSCjdXP/FhsgSz33uEiIN0reEJUn/EpYk/Utktq2nbzJ+4xmjMnN5PZwY2zIwSx/UnJCQgIODQ5ZtTwiRuhxV+iAlpRRDhw6lZs2alC1bFoCIiAgA/Pz8jNr6+fkZ5pkzceJEPDw8DD8FCxYEoPtu/fynU5s26FMHff7QUCHP65mwN89HKcW0388zdNVJErWKN8v5M7NjEHk9jEc1+ns4MadrxSw90YsnUutf6WLnCG3nQ5W+gIItH8OuCakPnX5BSVodyw9doc6UXUzZdp7o+CRey+vOol5VmNmxgn7k9lPLyBcB1vOsvrUjfAdN1jSh97bejNg3gt7betNkTZNs9RBEg/ADEHXjGQ0URF2H8JAsC0m84LlLiGeQviUsSfqXsCTpXyIzbT19kwFLj5k8CyQiMo4BS4+x9fRNi227bt26fPjhhwwdOhRvb28aNWrEtGnTKFeuHLly5aJgwYK8//77PHr0yLDMokWL8PT0ZNu2bbz22mu4urrStGlTbt58EqdWq2Xo0KF4enqSJ08ePvnkE9RTf7vGx8fz0Ucf4evri5OTEzVr1uTIkSOG+bt370aj0bBt2zYqVKiAs7Mz9evX5/bt22zZsoXXXnsNd3d3OnXqRExMjMWOkRDWkmMTtR9++CGnTp1ixYoVJvM0GuNkkVLKZFpKo0aNIjIy0vBz9epVADyjdCZJqWQ2QO5ILfHHTjznHryYuEQtH/18glk7/wFgQN1ifNepIm8F5Wf/iPqs6FuVmR2DWNG3KvtH1JckrRWl1r/SzcYW3pwCdUfrf98zGTYNzdSHLCml2Hr6Jo1n7GX0ur+4HR1PgdzOzHgniE0Da1K3lC/NyuVlTteK+MsXAdlGan1r15VdDN09lFsxt4za3465zdDdQ62frFUK7odB6GL9SPGfu6ZvuUe30m4jMs0Ln7uESIX0LWFJ0r+EJUn/Es+ilCImISldP9FxiYzd8Heq95MBjNtwhui4xHSt7+lkaHosXrwYOzs7Dhw4wPfff4+NjQ2zZs3i9OnTLF68mJ07d/LJJ58YLRMTE8M333zDTz/9xN69e7ly5QrDhw83zJ86dSoLFizgxx9/ZP/+/dy/f59169YZreOTTz5hzZo1LF68mGPHjlG8eHGaNGnC/fvGD6YeN24c3333HSEhIVy9epUOHTowY8YMli9fzqZNm9i+fTvffvtthvdbiOzOoqUPihYtyowZM2jVynxdwd9++42PPvqIf//9N0PrHThwIBs2bGDv3r0UKFDAMN3f3x/Qj6zNm/dJ0uj27dsmo2xTcnR0xNHRMUMxJEu6k/U1au8+iqffkqMcu/IQOxsNE9qUo0PlJ9/m2tpoqFYsT5bHJcx7kf5loNFA3RH62rWbhsPRBRBzD9rM04+6fQF//nuPSVvOceLqQwByu9gzsH4JulQthKOd8W3mTcvmpVGgv9RQyiZS61vTQ6ejzFz2KRQaNEw+PJl6BetlbRmEyGsQtg/C9sLlfRD5HH/YuKZ+HheZL1POXUKYIX1LWJL0L2FJ0r/Es8Qmagkcsy1T1qWAiKg4yo37PV3tz3zeBBeHjKV3ihcvztdff234vXTp0ob/BwQE8MUXXzBgwABmz55tmJ6YmMjcuXMpVqwYoB9A9/nnnxvmz5gxg1GjRtG2bVsA5s6dy7ZtT47J48ePmTNnDosWLaJZs2YAzJs3j+3bt/Pjjz/y8ccfG9p++eWX1KhRA4A+ffowatQoLl26RNGiRQFo164du3btYsSIERnabyGyO4smai9fvmw0VP5pjx8/Jjw8PN3rU0oxcOBA1q1bx+7duwkICDCaHxAQgL+/P9u3b6dChQqAvtbKnj17mDx58vPtRBrsfLK2Ru3FW9H0WnSEaw9icXeyY263SlQv5p2lMQgrqvIuuOSBNX3hzK8Q+wA6LgdHtwyv6uzNKL7eeo5d5/VfNjjb29K3VgB9axfFzck+1eXki4Ds73bsbWydzSdhFYqImAiO3T5GFf8qlgsi+pY+IRu2V//zIMx4vo09FKgMAbWhUHVY3x+iIzBfp1YD7vmgcHXLxSuEEEIIIcQrpHLlyka/79q1iwkTJnDmzBmioqJISkoiLi6Ox48fkytXLgBcXFwMSVqAvHnzcvv2bUD/DKGbN29SrVo1w3w7OzsqV65sGPF76dIlEhMTDQlYAHt7e4KDgzl79qxRPOXLlzf838/PDxcXF0OSNnna4cOHX/QwCJHtWPVhYlevXsXV1TXd7T/44AOWL1/Or7/+ipubm6HurIeHB87Ozmg0GgYPHsyECRMoUaIEJUqUYMKECbi4uNC5c+cMx2fn5wf37pmvB6rRYOfnh0vlShle7/Pad/EO7y89RnR8EoXzuLCgZxWK+aT/+ImXRJm3wckTVnbVJ8AWtYAuq8E1fV8aXHsQw7TtF1h3/DpKgZ2Nho7BBfmoQQl83ZzSXoF4KdyJyeS7AR7f0ydmL+/Tj5y9e954vsYW8lWAgFpQpBYUqgoOuZ7Mb/Y1rOqOvupxynPuf6O1m06SB4kJIYQQQohsy9neljOfN0lX28Nh9+m58Eia7Rb1qpKuBzY722f8Ojk5+QoQHh7Om2++Sf/+/fniiy/w8vJi//799OnTh8TEREM7e3vjAT0ajSZDZReS26anXGXKbWk0GrPb1ul06d62EDlFpidqf/31V3799VfD7z/88AM7dpjWQ3zw4AE7duygatWq6V73nDlzAH3h65QWLlxIz549AX29k9jYWN5//30ePHjAG2+8we+//46bW8ZHHPoMG0r06E/1t52nPPn8dwLxGz0KjW3WJA6WH7rCZ7+eRqtTVCmSm++7VcYrlzyV8ZVVrB702AjL2sHNE7CgCXRbB7kLp7rI/ccJ/G/XP/x0MJwErf4DrXn5vAxvXIoA71ypLideTj4uL3g3QOxD/cO9kksZ3Dr9VAMN+JfTj5gNqA2FqoGTe+rrC2wFHZbA1hHGDxZzz6dP0gaaL6EjhBBCCCFEdqDRaNJdfqBWCR/yejgRERmX2v1k+Hs4UauET5aUmTt69ChJSUlMnToVGxv9o4xWrVqVoXV4eHiQN29e/vzzT2rXrg1AUlISoaGhVKxYEdCXW3BwcGD//v2GwXSJiYkcPXqUwYMHZ94OCZGDZXqi9sSJEyxatAjQn6j27t3L3r17Tdq5urpStWpV/ve//6V73en5pkaj0TBu3DjGjRuX7vWmxr1BA9xn5uLWhIkk/Td6F/Qjbf1Gj8K9ceMX3kZatDrFpC1nmbdPf9vw2xXyM6ltOZPaoeIVlL8i9P4dfnob7l+CHxtDt7XgV8aoWUxCEgsPXGbu7ktExycBUK1oHkY2K83rBT2tELiwNF9nX+5z32ydWg0a/Fz8qOhbMWMrjY+GK38+KWUQcQrUU99g+wbqR8sG1NaXKXBJ+9t/I4GtoHRzfQL40S19TdrC1WUkrRBCCCGEeKnY2mgY2zKQAUuPpXY/GWNbBmbZs0CKFStGUlIS3377LS1btuTAgQPMnTs3w+sZNGgQkyZNokSJErz22mtMmzaNhw8fGubnypWLAQMG8PHHH+Pl5UWhQoX4+uuviYmJoU+fPpm4R0LkXJmeqB07dixjx44FwMbGhqVLlz5X2YHswr1xY9waNCDmaChJd+5g5+ODS+VKWTKSNiYhiUE/n2D7Gf2Tzoc2KsnA+sVNbgkQrzDv4tBnG/zUBu6chYXNoPMqKFSVRK2OVUevMmPHRe5ExwMQmNedEc1KU7uEt/Sjl9iQSkP4LPQzNGiMkrWa/y77RgSPSPtBYgkxcPXQk1IG10NBaY3b5CmuT8oW+a+cQTrLbzyTja2+PIIQQgghhBAvsaZl8zKna0XGbzzDzcg4w3R/DyfGtgykadm8z1g6cwUFBTFt2jQmT57MqFGjqF27NhMnTqR79+4ZWs+wYcO4efMmPXv2xMbGht69e/P2228TGRlpaDNp0iR0Oh3dunUjOjqaypUrs23bNnLnzp3ZuyVEjqRRGSkokkF79uwhMDAQnyx+4NaLioqKwsPDg8jISNzdn3GrrgXdioqjz+IjnL4ehYOdDVPaleetoPxWiSWnyA6vW3pYJM6Y+7CiI1w9hLJzIjR4Bp+cysu/dx8DUNDLmeGNS9GyfD5ssuhb2ZdNTuhfKWM8fOp/TLqwjFu2T15vf61iRMkuNKw5ynThpHi4dvRJKYNrR0CbYNzGs7A+iRpQB4rU1JclEC8sJ/QtyDlxCmM54XXLCTEK83LCa5cTYhSmcsrrllPiFMYy63WLi4sjLCyMgIAAnJye/1kfWp3icNh9bkfH4evmRHCAV5aNpBVCZI2MnC8s+jCxOnXqWHL1L62/b0TSZ9FRIqLi8MrlwLzulahUOIO3EItXi4sXdFvPg8WdyX19F0EH3icosR8PczXko/rF6fxGYRzsbKwdpcgq5zbTcMdk6qE45uTIHVtbfLRaKsYlYHtlMniVgVLN4MbxJ6UMrh6GpFjj9bjn/6+UwX8jZp9RA1kIIYQQQgiRcbY2GqoVy2PtMIQQ2YRFE7WgLx69fv16Dh06xIMHD0yeyqfRaPjxxx8tHUaO8cfZWwxccZyYBC3FfV1Z0KMKhfK4WDsskc2duRHF19vOsf9SLybbJ9LWdj/THOYSVycfTjUaWTs8kdV2jAEUtkCVuHjT+WvfBY0dJD42np7L50kpg4Da4FXU8PBEIYQQQgghhBBCWJZFE7X379+nXr16nD59GqUUGo3G8ECw5P9LolZPKcXCA5f5ctMZdApqFM/D7C6V8HC2t3ZoIhu7ej+GadsvsP7EdZQCOxt7TlacSDO7ZbiEzsVp5xiIvw8Nx0nC7VUSHQGOz3i9k+KBeHDOrS9hUKS2PjHrU0r6iRBCPA+dVh6EKIQQQgghXphFE7X/93//x7lz55g/fz5169alWLFibNu2jUKFCvHFF19w8eJFtm3bZskQcoQkrY7xG8/w05/hAHSsUpAvWpfF3lZuVRfm3X+cwHc7/2Hpn+EkaPWj1FuUz8vwxqUo4p0LVDnInRd2jIUDMyDmLrSYCbYWH0QvcooGY6HGYLCR84wQQryQMxtg6wiIuvFkmns+aDoZAltZLy5hllan5djtY9yJuYOPiw8VfSum/YBNIYQQQogsYtGszaZNm+jevTu9evXi3r17ANja2lKqVCmWLl1K3bp1GTVqFHPmzLFkGNladFwiHy4/zp4Ld9BoYFSz0vStVRSNjGoTZsQkJLFgfxhz9/zLo/gkQD/6emTT1yhXwONJQ40Gag4Glzyw8SM4vlT/wLF2C8De2TrBi+ylQBVJ0gohxIs6swFWdQeeejZv1E399A5LJFmbjewI38Gkw5O4FXPLMM3PxY+RwSNpWLihFSMTQgghhNCz6F/pERERBAcHA2Bnp88Jx8XFGea3bt2aDRs2WDKEbO3agxjazTnIngt3cLK3YU6XSvSrXUyStMJEolbH0j/DqTNlN9/8foFH8UmUyefOT32CWfZuVeMkbUoVu8E7S8HWEc5vhp/aQOzDLI1dWIGbP5DaeUSjf0hY4epZGVG6aXWKg5fu8euJ6xy8dA+tTqW9kBBCWINOqx9J+3SSFp5M2zpS305Y3a4ruxi6e6hRkhbgdsxthu4eyo7wHVaKTAghhBDiCYuOqPXy8iImJgYANzc37O3tuXr1qmG+vb09Dx48sGQI2daJqw95d/FR7j6Kx9fNkfk9KlO+gKe1wxLZjFKKzX9F8M3v5wm7q3/wU0EvZ4Y3LkXL8vmwsUlHUr90c+i2DlZ0hCshsKg5dF3zXzJPvJQafg6b+6NP1qZMIPzXX5pOypa1E7eevsn4jWe4GfnkC728Hk6MbRlI07J5rRiZEEKYER5iXO7AhIKo6/p2AbWyLCxh3vTQ6SgzSXWFQoOGyYcnU69gPSmDIIQQQgirsuiI2pIlS3L27Fn9hmxsqFChAosWLSI+Pp6YmBiWLFlC0aJFLRlCtrT5r5u88/1B7j6Kp7S/G+s/qCFJ2ldYaiMIQy7dpfX/DvDB8mOE3X1MnlwOjG9Vhj+G1uWtoPzpS9ImK1IDem2GXL5w6zQsaAL3/7XQHgmrK/2m/nZb96eSm+75su1tuFtP32TA0mNGSVqAiMg4Biw9xtbTN60UmRBCpOLRrbTbZKSdsKjbsbdTnadQRMREcOz2sSyMSAghhBDClEVH1DZu3JipU6cyc+ZMHB0dGTp0KB07dsTLywuNRkNsbCw//PCDJUPIVpRSzNlzia+3ngegfmlfZnWqgKujPODpVWVuBKG3qwO+bk6cuRkFgIuDLX1rFaVv7aIv1lf8y0Gf3+Gnt+FBGPzYBLquhryvv+huiOwosJV+NHUOeAq5VqcYv/FMqjcPa4DxG8/QKNAf24x8QSGEEJbk4p2+dq5+lo1DZJo7MXesHYIQQgghXnEWzRCOHj2a4cOH4+joCECHDh2wtbVl2bJl2Nra0q5dO9555x1LhpBtJCTp+HTdX/wSeg2AntWL8FmLQEk6vMK2n4lg+PqLJsmpu48SuPsoARsNdKtamA/rl8DHzTFzNuoVAL23wbK2EPEXLGoBHZfLLZkvKxvbHPHaHg67bzKSNiUF3IyM43DYfaoVy5N1gQkhrEKrUxwOu8/t6Dh83ZwIDvDKftdLDy7Drq/SaKTR38mQTWuCC1M+Lj7WDkEIIXKsiIgIunXrRkhICPb29jx8+NDaIaVJo9Gwbt06Wrdube1QhDCwaKJWo9EYkrTJ2rZtS9u2bS252WznYUwC/ZeG8ue/97HRwNiWZehRvYi1wxJWNmnLORSpj270dnVkTMsymf/HqZsf9NwEKzpD+H5Y2hba/Qivtczc7QiRTrejU0/SPk87IUTOle1rVSsFp1bBpmGQEA12zpAUS06rCf4q8nX25T73zdap1aDBz8WPir4VrRCZEOKVp9PmiLvg0jJ9+nRu3rzJiRMn8PBI5WHXIkcbNGgQ+/fv5/Tp07z22mucOHHC2iG9lCxao1bA5buPaTM7hD//vU8uB1t+7FlFkrQCgFtR8f/9T4etyyXs3E9g63IJ0AFwOzqew2H3LbNxJw/9A8VKtwBtPKzqDqGLLbMtIdLg5mSfrna+bk4WjkQIYU3bz0Rk71rVsQ9hzbuwrp8+SVuwKnzwJ3T4KUfVBH9VDak0BNAnZVNK/n1E8Ah5kJgQIuud2QAzysLiFrCmj/7fGWX103OYS5cuUalSJUqUKIGvr6/ZNhqNhsuXL2faNrVaLTqdLtPWJ55NKUXv3r1fmTvjrcXiidrHjx8zb948PvnkE/r06UPv3r2Nfvr06WPpEKzmcNh9Ws8+wL93H5PPw4nVA6pTr5T5E5Z4Ndm5nSZX8cm4FJ6Hc/6fcSk8j1zFJ2Pndhqw8AhCeydovxgqdgelg40fwb6p+tFCQmSRq/djmLT57DPbaNCPqAsO8MqaoIQQVqG/08RU8rTxG88YHriZ5cJDYG5NOL0aNLZQ71P93Sm5i+iTsYNPQ4/foO2P+n8H/yVJ2mymXqF6fOjZkjxJWqPp3klaPvRsScPCDa0UmRDilXVmg37ATNQN4+lRN/XTLZSs/f7778mfP79JgrNVq1b06NGDcePGERQUxIIFCyhUqBCurq4MGDAArVbL119/jb+/P76+vnz11ZMSQEWKFGHNmjUsWbIEjUZDz549nyu2DRs2UKJECZydnalXrx6LFy9Go9EYyigsWrQIT09PfvvtNwIDA3F0dCQ8PJwjR47QqFEjvL298fDwoE6dOhw7ZvyAyIsXL1K7dm2cnJwIDAxk+/btGYotJCSEoKAgnJycqFy5MuvXr0ej0RhGlWq1Wvr06UNAQADOzs6UKlWKmTNnGq2jZ8+etG7dmgkTJuDn54enpyfjx48nKSmJjz/+GC8vLwoUKMCCBQsMy1y+fBmNRsOqVauoVasWzs7OVKlShQsXLnDkyBEqV66Mq6srTZs25c6dJ7XW03NMMmrWrFl88MEHFC1a9IXWI57NoqUPDh8+TPPmzbl3716qbTQaDT/++KMlw7CKdcevMWL1XyRodbxewIN5PSrLaDBhxM71DE75V5tM19hF4pR/KXHXu+LrVtWyQdjaQctZkMtHn6T943N4fBcafwU2MuBeWNaf/95jwNJQHsQk4u5kR1RcUmo3DzO2pdT0FuJldysqHhtHF7PzrFarWpsIuyfB/mn6LzVzB0Db+VCgsnG7HFIT/FV2csdy3j35Hb2AE86O3LG1xUerJSg2Htsr33HcqSQVmvSwdphCiJxMKUiMSV9bnRa2fALPepzu1hFQtG76yiDYu4AmfdfK7du356OPPmLXrl00aNAAgAcPHrBt2zY2btxISEgIly5dYsuWLWzdupVLly7Rrl07wsLCKFmyJHv27CEkJITevXvToEEDqlatypEjR+jevTvu7u7MnDkTZ2fn9B2HFC5fvky7du0YNGgQ7777LsePH2f48OEm7WJiYpg4cSLz588nT548+Pr6EhYWRo8ePZg1axYAU6dO5c033+TixYu4ubmh0+lo06YN3t7e/Pnnn0RFRTF48OB0xxYdHU3Lli158803Wb58OeHh4SbL63Q6ChQowKpVq/D29iYkJIR+/fqRN29eOnToYGi3c+dOChQowN69ezlw4AB9+vTh4MGD1K5dm0OHDrFy5Ur69+9Po0aNKFiwoGG5sWPHMmPGDAoVKkTv3r3p1KmT4Xi7uLjQoUMHxowZw5w5cwwxP+uYADRr1ox9+/Y9c98fPXqU7uMkModFE7VDhw4lMTGRVatWUb9+fby8Xv7RUEoppm+/wKyd/wDQrKw/0zoE4ewgt1IJY87+WwHTz1ONRv8Z7+L/G5UKD7N8IBoNNBijf3r1tlHw52x9srb1bLBN3y3pQmTU8kNXGPPraZJ0ivIFPPihW2VOXH1gUpvSPzvVphRCWF2W1qq+d0lf6uDGf6NPgrpCs0ng6JZ1MYhMk/fIJHAAOwWB4YqkOIWdk8LOB5QG8h4cj7ZBF2ztLPrnkRDiZZYYAxPyZdLKlH6k7aSCaTcFGH0DHHKlq6mXlxdNmzZl+fLlhkTtL7/8gpeXFw0aNCAkJASdTseCBQtwc3MjMDCQevXqcf78eTZv3oyNjQ2lSpVi8uTJ7N69m6pVq+Lj44OjoyPOzs74+/s/1x7PnTuXUqVKMWXKFABKlSrF6dOnjUbuAiQmJjJ79mxef/11w7T69esbtfn+++/JnTs3e/bsoUWLFuzYsYOzZ89y+fJlChQoAMCECRNo1qxZumJbtmwZGo2GefPmGUbkXr9+nb59+xra2NvbM378eMPvAQEBhISEsGrVKqNErZeXF7NmzTIcx6+//pqYmBhGjx4NwKhRo5g0aRIHDhygY8eOhuWGDx9OkyZNAH2t2E6dOvHHH39Qo0YNAPr06cOiRYvSfUwA5s+fT2xsbLqOgcg6Fr0SCQ0NZfTo0bRr186Sm8k24hK1fLL6FBtO6m9dGFC3GB83LoWNjAITZijbSGw05hP4Gg0ou4ecvHucKv5Vsiagau9DLm9YPwD+WgWxD6DD4nR/4AuRHklaHV/8dobFB8MBaPl6Pqa0K4+TvS1NPfLSKNA/+z/tXQhhNTvO3qZmcW/yuDqm3fh5KQXHl8KWEZD4WF/XveVMKPO25bYpLM6X+zy65sytYx4kxT65/rJz1uJXMRL/gvf4+9A2ytRobsUohRAia3Tp0oV+/foxe/ZsHB0dWbZsGR07dsTWVn9+LFKkiGHUJYCfnx+2trbYpLjr0s/Pj9u3bz9zO+ZGbJYpUwZNitFKySM2z58/T5Uqxn/7BgcHm6zTwcGB8uXLG027ffs2Y8aMYefOndy6dQutVktMTAxXrlwB4OzZsxQqVMiQpAWoVq3aM2NP6fz585QvXx4npyd3SZuLbe7cucyfP5/w8HBiY2NJSEggKCjIqE2ZMmVMjmPZsmUNv9va2pInTx6TY5tyn/38/AAoV66c0bSUy6R1TADy58+f3kMgspBFE7Xu7u54e3tbchPZxr1H8fRdcpRjVx5iZ6Nhwtvl6FAlnd9+CZGKOzF30m6Umcp3AOfcsLIb/LMdlrwFnVeBy8s/Gl5Y3sOYBD5YfowD/+jL4XzcpBTv1y1mdKFma6PJ2tuahRDZhp+7I3fjzd8EmmzjyRvsOHOLLm8Uol/tovi6Z3JZqZj7sHEQnP2vLmCRWvD2XPAo8OzlRLYXdc2R6CO5TaYnxdpw/UBuqPGA2AfXrRCZEOKlYe+iH9maHuEhsCwdA9q6rIbC1dO37Qxo2bIlOp2OTZs2UaVKFfbt28e0adOerM7e+M5KjUZjdlpaD/J6esRmiRIl2Lx5s9kEoVLK6O+C5GlPc3Z2NmnXs2dP7ty5w4wZMyhcuDCOjo5Uq1aNhISEVNfz9DqeJT2xrVq1iiFDhjB16lSqVauGm5sbU6ZM4dChQ0btnvfYpmyTHMvT01Iuk9YxASl9kF1ZNFHbqlUrtm7dyoABAyy5Gav753Y0vRYd4er9WNyd7JjbrRLVi70aCWphWT4uPlm/0RKNoMcGWNYerh2Bhc2g61rwkG/bxPP75/Yj3l18hMv3YnBxsGX6O0E0KfN8t0UJIV5OI5uVZvj6i6nWqu5fpxgHLt3l1LVI5u8PY8mf4XQOLsR7dYqS1yPjtfBM/Lsb1vWH6JtgYwf1P4PqA9NXG1Bke3dOuqNP6z/9h7m+x9067o5zTymzI4R4ARpN+u9GLFYf3PPpHxxm9itKjX5+sfoW+RxydnamTZs2LFu2jH/++YeSJUtSqVKlTN+OuYRs4cKFKVKkiMn00qVLs3nzZqNpR48eTdd29u3bx+zZs3nzzTcBuHr1Knfv3jXMDwwM5MqVK9y4cYN8+fTlKQ4ePJje3aB06dIsW7aM+Ph4HB0dzca2b98+qlevzvvvv2+YdunSpXRvI7OldUxASh9kVxZ9WtCUKVO4desWAwcO5NKlS2a/xcjp9l+8y9uzQ7h6P5ZCXi6sfb+GJGlFuvg6+6Ix+WNBT4MGfxd/KvpWzOKo/lMwGHpvBbd8cOcc/NgY7lywTiwix9t1/jZv/+8Al+/FkN/TmTUDqkuSVghholGgP3O6VsTPwwFbl0vYuZ/A1uUSfh4OzOlakRHNSvPrBzVY1KsKFQt5kpCkY1HIZep8vZvR6/7i6v10PsDlaUnx8PtnsKS1Pkmbpzi8uwNqDpYk7UtEX+4gtdFTGpJi7ChkK9fw4sUorZbHhw4T+dsmHh86jNJqrR2SyK5sbKHp5P9+MfcFEtB0kkU/h7p06cKmTZtYsGABXbt2tdh20uu9997j3LlzjBgxggsXLrBq1SpDzdW0Rr8WL16cn376ibNnz3Lo0CG6dOli9ECzhg0bUqpUKbp3787JkyfZt28fn376abpj69y5Mzqdjn79+nH27Fm2bdvGN998YxRb8eLFOXr0KNu2bePChQt89tlnHDlyJINHIfOkdUxAn0gvXrz4M39S+ueffzhx4gQRERHExsZy4sQJTpw4YTRKV7y4TE3U2tjYYGtra/jJkycPhw8fZvbs2ZQsWRI7Ozuj+ba2ttjl4IL9Kw5focfCw0THJVG5cG7Wf1CD4r6u1g5L5BBDKg0BMEnWJv8+IngEttb8A9H3NejzO+QpAVHXYEETuBaqf0Jp2D74a7X+X51cgArzlFLM2/svfRYdITo+ieAiXmz4sAav5XW3dmhCiGzKzu1vXItPxqXwPJzz/4xL4Xm4Fp+MndvfgP6PobqlfFkzoDrL3n2DNwK8SNDqWH7oCvW+2c0nq09y+e7j9G/wznmY3xBCZgEKKvWC9/ZCvgqW2UGRrenu37d2CCIHi/r9d/5p0JArPXpwY/hwrvTowT8NGhL1++/WDk1kV4GtoMMScH9qNL97Pv30wFYW3XzyA9/Pnz9P586dLbqt9AgICGD16tWsXbuW8uXLM2fOHEMyNXkUa2oWLFjAgwcPqFChAt26deOjjz7C19fXMN/GxoZ169YRHx9PcHAw7777rslDyp7F3d2djRs3cuLECYKCgvj0008ZM2YMgKFubf/+/WnTpg3vvPMOb7zxBvfu3TMaXZvV0jomz+Pdd9+lQoUKfP/991y4cIEKFSpQoUIFbtxIZ8kPkS4alYnDXHv27JmhOh/JFi5cmFkhZIqoqCg8PDyIjIzE3d0drU4ZPdymcuHcfPP7eb7f+y8ArYPyMbldeRztZNSFNT39umVXKeM8/OAwkw5P4lbMLcN8fxd/RgSPoGHhhlaMMoXH9/T1k24cA1tHcHSFmHtP5rvn038bbOELCWvLCf0rO8UYn6Tl03WnWR16DYCOVQry+VtlcbCz6I0cOVJ2et2eJafEKYzlhNctOcb1f63ns9DPUE/dApr8Bea0utPMfjYe+vce3+78h/3/6G/ns9FA66D8vF+veOpfoCsFRxfAtk8hKRacveCt76C0PEgqI3JS/zpcvASuts++Vi+0eDG53jB9OIzIejmhb8GTOK+uXUv06E/155aU/vvbOP/MGbg3bmyFCIU5mdW/4uLiCAsLIyAgwOghUxmm0+pr1j66Ba5++pq0ckcHAF999RVz587l6tWr1g7FxLJly+jVqxeRkZEmI1WFeFpGzheZOpw1eVj6y2Tr6ZuM33iGm5FxhmmOdjbEJ+mLNA9pWJKPGhR/rgS1EA0LN6RewXocu32MOzF38HHxoaJvReuOpH1arjzQY6N+RO2t0xATbzw/6ias6p4l3/qKnOFOdDz9l4YSGv4AGw181iKQntWLyHlSCPFM00OnmyRpARQKDRomH55MvYL1TD4j3yiahzeK5iE0/AHf7rzI7vN3WHv8OutOXKdF+Xx8WK84pfyfPLmax3fh1w/hwhb970XrQes5piOaxEvFzs8P7t0zTaQBaDTY+fnhUjnz6zOKV8OdqdNwMte3lAKNhlsTJuLWoAGaNL4sEK8oG1sIqGXtKLKF2bNnU6VKFfLkycOBAweYMmUKH374obXDAmDJkiUULVqU/Pnzc/LkSUaMGEGHDh0kSSsynQxteobtZyIYsPSYUZIWMCRpe9cowqCGJST5IF6IjYLAcEWNM4rAcIVNdizlbO+sfxK2Wf8FvHWklEEQnL4eyVvf7Sc0/AHuTnYs7h1MrxoBcp4UQqTpduztVOcpFBExERy7fSzVNpUK52ZRr2A2fFiDhq/5oRRsPHmDJjP20v+nUP6+EQkXd8Dsavokra0DNJmgf2CmJGlfej7Dhur/8/Tn0X+/+40eJUk08dySbt1KfaZSJEVEEHM0NOsCEiKHunjxIm+99RaBgYF88cUXDBs2jHHjxll8uxMmTMDV1dXsT7NmzQCIiIiga9euvPbaawwZMoT27dvzww8/WDw28eqxaIHYlStXsmnTJpYsWWJ2fo8ePWjZsiXt2rWzZBjPbdKWcyhSv2DbcjqCT5sHYmsjCQjxfKJ+/51bEyaSFBFhmGbn74/f6FHZ6/ao8BCIflbdGQVR1/Xt5NvgV9bmv24ybNVJYhO1FPXJxfzulSnqI3W7hRCZ507MnTTblC/gyfwelTlzI4rvdl1ky+kIdv19heDzX1PGbqu+kU9paDsf/MtZOGKRXbg3aID7zFym111+ftnvuku8lJLupH3+Epnr6RKGwQFe8rd7Njd9+nSmT5+e5dvt378/HTp0MDsvecTsJ598wieffJKVYYlXlEUTtd999x3FihVLdb6trS3ffvtttk3U3oqKx8bRJdX5NyPjOBx2n2rF8mRhVOJlEfXHH2ZrWSXdusX1QYMhO9WyevSMUQLP0068VHQ6xcw/LjLzj4sA1C7pw7edKuDhbG/lyIQQLxsfF590tw3M587sLpW4fOYwDuv7kS8hDIBFSY3Z7/gRA+LzIze6v1rcGzfGrUEDYo6GknTnDnY+PrhUriQjaUWWsPNJ//lLvDhzJQzzejgxtmUg1QvlsmJkIjvy8vLCy8vL2mEIAVg4UXv27NlnJmErVKjAxo0bLRmCxd2Ojku7kRBm5KhaVq5+mdtOvDRiEpIYtuokW07rRyf1qRnAqGalsbOVyjpCiIzxdfblPvfN1qnVoMHPxY+KvhXTv0Kl4ND3FNk+BrTxJDl7s9D7YyZdKoT2nyh2/HOQ6sXy8FGDElQtKl+6vyo0trbywDCR6aQGcvay9fRNBiw9ZvJpEhEZx4Clx/imdQmrxCWEEOlh0b+kHz9+jO0zkkwajYbo6OgMrXPv3r20bNmSfPnyodFoWL9+vdH8R48e8eGHH1KgQAGcnZ157bXXmDNnzvOEny6+bi/wdEfxSstRtawKVwf3fEBqtwppwD2/vp14ZVx/GEu7OQfZcjoCe1sNX7ctz2ctAiVJK4R4LkMqDQH0SdmUkn8fETwi/Q/bjL4Fy9rB1hGgjYcSjbH74CB9+/Rn17C6dKxSEDsbDSGX7tHxhz/pMPcg+y7eQZlLsgghRBqkBnL2odUpxm88Y+YrP8OTNZi05VxWhiSEEBli0b+mAwICCAkJSXX+/v37KVSoUIbW+fjxY15//XW+++47s/OHDBnC1q1bWbp0KWfPnmXIkCEMHDiQX3/9NUPbAfBzd3xWWoq8Hvo6N0JYSrapZWVjC00n//fL0++K/35vOknfTrwSQsPv89Z3+zlzMwpvVwdW9K1KhyoFrR2WECIHq1eoHtPqTsPXxddoup+LH9PqTqNh4YbpW9H5LTCnGvyzA+yc4M1voPMqcNWvt1AeFya1Lc/uj+vStWohHGxtOHz5Pt1+PEybOSHsOndbErZCiAxxb9CA/DNn6EfWpmDn50f+7FTO7CUWm6Dl6OX7jN1w2uRh4Ckp9CUOhRAiu7Jo6YO3336bSZMm0ahRI3r16mU078cff+SXX37h448/ztA6mzVrZnjqnjkHDx6kR48e1K1bF4B+/frx/fffc/ToUd56660MbWtks9IMX38RDRh9I5ecphrbUh4kJiwrW9WyCmwFHZboRydFpXiwmHs+fZI2sJX1YhNZ6pejV/l03WkStDpey+vOvO6VKJA79XreQgiRXg0LN6RewXocu32MOzF38HHxoaJvxfSNpE2Igd//D47+qP/dr6z+gWG+r5ltXiC3C1+2LseH9Urw/d5LLD90heNXHtJr0RHK5fdgYP3iNAr0Q/P0CDkhhDBDaiBnnSStjgu3HnHq2kNOXnvIiauRXLgVjVYnX7IJIXI+iyZqR44cya+//sq7777LN998Q/ny5QE4efIk58+fp1SpUowePTpTt1mzZk02bNhA7969yZcvH7t37+bChQvMnDkz1WXi4+OJj3/yrVpUVBQAjQL9mePqZlKE3P+/IuRNy+bN1NjFyym1/pUja1kFtoLSzSE8RP/gMFc/fbkDGUlrFan1LUvR6hSTtpxl3j79A3malvFnaofXyeVo0Y8SYSVZ3b/EqyOtvmVrY0sV/yoZW+nNk7DmXbh7Qf97tQ+hwRiwc0xzUf11XRkG1C3G/H1h/HQwnL+uR9Lvp1BK+7sxsH4JmpX1x0a+nM8R5NwlLCmt/iU1kDOfUoqr92M5ce0hp67qE7N/XY8kLlFn0tbHzZFCXi6Ehj+wQqRCCJE5LPrXtZubGwcOHGDUqFGsXLmSs2fPApA7d24GDBjAl19+ibu7e6Zuc9asWfTt25cCBQpgZ2eHjY0N8+fPp2bNmqkuM3HiRMaPH292XtOyeWkU6M/hsPvcjo7D101f7kBG0or0Sq1/+QwbSvToT/W1q1Ima7N7LSsbWwioZe0oBM8+d2W2qLhEPlpxnN3n9eU4PmpQgsENSkji4iWWlf1LvFoytW/pdHDwO/jjc9Algqs/vD0HitXP8Kp83ZwY/eZrvFe7KD/uD2PJwXDORUTzwfJjlPB15cP6xWlRPp9cA2Zzcu4SliT9y/LuPorn1H+jZE9de8jJqw95EJNo0s7V0Y7yBTx4vaAnrxfw5PWCHvi7O6FTUHPyTiIi48zWqdWgL3F41eJ78uqJiIigW7duhISEYG9vz8OHD60dUpo0Gg3r1q2jdevW1g5FCAONyqIiXEop7t69i1IKHx+fTLmNzNyb6ptvvmHevHl88803FC5cmL179zJq1CjWrVtHw4bma5uZ+2a0YMGCREZGZnoiWVhOVFQUHh4e2e51e1b/4s8/uTVhIkkREYb5dv7++I0eJbWsspns2L+y6twVdvcx7y4+wqU7j3Gyt2Fq+yCal5c7CjJLduxbIJ+NL4vs2L8yrW9F3YB1/SFsj/730i2g5SzIlSdT4nwYk8CCA5dZeCCM6LgkAAK8c/FBveK8FZQP+xQPTtTq1Cv5pf5L3b+EVWXHvgXSvzLb4/gkTl+P5OS1h5y8qv/32oNYk3YOtja8ltctRVLWk6LeuVIdMLD19E0GLD0GmC9h+E3rErSrVuqFX7e4uDjCwsIICAjAyen5HzKu1Wmfr+RPNjNixAg2bdrEunXr8PDwwNfXN+2FrEwStVmvSJEiDB48mMGDB2doubVr1/L9998TGhrKvXv3OH78OEFBQUZt4uPjGT58OCtWrCA2NpYGDRowe/ZsChQoYGjz4MEDPvroIzZs2ABAq1at+Pbbb/H09DS0uXLlCh988AE7d+7E2dmZzp0788033+Dg4PC8u52h80WW3a+q0WjwsXC9zdjYWEaPHs26deto3rw5AOXLl+fEiRN88803qSZqHR0dcXRM+9Y4IZ7Hs/qX1LISLyIrzl37L97lg+XHiIxNJK+HE/O6V6Zsfg+LblNkD/LZKCwlU/rWmQ2w8SOIfQD2LtB0IlTsYfrE9Rfg6eLA0EYlebdWAEtCLjN/fxhhdx8z/JeTzPzjAu/XLU7bigXYee6WSZmsvFImy2rk3CUsSfrX80vU6jgfEf1fUlafmL14OxpzZWWL+7pSvoAHQf8lZkvndcPRLv1/HzUtm5c5XSumWsKweqFcmbFLmWJH+A4mHZ7ErZhbhml+Ln6MDB6Z/odoZhOXLl2iUqVKlChRItU2Go2GsLAwihQpkinb1Gq1aDQabGxs0m4scrTHjx9To0YN2rdvT9++fc22GTx4MBs3buTnn38mT548DBs2jBYtWhAaGortfzmWzp07c+3aNbZu3Qron2vVrVs3Nm7cCOj7VPPmzfHx8WH//v3cu3ePHj16oJTi22+/zZJ9zZLefOHCBTZv3sxPP/3EkiVLTH4yS2JiIomJiSZvUltbW3Q60xo2QmQHybWsPFo0J9cbwZKkFdmCUorFIZfpsfAwkbGJVCjkya8f1pAkrRDCuuIfwa8fwqpu+iRt3iB4by9U6pmpSdqU3J3s+bB+CfaPqM/IZqXJk8uBq/djGbX2L96YsIP+S4+ZPGE8IjKOAUuPsfX0TYvEJIQQ2ZlSirC7j/n1xHXGb/ybNrMPUHbsNlp8u59P151m1dFrnL+lT9Lm9XCiaRl/PmlaiuXvvsGpcY3ZMbQO0zoE0b1aEV4v6JmhJG2ypmXzsn9EfVb0rcrMjkGs6FuV/SPqZ6sv0HaE72Do7qFGSVqA2zG3Gbp7KDvCd1hku99//z358+c3yZG0atWKHj16MG7cOIKCgliwYAGFChXC1dWVAQMGoNVq+frrr/H398fX15evvvrKsGyRIkVYs2YNS5YsQaPR0LNnz+eKbcOGDZQoUQJnZ2fq1avH4sWL0Wg0hjIKixYtwtPTk99++43AwEAcHR0JDw/nyJEjNGrUCG9vbzw8PKhTpw7Hjh0zWvfFixepXbs2Tk5OBAYGsn379gzFFhISQlBQEE5OTlSuXJn169ej0Wg4ceIEoE/w9enTh4CAAJydnSlVqpTJs5J69uxJ69atmTBhAn5+fnh6ejJ+/HiSkpL4+OOP8fLyokCBAixYsMCwzOXLl9FoNKxatYpatWrh7OxMlSpVuHDhAkeOHKFy5cq4urrStGlT7ty5Y1guPcckI9Kzf3Xr1jUZKdu6dWtDf6hbty7h4eEMGTIEjUZjdKf9mjVrKFOmDI6OjhQpUoSpU6caradbt26MGTMm1QGYkZGR/Pjjj0ydOpWGDRtSoUIFli5dyl9//cWOHfr30tmzZ9m6dSvz58+nWrVqVKtWjXnz5vHbb79x/vx5AH7//XfOnDnD0qVLqVChAg0bNmTq1KnMmzcvy+reW3RE7a1bt+jRo4fhDWCuyoJGo6F79+7pXuejR4/4559/DL+HhYVx4sQJvLy8KFSoEHXq1OHjjz/G2dmZwoULs2fPHpYsWcK0adNefIeEEOIVkJCkY+yGv1lx+AoAbSrmZ8Lb5XCyly8RhBBZQKc1/9DK66H6B4bd/xfQQM3BUHc02D3/bWgZ4epoR/86xehRrQjLD19h7u5/uPMowWxbpY+Q8RvP0CjQ/5UogyCEeHXdjo7j5H81ZU9cfcipa5FExprWlXV3sjOUL0iuL+vn/vwlA9Jia6OhWrHMKYeTHkopYpNMSzeYo9VpmXh4IspMJd3kaZMOT+IN/zfSVQbB2c453eUl27dvz0cffcSuXbto0KABoL8dfNu2bWzcuJGQkBAuXbrEli1b2Lp1K5cuXaJdu3aEhYVRsmRJ9uzZQ0hICL1796ZBgwZUrVqVI0eO0L17d9zd3Zk5cybOzs7piiWly5cv065dOwYNGsS7777L8ePHGT58uEm7mJgYJk6cyPz588mTJw++vr6EhYXRo0cPZs2aBcDUqVN58803uXjxIm5ubuh0Otq0aYO3tzd//vknUVFRGbr1Pjo6mpYtW/Lmm2+yfPlywsPDTZbX6XQUKFCAVatW4e3tTUhICP369SNv3rx06NDB0G7nzp0UKFCAvXv3cuDAAfr06cPBgwepXbs2hw4dYuXKlfTv359GjRpRsGBBw3Jjx45lxowZFCpUiN69e9OpUyfD8XZxcaFDhw6MGTOGOXPmGGJ+1jEBaNasGfv27Xvmvj969ChD+/csa9eu5fXXX6dfv35Go2JDQ0Pp0KED48aN45133iEkJIT333+fPHnypDvpHxoaSmJiIo1TlJDMly8fZcuWJSQkhCZNmnDw4EE8PDx44403DG2qVq2Kh4cHISEhlCpVioMHD1K2bFny5ctnaNOkSRPi4+MJDQ2lXr166YrnRVg0Ufvhhx+yfft2BgwYQP369cmT58VPkkePHjU6MEOHDgWgR48eLFq0iJ9//plRo0bRpUsX7t+/T+HChfnqq6/o37//C29bCCFedvcfJ9B/aSiHw+6j0cCoZqXpW6toptQVF0KINJ3ZAFtH6OvPJnPPB4Wqw5n1oEsC9/zw9vdWe7Cls4MtfWoGUNw3Fz0WHEm1nQJuRsZxOOx+liYKhBDCkqLjEvnreqQhMXvy6kNuPHVXAYCDnQ1l8rnzegFPggrqE7NF8qReV/ZlEJsUyxvL30i7YTrdirlF9Z+rp6vtoc6HcLF3SVdbLy8vmjZtyvLlyw2J2l9++QUvLy8aNGhASEgIOp2OBQsW4ObmRmBgIPXq1eP8+fNs3rwZGxsbSpUqxeTJk9m9ezdVq1bFx8cHR0dHnJ2d8ff3f679nTt3LqVKlWLKlCkAlCpVitOnTxuN3AX9ndSzZ8/m9ddfN0yrX9/4IaLff/89uXPnZs+ePbRo0YIdO3Zw9uxZLl++bKhXOmHCBJo1a5au2JYtW4ZGo2HevHmGEbnXr183Sjba29sbPWwwICCAkJAQVq1aZZTI9PLyYtasWYbj+PXXXxMTE8Po0aMBGDVqFJMmTeLAgQN07NjRsNzw4cNp0qQJAIMGDaJTp0788ccf1KhRA4A+ffqwaNGidB8TgPnz5xMbm74vF9K7f8/i5eWFra0tbm5uRv1k2rRpNGjQgM8++wyAkiVLcubMGaZMmZLuRG1ERAQODg7kzp3baLqfnx8R/z0TKCIiwmztZF9fX6M2fn5+RvNz586Ng4ODoY2lWTRRu337dt577z2+++67TFtn3bp1zY7MTebv78/ChQszbXtCCPGqOBcRxbuLj3LtQSyujnbM6hRE/dJ+aS8ohBCZ4dxm2Nwfnh5dFHUDTq/W/z+wNbScAc65sbaHZp5Cbs7taNMEhhBC5AQJSTrORURx8upDTvyXmP3nziOe/nNco4ESvq6GB30FFfSkpJ8bDnZSNzS76tKlC/369WP27Nk4OjqybNkyOnbsaKjjWaRIEcOoS9Anu2xtbY3KTPr5+XH79u1nbsfciM0yZcoYDQJJHrF5/vx5qlSpYtQ2ODjYZJ0ODg6UL1/eaNrt27cZM2YMO3fu5NatW2i1WmJiYrhyRX+H4NmzZylUqJDRQ6WqVav2zNhTOn/+POXLlzd6CJS52ObOncv8+fMJDw8nNjaWhIQEkwdelSlTxuQ4li1b1vC7ra0tefLkMTm2Kfc5OZFYrlw5o2kpl0nrmADkz58/vYcg3fv3PM6ePctbb71lNK1GjRrMmDEDrVZr6JfPQyll1N/MDUB6njaWZNFErU6no0KFCpbchBBCiEzw+98RDFl5gscJWgrncWF+98qU8HNLe0EhhMgsO8ZgkqRNyTk3tP0RbLPsWbjP5OuWvtt109tOCCGsSadThN17/N+Dvh5y4lokZ29EkaA1fdZLfk9nXi/oYUjMls3vgatj9jg3W5OznTOHOh9KV9vQW6G8/8f7abab3WA2lfwqpWvbGdGyZUt0Oh2bNm2iSpUq7Nu3z6hcpL29vVF7jUZjdlpazwJ6esRmiRIl2Lx5s9kEoblEmLlBes7OpmUeevbsyZ07d5gxYwaFCxfG0dGRatWqkZCQkOp6MpJ0S09sq1atYsiQIUydOpVq1arh5ubGlClTOHTIuE8877FN2SY5lqenpVwmrWMCGSt9kJ79s7GxMTkuiYlpf7Gd3tf+Wfz9/UlISODBgwdGo2pv375N9erVDW1u3bplsuydO3cMyW9/f3+T1+zBgwckJiaajLS1FIueTatXr24orCyEECL7UUoxe/clvvn9PEpB9WJ5+F/niuTOlTU1H4UQwiA6Ahyf8UdT7AO4ctBqJQ+eFhzgRV4PJyIi48ymlzXonzAeHOCV1aEJIUSaIiLjOPlf6YKT1/R1ZaPjkkzaebrY6xOy/9WULV/AEx83RytEnP1pNJp0lx+onq86fi5+3I65bbZOrQYNfi5+VM9XPV01ajPK2dmZNm3asGzZMv755x9KlixJpUppJ4QzylxCtnDhwhQpUsRkeunSpdm8ebPRtKNHj6ZrO/v27WP27Nm8+eabAFy9epW7d+8a5gcGBnLlyhVu3LhhqD168ODB9O4GpUuXZtmyZcTHx+Po6Gg2tn379lG9enXef/9JAv7SpUvp3kZmS+uYQMZKH6Rn/3x8fLh588mDVLVaLadPnzYqX+rg4IBWqzVaLjAwkP379xtNCwkJoWTJkukeTVupUiXs7e3Zvn27oRTDzZs3OX36NF9//TWgH0UdGRnJ4cOHDSOiDx06RGRkpCGZW61aNb766itu3rxJ3rz6hw/+/vvvODo6WuQ9Yo5FE7XJdSbq169P27ZtLbkpIYQQGRSXqGXEmlP8ekJfC7J7tcJ81iIQe1u5TU0IkU09Mh0FYS22NhrGtgxkwNJjaDAeC5ycbh7bMlAeJCaEsLrI2ET+uhZplJi9FRVv0s7RzoZy+ZMTsh4EFfSkkJeLPKvAAmxtbBkZPJKhu4eiQWOUrNX89ykyIniERZK0ybp06ULLli35+++/6dq1q8W2k17vvfce06ZNY8SIEfTp04cTJ04Yaq6m1QeLFy/OTz/9ROXKlYmKijI8YD5Zw4YNKVWqFN27d2fq1KlERUXx6aefpju2zp078+mnn9KvXz9GjhzJlStX+Oabb4xiK168OEuWLGHbtm0EBATw008/ceTIEQICAjJ4JDJHWscEMlb6ID37V79+fYYOHcqmTZsoVqwY06dP5+HDh0brKVKkCHv37qVjx444Ojri7e3NsGHDqFKlCl988QXvvPMOBw8e5LvvvmP27NmG5e7fv29ItoO+HAXoR8D6+/vj4eFBnz59GDZsGHny5MHLy4vhw4dTrlw5GjZsCMBrr71G06ZN6du3L99//z0A/fr1o0WLFpQqVQqAxo0bExgYSLdu3ZgyZQr3799n+PDh9O3bF3d393Qfrxdh8YeJubm50aFDB/Lly0fRokVNsuEajYY//vjDkmEIIYR4SkRkHO/9dJST1yKxs9EwrlUZulYtbO2whBDi2VyzV93spmXzMqdrRcZvPMPNFA/T8fdwYmzLQJqWzWvF6IQQr6K4RC1nb+rryp66FsmJaw/5985jk3Y2Gijp50ZQQU9DYrakn5t8YZ+FGhZuyLS605h0eBK3Yp58Eenn4seI4BE0LNzQotuvX78+Xl5enD9/ns6dO1t0W+kREBDA6tWrGTZsGDNnzqRatWp8+umnDBgwwDCKNTULFiygX79+VKhQgUKFCjFhwgSGDx9umG9jY8O6devo06cPwcHBFClShFmzZtG0adN0xebu7s7GjRsZMGAAQUFBlCtXjjFjxtC5c2dD3dr+/ftz4sQJ3nnnHTQaDZ06deL9999ny5Ytz39QXkBaxySj0rN/vXv35uTJk3Tv3h07OzuGDBliNJoW4PPPP+e9996jWLFixMfHo5SiYsWKrFq1ijFjxvDFF1+QN29ePv/8c6MHiW3YsIFevXoZfk9+0NrYsWMZN24cANOnT8fOzo4OHToQGxtLgwYNWLRokVEectmyZXz00Uc0btwYgFatWhk9V8vW1pZNmzbx/vvvU6NGDZydnencubMhMZ8VNCqjhR8yoEiRIun69i0sLMxSITyXqKgoPDw8iIyMzLKMuXhxOeV1yylxCmM54XVLb4wnrj6k35Kj3I6Ox9PFntldKlK9mHcWRipSygl9C3JOnMJYTnjdDDF+VQL3hNuYr1OrAfd8MPgvsODoouel1SkOh93ndnQcvm76cgevwkjaHNW/snGMwlROed2sHadWp/j3ziNOpChfcPZmFIla0/NoQS9nXi/gaUjMlsnnjovDq1lXNrNet7i4OMLCwggICDB6yFRGaXVajt0+xp2YO/i4+FDRt6JFR9LmJF999RVz587l6tWr1g7FxLJly+jVqxeRkZEmI1WFeFpGzhcWPTNfvnzZkqsXQgjxDOYSB7+dusHHq0+RkKSjpJ8r87tXoVCe9NXSEkIIi2r4OWzuD6kVEmg6KVsmaUFfBqFasTzWDkMI8RJTSnEzMu6/B33pSxicvh7Fo3jTurJeuRwMNWVfL+jJ6wU88ZLnD2Rbtja2VPGvYu0wsoXZs2dTpUoV8uTJw4EDB5gyZQoffvihtcMCYMmSJRQtWpT8+fNz8uRJRowYQYcOHSRJKzLdq/kVmhBCvOS2nr5pcituLkdbHsfrC7c3KO3LjI5BuDnZp7YKIYTIWqXfBNclsHUERN14Mt09nz5JG9jKerEJIUQWexiTwKlrkYaasieuRnL3kWldWWd7W8oV8HiSmC3gSYHczlJXVuRIFy9e5Msvv+T+/fsUKlSIYcOGMWrUKItvd8KECUyYMMHsvFq1arFlyxYiIiIYM2YMERER5M2bl/bt2/PVV19ZPDbx6smSRG1sbCy7du3i33//BaBYsWLUrVtXvnkQQggL2H4mguHrL5rcPJycpG0c6MecrpVeidtyhRA5TGArKN0cwkP0Dw5z9YPC1bPtSFohhEjpecugxCVq+ftGJCevPnng1+V7MSbtbG00lPZ3+y8hq0/MFvdxxU7qyoqXxPTp05k+fXqWb7d///506NDB7LzkvNUnn3zCJ598kpVhiVeUxRO1K1asYODAgTx48IDkcrgajYbcuXPz7bff0qlTJ0uHIIQQr5RJW86hSD2p8df1yCyMRgghMsjGFgJqWTsKIYTIEHN3M+U182BBrU5x8XY0p65GGkoYnI+IJklnWle2SB4XwyjZ1wt6UCafB0728sWVEJnNy8sLLy8va4chBGDhRO327dvp2rUrfn5+jB8/nnLlyqGU4vTp08yePZtu3brh4+NDw4aWfZqhEEK8Sm5FxWPjmHrd2ZuRcRwOuy/1FMULeVUfniSEEEI87cndTDpsXcLQ2EWjktyIiAyg/9JjvFsrAA1w8lokp69HEpOgNVmHt6sjQQU9/kvKelK+gAeeLlJXVgghXjUWTdROnDiRgIAAjhw5Qu7cuQ3TW7duzfvvv09wcDATJ06URK0QQmSx29FxaTcSIhXpHTUkhBBCvAombTmHrdtZHP02YmP/5M4lXaIH8bdaMn+fcftcDv/VlS3oSdB/idm8Hk5SV1YIIYRlE7VHjx5l1KhRRknaZF5eXvTu3ZtJkyZZMgQhhBBm+Lo5WTsEkUOlVgM5IjKOAUuPMadrRUnWCiGEeKXc0x3HJf9qk+kau0ic8i8l7npX6haoT5My/gQV9KSoj6vchSKEEMIsi1Yd12q1ODmlngxwdnZGqzW97UMIIcTz83N3JLVLfw36kY/BAVKDSTwffQ1kU8nTxm88g9ZMnT0hhBDiZeXguwWApwfEJv/u6LeR5uX9aF+5ICX83CRJK4QQIlUWTdQGBgayYsUKEhMTTeYlJiayYsUKAgMDLRmCEEK8ckY2Kw1gkqxN/n1sy0D5A0E8t1tR8anOUzypgSyEEEK8Kmzso0yStMk0GrCxjyRSdyFrgxJCCJEjWTRR+/7773P06FHq1q3Lr7/+ysWLF7l48SLr16+nXr16hIaG8sEHH1gyBCGEeOU0CvRnTteK+HsY39Hg7+Ekt6WLLCE1kIUQQghj3p6pf9EpxMsgIiKCRo0akStXLjw9Pa0dTrpoNBrWr19v7TCEMGLRRG2vXr0YOXIkf/75J23atKF06dKULl2atm3bcvDgQUaOHEnPnj0tGYIQQrySmpbNy/4R9VnRtyozOwaxom9V9o+oL0lakSWkBrIQQghhzC+Xr7VDENmU0mp5fOgwkb9t4vGhw6gcWh5y+vTp3Lx5kxMnTnDhgowgF+YVKVKEGTNmZHi5tWvX0qRJE7y9vdFoNJw4ccKkTXx8PAMHDsTb25tcuXLRqlUrrl27ZtTmwYMHdOvWDQ8PDzw8POjWrRsPHz40anPlyhVatmxJrly58Pb25qOPPiIhIcGozV9//UWdOnVwdnYmf/78fP755yiVOeXfLPowMYAJEybQq1cv1q9fT1hYGEopihUrRuvWrSlevLilNy+EEK8sWxsN1YrlsXYY4iXj5+7I3XjM1qnVoB+5LTWQhRBCvEp87dy4rx6jzNQ/0CiFn6MnFX0rWiEykd1F/f47tyZMJCkiwjDNzt8fv9GjcG/c2IqRZdylS5eoVKkSJUqUSLWNRqMhLCyMIkWKZMo2tVotGo0GGxuLjkEU2cDjx4+pUaMG7du3p2/fvmbbDB48mI0bN/Lzzz+TJ08ehg0bRosWLQgNDcXW1haAzp07c+3aNbZu3QpAv3796NatGxs3bgT0fap58+b4+Piwf/9+7t27R48ePVBK8e233wIQFRVFo0aNqFevHkeOHOHChQv07NmTXLlyMWzYsBfe1yzpzSVKlODjjz9m9uzZzJkzh+HDh0uSVgghhMiBpAayEEIIYWzIg4eAPimbUvLvI+7dxzargxLZXtTvv3N90GCjJC1A0q1bXB80mKjff7fIdr///nvy58+PTqczmt6qVSt69OjBuHHjCAoKYsGCBRQqVAhXV1cGDBiAVqvl66+/xt/fH19fX7766ivDskWKFGHNmjUsWbIEjUbz3HdOb9iwgRIlSuDs7Ey9evVYvHgxGo3GMOJx0aJFeHp68ttvvxEYGIijoyPh4eEcOXKERo0a4e3tjYeHB3Xq1OHYsWNG67548SK1a9fGycmJwMBAtm/fnqHYQkJCCAoKwsnJicqVK7N+/XqjkZ1arZY+ffoQEBCAs7MzpUqVYubMmUbr6NmzJ61bt2bChAn4+fnh6enJ+PHjSUpK4v/Zu/P4qKr7/+OvO5MdkkAgCzthC4ZFtqABkS0solhKNeLCIlQEahVBQaCyVVnLom0BfyIiGttSt4oiIAqCRgGhWBEE5MumZFMgCQZIMjO/P4YMGbIwgUwmE95PH3kkOXc5n3vnYxI+c+45Tz/9NGFhYdSvX59Vq1Y5jjl27BiGYbB27Vq6detGYGAgcXFxHDp0iF27dtGpUyeqV69O//79ycjIcBznyj0pC1eur0ePHowfP96pbdCgQY586NGjB8ePH+fJJ5/EMAyMQm9uvf3227Rq1Qp/f38aN27MokWLnM4zdOhQpk+fTkJCQrHxZWZm8sorr7Bo0SISEhJo3749b7zxBt9++y2bN28G4MCBA2zYsIGVK1cSHx9PfHw8L7/8Mh988AEHDx4EYNOmTezfv5833niD9u3bk5CQwKJFi3j55ZfJysoCICkpiQsXLrB69Wpat27N4MGDmTp1KosXLy6XUbVuL9R++eWXPPjgg3Tu3JmmTZvSpEkTp4+mTZu6OwQREREpJ5oDWURExFnP0yksTv+ZiCseWY+0WFic/jMJP/8Ex5M9FJ1UFJvNhjUnx6UPS3Y2ac89D8UVdWw2wEba83OwZGe7dL6yFIfuvfdefv75Z7Zs2eJoO3PmDBs3buTBBx8E7KNjP/roIzZs2MA//vEPVq1axZ133smPP/7IZ599xvz58/nTn/7EV199BdiLgv379ycxMZGUlJQiBTxXHDt2jHvuuYdBgwaxd+9eHn30UaZNm1Zkv5ycHObOncvKlSv57rvviIiIIDs7m+HDh7N9+3a++uormjdvzoABA8jOzgbAarUyePBgzGYzX331FStWrGDy5Mkux5adnc3AgQNp06YNe/bs4c9//nOR461WK/Xr12ft2rXs37+f6dOnM3XqVNauXeu036effsqpU6fYtm0bixcvZubMmdx1113UrFmTHTt2MGbMGMaMGcPJkyedjpsxYwZ/+tOf2LNnDz4+Ptx///1MmjSJF154ge3bt3PkyBGmT5/uFHNp9wTgjjvuoHr16qV+lPX6SvPOO+9Qv359Zs+eTUpKCikpKQDs3r2bxMREhgwZwrfffsvMmTN59tlnWb16tcvn3r17N3l5efQtNBK9bt26tG7dmuRk+8/fL7/8ktDQUG655RbHPrfeeiuhoaFO+7Ru3Zq6des69unXrx8XL15k9+7djn26d++Ov7+/0z6nTp3i2LFjLsdcErdOfbBmzRoefvhhfH19adGiBQ0bNnRndyIiIlIB+reuQ5/YKHYePU169gUigu3THWgkrYiI3KgScs7TM+c8ewL8yTCbCbdY6HDh4uWRtOfSPBmeVADb+fMc7NCxnE5mH1l7KK6zS7vH7NmNERTk0r5hYWH079+fN998k969ewPw73//m7CwMHr37k1ycjJWq5VVq1YRHBxMbGwsPXv25ODBg6xfvx6TyURMTAzz589n69at3HrrrYSHh+Pv709gYCBRUVHXdMkrVqwgJiaGhQsX2q8pJoZ9+/Y5jdwFyMvLY9myZdx8882Otl69ejnt89JLL1GzZk0+++wz7rrrLjZv3syBAwc4duwY9evXB+zTdN5xxx0uxZaUlIRhGLz88suOEbk//fST0yP4vr6+zJo1y/F9dHQ0ycnJrF27lsTEREd7WFgYL774ouM+LliwgJycHKZOnQrAlClTmDdvHl988QVDhgxxHPfUU0/Rr18/AJ544gnuv/9+PvnkE7p27QrAqFGjnAqbV7snACtXruT8+fMu3QNXr680YWFhmM1mgoODnfJk8eLF9O7dm2effRaAFi1asH//fhYuXOjy6OzU1FT8/PyoWbOmU3tkZCSpl0atp6amEhFRdL7wiIgIp30iIyOdttesWRM/Pz+nfa6cvqPgmNTUVKKjo12KuSRuLdQ+//zzxMTEsHnzZqdqtIiIiHg3zYEsIiLizAzEXbhY/MbqkcW3i3jAgw8+yOjRo1m2bBn+/v4kJSUxZMgQxzyejRs3Jjg42LF/ZGQkZrPZaS7YyMhI0tPTS+3njjvuYPv27U5trVq1cnrk/dy5cwAcPHiQuLg4p307dy5aqPbz86Nt27ZObenp6UyfPp1PP/2UtLQ0LBYLOTk5nDhxArA/8t6wYUNHkRYgPj6+1NgLO3jwIG3btiUg4PITZcXFtmLFClauXMnx48c5f/48ubm5tGvXzmmfVq1aFbmPrVu3dnxvNpupVatWkXtb+JoLioJt2rRxait8zNXuCUC9evVcvQUuX9+1OHDgAL/5zW+c2rp27crSpUuxWCyOvLwWNpvNKd+MYuYSL499Cka1F3dsWbm1UHv8+HEWLFigIq2IiIiIiIhUTcFRkJtOiUtthtSFRl0qOiqpYEZgIDF7dru0b87XX3Ny9KNX3a/B/3uJoE6dXOq7LAYOHIjVauXDDz8kLi6O7du3s3jxYsd2X19f5/MbRrFtV85ze6UrR2w2b96c9evXF1sgvLIQVtB2pcDAwCL7jRgxgoyMDJYuXUqjRo3w9/cnPj6e3NzcEs9TloKaK7GtXbuWJ598kkWLFhEfH09wcDALFy5kx44dTvtd670tvE9BLFe2FT7mavcEii+kX6mgkO7K9ZlMpiL3JS8vr9Tzg+uvfWmioqLIzc3lzJkzTqNq09PT6dKli2OftLSiTzdkZGQ4it9RUVFFXrMzZ86Ql5fntE/qFXNLFxTJrxyNey3cWqitV6+eUxKIiIiIiIiIVCkJs2H9GOxLaxYuLlwqPPSfByYtJ1bVGYbh8vQD1bp2xScqivy0tOLnqTUMfCIjqda1K8Z1jCYsSWBgIIMHDyYpKYkffviBFi1a0LFjOU3bUEhxBdlGjRoVeWwcoGXLlqxfv96p7euvv3apn+3bt7Ns2TIGDBgAwMmTJ/n5558d22NjYzlx4gSnTp1yDCT88ssvXb0MWrZsSVJSEhcvXnTMS3plbNu3b6dLly6MGzfO0XbkyBGX+yhvV7snULapD1y5vvDwcMe8s2BfgGzfvn307NnT0ebn54flivm8Y2Nj+fzzz53akpOTadGihcujaTt27Iivry8ff/yxYyqGlJQU9u3bx4IFCwD7KOrMzEx27tzpGBG9Y8cOMjMzHcXc+Ph4nn/+eVJSUqhTx772xqZNm/D393f8PxIfH8/UqVPJzc3Fz8/PsU/dunWLze2ycutiYqNHjyYpKanIiyAiIiIiIiJSJbQcAIlrIOSKBTVD6trbY+/2TFxSaRlmM5FTp1z65oqRnZe+j5w6xS1F2gIPPvggH374IatWreKhhx5yWz+uevTRR/n++++ZPHkyhw4dYu3atY45V682+rVZs2a8/vrrHDhwgB07dvDggw8SWGiUcUJCAjExMQwbNoxvvvmG7du3F7tQWUkeeOABrFYro0eP5sCBA2zcuJG//OUvTrE1a9aMr7/+mo0bN3Lo0CGeffZZdu3aVca7UH6udk/AXkhv1qxZqR+Fz3e16+vVqxcffvghH374Id9//z3jxo3j7NmzTvs0btyYbdu28dNPPzkKxxMnTuSTTz7hz3/+M4cOHeK1117jb3/7G0899ZTjuNOnT7N37172798P2Kej2Lt3r2Nka2hoKKNGjXKc67///S8PPfQQbdq0ISEhAYCbbrqJ/v3788gjj/DVV1/x1Vdf8cgjj3DXXXcRExMDQN++fYmNjWXo0KH897//5ZNPPuGpp57ikUceISQkBLDng7+/PyNGjGDfvn28++67zJkzhwkTJpTL1AduLdR27twZf39/OnfuzKpVq9iyZQvbtm0r8iEiIiIiIiLitWLvhvH7YPgH8LtX7J/Hf6sirZQopG9f6r2wFJ8rHpX2iYyk3gtLCSm0er079OrVi7CwMA4ePMgDDzzg1r5cER0dzVtvvcU777xD27ZtWb58uaOYWjCKtSSrVq3izJkztG/fnqFDh/L44487LRplMpl49913uXjxIp07d+b3v/99kUXKShMSEsK6devYu3cv7dq1Y9q0aUyfPh3AMW/tmDFjGDx4MPfddx+33HILv/zyi9Po04p2tXtSVq5c38iRIxk+fDjDhg2je/fuREdHO42mBZg9ezbHjh2jadOmhIeHA9ChQwfWrl3LP//5T1q3bs306dOZPXu200Ji77//Pu3bt+fOO+8EYMiQIbRv354VK1Y49lmyZAmDBg0iMTGRrl27EhQUxLp165xG5SYlJdGmTRv69u1L3759adu2La+//rpju9ls5sMPPyQgIICuXbuSmJjIoEGDHIV5sBeFP/74Y3788Uc6derEuHHjmDBhAhMmTLjm+1uYYSvrxA9lUHiCZCh+sl3DMCrdiNusrCxCQ0PJzMx0VMyl8vOW181b4hRn3vC6eUOMUpS3vG7eEqc484bXzRtilOJ5w2vnDTFKUd7yunlLnOKsvF63CxcucPToUaKjo50WmSorm8VCzte7yc/IwCc8nKBOHd06ktabPP/886xYsYKTJ096OpQikpKSePjhh8nMzCwyUlXkSmX5eeHWOWpfffVVd55eREREPET/qBARERG5fobZTLVbOns6jEph2bJlxMXFUatWLb744gsWLlzIY4895umwAFizZg1NmjShXr16fPPNN0yePJnExEQVaaXcubVQO3z4cHeeXkRERDwga9Mm0ubMJb/Qaqc+UVFETp3i9sf0RERERKRqOnz4MM899xynT5+mYcOGTJw4kSlTpri93zlz5jBnzpxit3Xr1o2PPvqI1NRUpk+fTmpqKnXq1OHee+8t0/QJIq5ya6HWHbZt28bChQvZvXs3KSkpvPvuuwwaNMhpnwMHDjB58mQ+++wzrFYrrVq1Yu3atTRs2NAzQYuIiFQRWZ98QvbUaUVWKM5PS+OnJ8ZDBcypJiIiIiJVz5IlS1iyZEmF9ztmzBgSExOL3VYwYnbSpElMmjSpIsOSG5TXFWp//fVXbr75Zh5++GF+97vfFdl+5MgRbrvtNkaNGsWsWbMIDQ3lwIED1zVnjIiIiNhlLFpMQHHT29tsYBikzZlLcO/emgZBRERERLxCWFgYYWFhng5DBPDCQu0dd9zBHXfcUeL2adOmMWDAABYsWOBoa9KkSUWEJiIiUuXlp6VBSUVYm4381FRyvt6tudZERERERETKyOsKtaWxWq18+OGHTJo0iX79+vHf//6X6OhopkyZUmR6hMIuXrzIxYsXHd9nZWVVQLRyo1B+ibsot8Sdrie/8jMy3BGSVBH62SXupPwSd1J+SXFsxT1pJCJSSFl+TpjcGEeFS09P59y5c8ybN4/+/fuzadMmfvvb3zJ48GA+++yzEo+bO3cuoaGhjo8GDRpUYNRS1Sm/xF2UW+JO15NfPuHhboxMvJ1+dok7Kb/EnZRfUpivry8AOTk5Ho5ERCq7gp8TBT83SmPYvPjtH8MwnBYTO3XqFPXq1eP+++/nzTffdOx39913U61aNf7xj38Ue57i3hlt0KABmZmZhISEuPUapPxkZWURGhpa6V435VfVUBnzS7lVNVTG3IKS82tP19sI+OWXIouJAWAY+ERG0uyTzZqjtpKojPmln11Vh/JL3KUy5hYov6qK8syvlJQUzp49S0REBEFBQRiGUU5RikhVYLPZyMnJIT09nRo1alCnTp2rHlOlpj6oXbs2Pj4+xMbGOrXfdNNNfP755yUe5+/vj7+/v7vDkxuU8kvcRbkl7lRSfoVPnED21GlgGM7F2kv/MImcOkVFWimVfnaJOym/xJ2UX3KlqKgowP50r4hISWrUqOH4eXE1VapQ6+fnR1xcHAcPHnRqP3ToEI0aNfJQVCIiIlVHSO/ehLxQjbTn59gXFrvEJzKSyKlTCOnb14PRiYiIiFQcwzCoU6cOERER5OXleTocEamEfH19MZdhIIvXFWrPnTvHDz/84Pj+6NGj7N27l7CwMBo2bMjTTz/Nfffdx+23307Pnj3ZsGED69atY+vWrZ4LWkREpArZGWNi/jgzYQdN1DwHZ6rD6RgTk2NMJHg6OBEREZEKZjaby1SIEREpidctJvb111/Tvn172rdvD8CECRNo374906dPB+C3v/0tK1asYMGCBbRp04aVK1fy9ttvc9ttt3kybBERkSphy4ktTNg6gdQL6exvZOKLVib2NzKRdiGDCVsnsPn4Zk+HKCIiIiIi4pW8bkRtjx49uNr6ZyNHjmTkyJEVFJGIiMiNY8nuJdgo+nvYhg0Dg/k759OzQU/MJo0qERERERERKQuvG1ErIiIinpN+vuTFMmzYSM1JZU/6ngqMSEREREREpGpQoVZERETKVUZOhqdDEBERERER8Toq1IqIiEi5Cg8K93QIIiIiIiIiXkeFWhEREXFZRGAEBkax2wwMooKi6BDRoYKjEhERERER8X4q1IqIiIjLnuz4JECRYm3B95M7T9ZCYiIiIiIiItdAhVoRERFxWc+GPVncYzERQRFO7ZFBkSzusZiERgkeikxERERERMS7+Xg6ABEREfEuCY0S6NmgJ3vS95CRk0F4UDgdIjpoJK2IiIiIiMh1UKFWREREysxsMhMXFefpMERERERERKoMTX0gIiIiIiIiIiIi4mEq1IqIiIiIiIiIiIh4mAq1IiIiIiIiIiIiIh6mQq2IiIiIiIiIiIiIh6lQKyIiIiIiIiIiIuJhKtSKiIiIiIiIiIiIeJgKtSIiIiIiIiIiIiIepkKtiIiIiIiIiIiIiIepUCsiIiIiIiIiIiLiYSrUioiIiIiIiIiIiHiYCrUiIiIiIiIiIiIiHubj6QBERETEC1ktcDwZzqVB9Uho1AVMZk9HJSIiIiIi4rVUqBUREZGy2f8+bJgMWacut4XUhf7zIfZuz8UlIiIiIiLixTT1gYiIiLju+/WwdphzkRYgK8Xevv99z8QlIiIiIiLi5VSoFREREddtng7YitlwqW3DM/ZpEURERERERKRMVKgVERER12WnlrLRBlk/2eeuFRERERERkTJRoVZERETK17k0T0cgIiIiIiLidVSoFRERkfJVPdLTEYiIiIiIiHgdFWpFRETEdcFRgFHCRgNC6kGjLhUZkYiIiIiISJWgQq2IiIi4LmH2pS+uLNZe+r7/PDCZKzIiERERERGRKkGFWhEREXFdywGQuAZC6ji3h9S1t8fe7Zm4REREREREvJyPpwMQERERLxN7N7S8E44n2xcOqx5pn+5AI2lFRERERESumQq1xbDZbABkZWV5OBIpi4LXq+D1q6yUX97JG/JLueWdvCG3oIT8qnUz1Lr09blfKz4ouSpvyC/97PJeyi9xF2/ILVB+eStvyS8RuTGpUFuM7OxsABo0aODhSORaZGdnExoa6ukwSqT88m6VOb+UW96tMucWKL+8XWXOL+WW91N+ibtU5twC5Ze3q+z5JSI3JsOmt5GKsFqtnDp1iuDgYAzDvjhKVlYWDRo04OTJk4SEhHg4wqvztnjLqrjrs9lsZGdnU7duXUymyjv9svKr8vPW/KoKuQXeGbOrvDW3oGrkl7fFW1beml9VIbfAO2MuC+WXZ3ljzK7y1tyCqpFf3hZvWXlzfonIjUkjaothMpmoX79+sdtCQkK86heYt8VbVldenze8I6r88h7ell9VKbfAO2N2lbflFlSt/PK2eMvK2/KrKuUWeGfMZaH88ixvjNlV3pZbULXyy9viLStvzC8RuTHp7SMRERERERERERERD1OhVkRERERERERERMTDVKh1kb+/PzNmzMDf39/TobjE2+Itq6p2fd52Pd4Wb1lVpevzxmvxxphdVdWuzduux9viLauqdH3eeC3eGHNZVKXr88Zr8caYXVXVrs3brsfb4i2rqn59IlL1aDExEREREREREREREQ/TiFoRERERERERERERD1OhVkRERERERERERMTDVKgVERERERERERER8TAVakVEREREREREREQ8TIVaEREREREREREREQ9ToVZERERERERERETEw1SoFREREREREREREfEwFWpFREREREREREREPEyFWhEREREREREREREPU6FWRERERERERERExMNUqBURERERERERERHxMBVqRURERERERERERDzMx9MBVEZWq5VTp04RHByMYRieDkdcZLPZyM7Opm7duphMlfc9COWXd/KG/FJueSdvyC1Qfnkrb8gv5Zb3Un6Ju3hDboHyy1t5S36JyI1JhdpinDp1igYNGng6DLlGJ0+epH79+p4Oo0TKL+9WmfNLueXdKnNugfLL21Xm/FJueT/ll7hLZc4tUH55u8qeXyJyY1KhthjBwcGA/Qd3SEiIh6MRV2VlZdGgQQPH61dZKb+8kzfkl3LLO3lDboHyy1t5Q34pt7yX8kvcxRtyC5Rf3spb8ktEbkwq1Baj4LGVkJAQ/cL1QpX9sSPll3erzPml3PJulTm3QPnl7Spzfim3vJ/yS9ylMucWKL8qO4vVxs6jp0nPvkBEcACdo8Mwmy7nVGXPLxG5MalQKyIiIiIiIiJVxoZ9Kcxat5+UzAuOtjqhAcwYGEuXhtU8GJmISOk0c7aIiIiIiIiIVAkb9qUw9o09TkVagNTMC4x9Yw8f70/1UGQiIlenQq2IiIiIiIiIeD2L1casdfuxFbOtoG3eR99XZEgiImWiqQ9ERERERERExCvkW6xkXcjnbE4uZ8/nkZmTx9nzuWTm5PHtT5lFRtIWZgPSsi5WXLAiImVU7oXa7Oxsjh07Rps2bQD47rvvaNCggSZXFxEREREREREALuRZOJuTR+b5vCJFV0f7lW05eWRfzPd06CIiblPuhdpvvvmGMWPG8O233wLwwAMPsGzZMrp27VreXYmIiIiIiIiIh9hsNrIv5tuLqQUjW8/nORdgcy4VXK8oul7Mt15X38H+PoQG+VIjyJcagX6EBvlyMc/C5gPp5XR1IiIVr9wLtbfddhvt2rVjzZo1mM1mWrdurSKtiIiIiIiIVGkWq42dR0+Tnn2BiOAAOkeHYTYZng7LJXkWK1mXRrDai6yXCqyXCq6FR70Wbss8n4fFWtyMsK4xmwxqBPoSGuRLaKAvNQJ9qRHkZ/86yNexraAQW7A9JMAHH3PRJXcsVhu3zf+U1MwLxc5TawCRIf6cvOaIRUTcyy1z1C5YsIB+/fphMpnYsGGDO7oQERERERERqRQ27Eth1rr9TvOj1gkNYMbAWPq3rlMhMdhsNi7kWZ2nDihcdL30vb0g61yIPXed0wkE+JqoEehHjYKCq+Nz4aLr5e0FbdX9fTCM8itmm00GMwbGMvaNPRjgVKwt6OWZO1pyz+xy61JEpFyVa6HWZDI5fsjabPYfifXr18dms2EYBhaLpTy7ExEREREREfGoj/en8tR7h4uM4EzNvMDYN/aw/KEOZSrWWq2FphM4f+XUAc5F18xCUw2cPZ9H7nVOJxAS4OMYwVq46Foj0F5wLTyqtWDEa0igLwG+5uvqtzz1b12H5Q91KFI4j7pUOO/SsJoHoxMRKV25FmqtVvsvhVOnTtGvXz8Mw2Djxo3UqVMx7yCKiIiIiHfzykeHrRY4ngzn0qB6JDTqAqbKU7QQEfea99H32Cj6/3xB4fbZ/3xHWDU/si/kXy665uQ6Fssq3Hb2vH3E63XMJoCPybhi5Kqf0xQCjm2Fi66XCq6V/ueti/q3rkOf2Khif59kZWV5OjwRkRK5ZeqDyZMnM3HiRMxmM08//TRvvPGGO7oRERERkSqkMjw6XGb734cNkyHr1OW2kLrQfz7E3u25uESkwqRlXcTkH1Ti9ozsiyS+9FWZzxvoay52VGuNIOeia0GRtUaQvehazc9crtMJeCuzySC+aS1PhyEiUiblXqj9/PPP2bNnD2vWrAFg4cKFfPHFF1pQTERERERKVN6PDleI/e/D2mFwZdRZKfb2xDUq1ooIALWq+VGvZqDTCNcio14LLZ4VGuiLv49G5ouI3GjKvVDbtm1b/vGPfzjewUtKSqJhw4bl3Y2IiIiIVCGlPTpsALPW7adPbFTleSzXarGPpC12XfFLUW94BlreqWkQRIS/PdBBoztFROSqyr1QGxISQtu2bR3ft2nTpsR9t2/fTrdu3co7BBERERHxMqU9OmwDUjIvMOmtb2hTL9Qx2qzgIyTQA6PPjic7T3dQhA2yfrLvF62/d0WqssgQf36+CDasmIOOYvhkY8sPxpITjYGJqFD7/KgiIiJX45Y5aq/myy+/ZPr06Xz66adYLBZPhCAiIiIiXubtPT/x9p6fStwe6Gt2KuBeWdAtrd3XbHI9kJ9/gOS/urbvuTTXzysiXumZO1oy+ZN3CQx/n9jUs9Q8C2eqw/4mNTifcTczBg6rPE8DiIhIpVbuhdrDhw/zt7/9jcOHD1OrVi0eeugh+vXrB8D+/ft56qmn2LhxI4ZhcN9995V39yIiIiJSRfVqGUGgr5nM83lOH1kX8rDZ4HyehfN5FlKzLlz9ZFeo5md2Gp0besX8kWF+FmLObKHpibepkbHL5fNaqkUUM6GDiFQlPtUP0P3XNYx4z0rt7MvtPwefZnWfNfgEdwQq2RzbIiJSKZVrofa7776jS5cuZGdf/u305ptv8uqrr2I2mxk1ahRWq5WhQ4cydepUWrRoUZ7di4iIiIiXuvzocFEGEBUawMvDOhU7Ks1qtZF9Id+peHv2fK5zMfd8Hmdz8ooUebMv5APwa66FX3MtnMp0LvLGGCcYYt7CnebthBo5AFhsBlusN9PB9AM1OEdxA+WsNkilFsctLYm/3psjIpXax0lzmPiutUh7WDZMfMfKK34z6Tm9J2bNVy0iIldRroXaP//5z1y8eJEXXniB3r17c/jwYcaPH8+UKVM4c+YMCQkJLF26lGbNmpVntyIiIiLi5Z65oyVPvXcYA+dibUENdMbA2BIfHTaZDPt0BkG+Ze7XYrWRdUXx9lz2WWoe/YDo428Rlb3PsW+GOZKPfBN4x9aDA78G0yPvK5b7LsVqw6lYa710AbPyhjLg17wyxyQi3uWujWcAM1f+hDIBVmDQh7+w55Gviat3S8UHJyIiXqVcC7Xbt29n5MiR/PGPfwQgNjYWm83G4MGD+c1vfsO7775bnt2JiIiISBXRJzaK5dWDmbVuPymFRrVGhQYwY2As/Vu757Fhs8mgZjU/agb5wqk98P1rsO9tyD1n38HkAzEDoONwwpv0ZJjJzDDgyyO/cP/LVsbmjWeG7xrqctpxzlRqMStvKButnRkRHOCWuEWk8qiVDUYJg2VNQO1sSNm1A1SoFRGRqyjXQm1GRgYdO3Z0auvUqRMADz30UHl2JSIiIiJVTP/WdegTG8XOo6dJz75ARLB9pXS3LsJz/ix8+2/Y/RqkfXu5PawpdBgG7R6A6hFFDuscHUad0AA2ZXbm44ud6Gz6ngjOkk4NdlpbYsNEHa30XulYrBb2pO8hIyeD8KBwOkR00OPoUiFq/urpCERExBuUa6E2Pz+fwMBAp7aC72vWrFmeXYmIiIhIFWQ2GcQ3reXeTmw2OPEV7HkNvnsP8s9f6twfYn8DHYdDo65glFwgNpsMZgyMZewbe7Bh4itrrGObK9M1SMXbfHwz83bOIy0nzdEWGRTJM52fIaFRggcjkxtBs6Zxng5BRES8QLkWagGMEv6gLaldRERERKRC/PoLfPMm7FkDPx+63B4RCx2GQ9tECHJ9BGz/1nVY/lCHCp+uQcpuy4ktPLv7WWxXLFeXnpPOhK0TWNxjsYq1cs3ya4ViPXsOUzHbrIA1vAbV4zpXdFgiIuKFyr1Q+/DDDzNq1Kgi7XfccQcmk/OvLsMw+PVXPQMiIiIiIm5itcLRz+yjZw98ANZLi3v5BkHrwdBhBNTvVOro2dJ4ZLoGKbMlu5dgw4ZhtXHTSRs1z8GZ6nCggQEmg/k759OzQU9NgyDXpMHkqWRNmYoNnBYUK/i+0bOzMMzKLRERubpyLdTefvvtGjkrIiIiIp6XnQr/fQP++zqcOXa5vW57++jZ1r+DgJBy6apCpmuQ65J+Pp34EwYjPrZSO/ty+8/BsLqPiZ0xqexJ30NclB5Pl7ILqXeRkK5nSNsTQv75ywVZ3yALke2zCKl/oZSjRURELivXQu3WrVvL83QiIiIiIq6zWuCHzfaFwQ5tAJvF3u4fYp/WoMNwqNPWszGKR3Q8bGXih0UHlIRlw8R3rCwaDBm3pRVzpIgLNk8npMF5guudJyfDj/wLZnwCLASF52KYDNjwDLS8EzRiW0RErqLcpz4QEREREalQZ0/AntftI2izT11ub3CrfWGw2EHgF+Sx8MTzHtxiBcxcWao1YZ9DdMTHVgJ+l1LxgUnVkJ0K/gaGCapF5l6x0QZZP8HxZIju5pHwRETEe5RrobZHjx706tWLHj16EB8fj6+vb3meXkRERCoJi9WmOTnFLSz5uez59nUysk4QHtKQDm2GYvbxK2bHPDi43j569sinULBIVGAYtHsAOgyD8JgKjV0qr1rZYJQwmNEE1M6G+odOQ8cKDUtuJOc0YltERK6uXAu127dvZ9u2bRiGQUBAAF26dKFnz5707NmTzp07Y9YE6iIiIl5vw76UIqvc19Eq91IONn8+l3mHkkgzXy76R/53Cc+0eJCE26bYG345Yl8YbO+b8GvG5YOju9tHz7a8C3z8KzhyqQoKzy0qUu6qR3o6AhER8QLlWqg9c+YM27ZtY+vWrWzdupUtW7bwySefYBgGQUFBdOvWzVG47dixoxYeExER8TIf70/lqfcOF4xddEjNvMDYN/aw/KEOKtbKNdny5SKeTXkbm8m5Pd0EE35IYnHG9ySc/RmOf355Y/VIaPcgdBgKYU0qNmCpcn6o0QYtJSbXJDgKctOhyG9HAANC6kKjLhUdlYiIeKFyLdSGhIRw1113cddddwGQlZXFtm3b2LJlC1u3bmXTpk1s2LABwzAICQnh9ttv5z//+U95hiAiIiJuNO+j77FhxoSVzqbvieAs6dRgp7UlNkzMWrefPrFRmgZBymzJD2uxVTPDFW/k2wwDw2Zj/umd9Dx5CrNhgmYJ9oXBWvQDs6bakqvLDgykeu5FKDJLLYCN7MAgUhrfVNFhSVWRMBvWj8GeX4WLtZfyrf88LSQmIiIucetiYsUVbrds2cKSJUvYtm0bH3zwgTu7FxERkXKWlnWROwL3McN3DXWN0472U7YwZuUNY2NmZ3YePU1801oejFK8UbrZwFzC01Y2wyDVx4c9re4kru8CCK1fwdGJt1se+1tm7f0XBjaci7U2bBgsbZPIH0OreSo88XYtB0D1NbBhMmQVWtAwpK69SBt7t+diExERr+LWQm2B//3vf2zZsoUtW7awbds2zp49i4+PDx07arZ+ERERb9LbtJvlvsuLtEdxmuW+SxmbN5707HYVH5jcEDIimqtIK9fkWEwHnvcL4NH/vUf4hUxHe0ZgDV5qM4ijN8XROTrMgxGK14u9G1reCceT7QuHVY+0T3egkbQiIlIGbinUfvfdd47C7GeffcaZM2fw9fUlLi6OcePG0b17d7p27UpQUJA7uhcRERE3ecb3HwAYNvg13Y/8C2Z8AiwEhediM2CG7+scr/YHD0cpVVV4SENPhyBe6pk7WvLUe2a+qtOK2J//j7CL2Zz2D2Z/7SZYDRPLB8Zqyha5fiYzRHfzdBTiARaLhby8PE+HISKVkK+vL2az62/alWuh9r777mPr1q38/PPP+Pn5ccstt/DYY4/RvXt34uPjCQgIKM/uREREpILVMc5w7sdA0vaEOq2Q7hNoIbJDJnUb/EKk+XsgwnNBileKsNg4bbNhK2b6A8NmI9IKHdoM9UBkUhX0iY1iefVgZq3bz7dGM0d7ndAAZgyM1SKIInJNbDYbqampnD171tOhiEglVqNGDaKiojBKmOarsHIt1P773//G19eXkSNHMnnyZJo1a3b1g0RERMRrZP3oT/aumkXa88+b+OmLmtD1DCG/pnsgMvF2TzZL5NmUtzGuKNYaNvvCPJNbPIjZx89T4UkV0L91HfrERrHz6GnSsy8QERxA5+gwjaQVkWtWUKSNiIggKCjIpSKMiNw4bDYbOTk5pKfb/31Up87V3xgu10Lt2LFj+eyzz1i1ahWrVq2icePG9OjRgx49etC9e3caNtTjaiIiIt4s45sQ7M/HXPkPEftK12n/DSE4MLzYddVFStMzfiKL/xfAvENJpBV6OizSai/SJtw2xXPBSZVhNhla7FBEyoXFYnEUaWvV0s8VESleYGAgAOnp6URERFx1GoRyLdT+/e9/B+Dnn392zE+7detWXn31VQzDoGHDho6ibY8ePWjcuHF5di8iIiJuln/eDOaSyrAG+Tk+5PzsRzU9VCPXIOG2KfS8dSJ7vn2djKwThIc0pEOboRpJKyIilU7BnLRae0dErqbg50ReXl7FFmoL1K5dm3vvvZd7770XKFq4XbNmDQANGjTg2LFj7ghBREREPCT/59OeDkG8mNnHj7j2ozwdhoiIiEs03YGIXE1Zfk64pVB7pYLCbevWrbnppptYu3Yt27dv5+TJkxXRvYjIDclmsZDz9W7yMzLwCQ8nqFNHjDKsNukRVgscT4ZzaVA9Ehp1sa+gLF7FJzzc0yGIiIiIiIh4HbcWag8ePMiWLVvYunUrW7duJSMjA7BPptukSRN69uzpzu5FRG5YWZs2kTZnLvmpqY42n6goIqdOIaRvXw9GVor978OGyZB16nJbSF3oPx9i7/ZcXOLEJzIS288/FzsHrQ3wjYoiqFPHig5LRERERCqhESNGcPbsWd577z1PhyLiFcq1UHv48GGnwmxaWhq2Syv1NmzYkGHDhtGzZ0969uxJgwYNyrNrERG5JOuTT8ieOg0u/fwtkJ+Wxk9PjIcXlla+Yu3+92HtMOylvkKyUuztiWtUrK0k0of3o8GiJKyAqVC7FftyYmmP3Enzyj5yW0RERKSSsFht7Dx6mvTsC0QEB9A5OgyzSdMpiNyoyrVQGxMTg2EY2Gw26tWrxwMPPOAozEZHR5dnVwDMnTuXqVOn8sQTT7B06VLAPlp31qxZ/L//9/84c+YMt9xyC3//+99p1apVufcvIlIZZSxaTMAVRVrAXrg1DNLmzCW4d+/KMQ1Cfi5kp8KHEyhSpIVLbQZseAZa3qlpECqBuX6baTrYxIiPrdTOvtx+Ohhe62PmuN9GNlifxKzXSkRERKRUG/alMGvdflIyLzja6oQGMGNgLP1b16mwOHJzc/Hz08KdIpWB6eq7uO6+++5j+fLlHDx4kJMnT/L6668zcuRItxRpd+3axf/7f/+Ptm3bOrUvWLCAxYsX87e//Y1du3YRFRVFnz59yM7OLuFMIiJVS35aWskbbTbyU1PJ+Xq3+wLIOw9njsOPX8P362H3avhsIax/GtYOh1cHwF87wbyG8Fw4vNAGfs0o5YQ2yPrJPneteFz6+XR2xpj4wzgzMx8w8cLdJmY+YP9+R4xBak4qe9L3eDpMERERkUptw74Uxr6xx6lIC5CaeYGxb+xhw74Ut/Xdo0cPHnvsMSZMmEDt2rXp06cPixcvpk2bNlSrVo0GDRowbtw4zp075zhm9erV1KhRg40bN3LTTTdRvXp1+vfvT0rK5TgtFgsTJkygRo0a1KpVi0mTJjmesi5w8eJFHn/8cSIiIggICOC2225j165dju1bt27FMAw2btxI+/btCQwMpFevXqSnp/PRRx9x0003ERISwv33309OTo7b7pGIp5TriNrbb7+dgQMHUqeOe9/5OXfuHA8++CAvv/wyzz33nKPdZrOxdOlSpk2bxuDBgwF47bXXiIyM5M033+TRRx91a1wiIt4iP6O0wmgxLp6DX9PhXMalz+n24uq59CvaMyC3rG+MGRQ/mvYK50opQEuFs5kM9jcq/rG8jJwy5peIiIiIl7PZbJzPs7i0r8VqY8b735X2PBkz399P12a1XZoGIdDXXKZV5cFeKxk7dixffPEFNpuNDRs28OKLL9K4cWOOHj3KuHHjmDRpEsuWLXMck5OTw1/+8hdef/11TCYTDz30EE899RRJSUkALFq0iFWrVvHKK68QGxvLokWLePfdd+nVq5fjHJMmTeLtt9/mtddeo1GjRixYsIB+/frxww8/EBYW5thv5syZ/O1vfyMoKIjExEQSExPx9/fnzTff5Ny5c/z2t7/lr3/9K5MnTy7TdYtUduVaqP3jH//IH//4R+Li4hg8eDC/+c1vaNGiRXl2AcAf/vAH7rzzThISEpwKtUePHiU1NZW+heZe9Pf3p3v37iQnJ5dYqL148SIXL150fJ+VlVXuMcuN62r5ZbNYyPl6N/kZGfiEhxPUqWPleCRdKr3r+dnlE14bzp8todh6RRH2158hr4zvVpv9oFoEVA93/lwtHKoX/hwBad/BmoFXP2f1yLLFINflevIrPCjcHSFJFaG/u8SdlF/iTsovKc35PAux0zeWy7lsQGrWBdrM3OTS/vtn9yPIr2zlnWbNmrFgwQLH9y1btnR8HR0dzZ///GfGjh3rVKjNy8tjxYoVNG3aFIDHHnuM2bNnO7YvXbqUKVOm8Lvf/Q6AFStWsHHj5Xvy66+/snz5clavXs0dd9wBwMsvv8zHH3/MK6+8wtNPP+3Y97nnnqNr164AjBo1iilTpnDkyBGaNGkCwD333MOWLVtUqJUqp1wLtSkpKbz33nu89957PPvsszzzzDO0bNmSwYMHM2jQIDp2vP5VoP/5z3+yZ88ep6HxBVIvrW4eGen8j/nIyEiOHz9e4jnnzp3LrFmzrjs2keKUll9ZmzaRNmcu+ZdyF8AnKorIqVMq32JPUumUlFv51cF6vvi5bayAtZqVoI/ugo9yy9ahT2ChwusVxdYr2wNCwdV39Rt3hZC69oXDih1XYNi3N+pStnjlupSUXxGBEZzmNLZiXisDg8igSDpEdKiIEMVL6e8ucSfll7iT8kuqkk6dOjl9v2XLFubMmcP+/fvJysoiPz+fCxcu8Ouvv1KtWjUAgoKCHEVagDp16pCeng5AZmYmKSkpxMfHO7b7+PjQqVMnx/QHR44cIS8vz1GABfD19aVz584cOHDAKZ7C01xGRkYSFBTkKNIWtO3cufN6b4NIpVOuhdrw8HAeeeQRHnnkEc6dO8cHH3zAe++9x4svvsicOXOoX78+v/3tb/ntb39Lt27dMJnKNkXuyZMneeKJJ9i0aRMBAQEl7nflkH+bzVbqYwBTpkxhwoQJju+zsrJo0KBBmWITKUlJ+ZX1ySdkT51mX+CpkPy0NH56Yjy8sFTFWilVSbmV1B3GbbAXZQv/lLVif4xqdYKJBeRiBvALvmLUa3jxhdfqEeBX3fXia1mYzNB/PqwdRtFpEC7113+eFhKrYCXl15Mdn+TZ3c9iYDgVa41Lr9XkzpO1kJiUSn93iTspv8SdlF9SmkBfM/tn93Np351HTzPi1aKDz660+uE4OkeHXXW/QN+y/+1VUHwFOH78OAMGDGDMmDH8+c9/JiwsjM8//5xRo0aRl5fn2M/X19fpHAWLybuqYF9XajaF+zIMo9i+rVary32LeItyLdQWVr16dYYMGcKQIUPIzc1l8+bNvPvuu/zzn//kxRdfJCwsjIEDBzJ48GD69OlTauG1wO7du0lPT3camWuxWNi2bRt/+9vfOHjwIGAfWVt4ntz09PQio2wL8/f3x9/f/zquVqRkJeVXxqLFBBT3S81mA8Mgbc5cgnv31jQIUqKScuvTlj6cDzIY8bGV2oWmiz0dDKv7mNgZY2JIgz8Sd9tU8AuqwIhLEXs3JK6BDZMh69Tl9pC69iJt7N2ei+0GVVJ+9WzYk8XBi5m3cx5pOZfnDY4MimRy58kkNEqoyDDFC+nvLnEn5Ze4k/JLSmMYhsvTD3RrHk6d0ABSMy+U9DwZUaEBdGse7tIctdfr66+/Jj8/n0WLFjkG1K1du7ZM5wgNDaVOnTp89dVX3H777QDk5+eze/duOnSwP23VrFkz/Pz8+Pzzz3nggQcA+3QKX3/9NePHjy+/CxLxYm4r1Bbm5+fHgAEDGDBgADabjc8//5x3332X//znP6xZs4YZM2Ywffr0q56nd+/efPvtt05tDz/8MC1btmTy5Mk0adKEqKgoPv74Y9q3bw9Abm4un332GfPnz3fLtYlcq/y0NCipCGuzkZ+aSs7Xu6l2S+eKDUyqhJ0xJnY1N7jppI2a5+BMdTjQwMB26Q+9jGphladIWyD2bmh5JxxPti8cVj3SPt2BRmdWOgmNEujZoCd70veQkZNBeFA4HSI6aCStiIiIiAvMJoMZA2MZ+8aekp4nY8bA2Aop0gI0bdqU/Px8/vrXvzJw4EC++OILVqxYUebzPPHEE8ybN4/mzZtz0003sXjxYs6ePevYXq1aNcaOHcvTTz9NWFgYDRs2ZMGCBeTk5DBq1KhyvCIR71UhhdrCDMOgW7dudOvWjcWLF/O///3PaUL20gQHB9O6dWuntmrVqlGrVi1H+/jx45kzZw7NmzenefPmzJkzh6CgIMe7NSLeJD9DK6fLtbOZDPY3Kv6Pu/AGlXS+V5MZort5OgpxgdlkJi4qztNhiIiIiHil/q3rsPyhDsxat5+UzAuO9qjQAGYMjKV/6zqlHF2+2rVrx+LFi5k/fz5Tpkzh9ttvZ+7cuQwbNqxM55k4cSIpKSmMGDECk8nEyJEj+e1vf0tmZqZjn3nz5mG1Whk6dCjZ2dl06tSJjRs3UrNmzfK+LBGvZNjKMqFIJdSjRw/atWvH0qVLAfvcJrNmzeKll17izJkz3HLLLfz9738vUuAtTVZWFqGhoWRmZhISEuKmyKW8ecvrVhDnzmbNqX6VaQ0avvaaRtRWEt6QXwUxdn+5M6d9fsVWzHyyhs1GpH8NNtz3mUY/VhLekFvgPXGKM2943bwhRimeN7x23hCjFOUtr5u3xCnOyut1u3DhAkePHiU6OtqlqRxLYrHa2Hn0NOnZF4gIDqBzdFiFjaQVkYpRlp8Xbh1R+8Ybb7Bs2TJ++OEHfvnllyLbDcMgPz//uvrYunVrkXPOnDmTmTNnXtd5RdzNJzISfvmlyGJiABgGPpGRBHXqWHSbyFU8eetUnt39JwybzalYa1ya/3hyl5kq0oqIiIiIVAJmk0F801qeDkNEKgm3FWpnz57NrFmziIyMpEuXLhrGLnKF8IkTyJ46DQzDuVh7qbAWOXVK5VxIzGrRHKKVnH2xpyVFF3uqFsXkzs9osScRERERERGRSshthdrly5fTo0cPNmzYgK+vr7u6EfFaIb17E/JCNdKen2NfWOwSn8hIIqdOIaRvXw9GV4L978OGyZB16nJbSF3oP9++EJRUGlrsSURERERERMS7uK1Qm52dTWJiooq0IqUIqX+B4IFp5Bz5mfwLZnwCLAQ1BaP+hasfXNH2vw9rh+G8JimQlWJvT1yjYm0l442LPWmOLhEREREREblRua1Q2759e3788Ud3nV7E+32/HtaPwcBGtchC7ecqsPBps4ElD6x59s+Or3PBkm//bM2DvIvwwZMUKdLaTwIYsOEZaHmnpkGQa7ZhX0qRVW/reGDVWxERERERERFPcFuh9rnnnuOee+7hnnvu4eabb3ZXNyLea/N0Si58Av/5A6R9BzZL0cKp09cFH7lgzS/0deFtJRRjrde3mJ9TzFk/2eeuje5WTueUG8mGfSmMfWNPkf8jUjMvMPaNPSx/qIOKtSIiIiIiIlKlua1Q2717d1auXEnnzp2Jj4+nUaNGmK9YGMkwDF555RV3hSBSuWWngn8pj3RfzILP5lVcPAVMPmDyBfOlD5Ovvbh7/perH3su7er7iFzBYrUxa93+0sZrM2vdfvrERmkaBBEREREREamy3Fao/eqrrxg+fDh5eXls27at2H1UqBW5iujuEB4DZj97AdXsd6l4WujrgmKq09d+YPYp9LWL+5l8wGQqGsfR7fDaXVePt3rk1fcRucLOo6edpju4kg1IybzAzqOniW9aq+ICExEREREREalAbivUjh8/Hn9/f9atW0fXrl2pUaOGu7oSqbpuf7pyTCXQqAuE1LUvHFbsuEfDvr1Rl4qOTLxUnsXKnuNn2HIwg/f3/uTSMenZlXCRPREREREREZFy4rZC7f/+9z9mzZrFnXfe6a4uRLxbcBTkpuMVhU+TGfrPty9yhoFzzJceRe8/TwuJSanSsy/w2cEMth7MYNvhDLIvlG2O5IjgADdFJiIiIiIi1yM1NZWhQ4eSnJyMr68vZ8+e9XRIV2UYBu+++y6DBg3ydCgiDm4r1EZERODn5+eu04t4v4TZsH4MXlP4jL0bEtfAhsmQdepye0hde6yxd3suNqmULFYbe0+eZevBdLYcTGffT1lO28Oq+dG9RTi3N6/NvI++Jz37YklvWxAVGkDn6LAKiVtEREREpMJYLfZFmc+l2aeSa9Slcv070EVLliwhJSWFvXv3Ehoa6ulwxA2eeOIJPv/8c/bt28dNN93E3r17PR1SleS2Qu3IkSNJSkriscceK7KImIgALQdAdS8rfMbeDS3vrBJ/SIh7nP41l22HMthyMJ1thzI4k5PntP3m+qH0iImgZ8sI2tQLdSwOFuhnZuwbe0p624IZA2O1kJiIiIiIVC373y/h34PzK+e/B0tx5MgROnbsSPPmzUvcxzAMjh49SuPGjculT4vFgmEYmIpbZ0XKnc1mY+TIkezYsYP//e9/ng6nynJbNnft2hWbzUZ8fDyvvvoqW7ZsYdu2bUU+RG5osXdjefwbdv1mMet7jmfXbxZjeXxv5f6lbDLb581tc4/9s4q0NzSr1ca3P2by4ieH+e2yL+j43MeM/9de/rP3FGdy8ggJ8OGutnVYdO/N7JqWwH8eu40n+7SgXYMaToXX/q3rsPyhDkSFOk9vEBUawPKHOtC/dZ2KvjQREREREffZ/759arnCRVqwrwuydph9uxu89NJL1KtXD6vV6tR+9913M3z4cGbOnEm7du1YtWoVDRs2pHr16owdOxaLxcKCBQuIiooiIiKC559/3nFs48aNefvtt1mzZg2GYTBixIhriu3999+nefPmBAYG0rNnT1577TUMw3BMo7B69Wpq1KjBBx98QGxsLP7+/hw/fpxdu3bRp08fateuTWhoKN27d2fPnj1O5z58+DC33347AQEBxMbG8vHHH5cptuTkZNq1a0dAQACdOnXivffewzAMx6hSi8XCqFGjiI6OJjAwkJiYGF544QWnc4wYMYJBgwYxZ84cIiMjqVGjBrNmzSI/P5+nn36asLAw6tevz6pVqxzHHDt2DMMwWLt2Ld26dSMwMJC4uDgOHTrErl276NSpE9WrV6d///5kZGQ4jnPlnpTViy++yB/+8AeaNGlyXeeR0rltRG2fPn0cX48aNQrDcB4JZbPZMAwDi8XirhBEKr3Nxzczb+c80nLSHG2RP/yDZzo/Q0KjBA9GJlKyzPN5fH74Z7YcTGfrwQx+PnfRaftNdULoGRNOz5YRtG9QAx+za+8J9m9dhz6xUew8epr07AtEBNunO9BIWhERERGp9Gw2yMtxbV+rBT6aRPHrldgAwz7StkkP1wbG+AaB4drfzPfeey+PP/44W7ZsoXfv3gCcOXOGjRs3sm7dOpKTkzly5AgfffQRGzZs4MiRI9xzzz0cPXqUFi1a8Nlnn5GcnMzIkSPp3bs3t956K7t27WLYsGGEhITwwgsvEBgY6Np9KOTYsWPcc889PPHEE/z+97/nv//9L0899VSR/XJycpg7dy4rV66kVq1aREREcPToUYYPH86LL74IwKJFixgwYACHDx8mODgYq9XK4MGDqV27Nl999RVZWVmMHz/e5diys7MZOHAgAwYM4M033+T48eNFjrdardSvX5+1a9dSu3ZtkpOTGT16NHXq1CExMdGx36effkr9+vXZtm0bX3zxBaNGjeLLL7/k9ttvZ8eOHfzrX/9izJgx9OnThwYNGjiOmzFjBkuXLqVhw4aMHDmS+++/33G/g4KCSExMZPr06SxfvtwRc2n3BOCOO+5g+/btpV77uXPnXL5PUj7cVqh99dVX3XVqkSphy4ktPLv7WWxX/HJOz0lnwtYJLO6xWMVaqRRsNhsH07LZ8r19SoPdx89gsV7O22p+Zm5rXpueMRF0jwmnTmjZ/zArYDYZxDetVR5hi4iIiIhUnLwcmFO3nE5ms4+0ndfg6rsCTD0FftVc2jUsLIz+/fvz5ptvOgq1//73vwkLC6N3794kJydjtVpZtWoVwcHBxMbG0rNnTw4ePMj69esxmUzExMQwf/58tm7dyq233kp4eDj+/v4EBgYSFRV1TVe8YsUKYmJiWLhwIQAxMTHs27fPaeQuQF5eHsuWLePmm292tPXq1ctpn5deeomaNWvy2Wefcdddd7F582YOHDjAsWPHqF+/PgBz5szhjjvucCm2pKQkDMPg5ZdfdozI/emnn3jkkUcc+/j6+jJr1izH99HR0SQnJ7N27VqnQm1YWBgvvvii4z4uWLCAnJwcpk6dCsCUKVOYN28eX3zxBUOGDHEc99RTT9GvXz/APlfs/fffzyeffELXrl0B+wDJ1atXu3xPAFauXMn58+ddugdScdxSqM3NzSU6Opo6deqUOj+JyI1sye4lRYq0ADZsGBjM3zmfng16YtbUAuIBv17M54sffmbLwQy2HkwnJfOC0/ZmEdXto2ZjIujUOAw/H80LJSIiIiLiDR588EFGjx7NsmXL8Pf3JykpiSFDhjjWF2rcuLFj1CVAZGQkZrPZaS7YyMhI0tPTS+2nuBGbrVq1cnriumDE5sGDB4mLi3Pat3PnzkXO6efnR9u2bZ3a0tPTmT59Op9++ilpaWlYLBZycnI4ceIEAAcOHKBhw4aOIi1AfHx8qbEXdvDgQdq2bUtAwOVp2oqLbcWKFaxcuZLjx49z/vx5cnNzadeundM+rVq1KnIfW7du7fjebDZTq1atIve28DVHRkYC0KZNG6e2wsdc7Z4A1KtXz9VbIBXILYVas9lM7969WbRokQq1IiVIP5+OObD4IqwNG6k5qexJ30NcVFyx+4hcjcVqc3kaAZvNxpGMX9l6aTqDHUd/Ic9y+Y2EAF8TXZrWpmdMOD1iImgQFlRRlyEiIiIiUvn5BtlHtrrieDIk3XP1/R58y754syt9l8HAgQOxWq18+OGHxMXFsX37dhYvXnz5dL6+TvsbhlFs25Xz3F7pyhGbzZs3Z/369cUWCAumx7yy7UqBgYFF9hsxYgQZGRksXbqURo0a4e/vT3x8PLm5uSWe58pzlMaV2NauXcuTTz7JokWLiI+PJzg4mIULF7Jjxw6n/a713hbepyCWK9sKH3O1ewKa+qCycluhNioqqtj/GUTEdRk5GVffSaQYG/alMGvdfqeRsHVCA5gxMNaxMNf5XAtf/d8vbDmYzpaD6Zw87fzYS6NaQfSMiaBnywhuiQ4jwFeju0VEREREimUYLk8/QNNeEFLXvnBYsfPUGvbtTXu5ZfHmwMBABg8eTFJSEj/88AMtWrSgY8eO5d5PcQXZRo0a0bhx4yLtLVu2ZP369U5tX3/9tUv9bN++nWXLljFgwAAATp48yc8//+zYHhsby4kTJzh16hR169qnp/jyyy9dvQxatmxJUlISFy9exN/fv9jYtm/fTpcuXRg3bpyj7ciRIy73Ud6udk9AUx9UVm6bo/bee+/lrbfe4vHHHy/TOxUicll4ULinQxAv9PH+VJ5673CRP/lSMy8w5o093NepAWnZF/jyyC9czL/8rquf2cQtTcIcxdno2i7+oSkiIiIiIq4zmaH/fFg7DDBwLtZeqp/0n+eWIm2BBx98kIEDB/Ldd9/x0EMPua0fVz366KMsXryYyZMnM2rUKPbu3euYc/VqNaVmzZrx+uuv06lTJ7Kysnj66aedFjRLSEggJiaGYcOGsWjRIrKyspg2bZrLsT3wwANMmzaN0aNH88wzz3DixAn+8pe/OMXWrFkz1qxZw8aNG4mOjub1119n165dREdHl/FOlI+r3RMo+9QHP/zwA+fOnSM1NZXz58+zd+9ewF4I9/PzK6/Qb3hum1Tw97//PefOnaNv37588MEHfP/995w4caLIh8iNKiIwAoPif+EYGEQFRdEhokMFRyVVwbyPvi9x/ViAf319kq0HM7iYb6VejUAevKUhK4d14r/T+/D6qFsYeVu0irQiIiIiIu4UezckroGQOs7tIXXt7bF3u7X7Xr16ERYWxsGDB3nggQfc2pcroqOjeeutt3jnnXdo27Yty5cvdxRTC0axlmTVqlWcOXOG9u3bM3ToUB5//HEiIiIc200mE++++y4XL16kc+fO/P73vy+ySFlpQkJCWLduHXv37qVdu3ZMmzaN6dOnAzjmrR0zZgyDBw/mvvvu45ZbbuGXX35xGl1b0a52T67F73//e9q3b89LL73EoUOHaN++Pe3bt+fUKRen/BCXGDY3zU9gMpkwDKPYuTwKs1gs7uj+umRlZREaGkpmZiYhISGeDkdc5C2vW0Gc7337Hs/ufhbAaVGxguLt4h6LSWiU4JEYpShvyK+CGBuMX4vJv/R5qh7o3IARXaNpHlFdTz14mDfkFnhPnOLMG143b4hRiucNr503xChFecvr5i1xirPyet0uXLjA0aNHiY6OdlpkqsysFvuctefSoHqkfU5aLSgNwPPPP8+KFSs4efKkp0MpIikpiYcffpjMzMwiI1VFrlSWnxdum/pg+vTp+se/SCl6NuzJ4uDFzNs5j7ScNEd7ZFAkkztPVpFW3OqWJrVoERl89R1FRERERMR9TGaI7ubpKCqFZcuWERcXR61atfjiiy9YuHAhjz32mKfDAmDNmjU0adKEevXq8c033zB58mQSExNVpJVy57ZC7cyZM911apEqI6FRAj0b9GRP+h4ycjIIDwqnQ0QHzJX4HVSL1cbOo6dJz75ARHAAnaPDMJv0poy3iQi+jnf9RUREREREytnhw4d57rnnOH36NA0bNmTixIlMmTLF7f3OmTOHOXPmFLutW7dufPTRR6SmpjJ9+nRSU1OpU6cO9957b5mmTxBxldsKtSLiGrPJTFxUnKfDcMmGfSnMWreflMwLjrY6oQHMGBhL/9Z1SjlSKlJkiD8/Xyxx/ViiQu0FdhERERERkcpiyZIlLFmypML7HTNmDImJicVuKxgxO2nSJCZNmlSRYckNyu2FWovFwvfff8+ZM2ewWq1Ftt9+++3uDkFEysGGfSmMfWNPkeJfauYFxr6xh+UPdVCxtpJ45o6WPPXe4ZLWj2XGwFiNghYREREREQHCwsIIC9NAFqkc3FqonT9/PvPmzSMrK6vEfSrjYmIiFckbphKwWG3MWre/2BGaNuwFwFnr9tMnNqrSxX4j6hMbxfLqwUVGP0dp9LOIiIiIiIhIpeW2Qu3KlSuZMmUK3bt3p2/fvkybNo0nn3wSX19fXnnlFZo0acK4cePc1b2IV6gsUwlYrTbO51n4NTefnIuXPuda+PViPudzLXzzY6ZTjFeyASmZF9h59DTxTWtVWNxSsv6t69AnNqrSvwkgIiIiIiIiInZuK9SuWLGCW2+9lS1btvDLL78wbdo07rzzTnr16sUTTzxBu3btNJpWbmgf70/lqfcOl2kqAZvNRq7FSs5FCzl5FnIu5vNrbqHPufn8erHQ57xChdcrCrA5uZZLH/avy0N6dsnFXKl4ZpOhwrmIiIiIiIiIl3BbofbAgQM899xzABiGfQRXfn4+AHXq1GH06NG88MILjBw50l0hiFRq8z76HhvmIu0FhdvH//FfYqJ+cBRUC4qr+dbiJiAoH4YB1fx8CPIzU83fh0BfM9X8zeTmW/nmx8yrHh8RHOC22EREREREREREqjK3FWrNZjPVq1cHoFq1agCcPn3asb1x48YcPnzYXd2LVHppWRcx+QeVuD3XYuPbn0qe39nfx0Q1f3tR1f7hQzX/S5/9zAT5X/p8qfBa+HvHfv5mgnx9CPI3U83PhwBfk+ONlcIsVhu3zf+U1MwLxc5Ta2Cf/7RztCZgFxERERERERG5Fm4r1DZs2JATJ04A4O/vT4MGDdi+fTtDhgwBYNeuXVpVT+QqHr29Cb1aRthHt/rZi6lB/maCfM34mE0VFofZZDBjYCxj39iDAU7F2oKy7oyBsZr/VERERERE5AaUmprK0KFDSU5OxtfXl7Nnz3o6pKsyDIN3332XQYMGeToUEQe3VXpuv/121q1b5/j+3nvv5aWXXmLkyJGMGDGClStXMmDAAHd1L1Il9IiJ4JYmtWhdL5Sm4dWJCg0gJMC3Qou0Bfq3rsPyhzoQFeo8vUFUaECx8+mKiIiIiIhI6SxWC7tSd7H+/9azK3UXFqt3ruWzZMkSUlJS2Lt3L4cOHfJ0OFJJNW7cmKVLl5b5uHfeeYd+/fpRu3ZtDMNg7969Rfa5ePEif/zjH6lduzbVqlXj7rvv5scff3Ta58yZMwwdOpTQ0FBCQ0MZOnRokTcVTpw4wcCBA6lWrRq1a9fm8ccfJzc3t8wxXyu3jah94oknuPnmm7lw4QIBAQHMmjWLgwcP8tprrwHQt29f5s2b567uRSq9yBB/fr6IV00l0L91HfrERrHz6GnSsy8QEWyPUSNpRUREREREymbz8c3M2zmPtJw0R1tkUCTPdH6GhEYJHoys7I4cOULHjh1p3rx5ifsYhsHRo0dp3LhxufRpsVgwDAOTqeIHMknF+vXXX+natSv33nsvjzzySLH7jB8/nnXr1vHPf/6TWrVqMXHiRO666y52796N2WxfH+iBBx7gxx9/ZMOGDQCMHj2aoUOHOgaaWiwW7rzzTsLDw/n888/55ZdfGD58ODabjb/+9a8Vcq3lms1r1qzh2LFjAMTExPDoo48SEGAffVetWjXWrVvH6dOnyczM5KOPPtLUB3JDe+aOlsDlqQMKVPapBMwmg/imtfhNu3rEN61VKWMUERERERGpzDYf38yErROcirQA6TnpTNg6gc3HN7ul35deeol69ephtVqd2u+++26GDx/OzJkzadeuHatWraJhw4ZUr16dsWPHYrFYWLBgAVFRUURERPD88887jm3cuDFvv/02a9aswTAMRowYcU2xvf/++zRv3pzAwEB69uzJa6+9hmEYjhGPq1evpkaNGnzwwQfExsbi7+/P8ePH2bVrF3369KF27dqEhobSvXt39uzZ43Tuw4cPc/vttxMQEEBsbCwff/xxmWJLTk6mXbt2BAQE0KlTJ9577z2nkZ0Wi4VRo0YRHR1NYGAgMTExvPDCC07nGDFiBIMGDWLOnDlERkZSo0YNZs2aRX5+Pk8//TRhYWHUr1+fVatWOY45duwYhmGwdu1aunXrRmBgIHFxcRw6dIhdu3bRqVMnqlevTv/+/cnIyHAc58o9KQtXrq9Hjx6MHz/eqW3QoEGOfOjRowfHjx/nySefxDAMp/Vx3n77bVq1aoW/vz+NGzdm0aJFTucZOnQo06dPJyGh+DcwMjMzeeWVV1i0aBEJCQm0b9+eN954g2+//ZbNm+3/Lx04cIANGzawcuVK4uPjiY+P5+WXX+aDDz7g4MGDAGzatIn9+/fzxhtv0L59exISEli0aBEvv/wyWVklryFUnsq1UPvwww+TnJzs+D4zM5MOHTqwa9cuR1toaKhjkTGRG1mf2ChNJSAiIiIiIlIF2Gw2cvJyXPrIvpjN3J1zsRXzfKXt0n/zds4j+2K2S+ez2Yp7TrN49957Lz///DNbtmxxtJ05c4aNGzfy4IMPAvbRsR999BEbNmzgH//4B6tWreLOO+/kxx9/5LPPPmP+/Pn86U9/4quvvgLsRcH+/fuTmJhISkpKkQKeK44dO8Y999zDoEGD2Lt3L48++ijTpk0rsl9OTg5z585l5cqVfPfdd0RERJCdnc3w4cPZvn07X331Fc2bN2fAgAFkZ2cDYLVaGTx4MGazma+++ooVK1YwefJkl2PLzs5m4MCBtGnThj179vDnP/+5yPFWq5X69euzdu1a9u/fz/Tp05k6dSpr16512u/TTz/l1KlTbNu2jcWLFzNz5kzuuusuatasyY4dOxgzZgxjxozh5MmTTsfNmDGDP/3pT+zZswcfHx/uv/9+Jk2axAsvvMD27ds5cuQI06dPd4q5tHsCcMcdd1C9evVSP8p6faV55513qF+/PrNnzyYlJYWUlBQAdu/eTWJiIkOGDOHbb79l5syZPPvss6xevdrlc+/evZu8vDz69u3raKtbty6tW7d21Cm//PJLQkNDueWWWxz73HrrrYSGhjrt07p1a+rWrevYp1+/fly8eJHdu3e7HM/1KNepD6784ZCfn8/evXudEkFELtNUAiIiIiIiIt7vfP55bnnzlqvv6KK0nDS6/LOLS/vueGAHQb5BLu0bFhZG//79efPNN+nduzcA//73vwkLC6N3794kJydjtVpZtWoVwcHBxMbG0rNnTw4ePMj69esxmUzExMQwf/58tm7dyq233kp4eDj+/v4EBgYSFRV1Tde7YsUKYmJiWLhwIWB/Snvfvn1OI3cB8vLyWLZsGTfffLOjrVevXk77vPTSS9SsWZPPPvuMu+66i82bN3PgwAGOHTtG/fr1AZgzZw533HGHS7ElJSVhGAYvv/yyY0TuTz/95PQIvq+vL7NmzXJ8Hx0dTXJyMmvXriUxMdHRHhYWxosvvui4jwsWLCAnJ4epU6cCMGXKFObNm8cXX3zBkCFDHMc99dRT9OvXD7BPNXr//ffzySef0LVrVwBGjRrlVNi82j0BWLlyJefPn3fpHrh6faUJCwvDbDYTHBzslCeLFy+md+/ePPvsswC0aNGC/fv3s3DhQpdHZ6empuLn50fNmjWd2iMjI0lNTXXsExERUeTYiIgIp30iIyOdttesWRM/Pz/HPu7mtjlqRcQ1BVMJiIiIiIiIiLjbgw8+yOjRo1m2bBn+/v4kJSUxZMgQxzyejRs3Jjg42LF/ZGQkZrPZaS7YyMhI0tPTS+3njjvuYPv27U5trVq1cnrk/dy5cwAcPHiQuLg4p307d+5c5Jx+fn60bdvWqS09PZ3p06fz6aefkpaWhsViIScnhxMnTgD2R94bNmzoKNICxMfHlxp7YQcPHqRt27aOqT1Lim3FihWsXLmS48ePc/78eXJzc2nXrp3TPq1atSpyH1u3bu343mw2U6tWrSL3tvA1FxQS27Rp49RW+Jir3ROAevXquXoLXL6+a3HgwAF+85vfOLV17dqVpUuXYrFYHHl5LWw2m1O+Ff76evZxJxVqRURERERERESuQ6BPIDse2OHSvrvTdjPuk3FX3W9Z72V0jOzoUt9lMXDgQKxWKx9++CFxcXFs376dxYsXO7b7+vo67W8YRrFtV85ze6UrR2w2b96c9evXF1sgLK4QVtyUDoGBgUX2GzFiBBkZGSxdupRGjRrh7+9PfHw8ubm5JZ6nLEU3V2Jbu3YtTz75JIsWLSI+Pp7g4GAWLlzIjh3OOXGt97bwPgWxXNlW+Jir3RMovpB+pYJCuivXZzKZityXvLy8Us8Prr/2pYmKiiI3N5czZ844japNT0+nS5cujn3S0tKKHJuRkeEofkdFRRV5zc6cOUNeXl6RkbbuokKtiIiIiIiIiMh1MAzD5ekHutTtQmRQJOk56cXOU2tgEBkUSZe6XTCbrn00YUkCAwMZPHgwSUlJ/PDDD7Ro0YKOHa9eEC6r4gqyjRo1onHjxkXaW7Zsyfr1653avv76a5f62b59O8uWLWPAgAEAnDx5kp9//tmxPTY2lhMnTnDq1CnH3KNffvmlq5dBy5YtSUpK4uLFi/j7+xcb2/bt2+nSpQvjxl0uwB85csTlPsrb1e4JlG3qA1euLzw83DHvLNgXINu3bx89e/Z0tPn5+WGxWJyOi42N5fPPP3dqS05OpkWLFi6Ppu3YsSO+vr58/PHHjqkYUlJS2LdvHwsWLADso6gzMzPZuXOnY0T0jh07yMzMdBRz4+Pjef7550lJSaFOHfu6QZs2bcLf398t/48Up9wLta+++qrjBl+4cAHDMHjhhRd46623iuxrGAZ///vfyzsEEREREREREZFKyWwy80znZ5iwdQIGhlOx1sA+snBy58luKdIWePDBBxk4cCDfffcdDz30kNv6cdWjjz7K4sWLmTx5MqNGjWLv3r2OOVevNvq1WbNmvP7663Tq1ImsrCyefvppAgMvjzJOSEggJiaGYcOGsWjRIrKysopdqKwkDzzwANOmTWP06NE888wznDhxgr/85S9OsTVr1ow1a9awceNGoqOjef3119m1axfR0dFlvBPl42r3BMo29YEr19erVy8mTJjAhx9+SNOmTVmyZAlnz551Ok/jxo3Ztm0bQ4YMwd/fn9q1azNx4kTi4uL485//zH333ceXX37J3/72N5YtW+Y47vTp045iO9inowD7CNioqChCQ0MZNWoUEydOpFatWoSFhfHUU0/Rpk0bEhISALjpppvo378/jzzyCC+99BIAo0eP5q677iImJgaAvn37Ehsby9ChQ1m4cCGnT5/mqaee4pFHHiEkJMTl+3U9TFffpWw++eQTVqxYwYoVK1i9ejU2m41169Y52q78EBERERERERG5kSQ0SmBxj8VEBDkvbhQZFMniHotJaJTg1v579epFWFgYBw8e5IEHHnBrX66Ijo7mrbfe4p133qFt27YsX77cUUwtGMVaklWrVnHmzBnat2/P0KFDefzxx50WjTKZTLz77rtcvHiRzp078/vf/77IImWlCQkJYd26dezdu5d27doxbdo0pk+fDuCYt3bMmDEMHjyY++67j1tuuYVffvnFafRpRbvaPSkrV65v5MiRDB8+nGHDhtG9e3eio6OdRtMCzJ49m2PHjtG0aVPCw8MB6NChA2vXruWf//wnrVu3Zvr06cyePdtpIbH333+f9u3bc+eddwIwZMgQ2rdv71RXXLJkCYMGDSIxMZGuXbsSFBTEunXrnEblJiUl0aZNG/r27Uvfvn1p27Ytr7/+umO72Wzmww8/JCAggK5du5KYmMigQYMchfmKYNjKOvFDKY4fP17mYxo1alRe3ZebrKwsQkNDyczMrLCKuVw/b3ndvCVOceYNr5s3xChFecvr5i1xijNveN28IUYpnje8dt4QoxTlLa+bt8Qpzsrrdbtw4QJHjx4lOjraaZGpsrJYLexJ30NGTgbhQeF0iOjg1pG03uT5559nxYoVnDx50tOhFJGUlMTDDz9MZmZmkZGqIlcqy8+Lcp36oDIWXUVEREREREREKiOzyUxcVJynw6gUli1bRlxcHLVq1eKLL75g4cKFPPbYY54OC4A1a9bQpEkT6tWrxzfffMPkyZNJTExUkVbKnRYTExERERERERERjzp8+DDPPfccp0+fpmHDhkycOJEpU6a4vd85c+YwZ86cYrd169aNjz76iNTUVKZPn05qaip16tTh3nvvLdP0CSKucnuh9uuvv2bHjh2cOXMGq9XqtM0wDJ599ll3hyAiIiIiIiIiIpXYkiVLWLJkSYX3O2bMGBITE4vdVjBidtKkSUyaNKkiw5IblNsKtefPn2fw4MFs2rQJm82GYRgUTIdb8LUKtSIiIiIiIiIi4ilhYWGEhYV5OgwRAEzuOvHs2bPZtGkT06ZNY8uWLdhsNl577TU++ugjunXrRlxcHPv373dX9yIiIiIiIiIiIiJew22F2rfeeot7772X2bNn07p1awDq1atHv3792Lx5M7m5uaxevdpd3Zcri9XCrtRdrP+/9exK3YXFavF0SCIiIiIiIiIiIlKFuG3qg5MnTzJhwgQAzGYzALm5ufZOfXy4//77Wb58OXPnznVXCOVi8/HNzNs5j7ScNEdbZFAkz3R+hoRGCR6MTERERERERERERKoKt42oDQ4OxmKxOL42mUycOnXKsT00NJTU1FR3dV8utpzYwoStE5yKtADpOelM2DqBzcc3eygyERERERERERERqUrcVqht2rQpP/zwA2AfUduqVSveeustAGw2G++88w4NGjRwV/flYsnuJdiwYVhtxB630vU7K7HHrWC1AjB/5/zKOQ2C1QJHt8O3b9k/V8YYRURERERERERExMFtUx8kJCTw6quvsnjxYkwmE48++iiPPfYYTZs2xTAMjh49ypw5c9zVfblIP59O/AmDER9bqZ19uf3nYFjdx8bOmFT2pO8hLirOc0Feaf/7sGEyZF0evUxIXeg/H2Lv9lxcIiIiIiIiIlIlpaamMnToUJKTk/H19eXs2bOeDumqDMPg3XffZdCgQZ4ORcTBbSNqn3nmGT799FOsl0afjhs3joULFxIaGkrNmjWZM2cOkyZNclf35aLjYSsT37FSK9u5PSwbJr5jpfNBKxk5GZ4Jrjj734e1w5yLtABZKfb2/e97Ji4RERERERERKcJmsfDrjp1kfvAhv+7Yic3inU/ELlmyhJSUFPbu3cuhQ4c8HY5UUo0bN2bp0qVlPu6dd96hX79+1K5dG8Mw2Lt3b5F9Ll68yB//+Edq165NtWrVuPvuu/nxxx+d9jlz5gxDhw4lNDSU0NBQhg4dWuRNhRMnTjBw4ECqVatG7dq1efzxxx1rbhX49ttv6d69O4GBgdSrV4/Zs2djs9nKfF3FcduI2urVqxMTE+PUNnHiRCZOnOiuLsvdg1usgBnjinYTYAVGfGwl4A+1Kj6w4lgt9pG0FJcYNsCADc9AyzvBZK7g4ERERERERESksKxNm0ibM5f8Quv3+ERFETl1CiF9+3owsrI7cuQIHTt2pHnz5iXuU/B0dePGjculT4vFgmEYmExuG4MolcSvv/5K165duffee3nkkUeK3Wf8+PGsW7eOf/7zn9SqVYuJEydy1113sXv3bsxmex3sgQce4Mcff2TDhg0AjB49mqFDh7Ju3TrAnlN33nkn4eHhfP755/zyyy8MHz4cm83GX//6VwCysrLo06cPPXv2ZNeuXRw6dIgRI0ZQrVq1cql5elU2z507l7i4OIKDg4mIiGDQoEEcPHjQaR+bzcbMmTOpW7cugYGB9OjRg+++++6a+quVTZEibQETUDsbWp60XtO5i2WzQd55yDkNmT/BL0cgdR/8+LV9rtnDH8P+/8A3/4Ldq+GrFfD5EtgyF955pOhIWueTQ9ZPcDy5/OIVERERERERkTLL2rSJn54Y71SkBchPS+OnJ8aTtWmTW/p96aWXqFevnuPp5wJ33303w4cPZ+bMmbRr145Vq1bRsGFDqlevztixY7FYLCxYsICoqCgiIiJ4/vnnHcc2btyYt99+mzVr1mAYBiNGjLim2N5//32aN29OYGAgPXv25LXXXsMwDMeIx9WrV1OjRg0++OADYmNj8ff35/jx4+zatYs+ffpQu3ZtQkND6d69O3v27HE69+HDh7n99tsJCAggNjaWjz/+uEyxJScn065dOwICAujUqRPvvfee08hOi8XCqFGjiI6OJjAwkJiYGF544QWnc4wYMYJBgwYxZ84cIiMjqVGjBrNmzSI/P5+nn36asLAw6tevz6pVqxzHHDt2DMMwWLt2Ld26dSMwMJC4uDgOHTrErl276NSpE9WrV6d///5kZFx+4tuVe1IWrlxfjx49GD9+vFPboEGDHPnQo0cPjh8/zpNPPolhGBjG5Yrb22+/TatWrfD396dx48YsWrTI6TxDhw5l+vTpJCQkFBtfZmYmr7zyCosWLSIhIYH27dvzxhtv8O2337J582YADhw4wIYNG1i5ciXx8fHEx8fz8ssv88EHHzhqi5s2bWL//v288cYbtG/fnoSEBBYtWsTLL79MVlYWAElJSVy4cIHVq1fTunVrBg8ezNSpU1m8eHG5jKp124jaGTNm8Pbbb7Nv375it7dp04b77ruPP/3pTy6f87PPPuMPf/gDcXFx5OfnM23aNPr27cv+/fupVq0aAAsWLGDx4sWsXr2aFi1a8Nxzz9GnTx8OHjxIcHBwuVxbYda9GyHopL3Amn8B8nIg7wLkn3f+nJdzafuV285fPjb/QrnHV8S5NPf3ISIiIiIiInIDsdls2M6fd21fi4W05563D9YqeiIwIO35OVSLj8cwX/2JWCMw0KnoVZp7772Xxx9/nC1bttC7d2/A/jj4xo0bWbduHcnJyRw5coSPPvqIDRs2cOTIEe655x6OHj1KixYt+Oyzz0hOTmbkyJH07t2bW2+9lV27djFs2DBCQkJ44YUXCAwMdCmWwo4dO8Y999zDE088we9//3v++9//8tRTTxXZLycnh7lz57Jy5Upq1apFREQER48eZfjw4bz44osALFq0iAEDBnD48GGCg4OxWq0MHjyY2rVr89VXX5GVlVWkoFia7OxsBg4cyIABA3jzzTc5fvx4keOtViv169dn7dq11K5dm+TkZEaPHk2dOnVITEx07Pfpp59Sv359tm3bxhdffMGoUaP48ssvuf3229mxYwf/+te/GDNmDH369KFBgwaO42bMmMHSpUtp2LAhI0eO5P7773fc76CgIBITE5k+fTrLly93xFzaPQG444472L59e6nXfu7cuTJdX2neeecdbr75ZkaPHu00Knb37t0kJiYyc+ZM7rvvPpKTkxk3bhy1atVyuei/e/du8vLy6FtoJHrdunVp3bo1ycnJ9OvXjy+//JLQ0FBuueUWxz633noroaGhJCcnExMTw5dffknr1q2pW7euY59+/fpx8eJFdu/eTc+ePfnyyy/p3r07/v7+TvtMmTKFY8eOER0d7VLMJXFbofbdd9+lT58+JW7v27cvb731VpkKtQVDkwu8+uqrREREsHv3bm6//XZsNhtLly5l2rRpDB48GIDXXnuNyMhI3nzzTR599NFru5hS+Hzzd0jNvfqOZWXyAZ9A8A249Lnw14U++waBTwDk/AL737v6eatHln+sIiIiIiIiIjcw2/nzHOzQsZxOZh9Zeyius0u7x+zZjREU5NK+YWFh9O/fnzfffNNRqP33v/9NWFgYvXv3Jjk5GavVyqpVqwgODiY2NpaePXty8OBB1q9fj8lkIiYmhvnz57N161ZuvfVWwsPD8ff3JzAwkKioqGu65BUrVhATE8PChQvt1xQTw759+5xG7gLk5eWxbNkybr75Zkdbr169nPZ56aWXqFmzJp999hl33XUXmzdv5sCBAxw7doz69esDMGfOHO644w6XYktKSsIwDF5++WXHiNyffvrJqdjo6+vLrFmzHN9HR0eTnJzM2rVrnQqZYWFhvPjii477uGDBAnJycpg6dSoAU6ZMYd68eXzxxRcMGTLEcdxTTz1Fv379AHjiiSe4//77+eSTT+jatSsAo0aNYvXq1S7fE4CVK1dy3sU3F1y9vtKEhYVhNpsJDg52ypPFixfTu3dvnn32WQBatGjB/v37WbhwocuF2tTUVPz8/KhZs6ZTe2RkJKmXRq2npqYSERFR5NiIiAinfSIjnetmNWvWxM/Pz2mfK6fvKDgmNTW18hZqjx49SsuWLUvcHhMTw8qVK6+rj8zMTMD+Yhf0mZqa6lRB9/f3p3v37iQnJ5dYqL148SIXL150fF8wnNkn0AK5JoqfAMGGT5CFoJj6ENawaAHVleKqb+AVn4Mu72cu40tjtcDSnfaFw4qdp9aAkLrQqEvZzivXraT8Erleyi1xJ+WXuItyS9xJ+SXupPySquLBBx9k9OjRLFu2DH9/f5KSkhgyZIhjHs/GjRs7PZEcGRmJ2Wx2mgs2MjKS9PT0UvspbsRmq1atnEb/FozYPHjwIHFxcU77du5ctFDt5+dH27ZtndrS09OZPn06n376KWlpaVgsFnJycjhx4gRgf+S9YcOGjiItQHx8fKmxF3bw4EHatm1LQEBAqbGtWLGClStXcvz4cc6fP09ubi7t2rVz2qdVq1ZF7mPr1q0d35vNZmrVqlXk3ha+5oKiYJs2bZzaCh9ztXsCUK9ePVdvgcvXdy0OHDjAb37zG6e2rl27snTpUiwWiyMvr4XNZnPKt+JGnpfHPgVTHrg6sr00bivUAkVWTivszJkzWK5jNUObzcaECRO47bbbHEldUN2+svodGRnJ8ePHSzzX3Llznd4ZKBB+cxbZu2rhWIzrcu/287bPwvjNKojuds3XUW5MZug/H9YOwx5r4WLtpdj7z9NCYh5QUn6JXC/llriT8kvcRbkl7qT8EndSfklpjMBAYvbsdmnfnK+/5uToqz/x2+D/vURQp04u9V0WAwcOxGq18uGHHxIXF8f27dtZvHixY7uvr6/z+Q2j2LYr57m90pUjNps3b8769euLLRBeWQgraLtSYDHTPIwYMYKMjAyWLl1Ko0aN8Pf3Jz4+ntzc3BLPU5aCmiuxrV27lieffJJFixYRHx9PcHAwCxcuZMeOHU77Xeu9LbxPQSxXthU+5mr3BMo29YEr12cymYrcl7y8vFLPD66/9qWJiooiNzeXM2fOOI2qTU9Pp0uXLo590tKKTgeakZHhqCNGRUUVec3OnDlDXl6e0z6pV8wtXVAkv7IeeS3ctphYq1atHKumXclms/H++++XOuL2ah577DH+97//8Y9//KPItuJe4NL+J5wyZQqZmZmOj5MnTwIQclNN6nU9i0+g8/8gPkEW6nU9S0irWpVrhGrs3ZC4BkLqOLeH1LW3x97tmbhucCXll8j1Um6JOym/xF2UW+JOyi9xJ+WXlMYwDExBQS59VOvaFZ+oKCipTmEY+ERFUa1rV5fOV9ZRfIGBgQwePJikpCT+8Y9/0KJFCzp2LKdpGwqpV68ezZo1c3wANGrUqEgbQMuWLdm1a5fT8V9//bVL/Wzfvp3HH3+cAQMGOBak+vnnnx3bY2NjOXHiBKdOXV6A/csvv3T5Olq2bMn//vc/pxH1V8a2fft2unTpwrhx42jfvj3NmjXjyJEjLvdR3q52T8BeSN+7d2+pH4XPd7XrCw8PJyUlxfG9xWIpsm6Vn59fkUGbsbGxfP75505tycnJtGjRwuXRtB07dsTX19dpkbiUlBT27dvnKNTGx8eTmZnJzp07Hfvs2LGDzMxMp3327dvndB2bNm3C39/f8f9IfHw827Ztcyp6b9q0ibp16xaZEuFauG1E7ahRo3j00Ud5+OGHmTdvnqOqnJaWxuTJk/nqq6/429/+dk3n/uMf/8j777/Ptm3bnIauF8xxkZqaSp06l4uV6enppVa1/f39nSYBdkiYTUjuGILrXSAnw5f8C2Z8AiwEhedhmKicI1Rj74aWd8LxZPvCYdUj7cXkyhbnDaTE/BK5TsotcSfll7iLckvcSfkl7qT8kvJimM1ETp3CT0+MtxdrC48evFR0jZw6xaWFxK7Vgw8+yMCBA/nuu+946KGH3NaPqx599FEWL17M5MmTGTVqFHv37nXMuXq1QnSzZs14/fXX6dSpE1lZWTz99NNOC5olJCQQExPDsGHDWLRoEVlZWUybNs3l2B544AGmTZvG6NGjeeaZZzhx4gR/+ctfnGJr1qwZa9asYePGjURHR/P666+za9eu656v9Fpd7Z5A2aY+cOX6evXqxYQJE/jwww9p2rQpS5YsKfKkfePGjdm2bRtDhgzB39+f2rVrM3HiROLi4vjzn//Mfffdx5dffsnf/vY3li1b5jju9OnTTsX2gwcPAvY6YFRUFKGhoYwaNYqJEydSq1YtwsLCeOqpp2jTpg0JCQkA3HTTTfTv359HHnmEl156CYDRo0dz1113ERMTA9jX04qNjWXo0KEsXLiQ06dP89RTT/HII48QEhIC2PNh1qxZjBgxgqlTp3L48GHmzJnD9OnTy2XqA7eNqH3kkUd44IEHeO2116hbty516tShTp061K1blzVr1pCYmMjYsWPLdE6bzcZjjz3GO++8w6efflok4aOjo4mKinKqoOfm5vLZZ585quNl0nIAJK7BqFGHapG5hDY6T7XIXIwalXyEqslsn46hzT32zyrSioiIiIiIiFQaIX37Uu+FpfhcMajMJzKSei8sJaTQ2jvu0KtXL8LCwjh48CAPPPCAW/tyRXR0NG+99RbvvPMObdu2Zfny5Y5i6tXeIFm1ahVnzpyhffv2DB06lMcff9xp0SiTycS7777LxYsX6dy5M7///e+LLFJWmpCQENatW8fevXtp164d06ZNY/r06QCOeWvHjBnD4MGDue+++7jlllv45ZdfGDduXFlvQ7m52j0pK1eub+TIkQwfPpxhw4bRvXt3oqOj6dmzp9M+s2fP5tixYzRt2pTw8HAAOnTowNq1a/nnP/9J69atmT59OrNnz3ZaSOz999+nffv23HnnnQAMGTKE9u3bs2LFCsc+S5YsYdCgQSQmJtK1a1eCgoJYt26d06jcpKQk2rRpQ9++fenbty9t27bl9ddfd2w3m818+OGHBAQE0LVrVxITExk0aJCjMA8QGhrKxx9/zI8//kinTp0YN24cEyZMYMKECdd8fwszbGWd+KGM1q5dS1JSEj/88AM2m42YmBgefPBB7rnnnjKfa9y4cbz55pv85z//cVS7wX6TCt4ZmD9/PnPnzuXVV1+lefPmzJkzh61bt3Lw4EGnybBLk5WVRWhoKJmZmfaKudWiEapeoMjrVkl5S5zizBteN2+IUYryltfNW+IUZ97wunlDjFI8b3jtvCFGKcpbXjdviVOcldfrduHCBY4ePUp0dLTTIlNlZbNYyPl6N/kZGfiEhxPUqaNbR9J6k+eff54VK1ZUymlGkpKSePjhh8nMzCwyUlXkSmX5eeHWxcQAEhMTSUxMLJdzLV++HIAePXo4tb/66quOSvukSZM4f/4848aN48yZM9xyyy1s2rTJ5SJtsQpGqIqIiIiIiIiIlBPDbKbaLZ09HUalsGzZMuLi4qhVqxZffPEFCxcu5LHHHvN0WACsWbOGJk2aUK9ePb755hsmT55MYmKiirRS7txeqC1Prgz+NQyDmTNnMnPmTPcHJCIiIiIiIiIi1+3w4cM899xznD59moYNGzJx4kSmTJni9n7nzJnDnDlzit3WrVs3PvroI1JTU5k+fbpjTaR77723TNMniLjK7YXar7/+mh07dnDmzBmsVqvTNsMwePbZZ90dgoiIiIiIiIiIVGJLlixhyZIlFd7vmDFjSnwSvGDE7KRJk5g0aVJFhiU3KLcVas+fP8/gwYPZtGkTNpsNwzAcI2ILvlahVkREREREREREPCUsLIywsDBPhyECgMldJ549ezabNm1i2rRpbNmyBZvNxmuvvcZHH31Et27diIuLY//+/e7qXkRERERERERERMRruK1Q+9Zbb3Hvvfcye/ZsWrduDUC9evXo168fmzdvJjc3l9WrV7urexERERERERERt7pyikcRkSuV5eeE26Y+OHnyJBMmTADAbDYDkJuba+/Ux4f777+f5cuXM3fuXHeFICIiIiIiIiJS7vz8/DCZTJw6dYrw8HD8/PwwDMPTYYlIJWKz2cjNzSUjIwOTyYSfn99Vj3FboTY4OBiLxeL4uuAHWIHQ0FBSU1Pd1b2IiIiIiIiIiFuYTCaio6NJSUlxqnWIiFwpKCiIhg0bYjJdfWIDtxVqmzZtyg8//ADYR9S2atWKt956i5EjR2Kz2XjnnXdo0KCBu7oXEREREREREXEbPz8/GjZsSH5+vmOgmohIYWazGR8fH5dH3LutUJuQkMCrr77K4sWLMZlMPProozz22GM0bdoUwzA4evQoc+bMcVf3IiIiIiIiIiJuZRgGvr6++Pr6ejoUEakC3FaofeaZZxg6dChWqxWTycS4ceM4f/48SUlJmM1mHnnkESZNmuSu7kVERERERERERES8htsKtdWrVycmJsapbeLEiUycONFdXYqIiIiIiIiIiIh4pavPYisiIiIiIiIiIiIibuXWQu358+eZO3cunTt3platWtSuXZvOnTszb948zp8/786uRURERERERERERLyG26Y+SE9Pp2fPnhw4cICQkBCaNGmCzWbj0KFDTJ06lTfeeIMtW7YQHh7urhBEREREREREREREvILbRtQ+/fTTfP/99yxevJj09HT27NnDf//7X9LT01m0aBEHDhzg6aefdlf3IiIiIiIiIiIiIl7DbSNqP/jgA0aNGsX48eOd2v38/HjyySf57rvvePfdd93VvYiIiIiIiIiIiIjXcNuI2tzcXDp06FDi9k6dOpGbm+uu7kVERERERERERES8htsKtXFxcezZs6fE7bt376Zz587u6l5ERERERERERETEa7ht6oNFixbRu3dv2rRpw5gxY/D19QUgPz+fv//977zzzjt88skn7upeRERERERERERExGu4rVA7ceJEatWqxfjx45k+fTpNmjTBMAyOHDlCVlYWTZs2ZcKECU7HGIah4q2IiIiIiIiIiIjccNxWqP2///s/DMOgYcOGAJw+fRqAGjVqUKNGDfLy8jh69Ki7uhcRERERERERERHxGm4r1B47dsxdpxYRERERERERERGpUty2mJiIiIiIiIiIiIiIuEaFWhEREREREREREREPK7epD3r16lXmY7R4mIiIiIiIiIiIiEg5FmoLFg8Tz7NYbew8epr07AtEBAfQOToMs0mvjYiIiIiIiIiISGVVboXaa1k87Ndffy2v7uWSDftSmLVuPymZFxxtdUIDmDEwlv6t63gwMhERERERERERESmJR+aoTU5OZtSoUdStW9cT3VdZG/alMPaNPU5FWoDUzAuMfWMPG/aleCgyERERERERERERKU25jai9mvT0dF577TVWrVrFoUOHsNlstG3btqK6r/IsVhuz1u3HVsw2G2AAs9btp09slKZBEBERERERERERqWTcWqi1Wq2sX7+eV155hfXr15Ofn0/r1q2Z7oCUhQAAfnZJREFUO3cuv/vd72jatKk7u6+y8i1WUrMu8OOZ85c+cthz4kyRkbSF2YCUzAvsPHqa+Ka1Ki5YERERERERERERuSq3FGoPHz7MqlWrWLNmDSkpKdSpU4f777+f119/nRkzZjB48GB3dOs2Fb04V77FSkrmBUcR9qezlwuyP545T0rmBSzW4sbOXl16dsnFXBEREREREREREfGMci3UrlmzhldeeYXt27fj7+/P3XffzYgRI+jXrx9Hjx5lzZo15dldhXDH4lxXFmILj4z98cx5UrOuXoj1M5uoVzOQ+pc+rDb4166TV+07IjjgmmIWERERERERERER9ynXQu2IESNo1qwZy5cvZ8iQIYSGhpbn6Svcx/tTeeq9w0XmfS1YnGv5Qx2KLdbmWaykZl7gZKEi7E/XWYitXzPI6evw6v6YCo3qtVhtbDuUQWrmhWLnqTWAqFD7aGARERERERERERGpXMq1UBsQEMCRI0f417/+RVBQEL/73e8ICgoqzy4q1LyPvseGuUh7QSF02rv7OHs+j1NnLziKsD+dOU9K5nmuNjOBn4+J+jUCLxVjCxdhiy/EXo3ZZDBjYCxj39iDUShGsBdpAWYMjNVCYiIiIiIiIiIiIpVQuRZqU1NTSUpKYtWqVQwfPpw//OEP3HPPPYwYMYK6deuWZ1cVIi3rIib/kgvNv/yayzNvf1vstpILsUE0qBlI7TIWYl3Rv3Udlj/UochUDVHXOVWDiIiIiIiIiIiIuFe5FmpDQkIYO3YsY8eO5X//+x8rV67kzTff5LXXXiM8PBzDMDhz5kx5dulxLaOC6dCoZpHpCWpXK/9CrCv6t65Dn9ioCl38TERERERERERERK6PyV0nbtu2LS+++CKnTp0iKSmJNm3aADB69GjatGnD7Nmz+e6779zVfYWZMbAVc37bhnE9mnH3zXXp0LAmEcEBHinSFjCbDOKb1uI37eoR37SWirQiIiIiIiIiIiKVnNsKtQX8/PwYMmQIH3/8Mf/3f//Hn/70J7Kzs5k5cyY333yzu7u/LpEh/pRU4jSAOlqcS0RERERERERERMqB2wu1hTVq1IhZs2Zx9OhRPvroI373u99VZPdl9swdLQGKFGu1OJeIiIiIiIiIiIiUpwot1BYwDIN+/frxr3/9yxPdu6xPbBTLH+pAVGiAU3tUaADLH+qgxblERERERERERESkXJTrYmJVkRbnEhEREREREREREXdTodYFBYtziYiIiIiIiIiIiLiDCrXFsNlsAGRlZXk4EimLgter4PWrrJRf3skb8ku55Z28IbdA+eWtvCG/lFveS/kl7uINuQXKL2/lLfklIjcmFWqLkZ2dDUCDBg08HIlci+zsbEJDQz0dRomUX96tMueXcsu7VebcAuWXt6vM+aXc8n7KL3GXypxboPzydpU9v0TkxmTY9DZSEVarlVOnThEcHIxh2OeizcrKokGDBpw8eZKQkBAPR3h13hZvWRV3fTabjezsbOrWrYvJ5JF18lyi/Kr8vDW/qkJugXfG7CpvzS2oGvnlbfGWlbfmV1XILfDOmMtC+eVZ3hizq7w1t6Bq5Je3xVtW3pxfInJj0ojaYphMJurXr1/stpCQEK/6BeZt8ZbVldfnDe+IKr+8h7flV1XKLfDOmF3lbbkFVSu/vC3esvK2/KpKuQXeGXNZKL88yxtjdpW35RZUrfzytnjLyhvzS0RuTHr7SERERERERERERMTDVKgVERERERERERER8TAVal3k7+/PjBkz8Pf393QoLvG2eMuqql2ft12Pt8VbVlXp+rzxWrwxZldVtWvztuvxtnjLqipdnzdeizfGXBZV6fq88Vq8MWZXVbVr87br8bZ4y6qqX5+IVD1aTExERERERERERETEwzSiVkRERERERERERMTDVKgVERERERERERER8TAVakVEREREREREREQ8TIVaEREREREREREREQ9ToVZERERERERERETEw1SoFREREREREREREfEwFWpFREREREREREREPEyFWhEREREREREREREPU6FWRERERERERERExMNUqBURERERERERERHxMBVqRURERERERERERDxMhVoRERERERERERERD/PxdACVkdVq5dSpUwQHB2MYhqfDERfZbDays7OpW7cuJlPlfQ9C+eWdvCG/lFveyRtyC5Rf3sob8ku55b2UX+Iu3pBboPzyVt6SXyJyY1KhthinTp2iQYMGng5DrtHJkyepX7++p8MokfLLu1Xm/FJuebfKnFug/PJ2lTm/lFveT/kl7lKZcwuUX96usueXiNyYVKgtRnBwMGD/wR0SEuLhaMRVWVlZNGjQwPH6VVbKL+/kDfml3PJO3pBboPzyVt6QX8ot76X8EnfxhtwC5Ze38pb8EpEbkwq1xSh4bCUkJMT+C9dqgePJcC4NqkdCoy5gMns4ylJ4W7zlrLI/dlQkv8SrVOb8Um55t8qcW6D88naVOb+UW95P+SXuUplzC5Rf3q6y55eI3JhUqL2a/e/DhsmQdepyW0hd6D8fYu/2XFwl8bZ4RUREREREREREBM2cXZrv18PaYc5FT4CsFHv7/vc9E1dJ9r/vXfGKiIiIiIiIiIgIoBG1pds8HbAVs8EGGLDhGWh5Z/lOK2Cz2acusFnBZrnia+sV7Ze2WS1gyYMPJ1Z8vCIiIiIiIiIiInLdVKgtTXYq+Jc0b40Nsn6Cv3cG3yB7gbVIYdVyRXuhwqrNWujrQu3FFlrLw6V4jydDdDc39SEiIiIiInKDusHXChERkeunQu31+uWHiu/TMF36MNt/8Rsm+x8F+eevfuy5NPfHJyIiIiIiciPRWiEiIlIOVKi9Xr1nQFRbMIxLRdNLhVOnr02utxcUXh1tpqLtxa1OeXQ7vHbX1eOtHln+90BERERERORG9f16WD+GIk9HFqwVkrhGxVoREXGJCrWlCY6C3HSKn47AsL9D2vWJyvE4S6Mu9niyUig13kZdKjoyERERERGRqssTa5uIiEiVZPJ0AJVawuxLX1w5gvXS9/3nVZ5ftiaz/bEawCviFRERERERqQqyU0vZWGitEBERkau4rhG1OTk5HDt2jF9++QWbreg7iLfffvv1nN7zWg6A6mtKmGtoXuV7fCX2bvtjNd4Sr9hp0QERERERkapNa4WIiIgLrqlQm5OTw4QJE3j11VfJz88vst1ms2EYBhaL5boD9LjYu+2PqXhLIc3b4r3RadEBEREREZGqT2uFiIiIC66pUPvEE0/wyiuvMGDAAHr16kWtWrXKO67KxWSG6G6ejsJ13hbvjUqLDoiIF7NZLOR8vZv8jAx8wsMJ6tQRw6w3BUVE5AbkytomWitERERccE2F2vfee4/777+fpKSk8o5H5MahRQdExEtlbdpE2py55KdenpPPJyqKyKlTCOnb14ORiYiIeEDC7EsDMAyc/77XWiEiIlI217SYWE5ODj169CjnUERuMFp0QES8UNYnn/DTE+OdirQA+Wlp/PTEeLI2bfJQZCIiIh7ScoD9abiQOs7tIXX1lJyIiJTJNY2o7dixIz/88EN5xyIiV9KiAyJSyWQsWkxAMQuIYrOBYZA2Zy7BvXtrGgQREbmxaK0QEREpB9c0onbu3LmsWrWKXbt2lXc8IlKYFh0QkUomP62UN5BsNvJTU8n5enfFBSQiIlJZFKwV0uYe+2cVaUVEpIyuaUTtK6+8Qv369YmPjyc+Pp4mTZpgvmLkjGEYvPLKK+USpEiVVOqiA0BQLS06ICJeKT8jw9MhiIiIiIiIeJ1rKtSuXr3a8fUXX3zBF198UWQfFWpFrqLERQcuyfkFNk6DPrPAx7+ioxMRuWY+4eGeDkFERERERMTrXNPUB1ar9aofFoulvGMVqVpKW3SgRT/71zuWw8oE+FlzQotI5ZBfKxRrCdusQH54DYI6dazIkERERCoFi9XCrtRdrP+/9exK3YXFqn8Ti4hI2VzTiFoRKSelLTpwcAO8NxZS/wcv3Q53LoJ293s6YhG5wSX1NDHuXXtRtvC7vVbszwes7m1mgQGalU9ERG4km49vZt7OeaTlXJ7LPTIokmc6P0NCowQPRiYiIt7kmkbUFvj111/ZvHkzSUlJpJW2uIiIlKykRQdi+sPYL6BxN8j7Fd4bA++MhovZno1XRG5onzbKZtFgE6eDndtPB8OiwSY2RWeyJ32PZ4ITERHxgC0ntjBh6wSnIi1Aek46E7ZOYPPxzR6KTEREvM01j6hdvnw5U6ZMISsrC8Mw+Pjjj4mMjOT/t3ff4VGVaR/Hv5NeSAKBVGpEiiF0AgZW6U0BETECiqBIXQsCgspKU6lSLAv4CixVXRbEFUWaIqKhR1YRRMTQJCEIIQmGtMl5/xgzMiSESUjn97muXJhznjlznzO3k+Se59zPhQsXqF69Om+99RbDhg0rzFhFbj/ewfD4f2HXPPhqOnz/bzi7H/oug+CmJR2diNym9tVzYH8dE3edMah0BRIqwNHqJgwHEwAXUrSYmIiI3D7mH5yPkcuaEwYGJkzM2jeL9tXb4+ig+01ERCRvBZpRu379ev7+97/Tvn17lixZgmH89UPJz8+Pbt268d///rfQghS5rTk4QtsXYPAm8K4Gl36FJZ1h9z/ByGURMhGRYmA4mDhS04FvGzhwpKaDtUgL4OehxcREROT2EX81/ob7DAziUuJ0t4mIiNilQIXaOXPm0KFDBzZs2MADDzyQY3+LFi04fPjwLQcnIteoGQEjdkH9HpCVAVtehvcj4Y/fSzoyEbmN+Lv7Y8KU6z4TJgI9Amnm36yYoxIRsVOWGWJ2wQ/rLP9qsScpJrrbRERE7FGgQu0PP/zAgw8+eMP9QUFBxMff+FNFESkgD194ZLVlYTFHVzi+FRa1gZivSzoyEblNPN/8eYAcxdrs7ye0nKBbO0WkdDryCSwIgxU9YP0Qy78LwizbRYqY7jYRERF7FKhQ6+joiNl840+fz507h6enZ4GDEpE8mEwQ/hQM/RKq1IMrcbCiF3zxKpgzSzo6ESnn2tdoz7za/fE3Z9lsDzBnMa92f61sLSKl00+bYO3jkHTOdntSrGW7irVyC3S3iYiIFJYCFWobN27Mli1bct1nNptZu3Yt4eHhtxSYiNxEYBgM2wFNBwIG7HoDlt8Hl0+XdGQiUp79tIlO22ex5fQZlsWeZ1b87yyLPc/m02fptH2Wih0iUjptnwS5LPZk3bb5RbVBkALT3SYiIlJYClSoffrpp/n888/5xz/+we+/W/pjZmZm8uOPP9KnTx+OHDnCs88+W6iBikguXDzhgXeg7zJw9YYze2Hx31QoEZGi82exwxEIT03jvj9SCE9Nw1HFDhEpzZLj8thpQNJvcCqq2MKR8qV9jfbMazcPfw9/m+0BHgHMazdPd5uIiIjdnAryoEceeYQffviB6dOnM2PGDAC6d+8OgGEYTJ061fq9iBSDsIcguJml39pvB2HtQGgxBLq+Ds7uJR2diJQnyXHgmvvtnTbFjpB7ijUsEZFbduV8SUcgZVinmp1oX7090fHRXEi5gJ+HH838m2kmbQkyZxnsi7lEfHIq/l5utAzxxdHhRr/DiIiUDgUq1AK89tprPPjgg7z//vv89NNPGIZB3bp1eeyxx2jRokVhxigi9vANgSe3wJevwrdvwoGlcHqPZbatf/2Sjk5EbicqdohIGWT29EclNbkVjg6OhAeqBWBpsPlwLFM3HiE2MdW6LcjHjck9Q2ldQ+vpiEjpVaDWB9maN2/O3Llz+eyzz9i0aRMLFiygRYsW7Nq1i2nTpuX7eIsWLaJRo0Z4e3vj7e1NREQEn3/+uXW/YRhMmTKF4OBg3N3dadeuHT/++KPNMdLS0njmmWeoUqUKnp6e9OrVi7Nnz97KaYqUHY7O0HkaPPYRePpB/I/wf+3g4AowcuvLJiJSBCoElHQEIiI2Yo1KZN3gV6EsA84Zldln1gfbIuXB5sOxjFwdbVOkBYhLTGXk6mi2HcmrFYqISMm6pULtjXz99ddMnTo134+rVq0aM2fO5MCBAxw4cIAOHTrwwAMPWIuxs2fPZt68ebzzzjvs37+fwMBAOnfuTHJysvUYo0ePZsOGDXz44Yd88803XLlyhR49emA2q1+e3Ebu7Agjo6B2B8i8ChufhXVPQGpiSUcmImWdVyDcYGVrMIF3VajZujgjEhG5qZkZ/QFyFGuzv5+aMZD4PzKKOSoRKWzmLIOpG4/ktXQgMz//qThDEhHJlwK3PigKPXv2tPn+9ddfZ9GiRezZs4fQ0FAWLFjAxIkT6dOnDwArVqwgICCA999/n+HDh5OYmMjSpUtZtWoVnTpZGravXr2a6tWrs337drp27Vrs5yRSYir4w6PrIeotSzuEHzdY+tc+tAyq65YsESmgTtNg0wgsxdpr/wz6s3jbbSaoH5+IlDJfZDVnZMZoJjuvJJhL1u1xVGZqxkC2ZLVksJdbCUYoIva6kpZJXGIq55NSiUtMJe6af0/EX8kxk/ZaBnA+Ka34ghURyadSVai9ltls5j//+Q9//PEHERERxMTEEBcXR5cuXaxjXF1dadu2LVFRUQwfPpyDBw+SkZFhMyY4OJiwsDCioqJUqJXbj4MD/G001PqbZUbt5dPwr27Q4R/Q+jnLfhGR/Kh/H1RYCZsnQNK5v7Z7B1uKtKG9Si42EZEbCPB2ZWtaS7altaClw0/4c5l4KrIvqz4GDgT5WBYaEpGSk5Vl8PsfaZxPTLMUX5NSiUu8SlximqUom5TK+cRUktMySzrUHMxmMxkZmpUvIjk5Ozvj6Gj/RJZSV6j94YcfiIiIIDU1lQoVKrBhwwZCQ0OJiooCICDAtu9dQEAAp06dAiAuLg4XFxcqVaqUY0xc3I370KSlpZGW9tenaklJSYV1OiKlI7+qtYAR38DG0fDjR7B9Cvy6Ex58F7zUS7KsKhW5JeVWnvkV2gvq3w+noiwLh1UIsLQ70ExasYPeu6Qo3Si/Xuxen3EfH8fAgT1Zodb92Y1cJvcM1WrwclN6/yq41AxzrjNgs7edT7IUYzNv1Ez6Ol6uTgT4uBHk40aAtxuB3m4E+LiRmJLOG1t/LuKz+YthGMTFxXH58uVie04RKXsqVqxIYGAgJtPNf9codYXaevXqcejQIS5fvsz69esZNGgQO3futO6//qQMw7jpid5szIwZM/LsqWvOMhMdH82FlAv4efjRzL8ZjvpjVOx0s/wyzGZSDhwk88IFnPz88GjRHFM+Pm2xm5sP9F0GtdvDpvHw6w5Y3MZSrL2zY+E/nxS5m+WWyK24aX45OELIPcUXkJQbeu+SonSj/OocGsiiCl5M2XiYCxlHMTklY2R64ed8F1N6htEtLKgEopWyRu9fORmGweWUjGtmwKb+1ZbgmoLs5RT7ZpuaTOBXwZVAH0vxNfCaQmygj5t1u6dr7qUMc5bBmr2niUtMzbVPrQnLDPszBT9lG9lFWn9/fzw8POwqwojI7cMwDFJSUoiPjwcgKOjmv2+YDMO+peCzD2qP+fPnM3v27EJZwKtTp07Url2bCRMmULt2baKjo2natKl1/wMPPEDFihVZsWIFX375JR07duTSpUs2s2obN25M7969b/hDNbdPRqtXr05iYiL7EvYxc99Mzqect+4P8AjgxZYv0qlmp1s+Pyk8SUlJ+Pj4kJiYiLe3d0mHY5VXfrFnD+enzyDzmhnfToGBBLz8Et7XtPAodPE/wbonId6yUB+tn4UOr4CTS9E9ZxlXGvMrr9wqLTHKzZXG3ALlV3lRGvOrXOZWlvm2nGFe1vJLv9eXHaUxt6Ccvn/lIcOcRXxymrX4eu0M2LikvwqyaZlZdh3PzdnBMvPV+8+ZsNnF2GuKsH4VXHFyvLX2bJsPxzJydTSQazd93uhdh74R9W75dTObzfz888/4+/tTuXLlggcsIuXexYsXiY+Pp27dujdtg2D3jFp7p+iCfbNc7WUYBmlpaYSEhBAYGMi2bdushdr09HR27tzJrFmzAGjevDnOzs5s27aNyMhIAGJjYzl8+DCzZ8++4XO4urri6uqaY/uO0zt45eArGNd9FhefEs+Yr8Ywr908/VInN3Wj/Er64guSX54I131Wknn+PL89NxreXFB0xVr/+jD0C9j6D9i/xLLg2Klv4aGl4BtSNM8phe5GuSVSGJRfUlTKXW4d+eQGPZtnqWdzCdDv9VKUytP7V3JqBueTUonNMQM2jbgkS1/Yi3+kXf+nyg35err8VYC1Fl9d/9zmTqC3G97uTsUy47RbWBCLHmvG1I1HbBYWC/RxY3LPUFrX8CyU58nuSevh4VEoxxOR8iv7fSIjI6PwCrWPP/54kb+pvvzyy3Tv3p3q1auTnJzMhx9+yFdffcXmzZsxmUyMHj2a6dOnU6dOHerUqcP06dPx8PBgwIABAPj4+DBkyBDGjh1L5cqV8fX1Zdy4cTRs2JBOnfL/i9f8g/Nz/DIHYGBgwsSsfbNoX7292iBIgVyYOw+33H7zMQwwmTg/fQZeHTsWTRsEAGd3uH8u3NEO/vt3+O0gvHsv9JgPDfsWzXNKsVLbFhGRInbkE1j7OFz/+2JSrGV75EoVa0sJ/V4vxSEzI5PvPt1B8rk4vIIDadqjPU7Oxdtt0JxlcPFKmqUAm8sM2OwFuf5It+/uV2dHE/5etm0HsvvBBv35vb+3K65Opev/nW5hQXQODWRfzCXik1Px97IsGOjoYCr03sJqdyAiN5Of9wm7f2osX768ILHky/nz5xk4cCCxsbH4+PjQqFEjNm/eTOfOnQEYP348V69eZdSoUSQkJNCqVSu2bt2Kl5eX9Rjz58/HycmJyMhIrl69SseOHVm+fHm+VljLFn81Hkd3R0xZBnedMah0BRIqwNHqJgwHiEuJIzo+mvDA8EK7BnL7yDx/Hm6Ul4ZBZlwcKQcO4tmqZdEGcldPCGoC65+CM3tg/RBL/9rus8GlcD5tluK3/dR23d4pImWSOcvI9Q/rUifLbJlJm2sXRAMwweYXLQvvqfhX4rJ/r8+NgaHf6+WW7Vzyb5wWLsA35TIV/ty299WKZI4aTdunHimU50jNMBOXaJkFe/66nrDZRdn45DTM9i7I5eb0V9uBG/SD9fVwwaE0vgfbwdHBRERttSQoaYMHD+by5ct8/PHHJR2KSJlgd6F20aJFPPjggwQGBhZZMEuXLs1zv8lkYsqUKUyZMuWGY9zc3Hj77bd5++23CyWmlseyGLwtiyrJf2373QuWd3ZgXz0HLqRcKJTnEclN5oViyq+K1WHwZ7BzFnw9B75bDWf2Qd9/QWBY8cQghUa3d4pIWbX5cGyOW1WD/rxVtdQt9nQqyrbdQQ4GJP1mGaeF90qN3CdgWIpQ+r1eCuqbFeup9c+crfYqplzG9MYUdkKexVrDMEhIySA28eqfM2DTrDNfY//8Ny4plcSr9i3I5WACPy9XmyJsbv1gPVxK3drit6Uy8wGliBQLu9+Zn3nmGZ555hnCw8Pp06cPDzzwAHXr1i3K2Epc8+NZjP0s5xukbzKM/SiLuX3Ar6tfCUQmtwsnv2LML0cn6DDR8sfkR8Pg95/hvQ7Q9XUIf8qyBKuUCbq9U0TKom1H4hj38fEc715xiamMXB3Noseala5i7ZXzNx+Tn3FS5G42AcPPQ7/XS8E4LVkI/LVYVTYHIAtwXLiAvW3bc+FqprUf7LWzYs8npZFu54Jc7s6Of858dSXIx/3PGbCu1tmwQT7uVKngcssLcknxKC0fUKanp+PiooWlRUoDu9+9Y2NjWbRoEb6+vrzyyivcddddNGjQgFdeeYWDBw8WZYwl5vGvLP/m9gPXAIZ8YaJp5cbFG5SUG04BATcufppMOAUG4tGiefEGBRByL4z4Bup0BXMabBoH/34MUi5Z9meZIWYX/LDO8m+Wff2tpPjEX42/4b5rb+8UESlNZn7+0w2bCABM3XjE7tt5i8WVG7/X2qgQULRxiF06nPJi7EdZVE623Z49AaNLjA/N/JuVTHBS5lVKSczxN2M2B6ByymVem/kBT7//Ha99dpT3dsXw6fex7D+ZwJlLV61F2sqeLoQGedOxvj8DWtVgTOe6zH6oESuebMnW5+/lf5O7cGRaV3aMa8eHwyKY/0gTXuxen8FtQugWFkTTGpUI9HFTkbaM2Hw4lpGro22KtPDXB5SbD8cW2XO3a9eOp59+mjFjxlClShU6d+7MvHnzaNiwIZ6enlSvXp1Ro0Zx5coV62OWL19OxYoV2bJlC3fddRcVKlSgW7duxMb+FafZbGbMmDFUrFiRypUrM378eIzr1mVJS0vj2Wefxd/fHzc3N/72t7+xf/9+6/6vvvoKk8nEli1baNq0Ke7u7nTo0IH4+Hg+//xz7rrrLry9venfvz8pKSlFdo1ESordM2r9/PwYOnQoQ4cO5cqVK3z66ad8/PHHvPXWW0yfPp1q1arx4IMP8uCDD3LPPffg4FD2fzhUTMq64UJODkClRDNp0YdwKuoeolIu+Y0dQ/LLEy3F2mt/eP1ZvA14+aWiW0jsZjyrwIB/w55FsG0S/PQpnDsEzQfDwWVa2boc0O2dIlLanE9Kw8E195WzDSA2MZX739xFNV93vN2c8XZ3xtvN6c9/nfF2d7pmu+V7Lzfnwr99NP0P2D4V9r1rjS23ZzAwYfIOhpqtC/f5pUAe3WEphN1oxuPgL8w4vFLcUcntpAYpuNaslLMfbClekEvyxzAMrmbYN4nFnGUw+ZMf8+pyzpRPjtDmzip2/Rxzd3bM96JmK1asYOTIkXz77bcYhsHmzZt56623qFWrFjExMYwaNYrx48ezcOFC62NSUlJ44403WLVqFQ4ODjz22GOMGzeONWvWADB37lyWLVvG0qVLCQ0NZe7cuWzYsIEOHTpYjzF+/HjWr1/PihUrqFmzJrNnz6Zr16788ssv+Pr6WsdNmTKFd955Bw8PDyIjI4mMjMTV1ZX333+fK1eu8OCDD/L2228zYcKEfJ23SGlXoKY0FSpUoF+/fvTr14/09HS2b9/Ohg0b+PDDD3nrrbfw9fWlZ8+e9OnTh86dO+Pm5lbYcZcaxdZDVMod744d8X7Tk/PTZ5AZF2fd7hQQQMDLL+HdpUsJRoelYBwxyvIH5ron4dIJ2PFaznFa2bpM0u2dIlIW/XQ+mZ/OJ9984DUquDrlUdC9caHXy80JLzcn25lpJ7+F//4dEmIA+N0/At/zuzGw9ITMZpn4a3CowQSaqs1MqeB0MTHPCRgOFy4XzyKuctsa3KMF4Q/qg5vy7GqGmdBJWwrlWAYQl5RKwylb7Rp/ZFrXfPccvvPOO5k9+6/eyvXr17f+d0hICK+++iojR460KdRmZGSwePFiateuDcDTTz/NtGnTrPsXLFjASy+9xEMPPQTA4sWL2bLlr2vyxx9/sGjRIpYvX0737t0BeO+999i2bRtLly7lhRdesI597bXXaNOmDQBDhgzhpZde4sSJE9xxxx0A9O3blx07dqhQK+XOLXcPd3Fx4b777uO+++7DMAy++eYbNmzYwH//+19WrlzJ5MmTmTRpUmHEWioVaw9RKXe8u3TBq2NHUg4cJPPCBZz8/PBo0bzkZtLmJrgJDP0S5tWHjKu5DNDK1qWNv7s/l7iUa59aEyYCPAJ0e6eIlEnPdaxDoI8bSVczSErNIOlq5p//ZpCUmmmzPXtW05W0TK6kZXLuultL7eXp4oi/m5ln+YAH0j/FAYMEJz8+qfEic05Uo01GKyY7rySYS9bHxFGZaRkD+V90Nb7pbGhRmDJCEzCkoBI8fPBIu5JrX8EsIMGjInf3aF/cYYnkqUWLFjbf79ixg+nTp3PkyBGSkpLIzMwkNTWVP/74A09PTwA8PDysRVqAoKAg4uMtrYASExOJjY0lIiLCut/JyYkWLVpY2x+cOHGCjIwMawEWwNnZmZYtW3L06FGbeBo1amT974CAADw8PKxF2uxt+/btu9XLIFLqFOoyjyaTiXvuuYd77rmHefPm8f3335OWllaYT1GsnAIC4OJF29vSs5lMOAUElEwPUSlXTI6OpX/2RtwPNyjSZtPK1qXJ882f55WDr2DCZFOsNf15w+eElhO0kJiIlDoB3q78nkaut4GagEAfN57tWMfuomd6ZhbJqTkLuJZteRd5k1IzSEm3FHpDM35kDu9Sy8GyKNiHme14PfUxko94AGa20JJtaS1o6fAT/lwmnorsy6pPFg6QmMq+mEtE1K5cOBdJipQmYEhBZT41CtM/Z5OF7SIwWVjev8yjRuPkXKh/eksp5O7syJFpXe0auy/mEoP/tf+m45Y/EU7LEN+bjnN3zv/v9tnFV4BTp05x3333MWLECF599VV8fX355ptvGDJkCBkZGdZxzs7ONscwmUw5etDmJXvs9W0aDMPIse3a5zKZTLk+d1aWfYvwiZQlRfrT4tpPQMqiUt1DVKQ4aWXrMqV9jfbM85rHzH0zOZ/y12sS4BHAhJYT6FSzUwlGJ+WFOctgX8wl4pNT8fdyo2WIr2YNyi15sXt9xn18HBO2xdrsrJrcMzRfOebi5EDlCq5UruBaoHgyUq9g3jYN14P/hwmDNI9A/tf0VUwV7+bZq5kcOHmJLUcs77FZOLAnKzTX48QnF2wmrxQuTcCQovS3QQ/xnbsHTgsX4Jty2bo9waMi5lGjafvUIyUXnBQbk8lkd/uBe+r4EeTjRlxiap4fUN5Tx69Yfr86cOAAmZmZzJ0717re0Nq1a/N1DB8fH4KCgtizZw/33nsvAJmZmRw8eJBmzSx389155524uLjwzTffMGDAAMDSTuHAgQOMHj268E5IpAwrcKF29erVLFy4kF9++YWLFy/m2G8ymcjMzLyl4Epaqe8hKlJc7F2xWitblxqdanaiffX2RMdHcyHlAn4efjTzb6aZtFIoNh+OZerGIzarFAf5uDG5ZyjdwoJKMDIpyzqHBrKogleO3Aosidw6vQfnj0fhfOmE5fumj+HadTot3XzIvgcmrKqPtVCbF3+v8rtWQ1miCRhS1No+9QiZgx7iu093kHwuDq/gQO7u0V4zaSVXjg4mJvcMZeTq6EL7gPJW1K5dm8zMTN5++2169uzJt99+y+LFi/N9nOeee46ZM2dSp04d7rrrLubNm8fly5et+z09PRk5ciQvvPACvr6+1KhRg9mzZ5OSksKQIUMK8YxEyq4C/dSYNm0aU6dOJSAggNatW1OpUqXCjqvUKBM9REWKWs3W4B1sWTjsRp/5amXrUsfBgNBTBpkXDJz8DBx0R6cUgm1H4hj38fEc7wRxiamMXB3NoseaqVgrBdYtLIjOoYElN1s7PQW+fA32LAQM8AqGXm9Bnc45hrYM8bVrNpQ9t6xK0dMEDCkOTs5OhD+Y8/1CJDfdwoJY9FizUvEBZZMmTZg3bx6zZs3ipZde4t5772XGjBk8/vjj+TrO2LFjiY2NZfDgwTg4OPDkk0/y4IMPkpiYaB0zc+ZMsrKyGDhwIMnJybRo0YItW7aU67qSSH6YjPw0FPlTUFAQoaGhbN68OUefkPIgKSkJHx8fEhMT8fb2LulwxE5l5XUrK3HmcOQTWJv9gzqXz3wjV0Jor+KOqtiUhdft2hjZsyfnH6OBgfpjtBQqC7kFf8XZYtJ/uZCW+4eV2YWpbyZ0UBuEUqIs5FepifH0Xvh4JGTPom3yGHR9Hdwr3vAhmw/HMnJ1NJD7bKjy/sFFqXnt8nB9jIbZrAkYZUBZyC0oO3GKrcJ63VJTU4mJiSEkJAQ3t4LfPaF2UiLlX37eL3JbmPKmkpOTiYyMLJdFWhG5gdBelmKs93V/cHoHl/sibVmT9MUX/PbcaJsiLUDm+fP89txokrZuLaHIpDw4n3TjRUINIPbPxZNEyoyMq7BlIizrainSegXBgP9A73/mWaSFv2ZDBfrY/sId6ONW7ou0ZVX2Iq4+Pe7Hs1VLFWlFpMQ5OpiIqF2ZB5pUJaJ2ZRVpRW5zBWp90LRpU86ePVvYsYhIaRfaC+rfD6eiLAuHVQiwtDtQ39NS5cLcebjldrOEYYDJxPnpM/Dq2FF/nEqR0eJJUmac2WeZRXvxF8v3jQdAt+ngbv/tlyXerkFEREREyo0CFWpfe+01+vbtS9++fWncuHFhxyQipZmDI4TcU9JRSB4yz5+HGxVhDYPMuDhSDhzEs1XL3MeI3CItniSlXsZV2PE67P4nGFlQIdDSi7Zu1wIdLns2lIiIiIjIrShQobZt27YsWbKEli1bEhERQc2aNXG8rihgMplYunRpoQQpIiKFK/PChZIOQcqoAG9Xfk+74bKCWjxJSr8z+/+cRXvc8n3j/tBtRr5m0YqIiIiIFIUCFWr37NnDoEGDyMjI4Ouvv851jAq1IiKll5OfX0mHIGXUi93rM+7j45jIffGkyT1Ddcu3lE4ZqX/Oon3nr1m0PRdAve4lHZmIiIiICFDAxcRGjx6Nq6srGzdu5NKlS2RlZeX4MpvNhR2riIjYwSkgAEw3KJSZTDgFBuLRonnxBiXlRufQQC2eJGXP2QPw7j0Q9ZalSNuoH4zarSKtiIiIiJQqBZpR+/333zN16lTuv//+wo5HRERukd/YMSS/PNFSrL12UbE/i7cBL7+khcTklmjxJCkzMlLhq+kQ9fafs2gDoOebKtCKiIiISKlUoEKtv78/Li4uhR2LyG3JnGUmOj6aCykX8PPwo5l/MxwdSm8RzZxlqDhTynl37Ij3m56cnz6DzLg463angAACXn4J7y5dSjA6KS+0eJIUlUL7uXj2oKUX7e/HLN83egS6zQQP9VAWERERkdKpQIXaJ598kjVr1vD000/nWERMROy3/dR2Zu6byfmU89ZtAR4BvNjyRTrV7FSCkeVu8+FYpm48QmxiqnVbkI8bk3uG6nbnUsa7Sxe8OnYk5cBBMi9cwMnPD48WzTWTVkRKtUL5uZiZBl/NgG/f/GsWbY8FUP++oglaRERERKSQFKhHbZs2bTAMg4iICP71r3+xY8cOvv766xxfInJjO07vYMxXY2z+GAWIT4lnzFdj2H5qewlFlrvNh2MZuTrapkgLEJeYysjV0Ww+HFtCkcmNmBwd8WzVEp8e9+PZqqWKtCJSqhXKz8XfDsK798I38y1F2oaRMGqPirQiIiJFLC4ujs6dO+Pp6UnFihVLOhy7mEwmPv7445IOQ8RGgWbUdu7c2frfQ4YMwXTdojWGYWAymbSgmEge5h+cj2GzZrqFgYEJE7P2zaJ99falog2COctg6sYjuURrWfXdBEzdeITOoYFqgyAiIgVySz8XM9Pgq5l/zqI1g6c/9JgPd/UohshFRERuQZYZTkXBlfOWu0BqtoZS8Ddgfs2fP5/Y2FgOHTqEj49PSYcjReC5557jm2++4fDhw9x1110cOnSopEMqlwpUqP3Xv/5V2HGI3Hbir8bj6J77D2ADg7iUOKLjowkPDC/myHLaF3Mpx0zaaxlAbGIq+2IuqWeliIgUSIF/Lv4WDR+PggtHLd+H9YX75qgXrYiIlH5HPoHNEyDp3F/bvIOh2ywI7VVycRXAiRMnaN68OXXq1LnhGJPJRExMDLVq1SqU5zSbzZhMJhwcCnSzuOSTYRg8+eST7N27l++//76kwym38p3N6enphISE0Lp1awYNGpTnl4jcmgspF0o6BADik29cpC3IOBERkYKw+bmYmQZfTIMlnSxFWk8/eGQ19F2qIq2IiJR+Rz6BtY/bFmkBkmIt2498UiRP++6771K1alWysrJstvfq1YtBgwYxZcoUmjRpwrJly6hRowYVKlRg5MiRmM1mZs+eTWBgIP7+/rz++uvWx9aqVYv169ezcuVKTCYTgwcPLlBsn3zyCXXq1MHd3Z327duzYsUKTCYTly9fBmD58uVUrFiRTz/9lNDQUFxdXTl16hT79++nc+fOVKlSBR8fH9q2bUt0dLTNsY8fP869996Lm5sboaGhbNu2LV+xRUVF0aRJE9zc3GjRogUff/wxJpPJOqvUbDYzZMgQQkJCcHd3p169erz55ps2xxg8eDC9e/dm+vTpBAQEULFiRaZOnUpmZiYvvPACvr6+VKtWjWXLllkfc/LkSUwmE2vXruWee+7B3d2d8PBwfv75Z/bv30+LFi2oUKEC3bp148KFv35Psuea5Ndbb73F3//+d+64445bOo7kLd+FWkdHRzp27Mjnn39eFPGIyDX8PPxKOgQA/L3cCnWciIhIQVh/Lp77Dv6vHeyaa2l1EPYQjNoLd/Us0fhEROQ2ZhiQ/od9X6lJ8Pl4uGFzOSwzbVOT7Duekdtxcvfwww/z+++/s2PHDuu2hIQEtmzZwqOPPgpYZsd+/vnnbN68mQ8++IBly5Zx//33c/bsWXbu3MmsWbP4xz/+wZ49ewBLUbBbt25ERkYSGxubo0Bpj5MnT9K3b1969+7NoUOHGD58OBMnTswxLiUlhRkzZrBkyRJ+/PFH/P39SU5OZtCgQezatYs9e/ZQp04d7rvvPpKTkwHIysqiT58+ODo6smfPHhYvXsyECRPsji05OZmePXvSsGFDoqOjefXVV3M8Pisri2rVqrF27VqOHDnCpEmTePnll1m7dq3NuC+//JJz587x9ddfM2/ePKZMmUKPHj2oVKkSe/fuZcSIEYwYMYIzZ87YPG7y5Mn84x//IDo6GicnJ/r378/48eN588032bVrFydOnGDSpEk2Med1TQC6d+9OhQoV8vyS4pfv1geOjo4EBgZi5OONQERy8nf35xKXcu3HZ8JEgEcAzfyblUBkOYXXqoSnqyN/pOXed9oEBPq40TJEM5hERKRg7Pq56BsGX74Gu+ZZCrQeVSy9aMvY7aEiIlIOZaTA9OBCOphhmWk7s7p9w18+By6edg319fWlW7duvP/++3Ts2BGA//znP/j6+tKxY0eioqLIyspi2bJleHl5ERoaSvv27Tl27BibNm3CwcGBevXqMWvWLL766ivuvvtu/Pz8cHV1xd3dncDAwAKd8eLFi6lXrx5z5swBoF69ehw+fNhm5i5ARkYGCxcupHHjxtZtHTp0sBnz7rvvUqlSJXbu3EmPHj3Yvn07R48e5eTJk1SrVg2A6dOn0717d7tiW7NmDSaTiffee886I/e3335j6NCh1jHOzs5MnTrV+n1ISAhRUVGsXbuWyMhI63ZfX1/eeust63WcPXs2KSkpvPzyywC89NJLzJw5k2+//ZZ+/fpZHzdu3Di6du0KWHrF9u/fny+++II2bdoAlvWjli9fbvc1AViyZAlXr1616xpI8SlQI4+HH36YdevWqVgrcgueb/48YPnj81rZ309oOaFULCQG8PaXv+RZpAWY3DNUC4mJiEiB3fTn4p2ROC7pCF/PsRRpG/SBv+9TkVZERCSfHn30UdavX09aWhpgKUT269cPR0fL35+1atXCy8vLOj4gIIDQ0FCbXrABAQHEx8fn+TzXz9gEaNCgQa4zNo8dO0Z4uG0f+pYtW+Y4pouLC40aNbLZFh8fz4gRI6hbty4+Pj74+Phw5coVTp8+DcDRo0epUaOGtUgLEBERkWfs1zp27BiNGjXCze2vO0hzi23x4sW0aNECPz8/KlSowHvvvWeNIVuDBg1yXMeGDRtav3d0dKRy5co5ru215xwQEABg87jrX4+bXROAqlWrcuedd+b5JcWvQIuJPfXUU+zYsYMuXbrw3HPPceedd+Lh4ZFjXI0aNW45QJHyqn2N9szzmsfMfTM5n3Leuj3AI4AJLSfQqWanEozuL//c8QtvfnEcgL7Nq/HtL7/bLCwW6OPG5J6hdAsLKqkQRUSkHGhfoz3zLvdn5s9rOO/4V7E2wJzFBPc76PTJhL9m0d4/Fxr0LrlgRURErufsYZnZao9TUbCm783HPboOara277nzoWfPnmRlZfHZZ58RHh7Orl27mDdv3l+Hc3a2GW8ymXLddn2f2+tdP2OzTp06bNq0iapVq+YYaxgGJpMpx7brubu75xg3ePBgLly4wIIFC6hZsyaurq5ERESQnp5+w+Ncf4y82BPb2rVref7555k7dy4RERF4eXkxZ84c9u7dazOuoNf22jHZsVy/7drH3OyagKWQvmvXrjzP/cqVK3nul8JXoEJtWFgYJpMJwzD48ssvbzjObM59Bp6IWHSq2Yn21dsTHR/NhZQL+Hn40cy/WamZSfve178yZ8sxAF7sXp8RbWtjzjLYF3OJ+ORU/L0s7Q40k1ZERG7ZT5votH0W7TGIdnPlgqMjfmYzzVLTcOTPPm2hvS1FWs8qJRqqiIhIDiaT3e0HqN0BvIMtC4fl2qfWZNlfuwMUwd+G7u7u9OnThzVr1vDLL79Qt25dmjdvXujPk1tBtmbNmtSqVSvH9vr167Np0yabbQcOHLDreXbt2sXChQu57777ADhz5gy///67dX9oaCinT5/m3LlzBAdb2lPs3r3b3tOgfv36rFmzhrS0NFxdXXONbdeuXbRu3ZpRo0ZZt504ccLu5yhsN7smoNYHpVWBCrWTJk3K16cPInJjjg6OhAeG33xgMVv+bQyvbzoKwJjOdRnRtjYAjg4mImpXLsnQRESkPNo+CTBwBMJT03Lud/eFvsuK5A9WERGRYuXgCN1mwdrHsTSTu7ZY+2etpdvMIv2Z9+ijj9KzZ09+/PFHHnvssSJ7HnsNHz6cefPmMWHCBIYMGcKhQ4esPVdvVn+68847WbVqFS1atCApKYkXXngBd3d36/5OnTpRr149Hn/8cebOnUtSUlKuC5XdyIABA5g4cSLDhg3jxRdf5PTp07zxxhs2sd15552sXLmSLVu2EBISwqpVq9i/fz8hISH5vBKF42bXBHIvpOfll19+4cqVK8TFxXH16lUOHToEWArhLi4uhRX6ba9AhdopU6YUchgiUpqs2XuKKRuPAPBMhzt5tmOdEo5IRETKveQ4cM3jD7Grlyy3iobcU3wxiYiIFJXQXhC5EjZPsCwcls072FKkLeIe7B06dMDX15djx44xYMCAIn0ue4SEhLBu3TrGjh3Lm2++SUREBBMnTmTkyJHWWaw3smzZMoYNG0bTpk2pUaMG06dPZ9y4cdb9Dg4ObNiwgSFDhtCyZUtq1arFW2+9Rbdu3eyKzdvbm40bNzJy5EiaNGlCw4YNmTRpEgMGDLD2rR0xYgSHDh3ikUcewWQy0b9/f0aNGsXnn39e8ItyC252TQriqaeeYufOndbvmzZtCkBMTEyus6SlYEyGVgTLISkpCR8fHxITE/H29i7pcMROZeV1K+1xrj1whvHrvgdg+L138GL3+ppBT+l/3aBsxCg5lZXXrazEKbbKwutmjfFFL7zzKtQCPLQUGtrR00+KRZnKr1Ico+RUVl63shKn2Cqs1y01NZWYmBhCQkJsFpnKtyyz5YPIK+ehQoClJ63uHgHg9ddfZ/HixZw5c6akQ8lhzZo1PPHEEyQmJuaYqSpyvfy8XxRoRm02s9nMTz/9REJCQq5NpO+9995bObyIFLMN351lwnpLkfaJNrVUpBURkdKlQkBJRyAiIlK4HBx1t8ifFi5cSHh4OJUrV+bbb79lzpw5PP300yUdFgArV67kjjvuoGrVqvzvf/9jwoQJREZGqkgrhc6hoA+cNWsWVapUoVGjRrRt25b27dvn+MqvGTNmEB4ejpeXF/7+/vTu3Ztjx47ZjDEMgylTphAcHIy7uzvt2rXjxx9/tBmTlpbGM888Q5UqVfD09KRXr16cPXu2oKcqclv49PtzjF37PwwDHru7BpN6hKpIKyIixccrEGtfvhxM4F3VvpWvRUREpEw6fvw4DzzwAKGhobz66quMHTu2WFpvTp8+nQoVKuT61b17dwDi4uJ47LHHuOuuu3j++ed5+OGH+b//+78ij01uPwWaUbtkyRJeeukl2rZtS5cuXZg4cSLPP/88zs7OLF26lDvuuMNmpTt77dy5k7///e+Eh4eTmZnJxIkT6dKlC0eOHMHT07J64uzZs5k3bx7Lly+nbt26vPbaa3Tu3Jljx47h5eUFwOjRo9m4cSMffvghlStXZuzYsfTo0YODBw/i6KhbCESut/lwHM99eIgsAx5pUZ1pvcJUpBURkeLVaRpsGkFJLaoiIiIiJWv+/PnMnz+/2J93xIgRREZG5rove8bs+PHjGT9+fHGGJbepAhVqFy9ezN13382OHTu4ePEiEydO5P7776dDhw4899xzNGnSBLPZnO/jbt682eb7f/3rX/j7+3Pw4EHuvfdeDMNgwYIFTJw4kT59+gCwYsUKAgICeP/99xk+fDiJiYksXbqUVatW0alTJwBWr15N9erV2b59O127di3IKYuUW18cPc8zH0RjzjLo07Qq0/s0xMFBRVoRESlm9e+DCiW3qIqIiIjcnnx9ffH19S3pMESAAhZqjx49ymuvvQZgnXWXmZkJQFBQEMOGDePNN9/kySefvKXgEhMTAaz/w8TExBAXF0eXLl2sY1xdXWnbti1RUVEMHz6cgwcPkpGRYTMmODiYsLAwoqKici3UpqWlkZaWZv0+KSnpluIWuVZpzq+vf77AyNXRZJgNejQKYnbfRjiqSFtmlObckrJP+SVFJc/cCu0F9e/XoipSYHrvkqKk/BIRkaJWoB61jo6OVKhQAcDakuDSpUvW/bVq1eL48eO3FJhhGIwZM4a//e1vhIWFAZaeIAABAbYLSQQEBFj3xcXF4eLiQqVKlW445nozZszAx8fH+lW9evVbil3kWqU1v6J++Z2hKw+Qbs6ia4MA5j/SBCfHAretlhJQWnPrlmSZIWYX/LDO8m9W/u/OkMJRLvNLSoWb5lb2oioN+1r+VZFW8kHvXVKUlF8iIlLUClSVqVGjBqdPnwYsM1qrV6/Orl27rPv3799/y9PGn376ab7//ns++OCDHPuu751pGMZN+2nmNeall14iMTHR+nXmzJmCBy5yndKYX/tiLjFkxQHSMrPoWN+ft/s3w1lF2jKnNObWLTnyCSwIgxU9YP0Qy78LwizbpdiVu/ySUkO5JUVJ+SVFSfklIiJFrUCtD+699142btzIq6++CsDDDz/MggULuHr1KllZWaxevfqW2h4888wzfPLJJ3z99ddUq1bNuj0wMBCwzJoNCgqybo+Pj7fOsg0MDCQ9PZ2EhASbWbXx8fG0bp37SsGurq64uroWOF6RvJS2/Dp4KoEn/rWPqxlm7q3rx8LHmuHipCJtWVTacuuWHPkE1j6O7QJCQFKsZXvkSvWnLGblKr+kVFFuSVFSfklRUn6JiEhRK1B15rnnnmPkyJGkpqYCMHXqVLp3786KFStYtWoVnTt3ZubMmfk+rmEYPP3003z00Ud8+eWXhISE2OwPCQkhMDCQbdu2Wbelp6ezc+dOaxG2efPmODs724yJjY3l8OHDNyzUitwuvj97mcHL9vFHupnWtSvzfwOb4+qkW0qlhGWZLYsHXV+khb+2bX5RbRBERERERESkXLN7Ru3KlSu59957qVWrFvXq1aNevXrWfZ6enmzcuJHExESb/rX59fe//53333+f//73v3h5eVl7yvr4+ODu7o7JZGL06NFMnz6dOnXqUKdOHaZPn46HhwcDBgywjh0yZAhjx46lcuXK+Pr6Mm7cOBo2bEinTp0KFJdIefDjuUQGLt1HclomLWv5smRQC9ycVaSVUiDma9sV3nMwIOk3y+JCIfcUW1giIiIiImKfuLg4Bg4cSFRUFM7Ozly+fLmkQ7opk8nEhg0b6N27d0mHImJl94zaJ554gqioKOv3iYmJNGvWjP3791u3+fj4FLhIC7Bo0SISExNp164dQUFB1q9///vf1jHjx49n9OjRjBo1ihYtWvDbb7+xdetWvLy8rGPmz59P7969iYyMpE2bNnh4eLBx40YcHVWUktvTsbhkHluyl8SrGTSrUZFlT4Tj4VKgzicity7LDOe+g2/fhNV94f1H7HvclfNFG5eIiIiISDEzZ5nZH7efTb9uYn/cfsxl9C6y+fPnExsby6FDh/j5559LOhwppWrVqsWCBQvy/biPPvqIrl27UqVKFUwmE4cOHcoxJi0tjWeeeYYqVarg6elJr169OHv2rM2YhIQEBg4caF0UcuDAgTk+VDh9+jQ9e/bE09OTKlWq8Oyzz5Kenp7vmAvK7kqNYdjekpqZmcmhQ4dITk4utGCuf47cmEwmpkyZwpQpU244xs3Njbfffpu333670GITKat+ib/Co0v2kJCSQeNqPix/siUVXFWklWKUlQUXjlpmzsbsglPfQGpi/o9TIaDwYxMRERERKSHbT21n5r6ZnE/5a0JCgEcAL7Z8kU41y9YdwSdOnKB58+bUqVPnhmNMJhMxMTHUqlWrUJ7TbDZjMplwcNCaK+XdH3/8QZs2bXj44YcZOnRormNGjx7Nxo0b+fDDD6lcuTJjx46lR48eHDx40Dpxc8CAAZw9e5bNmzcDMGzYMAYOHMjGjRsBS07df//9+Pn58c0333Dx4kUGDRqEYRjFVmNUNouUYzG//8GA9/bw+5V0QoO8WflkK7zdnEs6LCnvDAN+Pw77l8LaQfBGHVjU2tJn9thnliKtqzfU7Q5dp8OwneAdDJhucEATeFeFmuozLiIiIiLlw/ZT2xnz1RibIi1AfEo8Y74aw/ZT24vked99912qVq1KVlaWzfZevXoxaNAgpkyZQpMmTVi2bBk1atSgQoUKjBw5ErPZzOzZswkMDMTf35/XX3/d+thatWqxfv16Vq5ciclkYvDgwQWK7ZNPPqFOnTq4u7vTvn17VqxYgclkss54XL58ORUrVuTTTz8lNDQUV1dXTp06xf79++ncuTNVqlTBx8eHtm3bEh0dbXPs48ePc++99+Lm5kZoaKjNukb2iIqKokmTJri5udGiRQs+/vhjm5mdZrOZIUOGEBISgru7O/Xq1ePNN9+0OcbgwYPp3bs306dPJyAggIoVKzJ16lQyMzN54YUX8PX1pVq1aixbtsz6mJMnT2IymVi7di333HMP7u7uhIeH8/PPP7N//35atGhBhQoV6NatGxcuXLA+zp5rkh/2nF+7du0YPXq0zbbevXtb86Fdu3acOnWK559/HpPJhMn0199/69evp0GDBri6ulKrVi3mzp1rc5yBAwcyadKkG7Y0TUxMZOnSpcydO5dOnTrRtGlTVq9ezQ8//MD27Zb/l44ePcrmzZtZsmQJERERRERE8N577/Hpp59y7NgxALZu3cqRI0dYvXo1TZs2pVOnTsydO5f33nuPpKSkAl+//NC0OpFy6sylFAa8t4f45DTqBXix+qlW+HioSCtFJOHUnzNmv4aTuyA51na/swfUiLD0mA25FwIbg+M1P4K6zYK1j2Mp1l57d8WfP7y7zQQHta8RERERkdLJMAyuZl61a6w5y8yMfTMwcllMN3vbzH0zaRXYCkc7fgd2d3K3KXrl5eGHH+bZZ59lx44ddOzYEbDcDr5lyxY2btxIVFQUJ06c4PPPP2fz5s2cOHGCvn37EhMTQ926ddm5cydRUVE8+eSTdOzYkbvvvpv9+/fz+OOP4+3tzZtvvom7u7tdsVzr5MmT9O3bl+eee46nnnqK7777jnHjxuUYl5KSwowZM1iyZAmVK1fG39+fmJgYBg0axFtvvQXA3Llzue+++zh+/DheXl5kZWXRp08fqlSpwp49e0hKSspRUMxLcnIyPXv25L777uP999/n1KlTOR6flZVFtWrVWLt2LVWqVCEqKophw4YRFBREZGSkddyXX35JtWrV+Prrr/n2228ZMmQIu3fv5t5772Xv3r38+9//ZsSIEXTu3Jnq1atbHzd58mQWLFhAjRo1ePLJJ+nfv7/1ent4eBAZGcmkSZNYtGiRNea8rglA9+7d2bVrV57nfuXKlXydX14++ugjGjduzLBhw2xmxR48eJDIyEimTJnCI488QlRUFKNGjaJy5cp2F/0PHjxIRkYGXbp0sW4LDg4mLCyMqKgounbtyu7du/Hx8aFVq1bWMXfffTc+Pj5ERUVRr149du/eTVhYGMHBwdYxXbt2JS0tjYMHD9K+fXu74rkVKtSKlEO/Xb5K//f2EJuYSm0/T9YMbYWvp0tJhyXlSdI5SxuDk38WZy+ftt3v6ArVW1qKsiH3QnAzcMojB0N7QeRK2DzBdmEx72BLkTa0V9Gch4iIiIhIIbiaeZVW77e6+UA7nU85T+sP7bujbO+AvXg4e9g11tfXl27duvH+++9bC7X/+c9/8PX1pWPHjkRFRZGVlcWyZcvw8vIiNDSU9u3bc+zYMTZt2oSDgwP16tVj1qxZfPXVV9x99934+fnh6uqKu7s7gYGBBTrfxYsXU69ePebMmQNAvXr1OHz4sM3MXYCMjAwWLlxI48aNrds6dOhgM+bdd9+lUqVK7Ny5kx49erB9+3aOHj3KyZMnqVatGgDTp0+ne/fudsW2Zs0aTCYT7733nnVG7m+//WZTbHR2dmbq1KnW70NCQoiKimLt2rU2hUxfX1/eeust63WcPXs2KSkpvPzyywC89NJLzJw5k2+//ZZ+/fpZHzdu3Di6du0KwHPPPUf//v354osvaNOmDQBDhgxh+fLldl8TgCVLlnD1qn0fLth7fnnx9fXF0dERLy8vmzyZN28eHTt25JVXXgGgbt26HDlyhDlz5thdqI2Li8PFxYVKlSrZbA8ICCAuLs46xt/fP8dj/f39bcYEBNi23KtUqRIuLi7WMUUtX4Xaf/3rX3zzzTcApKamYjKZePPNN1m3bl2OsSaTiX/+85+FE6WI2C0uMZUB7+3hbMJVQqp48sHQu6lSwbWkw5Ky7soFy0zZk7sshdmLv9jud3CCqs0tRdla91iKtM75/CQ9tBfUvx9ORVkWDqsQYGl3oJm0IiIiIiKF5tFHH2XYsGEsXLgQV1dX1qxZQ79+/ax9PGvVqmWzYHtAQACOjo42vWADAgKIj4/P83lym7HZoEEDm9m/2TM2jx07Rnh4uM3Yli1b5jimi4sLjRo1stkWHx/PpEmT+PLLLzl//jxms5mUlBROn7ZMJjl69Cg1atSwFmkBIiIi8oz9WseOHaNRo0a4ubnlGdvixYtZsmQJp06d4urVq6Snp9OkSRObMQ0aNMhxHcPCwqzfOzo6Urly5RzX9tpzzi4kNmzY0GbbtY+52TUBqFq1qr2XwO7zK4ijR4/ywAMP2Gxr06YNCxYswGw2W/OyIAzDsMm33GaeF2RMUcpXofaLL77giy++sNmW3XD3eirUihS/+ORUBizZw6mLKVT3def9oa3w93a7+QOlfMoyF7zoeTXB8tjsBcDif7Tdb3KAoMZ/FmbvhRp3g2uFW4/ZwdHSHkFEREREpAxxd3Jn74C9do09eP4go74YddNxCzsupHlAc7ueOz969uxJVlYWn332GeHh4ezatYt58+ZZ9zs727bMM5lMuW67vs/t9a6fsVmnTh02bdqUa4Ewt0JYbgvOu7vnbPMwePBgLly4wIIFC6hZsyaurq5ERESQnp5+w+Pkp+hmT2xr167l+eefZ+7cuURERODl5cWcOXPYu9c2Jwp6ba8dkx3L9duufczNrgnkr/WBPefn4OCQ47pkZGTkeXyw/7XPS2BgIOnp6SQkJNjMqo2Pj6d169bWMefPn8/x2AsXLliL34GBgTles4SEBDIyMnLMtC0qdhdqY2JiijIOEblFF6+k8diSvfx64Q+qVnTn/afuJsgn/72BpJw48skN2gjMyr2NQFoynN4DMTsthdnY/8H1PbMCwiyzZUPutRR93SsWetjmLIN9MZeIT07F38uNliG+ODoUzyeXIiIiIiIFZTKZ7G4/0Dq4NQEeAcSnxOfap9aEiQCPAFoHt7arR21+ubu706dPH9asWcMvv/xC3bp1ad785gXh/MqtIFuzZk1q1aqVY3v9+vXZtGmTzbYDBw7Y9Ty7du1i4cKF3HfffQCcOXOG33//3bo/NDSU06dPc+7cOWvv0d27d9t7GtSvX581a9aQlpaGq6trrrHt2rWL1q1bM2rUXwX4EydO2P0che1m1wTy1/rAnvPz8/MjNvavtUrMZjOHDx+26evq4uKC2Wy2eVxoaKj17v1sUVFR1K1b1+7ZtM2bN8fZ2Zlt27ZZWzHExsZy+PBhZs+eDVhmUScmJrJv3z7rjOi9e/eSmJhoLeZGRETw+uuvExsbS1BQEGBZYMzV1bVI/h/Jjd2F2po1axZlHCJyCy6npPPY0n38fP4KAd6uvD+0FdV97fslQcqhnzbBphHkKLQmxVoW7IpcCXU6w5m9lqJszNdwLhqyMm3HV6n7V2G21t/As0qRhr35cCxTNx4hNjHVui3Ix43JPUPpFhZUpM8tIiIiIlJcHB0cebHli4z5agwmTDbFWtOfi+lOaDmhSIq02R599FF69uzJjz/+yGOPPVZkz2Ov4cOHM2/ePCZMmMCQIUM4dOiQtefqzWa/3nnnnaxatYoWLVqQlJTECy+8YLOgWadOnahXrx6PP/44c+fOJSkpiYkTJ9od24ABA5g4cSLDhg3jxRdf5PTp07zxxhs2sd15552sXLmSLVu2EBISwqpVq9i/fz8hISH5vBKF42bXBPLX+sCe8+vQoQNjxozhs88+o3bt2syfP5/Lly/bHKdWrVp8/fXX9OvXD1dXV6pUqcLYsWMJDw/n1Vdf5ZFHHmH37t288847LFy40Pq4S5cuWYvtYGlHAZYZsIGBgfj4+DBkyBDGjh1L5cqV8fX1Zdy4cTRs2JBOnToBcNddd9GtWzeGDh3Ku+++C8CwYcPo0aMH9erVA6BLly6EhoYycOBA5syZw6VLlxg3bhxDhw7F29vb7ut1KxxuPkRESrPEqxkMXLqPo7FJVKngyvtD76ZmZc+SDktK0vZJ5CjSwp/bDFg/BGZUh5UPwK434Ow+S5G2Ui1oOhD6LIExP8HT+6HHPGjQu1iKtCNXR9sUacHSc3nk6mg2H469wSNFRERERMqeTjU7Ma/dPPw9bBc3CvAIYF67eXSq2alIn79Dhw74+vpy7NgxBgwYUKTPZY+QkBDWrVvHRx99RKNGjVi0aJG1mJo9i/VGli1bRkJCAk2bNmXgwIE8++yzNotGOTg4sGHDBtLS0mjZsiVPPfVUjkXK8uLt7c3GjRs5dOgQTZo0YeLEiUyaNAnA2rd2xIgR9OnTh0ceeYRWrVpx8eJFm9mnxe1m1yS/7Dm/J598kkGDBvH444/Ttm1bQkJCbGbTAkybNo2TJ09Su3Zt/Pz8AGjWrBlr167lww8/JCwsjEmTJjFt2jSbhcQ++eQTmjZtyv333w9Av379aNq0KYsXL7aOmT9/Pr179yYyMpI2bdrg4eHBxo0bbWblrlmzhoYNG9KlSxe6dOlCo0aNWLVqlXW/o6Mjn332GW5ubrRp04bIyEh69+5tLcwXB5OR38YP1zhw4AB79+4lISEhR/8Mk8lkXbGtrElKSsLHx4fExMRiq5jLrSsrr1thxpmcainSHjpzmcqeLnw47G7qBHjd/IGSb2Uhv6wxvuiFt6sd7QK8gi2zZUPuscycrVQyd06Yswz+NuvLHEXabCYg0MeNbyZ0KJdtEMpCbkHZiVNslYXXrSzEKLkrC69dWYhRciorr1tZiVNsFdbrlpqaSkxMDCEhITaLTOWXOctMdHw0F1Iu4OfhRzP/ZkU6k7Ysef3111m8eDFnzpwp6VByWLNmDU888QSJiYk5ZqqKXC8/7xf5Wkws29WrV+nTpw9bt261Nv3Nrvdm/3dZLtSKlAV/pGXy5PL9HDpzmYoezqx+qpWKtGK/rtPh7lFQTCtX5iXql99vWKQFyzzg2MRU9sVcIqJ25eILTERERESkiDk6OBIeGF7SYZQKCxcuJDw8nMqVK/Ptt98yZ84cnn766ZIOC4CVK1dyxx13ULVqVf73v/8xYcIEIiMjVaSVQlegQu20adPYunUrEydOpGPHjrRv354VK1bg7+/PjBkzuHr1KitXrizsWEXkT1fTzTy14gD7Tybg5ebE6iGtuCtIn+JLPgQ2KrEibXJqBtGnL7M/5hL7T17i4KkEux4Xn3zjYq6IiIiIiJRtx48f57XXXuPSpUvUqFGDsWPH8tJLLxX5806fPp3p06fnuu+ee+7h888/Jy4ujkmTJhEXF0dQUBAPP/xwvtoniNirQIXadevW8fDDDzNt2jQuXrwIWJoQd+jQgY4dOxIeHs7y5cuZMWNGoQYrIpCaYWbYqgPs/vUiFVydWPlkS8Kq+pR0WFKaeAVCejy596k1gXcw1GxdbOHEJ6Wy/2QC+09aCrNHY5PIKkDTHX+vgt9SJiIiIiIipdv8+fOZP39+sT/viBEjiIyMzHVf9ozZ8ePHM378+OIMS25TBSrUnjlzhjFjxgBYm/Kmp6dbDujkRP/+/Vm0aJEKtSKFLC3TzMjVB9l1/Hc8XBxZ/kQ4TWtUKumwpLTpNA02jcDS3fXaiuifM2i7zYQi6ntlGAYxv//xZ1HWUpw9dTElx7gavh60qFWJlrV8aVqjEoP+tY/ziak3Ki0T6ONGyxDfIolZRERERERuX76+vvj66m8NKR0KVKj18vLCbDZb/9vBwYFz585Z9/v4+BAXF1c4EYoIABnmLJ5+/zt2HLuAm7MDSweF06KWfphILurfBxVWwuYJkPTXezPewZYibWivQnuqTHMWR2KT2BdziQMnEzhw6hK/X0m3GWMyQf1Ab1rWqkSLWr6E1/Il0Md2duyUnqGMXB19o9Iyk3uGlsuFxERERERERESyFahQW7t2bX755RfAMqO2QYMGrFu3jieffBLDMPjoo4+oXr16oQYqcjvLNGcx+sNDbDtyHhcnB5Y8Hq5FlSRvob2g/v1wKgqunIcKAZZ2B7c4kzYlPZNDpy+z76SlMBt9OoGUdLPNGBcnB5pUq0h4iKUw27xmJbzdnPM8brewIBY91oypG4/YLCwW6OPG5J6hdAsLuqW4RUREREREREq7AhVqO3XqxL/+9S/mzZuHg4MDw4cP5+mnn6Z27dqYTCZiYmJu2IhZRPLHnGUw9j//47MfYnFxdODdgc35W50qJR2WlAUOjhByzy0d4uKVNA6cSrAs/HUqgR9/SyTzugaz3m5O1pmy4bUq0bCaD65O+S8IdwsLonNoIPtiLhGfnIq/l6XdgWbSioiIiIiIyO2gQIXaF198kYEDB5KVlYWDgwOjRo3i6tWrrFmzBkdHR4YOHaomyyKFICvLYML67/nvoXM4OZj456PNaF/Pv6TDkjLCnGXkq+hpGAZnE66yL+aSdeGvExf+yDEuyMfNUpQNsRRm6/p74VBIxVRHB5Nmi4uIiIiIiMhtqUCF2goVKlCvXj2bbWPHjmXs2LGFEpSIWIpmEz8+zLqDZ3F0MPFW/6Z0Dg0o6bCkjNh8ODZHG4Gg69oImLMMjsUlW4uy+09e4nxSWo5j1Q2oQItavrSs5UuLWpWoVsmj2M5DRERERERE5HZRoEKtiBQtwzCY8smPfLDvNA4mmBfZmPsaqken2GfbkTjGfXwc47rtcYmpjFgdzQNNgkm8msHBUwkkp2bajHF2NBFW1efPoqwvLWpWopKnS/EFLyIiIiIiInKbKlChdvLkyaxfv57Dhw/nur9hw4Y88sgj/OMf/7il4ERuR4Zh8PpnR1mx+xQmE8zu25gHmlQt6bCkDJn5+U8Y5OwRm124/e+hc9Ztni6ONKtZyVqYbVK9Iu4ut7bgmIiIiIiI3F7i4uIYOHAgUVFRODs7c/ny5ZIO6aZMJhMbNmygd+/eJR2KiJVDQR60YcMGOnfufMP9Xbp0Yd26dQUOSuR2Ys4y2H3iIv899Bu7T/zOrM0/seSbGACmP9iQvs2rlXCEUtbk1r7geoMiavLpM3/jf5O7sGpIK57pWIeI2pVVpBURERERKUaG2cwfe/eR+Oln/LF3H4bZXNIhFcj8+fOJjY3l0KFD/PzzzyUdjpRStWrVYsGCBfl+3EcffUTXrl2pUqUKJpOJQ4cO5RiTlpbGM888Q5UqVfD09KRXr16cPXvWZkxCQgIDBw7Ex8cHHx8fBg4cmONDhdOnT9OzZ088PT2pUqUKzz77LOnp6TZjfvjhB9q2bYu7uztVq1Zl2rRpGMb197QWTIFm1MbExFC/fv0b7q9Xrx5LliwpcFAit4vc+ohmm/ZAA/q3rFECUcntoFnNSoRV9SnpMEREREREbltJW7dyfvoMMuPirNucAgMJePklvLt0KcHI8u/EiRM0b96cOnXq3HCMyWQiJiaGWrVqFcpzms1mTCYTDg4FmoMoZcgff/xBmzZtePjhhxk6dGiuY0aPHs3GjRv58MMPqVy5MmPHjqVHjx4cPHgQR0fLhKQBAwZw9uxZNm/eDMCwYcMYOHAgGzduBCw5df/99+Pn58c333zDxYsXGTRoEIZh8PbbbwOQlJRE586dad++Pfv37+fnn39m8ODBeHp6FsraXQXO5rymsSckJGAuo58CiRSXbUfiGLk6OtciLYC/l2sxRyS3E38vt5IOQURERETktpW0dSu/PTfapkgLkHn+PL89N5qkrVuL5HnfffddqlatSlZWls32Xr16MWjQIKZMmUKTJk1YtmwZNWrUoEKFCowcORKz2czs2bMJDAzE39+f119/3frYWrVqsX79elauXInJZGLw4MEFiu2TTz6hTp06uLu70759e1asWIHJZLLWn5YvX07FihX59NNPCQ0NxdXVlVOnTrF//346d+5MlSpV8PHxoW3btkRHR9sc+/jx49x77724ubkRGhrKtm3b8hVbVFQUTZo0wc3NjRYtWvDxxx/bzOw0m80MGTKEkJAQ3N3dqVevHm+++abNMQYPHkzv3r2ZPn06AQEBVKxYkalTp5KZmckLL7yAr68v1apVY9myZdbHnDx5EpPJxNq1a7nnnntwd3cnPDycn3/+mf3799OiRQsqVKhAt27duHDhgvVx9lyT/LDn/Nq1a8fo0aNttvXu3duaD+3atePUqVM8//zzmEwmTCaTddz69etp0KABrq6u1KpVi7lz59ocZ+DAgUyaNIlOnTrlGl9iYiJLly5l7ty5dOrUiaZNm7J69Wp++OEHtm/fDsDRo0fZvHkzS5YsISIigoiICN577z0+/fRTjh07BsDWrVs5cuQIq1evpmnTpnTq1Im5c+fy3nvvkZSUBMCaNWtITU1l+fLlhIWF0adPH15++WXmzZtXKLNqC1SobdCggbXafD3DMPjkk0/ynHErItl9RHNnAqZuPII5q3CmzsvtJcDbFdMN9pmAIB83Wob4FmdIIiIiIiLlmmEYZKWk2PVlTk7m/GuvQ25FHcMADM6/Ph1zcrJdx8tPcejhhx/m999/Z8eOHdZtCQkJbNmyhUcffRSwzI79/PPP2bx5Mx988AHLli3j/vvv5+zZs+zcuZNZs2bxj3/8gz179gCWomC3bt2IjIwkNjY2RwHPHidPnqRv37707t2bQ4cOMXz4cCZOnJhjXEpKCjNmzGDJkiX8+OOP+Pv7k5yczKBBg9i1axd79uyhTp063HfffSQnJwOQlZVFnz59cHR0ZM+ePSxevJgJEybYHVtycjI9e/akYcOGREdH8+qrr+Z4fFZWFtWqVWPt2rUcOXKESZMm8fLLL7N27VqbcV9++SXnzp3j66+/Zt68eUyZMoUePXpQqVIl9u7dy4gRIxgxYgRnzpyxedzkyZP5xz/+QXR0NE5OTvTv35/x48fz5ptvsmvXLk6cOMGkSZNsYs7rmgB0796dChUq5PmV3/PLy0cffUS1atWYNm0asbGxxMbGAnDw4EEiIyPp168fP/zwA1OmTOGVV15h+fLldh/74MGDZGRk0OWamejBwcGEhYURFRUFwO7du/Hx8aFVq1bWMXfffTc+Pj42Y8LCwggODraO6dq1K2lpaRw8eNA6pm3btri6utqMOXfuHCdPnrQ75hspUOuDIUOGMHz4cJ544glmzpxJQEAAAOfPn2fChAns2bOHd95555aDEynPziel4eDqkes+A4hNTGVfzCUialcu3sCkzHuxe33GfXwcE9h8GJBdvJ3cMxRHhxuVckVEREREJL+Mq1c51qx5IR3MMrP25/CWdg2vF30Qk0fuf1tez9fXl27duvH+++/TsWNHAP7zn//g6+tLx44diYqKIisri2XLluHl5UVoaCjt27fn2LFjbNq0CQcHB+rVq8esWbP46quvuPvuu/Hz88PV1RV3d3cCAwMLdMqLFy+mXr16zJkzx3JO9epx+PBhm5m7ABkZGSxcuJDGjRtbt3Xo0MFmzLvvvkulSpXYuXMnPXr0YPv27Rw9epSTJ09SrZplDZjp06fTvXt3u2Jbs2YNJpOJ9957zzoj97fffrO5Bd/Z2ZmpU6davw8JCSEqKoq1a9cSGRlp3e7r68tbb71lvY6zZ88mJSWFl19+GYCXXnqJmTNn8u2339KvXz/r48aNG0fXrl0BeO655+jfvz9ffPEFbdq0ASx1umsLmze7JgBLlizh6tWrdl0De88vL76+vjg6OuLl5WWTJ/PmzaNjx4688sorANStW5cjR44wZ84cu2dnx8XF4eLiQqVKlWy2BwQEEPfnrPW4uDj8/f1zPNbf399mTHaNM1ulSpVwcXGxGXN9+47sx8TFxRESEmJXzDdSoELt0KFD2blzJytWrGDlypXWE42Pj8cwDB555BFGjhx5S4GJCMQn594WQSQvnUMDWVTBK0f/40AfNyb3DKVbWFAJRiciIiIiIiXp0UcfZdiwYSxcuBBXV1fWrFlDv379rH08a9WqhZeXl3V8QEAAjo6ONr1gAwICiI+Pz/N5unfvzq5du2y2NWjQwOaW9ytXrgBw7NgxwsPDbca2bJmzUO3i4kKjRo1stsXHxzNp0iS+/PJLzp8/j9lsJiUlhdOnTwOWW95r1KhhLdICRERE5Bn7tY4dO0ajRo1wc/urfVxusS1evJglS5Zw6tQprl69Snp6Ok2aNLEZ06BBgxzXMSwszPq9o6MjlStXznFtrz3n7KJgw4YNbbZd+5ibXROAqlWr2nsJ7D6/gjh69CgPPPCAzbY2bdqwYMECzGazNS8LwjAMm3y79r8Lc0z2rPbcHptfBSrUAqxevZpevXqxZs0afvnlFwzD4O677+bRRx+lb9++txyYiKiPqBRct7AgOocGsi/mEvHJqfh7WdodaCatiIiIiEjhM7m7Uy/6oF1jUw4c4Myw4TcdV/3/3sWjRQu7njs/evbsSVZWFp999hnh4eHs2rWLefPmWfc7OzvbHt9kynXb9X1ur3f9jM06deqwadOmXAuE1xfCsrddz93dPce4wYMHc+HCBRYsWEDNmjVxdXUlIiKC9PT0Gx4nPwU1e2Jbu3Ytzz//PHPnziUiIgIvLy/mzJnD3r17bcYV9NpeOyY7luu3XfuYm10TyL2Qfr3sQro95+fg4JDjumRkZOR5fLD/tc9LYGAg6enpJCQk2MyqjY+Pp3Xr1tYx58+fz/HYCxcuWIvfgYGBOV6zhIQEMjIybMbEXddbOrtIfv1s3IIocKEWIDIy0u4pziJiK8Dbld/TyLVPrQnL7Ef1EZVb4ehgUusMEREREZFiYDKZ7G4/4NmmDU6BgWSeP597n1qTCaeAADzbtMF0C7MJb8Td3Z0+ffpYJ97VrVuX5s0LqW3DNXIryNasWTPHbeMA9evXZ9OmTTbbDhw4YNfz7Nq1i4ULF3LfffcBcObMGX7//Xfr/tDQUE6fPs25c+esvUd3795t72lQv3591qxZQ1pamrUv6fWx7dq1i9atWzNq1CjrthMnTtj9HIXtZtcE8tf6wJ7z8/Pzs/adBcsCZIcPH6Z9+/bWbS4uLpjNZpvHhYaG8s0339hsi4qKom7dunbPpm3evDnOzs5s27bNWqeMjY3l8OHDzJ49G7DMok5MTGTfvn3WGdF79+4lMTHRWsyNiIjg9ddfJzY2lqAgy52oW7duxdXV1fr/SEREBC+//DLp6em4uLhYxwQHB+ea2/lVoMXEROTWvdjdsuDe9Z/jqY+oiIiIiIhI+WVydCTg5Zf+/Oa6v/n+/D7g5ZeKpEib7dFHH+Wzzz5j2bJlPPbYY0X2PPYaPnw4P/30ExMmTODnn39m7dq11p6rN5v9euedd7Jq1SqOHj3K3r17efTRR3G/ZpZxp06dqFevHo8//jj/+9//2LVrV64Lld3IgAEDyMrKYtiwYRw9epQtW7bwxhtv2MR25513cuDAAbZs2cLPP//MK6+8wv79+/N5FQrPza4JWArpd955Z55f1x7vZufXoUMHPvvsMz777DN++uknRo0axeXLl23G1KpVi6+//prffvvNWjgeO3YsX3zxBa+++io///wzK1as4J133mHcuHHWx126dIlDhw5x5MgRwNKO4tChQ9aZrT4+PgwZMsR6rO+++47HHnuMhg0b0qlTJwDuuusuunXrxtChQ9mzZw979uxh6NCh9OjRg3r16gHQpUsXQkNDGThwIN999x1ffPEF48aNY+jQoXh7ewOWfHB1dWXw4MEcPnyYDRs2MH36dMaMGVMorQ9uqVB74MAB/vnPf/Laa68xbdo0m69XX331loMTKc86hway6LFmBPrYtjcI9HFj0WPN1EdURERERESknPLu0oWqby7A6bpbpZ0CAqj65gK8r1m9vih06NABX19fjh07xoABA4r0uewREhLCunXr+Oijj2jUqBGLFi2yFlOzZ7HeyLJly0hISKBp06YMHDiQZ5991mbRKAcHBzZs2EBaWhotW7bkqaeeyrFIWV68vb3ZuHEjhw4dokmTJkycOJFJkyYBWPvWjhgxgj59+vDII4/QqlUrLl68aDP7tLjd7Jrklz3n9+STTzJo0CAef/xx2rZtS0hIiM1sWoBp06Zx8uRJateujZ+fHwDNmjVj7dq1fPjhh4SFhTFp0iSmTZtms5DYJ598QtOmTbn//vsB6NevH02bNmXx4sXWMfPnz6d3795ERkbSpk0bPDw82Lhxo82s3DVr1tCwYUO6dOlCly5daNSoEatWrbLud3R05LPPPsPNzY02bdoQGRlJ7969rYV5sBSFt23bxtmzZ2nRogWjRo1izJgxjBkzpsDX91omI7+NH4CrV6/Sp08ftm7dau0lcW3j3Oxt109nLiuSkpLw8fEhMTHRWjGX0q+svG7Xx2nOMtRHtAwoC/lVFmKUnMrK61ZW4hRbZeF1KwsxSu7KwmtXFmKUnMrK61ZW4hRbhfW6paamEhMTQ0hIiM0iU/llmM2kHDhI5oULOPn54dGieZHOpC1LXn/9dRYvXsyZM2dKOpQc1qxZwxNPPEFiYmKOmaoi18vP+0WBZtROmzaNrVu3MnHiRHbs2IFhGKxYsYLPP/+ce+65h/DwcOt05Pz4+uuv6dmzJ8HBwZhMJj7++GOb/YZhMGXKFIKDg3F3d6ddu3b8+OOPNmPS0tJ45plnqFKlCp6envTq1YuzZ88W5DRFikV2H9EHmlQlonZlFWlFRERERERuEyZHRzxbtcSnx/14tmp5WxdpFy5cyP79+/n1119ZtWoVc+bMYdCgQSUdFgArV67km2++ISYmho8//pgJEyYQGRmpIq0UugIVatetW8fDDz/MtGnTCAsLAyy9Lbp27cr27dtJT0+39hLJjz/++IPGjRvzzjvv5Lp/9uzZzJs3j3feeYf9+/cTGBhI586dSU5Oto4ZPXo0GzZs4MMPP+Sbb77hypUr9OjRo8zO7hURERERERERKe+OHz/OAw88QGhoKK+++ipjx45lypQpRf6806dPp0KFCrl+de/eHYC4uDgee+wx7rrrLp5//nkefvhh/u///q/IY5Pbj1NBHnTmzBlr74XsXg/p6emWAzo50b9/fxYtWsSMGTPyddzu3btb/ye4nmEYLFiwgIkTJ9KnTx8AVqxYQUBAAO+//z7Dhw8nMTGRpUuXsmrVKmuz4NWrV1O9enW2b99O165dC3K6IiIiIiIiIiJShObPn8/8+fOL/XlHjBhBZGRkrvuyZ8yOHz+e8ePHF2dYcpsqUKHWy8vLOkPVy8sLBwcHzp07Z93v4+NjXXmtsMTExBAXF0eXaxpqu7q60rZtW6Kiohg+fDgHDx4kIyPDZkxwcDBhYWFERUWpUCsiIiIiIiIiIla+vr74+vqWdBgiQAELtbVr1+aXX34BLDNqGzRowLp163jyyScxDIOPPvqI6tWrF2qg2YXfgOtWRAwICODUqVPWMS4uLlSqVCnHmLwKx2lpaaSlpVm/T0pKKqywRZRfUmSUW1KUlF9SVJRbUpSUX1KUlF+SmwKszy4it5n8vE8UqEdtp06d+M9//kNWVhYAw4cPZ/PmzdSuXZs6deqwfft2hgwZUpBD35TJZLvQkmEYObZd72ZjZsyYgY+Pj/WrsIvMcntTfklRUW5JUVJ+SVFRbklRUn5JUVJ+ybWcnZ0BSElJKeFIRKS0y36fyH7fyIvJKMDHP1euXOG3336jdu3aODlZJuXOnTuXNWvW4OjoSN++fRk/fvxNC6h5BmYysWHDBnr37g3Ar7/+Su3atYmOjqZp06bWcQ888AAVK1ZkxYoVfPnll3Ts2JFLly7ZzKpt3LgxvXv3ZurUqbk+V26fjFavXp3ExES8vb0LfA5SvJKSkvDx8Sl1r5vyq3wojfml3CofSmNugfKrvCiN+aXcKj+UX1JUSmNugfKrvCjM/IqNjeXy5cv4+/vj4eFxSzUQESl/DMMgJSWF+Ph4KlasSFBQ0E0fU6DWBxUqVKBevXo228aOHcvYsWMLcji7hISEEBgYyLZt26yF2vT0dHbu3MmsWbMAaN68Oc7Ozmzbts3aCDo2NpbDhw8ze/bsGx7b1dUVV1fXIotdbm/KLykqyi0pSsovKSrKLSlKyi8pSsovuV5gYCAA8fHxJRyJiJRmFStWtL5f3EyBCrVF5cqVK9bet2BZQOzQoUP4+vpSo0YNRo8ezfTp06lTpw516tRh+vTpeHh4MGDAAMCyiNmQIUMYO3YslStXxtfXl3HjxtGwYUM6depUUqclIiIiIiIiIuWMyWQiKCgIf39/MjIySjocESmFnJ2dcXR0tHt8gQu1V69eZcGCBWzYsIETJ05gMpm444476NOnD8899xzu7u75PuaBAwdo37699fsxY8YAMGjQIJYvX8748eO5evUqo0aNIiEhgVatWrF161a8vLysj5k/fz5OTk5ERkZy9epVOnbsyPLly/N1UURERERERERE7OHo6Kiag4gUigIVauPj42nfvj1Hjx7F29ubO+64A8Mw+Pnnn3n55ZdZvXo1O3bswM/PL1/HbdeuXZ4roZlMJqZMmcKUKVNuOMbNzY23336bt99+O1/PLSIiIiIiIiIiIlJSHAryoBdeeIGffvqJefPmER8fT3R0NN999x3x8fHMnTuXo0eP8sILLxR2rCIiIiIiIiIiIiLlUoFm1H766acMGTKE0aNH22x3cXHh+eef58cff2TDhg2FEZ+IiIiIiIiIiIhIuVegGbXp6ek0a9bshvtbtGhBenp6gYOSW2POMth94iL/PfQbu09cxJx143YSIiIiIiIiIiIiUvIKNKM2PDyc6OjoG+4/ePAgLVu2LHBQUnCbD8cydeMRYhNTrduCfNyY3DOUbmFBJRiZiIiIiIiIiIiI3EiBZtTOnTuXdevW8fbbb5ORkWHdnpmZyZtvvslHH33E3LlzCy1Isc/mw7GMXB1tU6QFiEtMZeTqaDYfji2hyERERERERERERCQvBZpRO3bsWCpXrszo0aOZNGkSd9xxByaTiRMnTpCUlETt2rUZM2aMzWNMJhNffPFFoQQtOZmzDKZuPEJuTQ4MwARM3XiEzqGBODqYijk6ERERERERERERyUuBCrW//vorJpOJGjVqAHDp0iUAKlasSMWKFcnIyCAmJqbwopSb2hdzKcdM2msZQGxiKvtiLhFRu3LxBSYiIiIiIiIiIiI3VaBC7cmTJws5DLlV8ck3LtIWZJyIiIiIiIiIiIgUnwIVaqX0SM0ws/lwHO/uPGHXeH8vtyKOSERERERERERERPJLhdoy6qe4JD7cd4YN3/1G4tWMm443AYE+brQM8S364ERERERERERERCRf7CrUdujQId8H1uJhhe+PtEw+/f4cH+w7w6Ezl63bq1Z0J7JFdfy9XXj5o8MANouKZS8dNrlnqBYSExERERERERERKYXsKtRmLx4mxc8wDL4/m8iH+8/wyaHf+CPdDICTg4nOoQH0a1mDv91ZxVqAreThwtSNR2wWFgv0cWNyz1C6hQWVyDmIiIiIiIiIiIhI3uwq1BZk8bA//vgj34+RvyRezeC/h37jg31nOBqbZN0eUsWTR8Kr81Czavh5ueZ4XLewIDqHBrIv5hLxyan4e1naHWgmrYiIiIiIiIiISOlV6D1qo6KiWLp0KevWrSMxMbGwD1+uGYbBgVMJfLDvNJt+iCU1IwsAFycHuocF0i+8Bnff4XvT2c2ODiYialcujpBFRERERERERESkEBRKoTY+Pp4VK1awbNkyfv75ZwzDoFGjRoVx6NvCpT/S+Sj6LB/uP8Mv8Ves2+sFeNGvZXUebFqVih4uJRihiIiIiIiIiIiIFKUCF2qzsrLYtGkTS5cuZdOmTWRmZhIWFsaMGTN46KGHqF27dmHGWe5kZRlEnbjIB/tPs/XHODLMluW/3J0d6dk4iH4ta9C0ekX1BhYREREREREREbkN5LtQe/z4cZYtW8bKlSuJjY0lKCiI/v37s2rVKiZPnkyfPn2KIs5yIz4plf8cPMu/95/h9KUU6/aGVX3o17I6vRoH4+XmXIIRioiIiIiIiIiISHGzu1C7cuVKli5dyq5du3B1daVXr14MHjyYrl27EhMTw8qVK4syzjLNnGXw1bF4Pth3hh3H4jFnWWbPerk68UDTYPqF1yCsqk8JRykiIiIiIiIiIiIlxe5C7eDBg7nzzjtZtGgR/fr1w8dHhcWbOZuQwtr9Z1h74CxxSanW7S1qVqJfyxrc3zAIdxfHEoxQRERERERERERESgO7C7Vubm6cOHGCf//733h4ePDQQw/h4eFRlLGVSemZWXxx9Dwf7D/DruMXMCyTZ6nk4UyfZtXoF16dOgFeJRukiIiIiIiIiIiIlCp2F2rj4uJYs2YNy5YtY9CgQfz973+nb9++DB48mODg4KKMscSZswz2xVwiPjkVfy83Wob44uhgu8jXrxeu8O/9Z1gffZbfr6Rbt7euXZl+LWvQtUEArk6aPSsiIiIiIiIiIiI52V2o9fb2ZuTIkYwcOZLvv/+eJUuW8P7777NixQr8/PwwmUwkJCQUZawlYvPhWKZuPEJs4l+tC4J83JjcM5R29fzZfDiOD/adZm/MJet+Py9XHm5ejUfCq1OzsmdJhC0iIiIiIiIiIiJliN2F2ms1atSIt956izfeeIOPPvqIpUuX8uWXXzJs2DAWLFjAww8/zEMPPUSDBg0KO95ite1IHOM+Po5x3fbYxFRGrI7Gw8WRlHQzAA4maFvXj34ta9Chvj/Ojg7FH7CIiIiIiIiIiIiUSQUq1GZzcXGhX79+9OvXj1OnTrFs2TJWrFjBlClTmDZtGpmZmYUVZ4mY+flPGNy4XUFKupkgb1ceaVmDyBbVCa7oXozRiYiIiIiIiIiISHlRaNM+a9asydSpU4mJieHzzz/noYceKqxDl5jzSWk3HfNGZBNGd6qrIq2IiIiIiIiIiIgU2C3NqM2NyWSia9eudO3atbAPXSr9fuXmxVwRERERERERERGRvKiR6i3y93Ir6RBERERERERERESkjFOhNg8B3q6YbrDPBAT5uNEyxLc4QxIREREREREREZFySIXaPLzYvT5AjmJt9veTe4bi6HCjUq6IiIiIiIiIiIiIfVSozUPn0EAWPdaMQB/b9gaBPm4seqwZ3cKCSigyERERERERERERKU8KfTGx8qZbWBCdQwPZF3OJ+ORU/L0s7Q40k1ZEREREREREREQKiwq1uTAMA4CkpCTrtgZ+zjTwcwbgjyvJJRKX5C379cp+/Uqr3PJLSr+ykF/KrbKpLOQWKL/KqrKQX8qtskv5JUWlLOQWKL/KqrKSXyJye1KhNhfJyZZCbPXq1Us4EimI5ORkfHx8SjqMG1J+lW2lOb+UW2Vbac4tUH6VdaU5v5RbZZ/yS4pKac4tUH6VdaU9v0Tk9mQy9DFSDllZWZw7dw4vLy9MJkuLg6SkJKpXr86ZM2fw9vYu4QhvrqzFm1+5nZ9hGCQnJxMcHIyDQ+ltv6z8Kv3Kan6Vh9yCshmzvcpqbkH5yK+yFm9+ldX8Kg+5BWUz5vxQfpWsshizvcpqbkH5yK+yFm9+leX8EpHbk2bU5sLBwYFq1arlus/b27tM/QAra/Hm1/XnVxY+EVV+lR1lLb/KU25B2YzZXmUtt6B85VdZize/ylp+lafcgrIZc34ov0pWWYzZXmUtt6B85VdZize/ymJ+icjtSR8fiYiIiIiIiIiIiJQwFWpFRERERERERERESpgKtXZydXVl8uTJuLq6lnQodilr8eZXeTu/snY+ZS3e/CpP51cWz6Usxmyv8nZuZe18ylq8+VWezq8snktZjDk/ytP5lcVzKYsx26u8nVtZO5+yFm9+lffzE5HyR4uJiYiIiIiIiIiIiJQwzagVERERERERERERKWEq1IqIiIiIiIiIiIiUMBVqRUREREREREREREqYCrUiIiIiIiIiIiIiJey2KtR+/fXX9OzZk+DgYEwmEx9//LHNfsMwmDJlCsHBwbi7u9OuXTt+/PFHmzFpaWk888wzVKlSBU9PT3r16sXZs2dtxiQkJDBw4EB8fHzw8fFh4MCBXL58Od/xTpkyBZPJZPMVGBhYauPNS2m69qdPn6Znz554enpSpUoVnn32WdLT08vN+dmjPOUWlK7rr/xSfim/lF/2Kk3XXrlVvnILStf1V34pv5Rfyi97laZrXxS5JSJyU8ZtZNOmTcbEiRON9evXG4CxYcMGm/0zZ840vLy8jPXr1xs//PCD8cgjjxhBQUFGUlKSdcyIESOMqlWrGtu2bTOio6ON9u3bG40bNzYyMzOtY7p162aEhYUZUVFRRlRUlBEWFmb06NEj3/FOnjzZaNCggREbG2v9io+PL7Xx5qW0XPvMzEwjLCzMaN++vREdHW1s27bNCA4ONp5++ulycX72Kk+5ZRil5/orvyyUX8ov5Zd9Ssu1V25ZlKfcMozSc/2VXxbKL+WX8ss+peXaF1VuiYjczG1VqL3W9W/6WVlZRmBgoDFz5kzrttTUVMPHx8dYvHixYRiGcfnyZcPZ2dn48MMPrWN+++03w8HBwdi8ebNhGIZx5MgRAzD27NljHbN7924DMH766ad8xTh58mSjcePGue4rjfHaqySv/aZNmwwHBwfjt99+s4754IMPDFdXVyMxMbHMn5+9ymtuGYbyqyjPz17KL+WX8iv/lFsl/1qV19wyDOVXUZ6fvZRfyi/lV/6V99wSEcnNbdX6IC8xMTHExcXRpUsX6zZXV1fatm1LVFQUAAcPHiQjI8NmTHBwMGFhYdYxu3fvxsfHh1atWlnH3H333fj4+FjH5Mfx48cJDg4mJCSEfv368euvv5bqeAuiOM9l9+7dhIWFERwcbB3TtWtX0tLSOHjwYJk/v/y4HXILlF+FeX75ofxSfim/bo1yS7lVlJRfyq+ipPxSfhWV8p5bIiJwm/WozUtcXBwAAQEBNtsDAgKs++Li4nBxcaFSpUp5jvH3989xfH9/f+sYe7Vq1YqVK1eyZcsW3nvvPeLi4mjdujUXL14slfEWVHGeS1xcXI7nqVSpEi4uLkV2vqXxtbpdcis7juzYrqX8Un4VBuWX8quoKLeUW0VJ+aX8KkrKL+VXUSnvuSUiAuBU0gGUNiaTyeZ7wzBybLve9WNyG2/Pca7XvXt36383bNiQiIgIateuzYoVK7j77rtLXby3qrjOpaTOtzS9VrdbbuUWi/JL+VWYlF85Kb8Kh3IrJ+VW4VF+5aT8KjzKr5yUX4WjvOeWiNzeNKP2T9mrYl7/6Vh8fLz1k7TAwEDS09NJSEjIc8z58+dzHP/ChQs5PpHLL09PTxo2bMjx48fLRLz2Ks5zCQwMzPE8CQkJZGRkFNn5loXXqrzmVnYcoPxSfhUN5VfJv17lNb+UWyX/WpXX3MqOA5Rfyq+iofwq+dervOZXec8tERFQodYqJCSEwMBAtm3bZt2Wnp7Ozp07ad26NQDNmzfH2dnZZkxsbCyHDx+2jomIiCAxMZF9+/ZZx+zdu5fExETrmIJKS0vj6NGjBAUFlYl47VWc5xIREcHhw4eJjY21jtm6dSuurq40b968zJ9fQZXX3ALlV2GeX0Epv5Rfyq/8U26V/GtVXnMLlF+FeX4FpfxSfim/8q+855aICAC3shJZWZOcnGx89913xnfffWcAxrx584zvvvvOOHXqlGEYhjFz5kzDx8fH+Oijj4wffvjB6N+/vxEUFGQkJSVZjzFixAijWrVqxvbt243o6GijQ4cORuPGjY3MzEzrmG7duhmNGjUydu/ebezevdto2LCh0aNHj3zHO3bsWOOrr74yfv31V2PPnj1Gjx49DC8vL+PkyZOlMt68lJZrn5mZaYSFhRkdO3Y0oqOjje3btxvVqlUznn766XJxfvYqT7llGKXn+iu/LJRfyi/ll31Ky7VXblmUp9wyjNJz/ZVfFsov5Zfyyz6l5doXVW6JiNzMbVWo3bFjhwHk+Bo0aJBhGIaRlZVlTJ482QgMDDRcXV2Ne++91/jhhx9sjnH16lXj6aefNnx9fQ13d3ejR48exunTp23GXLx40Xj00UcNLy8vw8vLy3j00UeNhISEfMf7yCOPGEFBQYazs7MRHBxs9OnTx/jxxx+t+0tbvHkpTdf+1KlTxv3332+4u7sbvr6+xtNPP22kpqaWm/OzR3nKLcMoXddf+aX8Un4pv+xVmq69cqt85ZZhlK7rr/xSfim/lF/2Kk3XvihyS0TkZkyGYRj5m4MrIiIiIiIiIiIiIoVJPWpFRERERERERERESpgKtSIiIiIiIiIiIiIlTIVaERERERERERERkRKmQq2IiIiIiIiIiIhICVOhVkRERERERERERKSEqVArIiIiIiIiIiIiUsJUqBUREREREREREREpYSrUioiIiIiIiIiIiJQwFWpFRERERERERERESpgKtSIiIiIiIiIiIiIlTIVaERERERERERERkRKmQq2IiIiIiIiIiIhICft/NUk/jpope98AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x600 with 21 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yaxis_type = 'abs'\n",
    "assert(yaxis_type in ['delta_random', 'abs'])\n",
    "datasets = ['flan_v2', 'dolly', 'stanford_alpaca', 'oasst1', 'ultrachat200kv2', 'wizardlmv2', 'sharegptv2']\n",
    "task_names = ['MMLU/0-shot', 'GSM/CoT', 'BBH/Direct', 'TydiQA/GP', 'Codex-Eval/Pass@1', 'AlpacaFarm/WR*',]\n",
    "task_names = ['nonchat', 'AlpacaFarm/WR*', 'AlpacaFarm/Len',]\n",
    "# task_names = ['AlpacaFarm/WR*',]\n",
    "\n",
    "\n",
    "ncols = len(datasets)\n",
    "nrows = len(task_names)\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(2*ncols,2*nrows), sharey='row', sharex=True)\n",
    "\n",
    "xs_possible = []\n",
    "\n",
    "for axi, task_name in enumerate(task_names):\n",
    "    d = D[task_name]\n",
    "    for axj, dataset in enumerate(datasets):\n",
    "        ax = axs.reshape(nrows, ncols)[axi, axj]\n",
    "        sort_by_types = sorted(set(x.sort_by_type for x in d.keys()))\n",
    "\n",
    "        for i, sort_by_type in enumerate(sort_by_types):\n",
    "            xs = sorted([x.subset_size for x in d.keys()\n",
    "                         if x.dataset == dataset and x.sort_by_type==sort_by_type])\n",
    "            xs_possible += list(set(xs) - set(xs_possible))\n",
    "            ys = [d[DKey(sort_by_type, dataset, x)] for x in xs]\n",
    "            if yaxis_type == 'delta_random':\n",
    "                ys = [y-d[DKey('vmf+grad+gamma=1', dataset, x)] for x, y in zip(xs, ys)] \n",
    "            ax.plot(xs, ys, 'o-', label=sort_by_type)\n",
    "        \n",
    "#         ax.set_yscale('log')\n",
    "            \n",
    "for axi, task_name in enumerate(task_names):\n",
    "    axs.reshape(nrows, ncols)[axi, 0].set_ylabel('△ '+task_name if yaxis_type.startswith('delta') else task_name, fontsize=13)\n",
    "    axs.reshape(nrows, ncols)[axi, -1].legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "for axj, dataset in enumerate(datasets):\n",
    "    axs.reshape(nrows, ncols)[0, axj].set_title(dataset, fontsize=15)\n",
    "    axs.reshape(nrows, ncols)[0, axj].set_xticks(xs_possible, xs_possible)\n",
    "\n",
    "space = 0.05\n",
    "fig.subplots_adjust(wspace=space, hspace=space)  # Adjust the value as needed\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa5fdb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.1 , 0.2 , 0.4 , 0.8 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.array([1_000, 5_000, 10_000, 20_000, 40_000])\n",
    "xs/50_000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
