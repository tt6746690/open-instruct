{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "    get_run_statistics)\n",
    "import pandas as pd\n",
    "import json\n",
    "import platform\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "arch = platform.uname().processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir)\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|▏         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "# !cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"Running on $SLURM_JOB_NODELIST\"\n",
      "echo \"======\"\n",
      "\n",
      "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
      "master_port=10002\n",
      "RDZV_ENDPOINT=$master_addr:$master_port\n",
      "\n",
      "source ~/.profile\n",
      "conda activate open-instruct\n",
      "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
      "\n",
      "echo {cmd}\n",
      "echo \"======\"\n",
      "srun {cmd}\n",
      "\n",
      "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=256), micro-bsz=1,  grad-ckpt,  a100_80gb, 37s/it, 43hrs# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n",
    "\n",
    "# aimos\n",
    "# shared: humanmix, max_sequence_length=2048. 1 node = 6x v100_32gb\n",
    "\n",
    "# take-aways: \n",
    "#   (1) fsdp (v4.28.1, v4.32.0.dev0 are pretty similar in terms of speed. don't use v4.31.0)\n",
    "#   (2) micro-bsz=1->2 seems to be the best here. leads to ~30% reduction in runtime.\n",
    "#   (3) increasing #nodes almost linear reduction in time, e.g., Use 4x nodes cost 30% time (25% if linear.)\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=none, nodes=1, 74s/it, 89hrs, loss=0\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 72s/it, 86hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=none, nodes=1, 52s/it, 66hrs, loss=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 49s/it, 61hrs\n",
    "# - resume from lastest checkpt (trained 4.28.1, resume 4.32.0.dev0), did\n",
    "#   did not found `optimizer.bin` \n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# # nodes > 1\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=4, 14s/it, 18hrs\n",
    "\n",
    "# take-away: \n",
    "#   (1) torch_dtype=float16 gives loss=0. setting torch_dtype=float32 solves the issue but takes more memory and compute\n",
    "#   (2) mbsz=2 oom for nodes=1 but works fine with nodes=2. more nodes -> potentially larger mbsz.\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, 139s/it, 166hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=2, 33s/it, 41hrs, loss!=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 17s/it, 21hrs\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 18s/it, 21hrs\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=5, 4s/it, 21.7hrs\n",
    "\n",
    "\n",
    "#\n",
    "# deepspeed\n",
    "# shared: llama7b, deepspeed, grad-ckpt\n",
    "# take-aways\n",
    "#   (1) with deepspeed, using mixed-precision gives x50% speed improvement\n",
    "#   (2) no loss overflow issues. deepspeed has good mixed-precision integration, loss_scaler handles it.\n",
    "# \n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 161s/it, 192hrs, loss ok, loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=none, nodes=1, 259s/it, 309hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=fp16, nodes=1, 169s/it, 202hrs, loss ok. loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=none, nodes=1, 258s/it, 307hrs, loss ok.\n",
    "#\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 108s/it, 135hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3,offload), float32, mp=fp16, nodes=1,  96s/it, 120hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3), float32, mp=fp16, nodes=1, 123s/it, 147hrs, loss ok.\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=1, 66s/it, 83hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=1, oom\n",
    "#\n",
    "# nodes>1\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=2, error with downloading config.json\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=4, \n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=4, 16s/it, 21hrs\n",
    "# llama7b, micro-bsz=6, deepspeed(s=3), float32, mp=fp16, nodes=4,  3s/it, 17hrs\n",
    "\n",
    "shell_scripts_template_slurm = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template_lsf = \"\"\"\n",
    "echo \"Running on $LSB_DJOB_HOSTFILE\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\")\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "echo {cmd}\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$LSB_JOBID*.out\" ] && mv {log_dir}/$LSB_JOBID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf\n",
    "\n",
    "print(shell_scripts_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s/it, 21.7hrs\n"
     ]
    }
   ],
   "source": [
    "t = '5:31:18'\n",
    "n = 1148\n",
    "total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a669464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot\n",
      "sharegpt\n",
      "dolly\n",
      "gpt4_alpaca\n",
      "flan_v2\n",
      "super_ni\n",
      "stanford_alpaca\n",
      "baize\n",
      "code_alpaca\n",
      "self_instruct\n",
      "unnatural_instructions\n",
      "oasst1\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "p = '../data/processed/'\n",
    "for x in os.listdir(p):\n",
    "    y = os.path.join(p, x)\n",
    "    if os.path.isdir(y):\n",
    "        d = os.path.join(y, os.listdir(y)[0])\n",
    "    else:\n",
    "        continue\n",
    "    d = d[3:]\n",
    "    if 'shuffled' in d:\n",
    "        continue\n",
    "#     print(f\"train_file = '{d}'; abbr_train_file = '{x}'\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results/baselines/huggyllama/llama-7b using 6 GPUs, 2 batch size per GPU, 2 gradient accumulation steps.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"ft:unnatural_instructions\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 3\n",
      "}\n",
      "[{'args': 'sbatch --job-name=ft:unnatural_instructions --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_1:3.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp3faerpqy', 'job_id': 694108}, {'args': 'sbatch --job-name=ft:unnatural_instructions --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_2:3.out --time=6:00:00 --dependency=afterany:694108 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp2u96uoph', 'job_id': 694109}, {'args': 'sbatch --job-name=ft:unnatural_instructions --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J_3:3.out --time=6:00:00 --dependency=afterany:694109 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp4hzk1u3m', 'job_id': 694110}]\n"
     ]
    }
   ],
   "source": [
    "job_name = 'ft'\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "job_duration = 6 if arch == 'ppc64le' else 12\n",
    "job_duration = 6\n",
    "# shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "nodes = 5 # 128/(5*6*2)~=2.1\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem = 512 if arch == 'ppc64le' else 64\n",
    "num_gpus = 1; gpu_type = 'a100_80gb'\n",
    "num_gpus = 6; gpu_type = 'v100'\n",
    "debug_mode = test_run\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; abbr_model_name = 'llama-7b'\n",
    "model_name_or_path = 'results/baselines/huggyllama/llama-7b'; abbr_model_name = 'llama-7b'\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'\n",
    "max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; abbr_model_name = 'gpt2; max_seq_length = 1024\n",
    "\n",
    "\n",
    "train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "# train_file = 'data/processed/dolly_oasst1.jsonl'; abbr_train_file = 'dolly:oasst1'\n",
    "# train_file = 'data/processed/cot_flanv2.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "\n",
    "# train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "\n",
    "# train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "\n",
    "# train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "# train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "job_name += ':'+abbr_train_file\n",
    "\n",
    "\n",
    "num_train_epochs = 2\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128 # 128\n",
    "gradient_acc_steps = int(total_batch_size/(num_gpus*nodes)/batch_size_per_gpu)\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "\n",
    "mixed_precision = 'fp16' if arch == 'ppc64le' else 'bf16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = True\n",
    "load_in_8bit = False\n",
    "\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "if test_run:\n",
    "    output_dirname = 'jpt_'+output_dirname\n",
    "# if not test_run:\n",
    "#     output_dirname += \\\n",
    "#         ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "#         ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "#         '_gradckpt='+str(gradient_checkpointing)+\\\n",
    "#         '_mbsz='+str(batch_size_per_gpu)+\\\n",
    "#         '_dtype='+torch_dtype+\\\n",
    "#         ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "#         '_seqlen='+str(max_seq_length)+\\\n",
    "#         '_nodes='+str(nodes)\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "#     {'--tf32 True' if arch == 'x86_64' else ''} \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "{'!cd .. && ' if test_run else ''}{exe}\n",
    "    open_instruct/finetune_trainer.py \\\n",
    "    --model_name_or_path={model_name_or_path} \\\n",
    "    --tokenizer_name={model_name_or_path} \\\n",
    "    {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "    --use_fast_tokenizer=True \\\n",
    "    --train_file={train_file} \\\n",
    "    --max_seq_length={max_seq_length} \\\n",
    "    {'--use_lora' if use_lora else ''}\n",
    "    {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "    {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "    {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers=16 \\\n",
    "    --per_device_train_batch_size={batch_size_per_gpu} \\\n",
    "    --gradient_accumulation_steps={gradient_acc_steps} \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --lr_scheduler_type=linear \\\n",
    "    --warmup_ratio=0.03 \\\n",
    "    --optim={optimizer} \\\n",
    "    --weight_decay=0. \\\n",
    "    --evaluation_strategy=\"no\" \\\n",
    "    --logging_steps=1 \\\n",
    "    --save_strategy={save_strategy} \\\n",
    "    --save_steps={save_steps} \\\n",
    "    --save_total_limit=1 \\\n",
    "    --num_train_epochs={num_train_epochs} \\\n",
    "    {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "    {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "        if fsdp else ''}\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "    --report_to=tensorboard \\\n",
    "    --torch_dtype={torch_dtype} \\\n",
    "    --dataloader_num_workers=8 \\\n",
    "    {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "    {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "    --output_dir=\"{output_dir}\" \\\n",
    "    {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "\"\"\" \n",
    "#    --overwrite_cache\n",
    "\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "if test_run:\n",
    "    print()\n",
    "    print(cmd)\n",
    "\n",
    "shell_scripts = shell_scripts_template.format(\n",
    "    cmd=cmd,\n",
    "    log_dir=os.getcwd(),\n",
    "    save_dir=output_dir)\n",
    "out = submit_job(\n",
    "    shell_scripts, \n",
    "    job_name=job_name, \n",
    "    nodes=nodes,\n",
    "    num_cpus=num_cpus,\n",
    "    cpu_mem=cpu_mem,\n",
    "    num_gpus=num_gpus,\n",
    "    gpu_type=gpu_type,\n",
    "    test_run=test_run,\n",
    "    job_duration=job_duration,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7fbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(error_file, 'r') as f:\n",
    "    s = json.load(f)\n",
    "s = s['message']\n",
    "print(s['message'])\n",
    "print()\n",
    "print(s['extraInfo']['timestamp'])\n",
    "print(s['extraInfo']['py_callstack'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resource_for_task(task_name):\n",
    "    batch_size = 10\n",
    "    if task_name == 'gsm':\n",
    "        job_duration = 1 # 10min for n=200\n",
    "    elif task_name == 'bbh_s=0':\n",
    "        job_duration = 1\n",
    "    elif task_name == 'bbh_s=3':\n",
    "        job_duration = 1\n",
    "        batch_size = 5 # for longer prompts.\n",
    "    elif task_name == 'mmlu':\n",
    "        job_duration = 1\n",
    "    elif task_name == 'humaneval':\n",
    "        job_duration = 1 # pass@1: 10min, pass@10: 100min\n",
    "    else:\n",
    "        job_duration = 1\n",
    "    return batch_size, job_duration\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b68375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_flan_v2\" --save_dir \"results/llama-7b_flan_v2/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_dolly\" --save_dir \"results/llama-7b_dolly/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_oasst1\" --save_dir \"results/llama-7b_oasst1/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_gpt4_alpaca\" --save_dir \"results/llama-7b_gpt4_alpaca/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_stanford_alpaca\" --save_dir \"results/llama-7b_stanford_alpaca/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_code_alpaca\" --save_dir \"results/llama-7b_code_alpaca/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_baize\" --save_dir \"results/llama-7b_baize/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_self_instruct\" --save_dir \"results/llama-7b_self_instruct/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_flan_v2\" --save_dir \"results/llama-7b_flan_v2/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_dolly\" --save_dir \"results/llama-7b_dolly/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_oasst1\" --save_dir \"results/llama-7b_oasst1/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_gpt4_alpaca\" --save_dir \"results/llama-7b_gpt4_alpaca/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_stanford_alpaca\" --save_dir \"results/llama-7b_stanford_alpaca/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_code_alpaca\" --save_dir \"results/llama-7b_code_alpaca/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_baize\" --save_dir \"results/llama-7b_baize/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/llama-7b_self_instruct\" --save_dir \"results/llama-7b_self_instruct/eval/humaneval_chatfmt\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1 --use_chat_format\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "task_name = 'mmlu'\n",
    "task_name = 'gsm'\n",
    "task_name = 'bbh_s=0'\n",
    "task_name = 'bbh_s=3'\n",
    "task_name = 'humaneval'\n",
    "job_name = f'eval.{task_name}'\n",
    "job_name = 'eval'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "use_chat_format = True\n",
    "eval_final_model = True\n",
    "\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "models = []\n",
    "# models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "# models += ['t5-11b']\n",
    "# models += ['google/flan-t5-small', 'google/flan-t5-base', 'google/flan-t5-large', 'google/flan-t5-xl', 'google/flan-t5-xxl']\n",
    "# models += ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "# models += [os.path.join('results/baselines', x) for x in [\n",
    "#     'huggyllama/llama-7b',  # , 'mosaicml/mpt-7b'\n",
    "# ]]\n",
    "\n",
    "# models += ['results/huggyllama:llama-7b_human_mix-trainer_savebystep/']\n",
    "# models = [glob.glob(os.path.join(x, 'checkpoint*')) for x in models]\n",
    "# models = [x for l in models for x in l]\n",
    "\n",
    "# models += [x if eval_final_model else get_last_checkpoint(x) for x in [\n",
    "#     'results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix',\n",
    "#     'results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix',\n",
    "#     'results/huggyllama:llama-7b_humanmix',\n",
    "# ]]\n",
    "\n",
    "datasets = [\n",
    "#     'super_ni', \n",
    "#     'cot', \n",
    "    'flan_v2', 'dolly', 'oasst1',\n",
    "    'gpt4_alpaca', 'stanford_alpaca', 'code_alpaca', 'baize', 'self_instruct', \n",
    "#     'sharegpt',\n",
    "#     'unnatural_instructions',\n",
    "#     'humanmix',\n",
    "]\n",
    "\n",
    "models = [f'results/llama-7b_{x}' for x in datasets]\n",
    "\n",
    "\n",
    "task_names = [\n",
    "#     'mmlu',\n",
    "    'gsm',\n",
    "#     'bbh_s=0',\n",
    "    'humaneval',\n",
    "]\n",
    "\n",
    "options_list = itertools.product(\n",
    "    task_names,\n",
    "    models,\n",
    ")\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    job_name = f'eval.{task_name}'\n",
    "    batch_size, job_duration = get_resource_for_task(task_name)\n",
    "    \n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    if use_chat_format:\n",
    "        save_dir += '_chatfmt'\n",
    "    \n",
    "    if task_name == 'mmlu':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain 0 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name == 'gsm':\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 200 \\\n",
    "            --n_shot 8 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('bbh'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_cot' if 's=0' in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    if task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "            {'--load_in_8bit' if load_in_8bit else ''}\n",
    "        \"\"\"\n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    \n",
    "\n",
    "#     if test_run: print(out['jbsub_cmd'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e9a23e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26 16:32:07,845] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Loading data...\n",
      "Loading model and tokenizer...\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:15<00:00,  5.22s/it]\n",
      "Generating Completions:   0%|                           | 0/200 [00:00<?, ?it/s]/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd .. && CUDA_VISIBLE_DEVICES=0 python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/llama-7b_cot\" --save_dir \"results/llama-7b_cot/eval/gsm_chatfmt\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --use_chat_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019eae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'bbh_s=3_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'gsm', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n",
      "['bbh_s=0_chatfmt', 'gsm_chatfmt', 'mmlu_chatfmt', 'humaneval_chatfmt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>chatfmt</th>\n",
       "      <th>chatfmt</th>\n",
       "      <th>chatfmt</th>\n",
       "      <th>chatfmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>32.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+super_ni</td>\n",
       "      <td>43.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+cot</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.9</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+flan_v2</td>\n",
       "      <td>43.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+dolly</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+oasst1</td>\n",
       "      <td>34.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+gpt4_alpaca</td>\n",
       "      <td>39.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>31.7</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+stanford_alpaca</td>\n",
       "      <td>41.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>32.9</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+code_alpaca</td>\n",
       "      <td>34.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+baize</td>\n",
       "      <td>38.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.9</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b+self_instruct</td>\n",
       "      <td>35.7</td>\n",
       "      <td>6.5</td>\n",
       "      <td>32.4</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model MMLU/0-shot GSM/CoT BBH/Direct Codex-Eval/Pass@1\n",
       "                                chatfmt chatfmt    chatfmt           chatfmt\n",
       "0                  llama-7b        32.5    11.0       33.0               5.5\n",
       "0         llama-7b+super_ni        43.5     3.0       34.1               7.9\n",
       "0              llama-7b+cot        37.0    28.0       34.9              10.4\n",
       "0          llama-7b+flan_v2        43.2    12.0       34.4              12.8\n",
       "0            llama-7b+dolly        37.2    13.0       30.6              11.6\n",
       "0           llama-7b+oasst1        34.1     7.5       29.7               2.4\n",
       "0      llama-7b+gpt4_alpaca        39.2     8.5       31.7              12.2\n",
       "0  llama-7b+stanford_alpaca        41.7    10.5       32.9              11.0\n",
       "0      llama-7b+code_alpaca        34.9    11.0       31.6              14.6\n",
       "0            llama-7b+baize        38.7    10.0       31.9              11.6\n",
       "0    llama-7b+self_instruct        35.7     6.5       32.4               7.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir, run_name=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        self.run_name = run_name if run_name else self.save_dir\n",
    "        \n",
    "    def get_result_df(self):\n",
    "\n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "        print(task_names)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "\n",
    "            columns = [f'{task_name}/{k}' for k in metrics.keys()]\n",
    "            # columns = pd.MultiIndex.from_tuples(columns)\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        mapper = {\n",
    "            'mmlu/average_acc': 'MMLU/0-shot',\n",
    "            'mmlu_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'gsm/exact_match': 'GSM/CoT',\n",
    "            'gsm_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            'bbh_s=0/average_exact_match': 'BBH/Direct',\n",
    "            'bbh_s=0_chatfmt/average_exact_match': 'BBH/Direct_chatfmt',\n",
    "            'bbh_s=3/average_exact_match': 'BBH/CoT', \n",
    "            'bbh_s=3_chatfmt/average_exact_match': 'BBH/CoT_chatfmt',\n",
    "            'humaneval/pass@1': 'Codex-Eval/Pass@1',\n",
    "            'humaneval_chatfmt/pass@1': 'Codex-Eval/Pass@1_chatfmt',\n",
    "        }\n",
    "        cols = [col for col in mapper.keys() if col in df]\n",
    "        for col in cols:\n",
    "            df[col] = df[col]*100\n",
    "        df = df[cols]\n",
    "        df = df.rename(columns=mapper)\n",
    "        df.insert(0, 'Model', [self.run_name])\n",
    "        return df\n",
    "\n",
    "# get_last_checkpoint(v)\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama-7b+humanmix', '../results/huggyllama:llama-7b_humanmix'),\n",
    "#     ('llama-7b+lora(r=8,a=8)', '../results/huggyllama:llama-7b+lora(r=8,a=8)_humanmix'),\n",
    "#     ('llama-7b+lora(r=128,a=128)', '../results/huggyllama:llama-7b+lora(r=128,a=128)_humanmix'),\n",
    "]\n",
    "datasets = [\n",
    "    'super_ni', 'cot', 'flan_v2', 'dolly', 'oasst1',\n",
    "    'gpt4_alpaca', 'stanford_alpaca', 'code_alpaca', 'baize', 'self_instruct', \n",
    "#     'sharegpt',\n",
    "#     'unnatural_instructions',\n",
    "#     'humanmix',\n",
    "]\n",
    "\n",
    "save_dirs += [(f'llama-7b+{x}', f'../results/llama-7b_{x}') for x in datasets]\n",
    "\n",
    "dfs = []\n",
    "for model_name, save_dir in save_dirs:\n",
    "    r = EvalResults(save_dir, model_name)\n",
    "    df = r.get_result_df()\n",
    "    dfs.append(df)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    dfc = df.copy()\n",
    "    cols = [x.split('_') for x in df.columns]\n",
    "    cols = [x+[''] if len(x)==1 else x for x in cols]\n",
    "    dfc.columns = pd.MultiIndex.from_tuples(cols)\n",
    "    display(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dată în istoria Uniunii Europene, la drepturile persoanelor care aparţin acestor minorităţi şi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
