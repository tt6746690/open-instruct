{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3da1794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ppc64le', 'dcs')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_setup; jpt_setup()\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import re\n",
    "from llm.submit import (\n",
    "    multiline_to_singleline,\n",
    "    submit_job_ccc,\n",
    "    submit_job_aimos,\n",
    "    submit_job,\n",
    "    get_run_statistics)\n",
    "import pandas as pd\n",
    "import json\n",
    "import platform\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shlex\n",
    "import datetime\n",
    "import itertools\n",
    "import socket\n",
    "\n",
    "import base64\n",
    "string_to_alphanumeric = lambda s: base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')\n",
    "alphanumeric_to_string = lambda a: base64.urlsafe_b64decode(a).decode('utf-8')\n",
    "\n",
    "arch = platform.uname().processor\n",
    "hostname = socket.gethostname()\n",
    "cluster = 'ccc' if hostname.startswith('ccc') else ('dcs' if hostname.startswith('dcs') else 'npl')\n",
    "arch, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8323654",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_name = 'ft'\n",
    "# test_run = 1\n",
    "# test_run = bool(test_run)\n",
    "\n",
    "# queue = 'x86_12h' # 'x86_12h'\n",
    "# num_cpus = 20\n",
    "# num_gpus = 1\n",
    "# cpu_mem = 32\n",
    "# require = 'a100_80gb'\n",
    "\n",
    "# # model_name_or_path = 'mosaicml/mpt-7b'; max_seq_length = 2048\n",
    "# # model_name_or_path = 'gpt2'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-Large'; max_seq_length = 1024\n",
    "# # model_name_or_path = 'gpt2-xl'; max_seq_length = 1024\n",
    "# model_name_or_path = 'huggyllama/llama-7b'; max_seq_length = 2048\n",
    "\n",
    "\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; train_file_short = 'oasst1'\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; train_file_short = 'human_mix'\n",
    "# # train_file = 'data/processed/flanv2_cot_oasst1_dolly_shuffled.jsonl'; train_file_short = 'human_mix_shuffled'\n",
    "\n",
    "# output_dir = f\"results/{model_name_or_path.replace('/', ':')}_{train_file_short}\"\n",
    "# if test_run:\n",
    "#     output_dir = 'jpt_' + output_dir\n",
    "\n",
    "# use_deepspeed = False\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_no_offloading_accelerate_setauto.conf'\n",
    "# # deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate.conf'\n",
    "# deepspeed_config_file = 'ds_configs/stage3_offloading_accelerate_setauto.conf'\n",
    "\n",
    "# use_lora = True\n",
    "# lora_rank = 4\n",
    "# lora_alpha = lora_rank\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# batch_size_per_gpu = 1\n",
    "# total_batch_size = 128\n",
    "# mixed_precision = 'bf16' # 'bf16', 'fp16'\n",
    "# checkpointing_steps = None # every n steps, where n='1' or every 'epoch'\n",
    "\n",
    "# gradient_acc_steps = int(total_batch_size/num_gpus/batch_size_per_gpu)\n",
    "\n",
    "# print(f\"Training {model_name_or_path} \"\n",
    "#       f\"using {num_gpus} GPUs, \"\n",
    "#       f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "#       f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "# # do use fast tokenizer since mpt-7b does not have a fast tokenizer counter-part\n",
    "# #     --use_slow_tokenizer \\\n",
    "# # do not use flash attention, since having problem installing flash-attn with cuda 12.1\n",
    "# #     --use_flash_attn \\\n",
    "\n",
    "# cmd = f\"\"\"\n",
    "# {'!cd .. && ' if test_run else ''}accelerate launch \\\n",
    "#     --mixed_precision {mixed_precision} \\\n",
    "#     --num_machines 1 \\\n",
    "#     --num_processes {num_gpus} \\\n",
    "#     {'--use_deepspeed' if use_deepspeed else ''}\n",
    "#     {'--deepspeed_config_file '+deepspeed_config_file if use_deepspeed else ''}\n",
    "#     open_instruct/finetune.py \\\n",
    "#     --model_name_or_path {model_name_or_path} \\\n",
    "#     --tokenizer_name {model_name_or_path} \\\n",
    "#     --train_file {train_file} \\\n",
    "#     --max_seq_length {max_seq_length} \\\n",
    "#     {'--use_lora' if use_lora else ''}\n",
    "#     --lora_rank {lora_rank} \\\n",
    "#     --lora_alpha {lora_alpha} \\\n",
    "#     --lora_dropout {lora_dropout} \\\n",
    "#     --preprocessing_num_workers 16 \\\n",
    "#     --per_device_train_batch_size {batch_size_per_gpu} \\\n",
    "#     --gradient_accumulation_steps {gradient_acc_steps} \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --lr_scheduler_type linear \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --num_train_epochs 2 \\\n",
    "#     --output_dir {output_dir} \\\n",
    "#     --with_tracking \\\n",
    "#     --report_to tensorboard \\\n",
    "#     {'--checkpointing_steps '+str(checkpointing_steps) if checkpointing_steps else ''}\n",
    "#     --logging_steps 1\n",
    "# \"\"\"\n",
    "\n",
    "# # things to test to see its effects on (1) eval perf (2) runtime.\n",
    "# #\n",
    "# # - int8\n",
    "# # - mixed_precision bf16 or no\n",
    "# # - with/without LoRA\n",
    "# # - LoRA's rank/alpha (alpha typically set to 2*rank)\n",
    "# # - batch size\n",
    "# # - micro-batch size (largest without running out of memory)\n",
    "\n",
    "\n",
    "# cmd = multiline_to_singleline(cmd)\n",
    "# if test_run:\n",
    "#     print()\n",
    "#     print(cmd)\n",
    "\n",
    "# shell_scripts = shell_scripts_template.format(\n",
    "#     cmd=cmd,\n",
    "#     log_dir=os.getcwd(),\n",
    "#     save_dir=output_dir)\n",
    "# out = submit_job_ccc(\n",
    "#     shell_scripts, \n",
    "#     job_name=job_name, \n",
    "#     queue=queue,\n",
    "#     num_cpus=num_cpus,\n",
    "#     cpu_mem=cpu_mem,\n",
    "#     require=require,\n",
    "#     num_gpus=num_gpus,\n",
    "#     test_run=test_run,\n",
    "# )\n",
    "# if not test_run:\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# llama7b+lora, micro-bsz=1, bsz=128, \n",
    "#     66gb gpu mem, in it 28gb for torch tensor. so fit on 1 a100_80gb\n",
    "\n",
    "#   1%|‚ñè         | 57/4222 [22:24<25:53:47, 22.38s/it]07/11/2023 23:13:55 - INFO - \n",
    "# __main__ -   Step: 57, LR: 9.047619047619049e-06, Loss: 2.2924644947052\n",
    "# \n",
    "\n",
    "\n",
    "# !cd .. && accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 1 open_instruct/finetune.py --model_name_or_path huggyllama/llama-7b --tokenizer_name huggyllama/llama-7b --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl --max_seq_length 2048 --use_lora --lora_rank 4 --lora_alpha 4 --lora_dropout 0.05 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 128 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 2 --output_dir results/huggyllama:llama-7b_human_mixjpt_results/huggyllama:llama-7b_human_mix --with_tracking --report_to tensorboard --logging_steps 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6c8b",
   "metadata": {},
   "source": [
    "# Finetuning with openinstruct/finetune_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2170c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"Running on $SLURM_JOB_NODELIST\"\n",
      "echo \"======\"\n",
      "\n",
      "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
      "master_port=10002\n",
      "RDZV_ENDPOINT=$master_addr:$master_port\n",
      "\n",
      "source ~/.profile\n",
      "conda activate open-instruct\n",
      "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
      "\n",
      "set -e\n",
      "set -x\n",
      "echo \"======\"\n",
      "srun {cmd}\n",
      "\n",
      "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runtime:\n",
    "# shared: humanmix, max_sequence_length=2048.\n",
    "# just a single process if num_gpus=1 otherwise use python fsdp with gradient-checkpointing.\n",
    "#\n",
    "# llama7b, micro-bsz=2, grad-ckpt, 2xa100_80gb: 27s/it, 32hrs\n",
    "# \n",
    "# llama7b+lora(r=4),   micro-bsz=1, no-grad-ckpt,a100_80gb: 27s/it, 32hrs, 66gb gpu mem\n",
    "# llama7b+lora(r=16),  micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 67gb gpu mem\n",
    "# llama7b+lora(r=256), micro-bsz=1, no-grad-ckpt,a100_80gb: 24s/it, 28hrs, 70gb gpu mem\n",
    "#\n",
    "# llama7b+lora(r=4),   micro-bsz=1,  grad-ckpt,  a100_80gb: 32s/it, 38hrs\n",
    "# llama7b+lora(r=256), micro-bsz=1,  grad-ckpt,  a100_80gb, 37s/it, 43hrs# llama7b+lora(r=4),   micro-bsz=2,  grad-ckpt,  a100_80gb: 32s/it, 38hrs (+20% runtime with grad-ckpt)\n",
    "# llama7b+lora(r=4),   micro-bsz=16, grad-ckpt,  a100_80gb: 68s/it, 80hrs, 75gb gpu mem\n",
    "# llama7b+lora(r=128), micro-bsz=16, grad-ckpt,  a100_80gb: 70s/it, 82hrs\n",
    "# \n",
    "# take-aways\n",
    "# - larger batch should speed up training. however, it also implies that the batch \n",
    "#   will be padded to longer sequence length, due to chat data, thereby increasing\n",
    "#   compute required to process a batch. Therefore, micro-bsz=1 seems to be ok.\n",
    "#\n",
    "\n",
    "# aimos\n",
    "# shared: humanmix, max_sequence_length=2048. 1 node = 6x v100_32gb\n",
    "\n",
    "# take-aways: \n",
    "#   (1) fsdp (v4.28.1, v4.32.0.dev0 are pretty similar in terms of speed. don't use v4.31.0)\n",
    "#   (2) micro-bsz=1->2 seems to be the best here. leads to ~30% reduction in runtime.\n",
    "#   (3) increasing #nodes almost linear reduction in time, e.g., Use 4x nodes cost 30% time (25% if linear.)\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=none, nodes=1, 74s/it, 89hrs, loss=0\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 72s/it, 86hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=none, nodes=1, 52s/it, 66hrs, loss=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, 49s/it, 61hrs\n",
    "# - resume from lastest checkpt (trained 4.28.1, resume 4.32.0.dev0), did\n",
    "#   did not found `optimizer.bin` \n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float16, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, no-grad-ckpt, float16, mp=fp16, nodes=1, oom\n",
    "# # nodes > 1\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float16, mp=fp16, nodes=4, 14s/it, 18hrs\n",
    "\n",
    "# take-away: \n",
    "#   (1) torch_dtype=float16 gives loss=0. setting torch_dtype=float32 solves the issue but takes more memory and compute\n",
    "#   (2) mbsz=2 oom for nodes=1 but works fine with nodes=2. more nodes -> potentially larger mbsz.\n",
    "\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=none, nodes=1, oom\n",
    "# llama7b, micro-bsz=1, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, 139s/it, 166hrs\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=1, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=2, 33s/it, 41hrs, loss!=0\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 17s/it, 21hrs\n",
    "# llama7b, micro-bsz=3, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, 18s/it, 21hrs\n",
    "# llama7b, micro-bsz=4, fsdp, grad-ckpt, float32, mp=fp16, nodes=4, oom\n",
    "# llama7b, micro-bsz=2, fsdp, grad-ckpt, float32, mp=fp16, nodes=5, 4s/it, 21.7hrs\n",
    "\n",
    "\n",
    "#\n",
    "# deepspeed\n",
    "# shared: llama7b, deepspeed, grad-ckpt\n",
    "# take-aways\n",
    "#   (1) with deepspeed, using mixed-precision gives x50% speed improvement\n",
    "#   (2) no loss overflow issues. deepspeed has good mixed-precision integration, loss_scaler handles it.\n",
    "# \n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 161s/it, 192hrs, loss ok, loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float32, mp=none, nodes=1, 259s/it, 309hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=fp16, nodes=1, 169s/it, 202hrs, loss ok. loss_scaler early on in use.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3,offload), float16, mp=none, nodes=1, 258s/it, 307hrs, loss ok.\n",
    "#\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3,offload), float32, mp=fp16, nodes=1, 108s/it, 135hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3,offload), float32, mp=fp16, nodes=1,  96s/it, 120hrs, loss ok.\n",
    "# llama7b, micro-bsz=1, deepspeed(s=3), float32, mp=fp16, nodes=1, 123s/it, 147hrs, loss ok.\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=1, 66s/it, 83hrs, loss ok.\n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=1, oom\n",
    "#\n",
    "# nodes>1\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=2, error with downloading config.json\n",
    "# llama7b, micro-bsz=2, deepspeed(s=3), float32, mp=fp16, nodes=4, \n",
    "# llama7b, micro-bsz=4, deepspeed(s=3), float32, mp=fp16, nodes=4, 16s/it, 21hrs\n",
    "# llama7b, micro-bsz=6, deepspeed(s=3), float32, mp=fp16, nodes=4,  3s/it, 17hrs\n",
    "\n",
    "\n",
    "# 1 node, 1 v100_32gb, no-grad-ckpt, nodes=1\n",
    "#\n",
    "# gpt2, micro-bsz=1, float32, mp=16, 7s/it, 8.6hrs\n",
    "# gpt2, micro-bsz=2, float32, mp=16, 5s/it, 5.6hrs\n",
    "# gpt2, micro-bsz=4, float32, mp=16, 5s/it, 6.3hrs\n",
    "# gpt2, micro-bsz=8, float32, mp=16, 6s/it, 6.6hrs\n",
    "# gpt2, micro-bsz=16, float32, mp=16, oom\n",
    "# gpt2-medium, micro-bsz=2, float32, mp=16, 14s/it, 17.0hrs\n",
    "# gpt2-medium, micro-bsz=2, float32, mp=16, 10s/it, 12.2hrs\n",
    "# gpt2-medium, micro-bsz=4, float32, mp=16, oom\n",
    "# gpt2-medium, micro-bsz=8, float32, mp=16, oom\n",
    "# \n",
    "\n",
    "\n",
    "shell_scripts_template_slurm = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template_lsf = \"\"\"\n",
    "echo \"Running on $LSB_DJOB_HOSTFILE\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(head -n 1 \"$LSB_DJOB_HOSTFILE\")\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$LSB_JOBID*.out\" ] && mv {log_dir}/$LSB_JOBID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "shell_scripts_template = shell_scripts_template_slurm \\\n",
    "    if arch == 'ppc64le' else shell_scripts_template_lsf\n",
    "\n",
    "print(shell_scripts_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d611cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10s/it, 12.2hrs\n"
     ]
    }
   ],
   "source": [
    "t = '0:08:49'\n",
    "n = 51\n",
    "total = 4228; nnodes = 1\n",
    "# total = 4512; nnodes = 4\n",
    "# total = 4296; nnodes = 1\n",
    "# total = 2254; nnodes = 2\n",
    "# total = 1128; nnodes = 4\n",
    "# total = 1074; nnodes = 4\n",
    "# total = 1252; nnodes = 4\n",
    "\n",
    "l = [int(x) for x in t.split(':')]\n",
    "t = l[0]*60*60+l[1]*60+l[2]\n",
    "# t = t/60/60 # in hr\n",
    "\n",
    "print(f'{t/n/nnodes:.0f}s/it, {t/n*total/60/60:.1f}hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a669464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot\n",
      "sharegpt\n",
      "dolly\n",
      "gpt4_alpaca\n",
      "flan_v2\n",
      "super_ni\n",
      "stanford_alpaca\n",
      "baize\n",
      "code_alpaca\n",
      "self_instruct\n",
      "unnatural_instructions\n",
      "oasst1\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "p = '../data/processed/'\n",
    "for x in os.listdir(p):\n",
    "    y = os.path.join(p, x)\n",
    "    if os.path.isdir(y):\n",
    "        d = os.path.join(y, os.listdir(y)[0])\n",
    "    else:\n",
    "        continue\n",
    "    d = d[3:]\n",
    "    if 'shuffled' in d:\n",
    "        continue\n",
    "#     print(f\"train_file = '{d}'; abbr_train_file = '{x}'\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80cee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51bf7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'baize': 100000},\n",
       " {'code_alpaca': 100000},\n",
       " {'cot': 100000},\n",
       " {'dolly': 100000},\n",
       " {'gpt4_alpaca': 100000},\n",
       " {'oasst1': 100000},\n",
       " {'self_instruct': 100000},\n",
       " {'sharegpt': 100000},\n",
       " {'stanford_alpaca': 100000},\n",
       " {'super_ni': 100000},\n",
       " {'unnatural_instructions': 100000},\n",
       " {'cot': 50000, 'flan_v2': 50000, 'dolly': 50000, 'oasst1': 50000},\n",
       " {'cot': 40031, 'flan_v2': 40031, 'dolly': 6009, 'oasst1': 13928},\n",
       " {'cot': 97570, 'flan_v2': 97570, 'dolly': 1464, 'oasst1': 3394},\n",
       " {'baize': 16666,\n",
       "  'code_alpaca': 16666,\n",
       "  'cot': 16666,\n",
       "  'dolly': 16666,\n",
       "  'flan_v2': 16666,\n",
       "  'gpt4_alpaca': 16666,\n",
       "  'oasst1': 16666,\n",
       "  'self_instruct': 16666,\n",
       "  'sharegpt': 16666,\n",
       "  'stanford_alpaca': 16666,\n",
       "  'super_ni': 16666,\n",
       "  'unnatural_instructions': 16666},\n",
       " {'cot': 15356, 'flan_v2': 182740, 'dolly': 894, 'oasst1': 1814},\n",
       " {'cot': 22540, 'flan_v2': 174520, 'dolly': 2790, 'oasst1': 278}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# how to sample mixture sample size?\n",
    "# \n",
    "# approaches: \n",
    "# (1) want sufficient coverage for #datapoints/dataset, #datasets used, total sample size.\n",
    "#  Use 5k as a unit of data, sample different #unit/dataset, and vary total units of data.\n",
    "# (2) specify a total sample size and a mixture weight. this answers the question, given a \n",
    "#  fixed compute budget, what is the optimal mixture. this seems to be a simpler approach.\n",
    "#\n",
    "# experiments\n",
    "# (1) first use samples from a single dataset for tuning. \n",
    "# (2)\n",
    "# \n",
    "\n",
    "\n",
    "datasets = ['baize', 'code_alpaca', 'cot', 'dolly', 'flan_v2', 'gpt4_alpaca', 'oasst1', 'self_instruct', 'sharegpt', 'stanford_alpaca', 'super_ni', 'unnatural_instructions']\n",
    "total_data_points = 200000\n",
    "\n",
    "subsample_mixture_list = []\n",
    "subsample_mixture_list += [\n",
    "    {k: 100000} for k in datasets if k != 'flan_v2'\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51c8d72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results/baselines/gpt2-medium using 1 GPUs, 2 batch size per GPU, 64 gradient accumulation steps.\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"ft2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=ft2 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:1 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpr8_ynorf', 'job_id': 711159}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"ft2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=ft2 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:1 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpag3ixn4l', 'job_id': 711160}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"ft2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 144,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=ft2 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=144 --mem=512GB --gres=gpu:1 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp3bm0gpoe', 'job_id': 711161}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ft1: reproduce open-instruct table with llama7b\n",
    "job_name = 'ft1'\n",
    "\n",
    "# ft2: test mixture weights\n",
    "# vary mixture weights\n",
    "job_name = 'ft2'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "# specify `job_duration` to chain jobs for >12hr jobs.\n",
    "job_duration = 6 if arch == 'ppc64le' else 12\n",
    "# shell_scripts_modification_fn = lambda x: x.replace('--overwrite_output_dir ', '')\n",
    "\n",
    "num_cpus = 144 if arch == 'ppc64le' else 32\n",
    "cpu_mem =  512 if arch == 'ppc64le' else 64\n",
    "nodes = 5; num_gpus = 6; gpu_type = 'v100'; job_duration = 24 # llama7b 128/(5*6*2)~=2.1\n",
    "nodes = 1; num_gpus = 1; gpu_type = 'v100'; job_duration = 6  # gpt2\n",
    "debug_mode = test_run\n",
    "\n",
    "overwrite_output_dir = True if test_run else False # always continue from ckpt if run from cluster.\n",
    "save_strategy = 'steps'\n",
    "save_steps = 100\n",
    "\n",
    "# model_name_or_path = 'results/baselines/huggyllama/llama-7b'; abbr_model_name = 'llama-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'results/baselines/NousResearch/Llama-2-7b-hf'; abbr_model_name = 'llama2-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'; abbr_model_name = 'mpt-7b'; max_seq_length = 2048\n",
    "# model_name_or_path = 'gpt2'; abbr_model_name = 'gpt2'; max_seq_length = 1024\n",
    "# model_name_or_path = 'gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "model_name_or_path = 'results/baselines/gpt2-medium'; abbr_model_name = 'gpt2m'; max_seq_length = 1024\n",
    "\n",
    "\n",
    "train_file = 'data/processed/all.jsonl'; abbr_train_file = 'all'\n",
    "# subsample_mixture = {'flan_v2': 100000}\n",
    "# subsample_mixture = dict(sorted(subsample_mixture.items()))\n",
    "\n",
    "total_data_points = 100000\n",
    "total_data_points = 200000\n",
    "total_data_points = 50000\n",
    "total_data_points = 10000\n",
    "subsample_mixture_list = []\n",
    "# subsample_mixture_list += [\n",
    "#     {k: total_data_points} for k in datasets\n",
    "# ]\n",
    "subsample_mixture_list += [\n",
    "    {k: int(total_data_points/4) for k in ['cot', 'flan_v2', 'dolly', 'oasst1']}\n",
    "]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.48785105, 'flan_v2': 0.48785105, 'dolly': 0.00732313, 'oasst1': 0.01697478}.items())\n",
    "] # humanmix mixture.\n",
    "# subsample_mixture_list += [\n",
    "#     {k: int(total_data_points/len(datasets)) for k in datasets} \n",
    "# ]\n",
    "# subsample_mixture_list += [\n",
    "#     dict((k, int(v*total_data_points)) for k, v in\n",
    "#     {'cot': .07678, 'flan_v2': .9137, 'dolly': .004471, 'oasst1': .009072}.items())\n",
    "# ]\n",
    "subsample_mixture_list += [\n",
    "    dict((k, int(v*total_data_points)) for k, v in\n",
    "    {'cot': 0.1127, 'flan_v2': 0.8726, 'dolly': 0.01395, 'oasst1': 0.001391}.items())\n",
    "]\n",
    "subsample_mixture_list = [dict(sorted(d.items())) for d in subsample_mixture_list]\n",
    "\n",
    "\n",
    "# subsample_mixture_list = [None]\n",
    "# train_file = 'data/processed/flanv2_cot_oasst1_dolly.jsonl'; abbr_train_file = 'humanmix'\n",
    "# train_file = 'data/processed/dolly_oasst1.jsonl'; abbr_train_file = 'dolly:oasst1'\n",
    "# train_file = 'data/processed/cot_flanv2.jsonl'; abbr_train_file = 'cot:flanv2'\n",
    "\n",
    "# train_file = 'data/processed/super_ni/super_ni_data.jsonl'; abbr_train_file = 'super_ni'\n",
    "# train_file = 'data/processed/cot/cot_data.jsonl'; abbr_train_file = 'cot'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'; abbr_train_file = 'flan_v2'\n",
    "# train_file = 'data/processed/dolly/dolly_data.jsonl'; abbr_train_file = 'dolly'\n",
    "# train_file = 'data/processed/oasst1/oasst1_data.jsonl'; abbr_train_file = 'oasst1'\n",
    "\n",
    "# train_file = 'data/processed/stanford_alpaca/stanford_alpaca_data.jsonl'; abbr_train_file = 'stanford_alpaca'\n",
    "# train_file = 'data/processed/baize/baize_data.jsonl'; abbr_train_file = 'baize'\n",
    "# train_file = 'data/processed/self_instruct/self_instruct_data.jsonl'; abbr_train_file = 'self_instruct'\n",
    "\n",
    "# train_file = 'data/processed/code_alpaca/code_alpaca_data.jsonl'; abbr_train_file = 'code_alpaca'\n",
    "# train_file = 'data/processed/unnatural_instructions/unnatural_instructions_data.jsonl'; abbr_train_file = 'unnatural_instructions'\n",
    "# train_file = 'data/processed/sharegpt/sharegpt_data.jsonl'; abbr_train_file = 'sharegpt'\n",
    "# train_file = 'data/processed/gpt4_alpaca/gpt4_alpaca_data.jsonl'; abbr_train_file = 'gpt4_alpaca'\n",
    "\n",
    "\n",
    "num_train_epochs = 1\n",
    "batch_size_per_gpu = 2\n",
    "total_batch_size = 128 # 128\n",
    "gradient_acc_steps = int(total_batch_size/(num_gpus*nodes)/batch_size_per_gpu)\n",
    "optimizer = 'adamw_hf' # 'adafactor'\n",
    "\n",
    "deepspeed = ''; fsdp = False if num_gpus == 1 else \"full_shard auto_wrap\"  # full_shard, shard_grad_op\n",
    "if 'gpt2' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'GPT2Block'\n",
    "elif 'llama' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'LlamaDecoderLayer'\n",
    "elif 'mpt' in abbr_model_name: fsdp_transformer_layer_cls_to_wrap = 'MPTBlock'\n",
    "else: raise ValueError('Not sure how to set `fsdp_transformer_layer_cls_to_wrap`')\n",
    "    \n",
    "# deepspeed = './ds_configs/ds_zero3_cpu_offload.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/ds_zero3.json'; fsdp = False\n",
    "# deepspeed = './ds_configs/stage3_no_offloading.conf'; fsdp = False # error with loading... something wrong with the config.\n",
    "\n",
    "fsdp = False; deepspeed = False\n",
    "\n",
    "if fsdp and deepspeed:\n",
    "    raise ValueError('either fsdp or deepspeed, not both')\n",
    "\n",
    "use_lora = False\n",
    "lora_rank = 256 # test {8, 16, 32, 128} # just [128, 8] for now.\n",
    "lora_alpha = lora_rank \n",
    "lora_dropout = 0.05\n",
    "if use_lora:\n",
    "    abbr_model_name += f'+lora(r={lora_rank},a={lora_alpha})'\n",
    "\n",
    "mixed_precision = 'bf16' if arch == 'x86_64' else 'fp16' # mixed_precision = ''\n",
    "torch_dtype = 'bfloat16' if arch=='x86_64' else 'float16'; torch_dtype = 'float32'\n",
    "\n",
    "gradient_checkpointing = False\n",
    "load_in_8bit = False\n",
    "\n",
    "\n",
    "print(f\"Training {model_name_or_path} \"\n",
    "      f\"using {num_gpus} GPUs, \"\n",
    "      f\"{batch_size_per_gpu} batch size per GPU, \"\n",
    "      f\"{gradient_acc_steps} gradient accumulation steps.\")\n",
    "\n",
    "\n",
    "if nodes == 1:\n",
    "    exe = 'python' if num_gpus==1 else \\\n",
    "        f\"torchrun --nproc_per_node={num_gpus} --master_port=10002\"\n",
    "else:\n",
    "    exe = f\"torchrun --nnodes={nodes} --nproc_per_node={num_gpus} --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT\"\n",
    "\n",
    "if test_run:\n",
    "    exe = f\"CUDA_VISIBLE_DEVICES={','.join(map(str, range(num_gpus)))} {exe}\"\n",
    "if test_run and debug_mode:\n",
    "    exe = 'TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO ' + exe\n",
    "    error_file='/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/error_file'\n",
    "    exe = f'TORCHELASTIC_ERROR_FILE={error_file} {exe}'\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    subsample_mixture_list,\n",
    ")\n",
    "\n",
    "output_dirname_list = []\n",
    "for (subsample_mixture,) in options_list:\n",
    "\n",
    "    output_dirname = f\"{abbr_model_name}_{abbr_train_file}:{int(total_data_points/1000)}k\"\n",
    "    if test_run:\n",
    "        output_dirname = 'jpt_'+output_dirname\n",
    "        \n",
    "    if job_name == 'ft2':\n",
    "        if subsample_mixture is not None:\n",
    "            assert(abbr_train_file=='all')\n",
    "            output_dirname += \\\n",
    "                '_mix='+','.join(f'{k}:{v}' for k,v in subsample_mixture.items())\n",
    "            \n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         '_ep='+str(num_train_epochs)\n",
    "    # if not test_run:\n",
    "    #     output_dirname += \\\n",
    "    #         ('_fsdp='+fsdp.split(' ')[0] if fsdp else '')+\\\n",
    "    #         ('_deepspeed='+os.path.basename(deepspeed).split('.')[0] if deepspeed else '')+\\\n",
    "    #         ('_gradckpt='+str(gradient_checkpointing) if gradient_checkpointing else '')+\\\n",
    "    #         '_mbsz='+str(batch_size_per_gpu)+\\\n",
    "    #         '_dtype='+torch_dtype+\\\n",
    "    #         ('_mp='+str(mixed_precision) if mixed_precision else '_mp=none')+\\\n",
    "    #         '_seqlen='+str(max_seq_length)+\\\n",
    "    #         '_nodes='+str(nodes)\n",
    "    output_dir = os.path.join('results', job_name, output_dirname)\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    {'!cd .. && ' if test_run else ''}{exe}\n",
    "        open_instruct/finetune_trainer.py \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --tokenizer_name={model_name_or_path} \\\n",
    "        {'--load_in_8bit' if load_in_8bit else ''} \\\n",
    "        --use_fast_tokenizer=True \\\n",
    "        --train_file={train_file} \\\n",
    "        --max_seq_length={max_seq_length} \\\n",
    "        {'--use_lora' if use_lora else ''}\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''}\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''}\n",
    "        {'--lora_dropout='+str(lora_dropout) if use_lora else ''}\n",
    "        --do_train \\\n",
    "        --preprocessing_num_workers=16 \\\n",
    "        --per_device_train_batch_size={batch_size_per_gpu} \\\n",
    "        --gradient_accumulation_steps={gradient_acc_steps} \\\n",
    "        --learning_rate=2e-5 \\\n",
    "        --lr_scheduler_type=linear \\\n",
    "        --warmup_ratio=0.03 \\\n",
    "        --optim={optimizer} \\\n",
    "        --weight_decay=0. \\\n",
    "        --evaluation_strategy=\"no\" \\\n",
    "        --logging_steps=1 \\\n",
    "        --save_strategy={save_strategy} \\\n",
    "        --save_steps={save_steps} \\\n",
    "        --save_total_limit=1 \\\n",
    "        --num_train_epochs={num_train_epochs} \\\n",
    "        {'--fsdp=\"'+fsdp+'\"' if fsdp else ''}\n",
    "        {'--fsdp_transformer_layer_cls_to_wrap=\"'+fsdp_transformer_layer_cls_to_wrap+'\"' \n",
    "            if fsdp else ''}\n",
    "        {'--gradient_checkpointing' if gradient_checkpointing  else ''}\n",
    "        --report_to=tensorboard \\\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --dataloader_num_workers=8 \\\n",
    "        {f'--{mixed_precision}=True' if mixed_precision else ''} \\\n",
    "        {'--overwrite_output_dir' if overwrite_output_dir else ''} \\\n",
    "        {'--deepspeed='+deepspeed if deepspeed else ''} \\\n",
    "        {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "            if subsample_mixture else ''} \\\n",
    "        --output_dir=\"{output_dir}\" \\\n",
    "    \"\"\" \n",
    "    #    --overwrite_cache\n",
    "\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    if test_run:\n",
    "        print()\n",
    "        print(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=output_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name, \n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        gpu_type=gpu_type,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5dd1dc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anB0X2dwdDJtX2FsbF9taXg9YmFpemU6MTAwMDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9Y29kZV9hbHBhY2E6MTAwMDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9Y290OjEwMDAwMA==',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9ZG9sbHk6MTAwMDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9Zmxhbl92MjoxMDAwMDA=',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9Z3B0NF9hbHBhY2E6MTAwMDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9b2Fzc3QxOjEwMDAwMA==',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9c2VsZl9pbnN0cnVjdDoxMDAwMDA=',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9c2hhcmVncHQ6MTAwMDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9c3RhbmZvcmRfYWxwYWNhOjEwMDAwMA==',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9c3VwZXJfbmk6MTAwMDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9dW5uYXR1cmFsX2luc3RydWN0aW9uczoxMDAwMDA=',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9Y290OjI1MDAwLGRvbGx5OjI1MDAwLGZsYW5fdjI6MjUwMDAsb2Fzc3QxOjI1MDAw',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9Y290OjQwMDMxLGRvbGx5OjYwMDksZmxhbl92Mjo0MDAzMSxvYXNzdDE6MTM5Mjg=',\n",
       " 'anB0X2dwdDJtX2FsbF9taXg9YmFpemU6ODMzMyxjb2RlX2FscGFjYTo4MzMzLGNvdDo4MzMzLGRvbGx5OjgzMzMsZmxhbl92Mjo4MzMzLGdwdDRfYWxwYWNhOjgzMzMsb2Fzc3QxOjgzMzMsc2VsZl9pbnN0cnVjdDo4MzMzLHNoYXJlZ3B0OjgzMzMsc3RhbmZvcmRfYWxwYWNhOjgzMzMsc3VwZXJfbmk6ODMzMyx1bm5hdHVyYWxfaW5zdHJ1Y3Rpb25zOjgzMzM=']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [len(x) for x in output_dirname_list]\n",
    "output_dirname_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e37eadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2m_all_mix=baize:100000',\n",
       " 'gpt2m_all_mix=code_alpaca:100000',\n",
       " 'gpt2m_all_mix=cot:100000',\n",
       " 'gpt2m_all_mix=dolly:100000',\n",
       " 'gpt2m_all_mix=flan_v2:100000',\n",
       " 'gpt2m_all_mix=gpt4_alpaca:100000',\n",
       " 'gpt2m_all_mix=oasst1:100000',\n",
       " 'gpt2m_all_mix=self_instruct:100000',\n",
       " 'gpt2m_all_mix=sharegpt:100000',\n",
       " 'gpt2m_all_mix=stanford_alpaca:100000',\n",
       " 'gpt2m_all_mix=super_ni:100000',\n",
       " 'gpt2m_all_mix=unnatural_instructions:100000',\n",
       " 'gpt2m_all_mix=cot:25000,dolly:25000,flan_v2:25000,oasst1:25000',\n",
       " 'gpt2m_all_mix=cot:40031,dolly:6009,flan_v2:40031,oasst1:13928',\n",
       " 'gpt2m_all_mix=baize:8333,code_alpaca:8333,cot:8333,dolly:8333,flan_v2:8333,gpt4_alpaca:8333,oasst1:8333,self_instruct:8333,sharegpt:8333,stanford_alpaca:8333,super_ni:8333,unnatural_instructions:8333']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [x[4:] for x in output_dirname_list]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5329b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finetune_with_hf_trainer.sh',\n",
       " 'explore_tulu_paper_results.ipynb',\n",
       " 'split_sharegpt_conversations.py',\n",
       " 'submit_finetune_jobs.py',\n",
       " 'prepare_eval_data.sh',\n",
       " 'save_hf_models_to_local_disk.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'weight_diff.py',\n",
       " 'eval_tydiqa.ipynb',\n",
       " 'eval_humaneval.ipynb',\n",
       " 'finetune_with_accelerate.sh',\n",
       " 'eval',\n",
       " 'gen_cmds.ipynb',\n",
       " 'convert_llama_weights_to_hf.sh',\n",
       " 'eval_mmlu.ipynb',\n",
       " 'process_datasets.ipynb',\n",
       " 'results',\n",
       " 'prepare_train_data.sh',\n",
       " 'baselines',\n",
       " 'zguo_lora_finetune.py',\n",
       " 'eval_bbh.ipynb',\n",
       " 'submit_eval_jobs.py',\n",
       " 'eval_gsm.ipynb',\n",
       " 'resample_flan_v2.py',\n",
       " 'finetune_trainer.ipynb',\n",
       " 'data',\n",
       " 'split_sharegpt_conversations.ipynb',\n",
       " 'get_statistics.sh',\n",
       " 'finetune_lora_with_accelerate.sh']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831fe55",
   "metadata": {},
   "source": [
    "# eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41e3fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_all_symlinks(directory, verbose=False):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files + dirs:\n",
    "            path = os.path.join(root, name)\n",
    "            if os.path.islink(path):\n",
    "                os.unlink(path)\n",
    "                if verbose:\n",
    "                    print(f\"Removed symlink: {path}\")\n",
    "                \n",
    "import uuid\n",
    "\n",
    "def create_unique_symlinks(file_paths, verbose=False):\n",
    "    \"\"\"Create symlinks for each `file` in `files` in the same directory, with a unique name. \"\"\"\n",
    "    dirs = [os.path.dirname(x) for x in file_paths]\n",
    "\n",
    "    symlink_path_dict = {}\n",
    "    for directory, path in zip(dirs, file_paths):\n",
    "        if os.path.isdir(path):\n",
    "            symlink_name = f\"symlink_{str(uuid.uuid4())[:8]}\"  # Generate a unique symlink name\n",
    "            symlink_path = os.path.join(directory, symlink_name)\n",
    "            try:\n",
    "                os.symlink(os.path.abspath(path), symlink_path)\n",
    "                if verbose:\n",
    "                    print(f\"Created symlink: {symlink_path} -> {path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Failed to create symlink: {path}. Error: {e}\")\n",
    "            symlink_path_dict.update({path: symlink_path})\n",
    "    return symlink_path_dict\n",
    "\n",
    "\n",
    "def get_resource_for_task(task_name, model_name_or_path):\n",
    "    if 'gpt2' in model_name_or_path:\n",
    "        return 50, 1\n",
    "    \n",
    "    batch_size = 10\n",
    "    if task_name == 'gsm':\n",
    "        job_duration = 1 # 10min for n=200\n",
    "    elif task_name == 'bbh_s=0':\n",
    "        job_duration = 1\n",
    "    elif task_name == 'bbh_s=3':\n",
    "        job_duration = 1\n",
    "        batch_size = 5 # for longer prompts.\n",
    "    elif task_name == 'mmlu':\n",
    "        job_duration = 1\n",
    "    elif task_name == 'humaneval':\n",
    "        job_duration = 1 # pass@1: 10min, pass@10: 100min\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        job_duration = 1\n",
    "    else:\n",
    "        job_duration = 1\n",
    "    return batch_size, job_duration\n",
    "\n",
    "def finished_training(path):\n",
    "    return os.path.isfile(os.path.join(path, 'config.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4395657f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed symlink: results/ft2/symlink_71748a62\n",
      "Removed symlink: results/ft2/symlink_b69216e3\n",
      "Removed symlink: results/ft2/symlink_2cd61ebf\n",
      "Removed symlink: results/ft2/symlink_864006d6\n",
      "Removed symlink: results/ft2/symlink_6362993c\n",
      "Removed symlink: results/ft2/symlink_aca1e04a\n",
      "Removed symlink: results/ft2/symlink_6b7c9bf5\n",
      "Removed symlink: results/ft2/symlink_1fd9d380\n",
      "Removed symlink: results/ft2/symlink_d5497355\n",
      "Removed symlink: results/ft2/symlink_f1d184cc\n",
      "Removed symlink: results/ft2/symlink_6c843505\n",
      "Removed symlink: results/ft2/symlink_7e067943\n",
      "Removed symlink: results/ft2/symlink_5ca9537e\n",
      "Removed symlink: results/ft2/symlink_0ddb8c03\n",
      "Removed symlink: results/ft2/symlink_d58e4c30\n",
      "Removed symlink: results/ft2/symlink_431fda5a\n",
      "Removed symlink: results/ft2/symlink_bade8d34\n",
      "Removed symlink: results/ft2/symlink_49414029\n",
      "Removed symlink: results/ft2/symlink_33664be4\n",
      "Removed symlink: results/ft2/symlink_435568ed\n",
      "Removed symlink: results/ft2/symlink_52918cf7\n",
      "Removed symlink: results/ft2/symlink_06e3dec5\n",
      "Removed symlink: results/ft2/symlink_f693a1f2\n",
      "Removed symlink: results/ft2/symlink_ef41f3b8\n",
      "Removed symlink: results/ft2/symlink_14c41479\n",
      "Removed symlink: results/ft2/symlink_49d1a885\n",
      "Created symlink: results/ft2/symlink_bf5d69f7 -> results/ft2/gpt2m_all_mix=sharegpt:100000\n",
      "Created symlink: results/ft2/symlink_ebdc4ae0 -> results/ft2/gpt2m_all_mix=baize:16666,code_alpaca:16666,cot:16666,dolly:16666,flan_v2:16666,gpt4_alpaca:16666,oasst1:16666,self_instruct:16666,sharegpt:16666,stanford_alpaca:16666,super_ni:16666,unnatural_instructions:16666\n",
      "Created symlink: results/ft2/symlink_1099903b -> results/ft2/gpt2m_all_mix=cot:50000,dolly:50000,flan_v2:50000,oasst1:50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['results/ft2/symlink_bf5d69f7',\n",
       " 'results/ft2/symlink_ebdc4ae0',\n",
       " 'results/ft2/symlink_1099903b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = 'results/ft2'\n",
    "remove_all_symlinks(exp_dir)\n",
    "subdirs = [x for x in os.listdir(exp_dir) \n",
    "           if 'gpt2m_all_mix=baize:16666,cod' in x or\n",
    "           'gpt2m_all_mix=cot:50000,dolly:50' in x or \n",
    "           'gpt2m_all_mix=sharegpt:100000' in x\n",
    "          ]\n",
    "symlink_path_dict = create_unique_symlinks(exp_dir, subdirs)\n",
    "list(symlink_path_dict.values())\n",
    "# symlink_path_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f98a946f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed symlink: results/ft2/symlink_a106378f\n",
      "Removed symlink: results/ft2/symlink_ebc791b1\n",
      "Removed symlink: results/ft2/symlink_201f777f\n",
      "Removed symlink: results/ft2/symlink_b8646dd0\n",
      "Removed symlink: results/ft2/symlink_70242ee9\n",
      "Removed symlink: results/ft2/symlink_09660630\n",
      "Removed symlink: results/ft2/symlink_031d2f87\n",
      "Removed symlink: results/ft2/symlink_793a0e7d\n",
      "Removed symlink: results/ft2/symlink_a5389b31\n",
      "Removed symlink: results/ft2/symlink_8f01e528\n",
      "Removed symlink: results/ft2/symlink_8e98a6fd\n",
      "Removed symlink: results/ft2/symlink_ebc8ab2c\n",
      "Removed symlink: results/ft2/symlink_9146fd57\n",
      "Removed symlink: results/ft2/symlink_37743b28\n",
      "Removed symlink: results/ft2/symlink_dc39419f\n",
      "Removed symlink: results/ft2/symlink_cbdb8856\n",
      "Removed symlink: results/ft2/symlink_3c05776c\n",
      "Removed symlink: results/ft2/symlink_1566fe22\n",
      "Removed symlink: results/ft2/symlink_d33ce11d\n",
      "Removed symlink: results/ft2/symlink_8719673b\n",
      "Removed symlink: results/ft2/symlink_925e6d3d\n",
      "Removed symlink: results/ft2/symlink_65eadeca\n",
      "Removed symlink: results/ft2/symlink_8ce5a47c\n",
      "Removed symlink: results/ft2/symlink_efd25355\n",
      "Removed symlink: results/ft2/symlink_839b5475\n",
      "Removed symlink: results/ft2/symlink_e32cd2ff\n",
      "Removed symlink: results/ft2/symlink_419eb493\n",
      "Removed symlink: results/ft2/symlink_15ce15bc\n",
      "Removed symlink: results/ft2/symlink_f310c77e\n",
      "Removed symlink: results/ft2/symlink_f0bee416\n",
      "Removed symlink: results/ft2/symlink_329f1bee\n",
      "Removed symlink: results/ft2/symlink_98fcd469\n",
      "Removed symlink: results/ft2/symlink_6d938eb6\n",
      "Removed symlink: results/ft2/symlink_8cf7ea06\n",
      "Removed symlink: results/ft2/symlink_48c6679b\n",
      "Removed symlink: results/ft2/symlink_a5de86fd\n",
      "Removed symlink: results/ft2/symlink_3f6e860d\n",
      "Removed symlink: results/ft2/symlink_bb095ae5\n",
      "Removed symlink: results/ft2/symlink_80ca8aaf\n",
      "Removed symlink: results/ft2/symlink_2b1ecc2a\n",
      "Removed symlink: results/ft2/symlink_21a2896e\n",
      "Removed symlink: results/ft2/symlink_9a091f77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('results/ft2/symlink_f9c349a0', 'mmlu_s=0_chatfmt'),\n",
       " ('results/ft2/symlink_f9c349a0', 'gsm_s=8_chatfmt'),\n",
       " ('results/ft2/symlink_f9c349a0', 'gsm_s=8_cot_chatfmt'),\n",
       " ('results/ft2/symlink_f9c349a0', 'bbh_s=3_chatfmt'),\n",
       " ('results/ft2/symlink_f9c349a0', 'humaneval_chatfmt'),\n",
       " ('results/ft2/symlink_f9c349a0', 'tydiqa_s=1_cb_chatfmt'),\n",
       " ('results/ft2/symlink_f9c349a0', 'tydiqa_s=1_gp_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'mmlu_s=0_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'gsm_s=8_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'gsm_s=8_cot_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'bbh_s=3_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'humaneval_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'tydiqa_s=1_cb_chatfmt'),\n",
       " ('results/ft2/symlink_1c0d60eb', 'tydiqa_s=1_gp_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'mmlu_s=0_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'gsm_s=8_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'gsm_s=8_cot_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'bbh_s=3_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'humaneval_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'tydiqa_s=1_cb_chatfmt'),\n",
       " ('results/ft2/symlink_b966a456', 'tydiqa_s=1_gp_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'mmlu_s=0_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'gsm_s=8_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'gsm_s=8_cot_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'bbh_s=3_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'humaneval_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'tydiqa_s=1_cb_chatfmt'),\n",
       " ('results/ft2/symlink_d29dbf48', 'tydiqa_s=1_gp_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'mmlu_s=0_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'gsm_s=8_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'gsm_s=8_cot_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'bbh_s=3_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'humaneval_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'tydiqa_s=1_cb_chatfmt'),\n",
       " ('results/ft2/symlink_674cfbd4', 'tydiqa_s=1_gp_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'mmlu_s=0_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'gsm_s=8_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'gsm_s=8_cot_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'bbh_s=3_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'humaneval_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'tydiqa_s=1_cb_chatfmt'),\n",
       " ('results/ft2/symlink_b916e3cf', 'tydiqa_s=1_gp_chatfmt')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    ## 'mmlu_s=5', # some exceed max_seq_len error.\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    ## 'bbh_s=3_cot', # runtime too long.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb',\n",
    "    'tydiqa_s=1_gp',\n",
    "]\n",
    "# `use_chat_format`\n",
    "task_names = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "remove_all_symlinks(exp_dir)\n",
    "exp_dir = 'results/ft2/'\n",
    "model_and_task_name = []\n",
    "for subdir in os.listdir(exp_dir):\n",
    "    subdir_path = os.path.join(exp_dir, subdir)\n",
    "    for task_name in task_names:\n",
    "        if not os.path.islink(subdir_path) and \\\n",
    "            not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "            model_and_task_name.append((subdir_path, task_name))\n",
    "# create symlink for each directory.\n",
    "symlink_path_dict = create_unique_symlinks(\n",
    "    list([x[0] for x in model_and_task_name]))\n",
    "model_and_task_name = list(map(lambda x: (symlink_path_dict[x[0]], x[1]), model_and_task_name))\n",
    "model_and_task_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e9b68375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m eval.mmlu.run_eval --data_dir data/eval/mmlu --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/mmlu_s=0\" --eval_batch_size 10 --ntrain 0\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.mmlu_s=0\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/gsm_s=8\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --max_new_tokens 256 --no_cot\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.gsm.run_eval --data_dir data/eval/gsm/ --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/gsm_s=8_cot\" --eval_batch_size 10 --max_num_examples 200 --n_shot 8 --max_new_tokens 256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.gsm_s=8_cot\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.bbh.run_eval --data_dir data/eval/bbh/ --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/bbh_s=3\" --eval_batch_size 5 --max_new_tokens 256 --n_shot 3 --no_cot\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.bbh_s=3\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.codex_humaneval.run_eval --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/humaneval\" --eval_batch_size 10 --eval_pass_at_ks 1 --unbiased_sampling_size_n 1 --temperature 0.1\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.humaneval\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/tydiqa_s=1_cb\" --eval_batch_size 10 --no_context\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_cb\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python -m eval.tydiqa.run_eval --data_dir data/eval/tydiqa --n_shot 1 --max_num_examples_per_lang 100 --max_context_length 512 --model_name_or_path \"results/baselines/huggyllama/llama-7b\" --save_dir \"results/baselines/huggyllama/llama-7b/eval/tydiqa_s=1_gp\" --eval_batch_size 10\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"eval.tydiqa_s=1_gp\",\n",
      "    \"num_cpus\": 24,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task_name = 'mmlu'\n",
    "task_name = 'gsm'\n",
    "task_name = 'bbh_s=0'\n",
    "task_name = 'bbh_s=3'\n",
    "task_name = 'humaneval'\n",
    "task_name = 'tydiqa_cb'\n",
    "task_name = 'tydiqa_gp'\n",
    "job_name = f'eval.{task_name}'\n",
    "job_name = 'eval'\n",
    "\n",
    "test_run = 1\n",
    "eval_rest = 0\n",
    "test_run = bool(test_run)\n",
    "eval_test = bool(eval_rest)\n",
    "num_cpus = 10; cpu_mem = 32 # mem usage quite small for llama7b+lora on bbh\n",
    "num_cpus = 24; cpu_mem = 64\n",
    "\n",
    "models = []\n",
    "models += [os.path.join('results/baselines', x) for x in [\n",
    "    'huggyllama/llama-7b', \n",
    "#     'NousResearch/Llama-2-7b-hf',\n",
    "#     'gpt2',\n",
    "#     'gpt2-medium',\n",
    "]]\n",
    "\n",
    "\n",
    "task_names = [\n",
    "    'mmlu_s=0',\n",
    "    ## 'mmlu_s=5', # some exceed max_seq_len error.\n",
    "    'gsm_s=8',\n",
    "    'gsm_s=8_cot',\n",
    "    'bbh_s=3',\n",
    "    ## 'bbh_s=3_cot', # runtime too long.\n",
    "    'humaneval',\n",
    "    'tydiqa_s=1_cb',\n",
    "    'tydiqa_s=1_gp',\n",
    "]\n",
    "# `use_chat_format`\n",
    "task_names = [x+'_chatfmt' for x in task_names]\n",
    "\n",
    "\n",
    "# exp_dir = 'results/ft1'\n",
    "# exp_dir = 'results/ft2/'\n",
    "\n",
    "\n",
    "if eval_rest:\n",
    "    remove_all_symlinks(exp_dir)\n",
    "    task_name_and_model = []\n",
    "    for subdir in os.listdir(exp_dir):\n",
    "        \n",
    "        subdir_path = os.path.join(exp_dir, subdir)\n",
    "        for task_name in task_names:\n",
    "            if not os.path.islink(subdir_path) and \\\n",
    "                not os.path.isfile(os.path.join(subdir_path, 'eval', task_name, 'metrics.json')):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "                print((task_name, subdir_path))\n",
    "    # create symlink for each directory.\n",
    "    symlink_path_dict = create_unique_symlinks(\n",
    "        list([x[1] for x in task_name_and_model]))\n",
    "    options_list = list(map(lambda x: (x[0], symlink_path_dict[x[1]]), task_name_and_model))\n",
    "else:\n",
    "    options_list = itertools.product(\n",
    "        task_names,\n",
    "        models,\n",
    "    )\n",
    "\n",
    "info = {}  \n",
    "cmds = []\n",
    "for task_name, model_name_or_path in options_list:\n",
    "    \n",
    "    use_chat_format = 'chatfmt' in task_name\n",
    "    \n",
    "#     with open(os.path.join(model_name_or_path, 'ft_args.json'), 'r') as f:\n",
    "#         ft_args = json.load(f)\n",
    "        \n",
    "#     # note `model_name_or_path` could be anything, e.g., soft links with arbitrary names.\n",
    "#     # but `ft_args_model_name_or_path` indicates the finetuned model name.\n",
    "#     ft_args_model_name_or_path = ft_args['model_args']['model_name_or_path']\n",
    "    ft_args_model_name_or_path = 'llama-7b'\n",
    "    if 'gpt2' in ft_args_model_name_or_path:\n",
    "        tydiqa_max_context_length = 400 # max ctx len without exceeding max_seq_len\n",
    "    else:\n",
    "        tydiqa_max_context_length = 512\n",
    "    \n",
    "    job_name = f'eval.{task_name}'\n",
    "    batch_size, job_duration = get_resource_for_task(\n",
    "        task_name, ft_args_model_name_or_path)\n",
    "    \n",
    "    run_id = model_name_or_path\n",
    "    save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "    \n",
    "    if task_name.startswith('mmlu'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 5)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.mmlu.run_eval \\\n",
    "            --data_dir data/eval/mmlu \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --ntrain {n_shot} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('gsm'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 8)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.gsm.run_eval \\\n",
    "            --data_dir data/eval/gsm/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_num_examples 200 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''} \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('bbh'):\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot <= 3)\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.bbh.run_eval \\\n",
    "            --data_dir data/eval/bbh/ \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --max_new_tokens 256 \\\n",
    "            --n_shot {n_shot} \\\n",
    "            {'--no_cot' if 'cot' not in task_name else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('humaneval'):\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.codex_humaneval.run_eval \\\n",
    "            --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            --eval_pass_at_ks 1 \\\n",
    "            --unbiased_sampling_size_n 1 \\\n",
    "            --temperature 0.1 \\\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    elif task_name.startswith('tydiqa'):\n",
    "        no_context = 'cb' in task_name\n",
    "        match = re.search(r's=(\\d+)', task_name)\n",
    "        n_shot = int(match.group(1))\n",
    "        assert(n_shot in [0,1])\n",
    "        cmd = f\"\"\"\n",
    "        python -m eval.tydiqa.run_eval \\\n",
    "            --data_dir data/eval/tydiqa \\\n",
    "            --n_shot {n_shot} \\\n",
    "            --max_num_examples_per_lang 100 \\\n",
    "            --max_context_length {tydiqa_max_context_length} \\\n",
    "            --model_name_or_path \"{model_name_or_path}\" \\\n",
    "            --save_dir \"{save_dir}\" \\\n",
    "            --eval_batch_size {batch_size} \\\n",
    "            {'--no_context' if no_context else ''}\n",
    "            {'--use_chat_format' if use_chat_format else ''}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f'{task_name} not supported.')\n",
    "        \n",
    "        \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "    \n",
    "    # submit\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd,\n",
    "        log_dir=os.getcwd(),\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=job_name,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=1,\n",
    "        test_run=test_run,\n",
    "        job_duration=job_duration,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f8911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7aef11ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b9c70_row0_col0, #T_b9c70_row0_col1, #T_b9c70_row0_col2, #T_b9c70_row1_col0, #T_b9c70_row1_col1, #T_b9c70_row1_col2, #T_b9c70_row2_col0, #T_b9c70_row2_col1, #T_b9c70_row2_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b9c70_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c70_row0_col4, #T_b9c70_row0_col5, #T_b9c70_row0_col9, #T_b9c70_row0_col10, #T_b9c70_row0_col11, #T_b9c70_row1_col3, #T_b9c70_row1_col5, #T_b9c70_row1_col6, #T_b9c70_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c70_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a688;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b9c70_row0_col7, #T_b9c70_row1_col4, #T_b9c70_row1_col7, #T_b9c70_row1_col8, #T_b9c70_row1_col9, #T_b9c70_row2_col3, #T_b9c70_row2_col4, #T_b9c70_row2_col5, #T_b9c70_row2_col6, #T_b9c70_row2_col7, #T_b9c70_row2_col10, #T_b9c70_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c70_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c70_row1_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e9785d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c70_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #ec8165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b9c70_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b9c70\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b9c70_level0_col0\" class=\"col_heading level0 col0\" >total_train_samples</th>\n",
       "      <th id=\"T_b9c70_level0_col1\" class=\"col_heading level0 col1\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_b9c70_level0_col2\" class=\"col_heading level0 col2\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_b9c70_level0_col3\" class=\"col_heading level0 col3\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_b9c70_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_b9c70_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_b9c70_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_b9c70_level0_col7\" class=\"col_heading level0 col7\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_b9c70_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_b9c70_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_b9c70_level0_col10\" class=\"col_heading level0 col10\" >Average</th>\n",
       "      <th id=\"T_b9c70_level0_col11\" class=\"col_heading level0 col11\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c70_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b9c70_row0_col0\" class=\"data row0 col0\" >9998</td>\n",
       "      <td id=\"T_b9c70_row0_col1\" class=\"data row0 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_b9c70_row0_col2\" class=\"data row0 col2\" >{'cot': 4878, 'dolly': 73, 'flan_v2': 4878, 'oasst1': 169}</td>\n",
       "      <td id=\"T_b9c70_row0_col3\" class=\"data row0 col3\" >22.95</td>\n",
       "      <td id=\"T_b9c70_row0_col4\" class=\"data row0 col4\" >4.00</td>\n",
       "      <td id=\"T_b9c70_row0_col5\" class=\"data row0 col5\" >3.50</td>\n",
       "      <td id=\"T_b9c70_row0_col6\" class=\"data row0 col6\" >28.11</td>\n",
       "      <td id=\"T_b9c70_row0_col7\" class=\"data row0 col7\" >0.00</td>\n",
       "      <td id=\"T_b9c70_row0_col8\" class=\"data row0 col8\" >1.08</td>\n",
       "      <td id=\"T_b9c70_row0_col9\" class=\"data row0 col9\" >6.65</td>\n",
       "      <td id=\"T_b9c70_row0_col10\" class=\"data row0 col10\" >9.47</td>\n",
       "      <td id=\"T_b9c70_row0_col11\" class=\"data row0 col11\" >-15.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c70_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b9c70_row1_col0\" class=\"data row1 col0\" >10005</td>\n",
       "      <td id=\"T_b9c70_row1_col1\" class=\"data row1 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_b9c70_row1_col2\" class=\"data row1 col2\" >{'cot': 1127, 'dolly': 139, 'flan_v2': 8726, 'oasst1': 13}</td>\n",
       "      <td id=\"T_b9c70_row1_col3\" class=\"data row1 col3\" >23.16</td>\n",
       "      <td id=\"T_b9c70_row1_col4\" class=\"data row1 col4\" >3.00</td>\n",
       "      <td id=\"T_b9c70_row1_col5\" class=\"data row1 col5\" >3.50</td>\n",
       "      <td id=\"T_b9c70_row1_col6\" class=\"data row1 col6\" >29.47</td>\n",
       "      <td id=\"T_b9c70_row1_col7\" class=\"data row1 col7\" >0.00</td>\n",
       "      <td id=\"T_b9c70_row1_col8\" class=\"data row1 col8\" >0.92</td>\n",
       "      <td id=\"T_b9c70_row1_col9\" class=\"data row1 col9\" >4.98</td>\n",
       "      <td id=\"T_b9c70_row1_col10\" class=\"data row1 col10\" >9.29</td>\n",
       "      <td id=\"T_b9c70_row1_col11\" class=\"data row1 col11\" >-16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9c70_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b9c70_row2_col0\" class=\"data row2 col0\" >10000</td>\n",
       "      <td id=\"T_b9c70_row2_col1\" class=\"data row2 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_b9c70_row2_col2\" class=\"data row2 col2\" >{'cot': 2500, 'dolly': 2500, 'flan_v2': 2500, 'oasst1': 2500}</td>\n",
       "      <td id=\"T_b9c70_row2_col3\" class=\"data row2 col3\" >22.85</td>\n",
       "      <td id=\"T_b9c70_row2_col4\" class=\"data row2 col4\" >3.00</td>\n",
       "      <td id=\"T_b9c70_row2_col5\" class=\"data row2 col5\" >1.00</td>\n",
       "      <td id=\"T_b9c70_row2_col6\" class=\"data row2 col6\" >24.64</td>\n",
       "      <td id=\"T_b9c70_row2_col7\" class=\"data row2 col7\" >0.00</td>\n",
       "      <td id=\"T_b9c70_row2_col8\" class=\"data row2 col8\" >1.95</td>\n",
       "      <td id=\"T_b9c70_row2_col9\" class=\"data row2 col9\" >5.61</td>\n",
       "      <td id=\"T_b9c70_row2_col10\" class=\"data row2 col10\" >8.44</td>\n",
       "      <td id=\"T_b9c70_row2_col11\" class=\"data row2 col11\" >-19.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff6f0b6230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_715f7_row0_col0, #T_715f7_row0_col1, #T_715f7_row0_col2, #T_715f7_row1_col0, #T_715f7_row1_col1, #T_715f7_row1_col2, #T_715f7_row2_col0, #T_715f7_row2_col1, #T_715f7_row2_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_715f7_row0_col3, #T_715f7_row0_col4, #T_715f7_row0_col6, #T_715f7_row0_col8, #T_715f7_row0_col10, #T_715f7_row0_col11, #T_715f7_row1_col4, #T_715f7_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_715f7_row0_col5, #T_715f7_row0_col7, #T_715f7_row1_col5, #T_715f7_row1_col7, #T_715f7_row1_col8, #T_715f7_row1_col9, #T_715f7_row1_col10, #T_715f7_row2_col3, #T_715f7_row2_col4, #T_715f7_row2_col5, #T_715f7_row2_col6, #T_715f7_row2_col7, #T_715f7_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_715f7_row0_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_715f7_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_715f7_row1_col6, #T_715f7_row2_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ac8e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_715f7_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_715f7_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_715f7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_715f7_level0_col0\" class=\"col_heading level0 col0\" >total_train_samples</th>\n",
       "      <th id=\"T_715f7_level0_col1\" class=\"col_heading level0 col1\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_715f7_level0_col2\" class=\"col_heading level0 col2\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_715f7_level0_col3\" class=\"col_heading level0 col3\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_715f7_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_715f7_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_715f7_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_715f7_level0_col7\" class=\"col_heading level0 col7\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_715f7_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_715f7_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_715f7_level0_col10\" class=\"col_heading level0 col10\" >Average</th>\n",
       "      <th id=\"T_715f7_level0_col11\" class=\"col_heading level0 col11\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_715f7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_715f7_row0_col0\" class=\"data row0 col0\" >50000</td>\n",
       "      <td id=\"T_715f7_row0_col1\" class=\"data row0 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_715f7_row0_col2\" class=\"data row0 col2\" >{'cot': 12500, 'dolly': 12500, 'flan_v2': 12500, 'oasst1': 12500}</td>\n",
       "      <td id=\"T_715f7_row0_col3\" class=\"data row0 col3\" >22.95</td>\n",
       "      <td id=\"T_715f7_row0_col4\" class=\"data row0 col4\" >3.00</td>\n",
       "      <td id=\"T_715f7_row0_col5\" class=\"data row0 col5\" >3.00</td>\n",
       "      <td id=\"T_715f7_row0_col6\" class=\"data row0 col6\" >30.28</td>\n",
       "      <td id=\"T_715f7_row0_col7\" class=\"data row0 col7\" >0.00</td>\n",
       "      <td id=\"T_715f7_row0_col8\" class=\"data row0 col8\" >1.90</td>\n",
       "      <td id=\"T_715f7_row0_col9\" class=\"data row0 col9\" >7.08</td>\n",
       "      <td id=\"T_715f7_row0_col10\" class=\"data row0 col10\" >9.75</td>\n",
       "      <td id=\"T_715f7_row0_col11\" class=\"data row0 col11\" >-12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_715f7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_715f7_row1_col0\" class=\"data row1 col0\" >49998</td>\n",
       "      <td id=\"T_715f7_row1_col1\" class=\"data row1 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_715f7_row1_col2\" class=\"data row1 col2\" >{'cot': 24392, 'dolly': 366, 'flan_v2': 24392, 'oasst1': 848}</td>\n",
       "      <td id=\"T_715f7_row1_col3\" class=\"data row1 col3\" >22.90</td>\n",
       "      <td id=\"T_715f7_row1_col4\" class=\"data row1 col4\" >3.00</td>\n",
       "      <td id=\"T_715f7_row1_col5\" class=\"data row1 col5\" >3.00</td>\n",
       "      <td id=\"T_715f7_row1_col6\" class=\"data row1 col6\" >30.03</td>\n",
       "      <td id=\"T_715f7_row1_col7\" class=\"data row1 col7\" >0.00</td>\n",
       "      <td id=\"T_715f7_row1_col8\" class=\"data row1 col8\" >1.45</td>\n",
       "      <td id=\"T_715f7_row1_col9\" class=\"data row1 col9\" >6.06</td>\n",
       "      <td id=\"T_715f7_row1_col10\" class=\"data row1 col10\" >9.49</td>\n",
       "      <td id=\"T_715f7_row1_col11\" class=\"data row1 col11\" >-15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_715f7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_715f7_row2_col0\" class=\"data row2 col0\" >50031</td>\n",
       "      <td id=\"T_715f7_row2_col1\" class=\"data row2 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_715f7_row2_col2\" class=\"data row2 col2\" >{'cot': 5635, 'dolly': 697, 'flan_v2': 43630, 'oasst1': 69}</td>\n",
       "      <td id=\"T_715f7_row2_col3\" class=\"data row2 col3\" >22.85</td>\n",
       "      <td id=\"T_715f7_row2_col4\" class=\"data row2 col4\" >2.50</td>\n",
       "      <td id=\"T_715f7_row2_col5\" class=\"data row2 col5\" >3.00</td>\n",
       "      <td id=\"T_715f7_row2_col6\" class=\"data row2 col6\" >29.43</td>\n",
       "      <td id=\"T_715f7_row2_col7\" class=\"data row2 col7\" >0.00</td>\n",
       "      <td id=\"T_715f7_row2_col8\" class=\"data row2 col8\" >1.46</td>\n",
       "      <td id=\"T_715f7_row2_col9\" class=\"data row2 col9\" >8.44</td>\n",
       "      <td id=\"T_715f7_row2_col10\" class=\"data row2 col10\" >9.67</td>\n",
       "      <td id=\"T_715f7_row2_col11\" class=\"data row2 col11\" >-15.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff6f0b6230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3719e_row0_col0, #T_3719e_row0_col1, #T_3719e_row0_col2, #T_3719e_row1_col0, #T_3719e_row1_col1, #T_3719e_row1_col2, #T_3719e_row2_col0, #T_3719e_row2_col1, #T_3719e_row2_col2, #T_3719e_row3_col0, #T_3719e_row3_col1, #T_3719e_row3_col2, #T_3719e_row4_col0, #T_3719e_row4_col1, #T_3719e_row4_col2, #T_3719e_row5_col0, #T_3719e_row5_col1, #T_3719e_row5_col2, #T_3719e_row6_col0, #T_3719e_row6_col1, #T_3719e_row6_col2, #T_3719e_row7_col0, #T_3719e_row7_col1, #T_3719e_row7_col2, #T_3719e_row8_col0, #T_3719e_row8_col1, #T_3719e_row8_col2, #T_3719e_row9_col0, #T_3719e_row9_col1, #T_3719e_row9_col2, #T_3719e_row10_col0, #T_3719e_row10_col1, #T_3719e_row10_col2, #T_3719e_row11_col0, #T_3719e_row11_col1, #T_3719e_row11_col2, #T_3719e_row12_col0, #T_3719e_row12_col1, #T_3719e_row12_col2, #T_3719e_row13_col0, #T_3719e_row13_col1, #T_3719e_row13_col2, #T_3719e_row14_col0, #T_3719e_row14_col1, #T_3719e_row14_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_3719e_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row0_col4, #T_3719e_row4_col4, #T_3719e_row5_col4, #T_3719e_row9_col4, #T_3719e_row10_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #e26952;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row0_col5, #T_3719e_row0_col9, #T_3719e_row0_col10, #T_3719e_row0_col11, #T_3719e_row1_col3, #T_3719e_row1_col4, #T_3719e_row4_col6, #T_3719e_row5_col7, #T_3719e_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row0_col7, #T_3719e_row2_col7, #T_3719e_row3_col7, #T_3719e_row6_col7, #T_3719e_row7_col5, #T_3719e_row7_col7, #T_3719e_row10_col8, #T_3719e_row11_col4, #T_3719e_row11_col7, #T_3719e_row11_col9, #T_3719e_row12_col3, #T_3719e_row12_col6, #T_3719e_row12_col7, #T_3719e_row12_col10, #T_3719e_row13_col5, #T_3719e_row13_col7, #T_3719e_row14_col7, #T_3719e_row14_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row1_col5, #T_3719e_row2_col5, #T_3719e_row3_col5, #T_3719e_row6_col5, #T_3719e_row8_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c73635;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row1_col7, #T_3719e_row3_col9, #T_3719e_row9_col7, #T_3719e_row11_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row1_col10, #T_3719e_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row2_col3, #T_3719e_row14_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row2_col4, #T_3719e_row3_col4, #T_3719e_row12_col4, #T_3719e_row13_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row2_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #c12b30;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #eb7d62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row2_col9, #T_3719e_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row2_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row3_col3, #T_3719e_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #d2dbe8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row3_col11, #T_3719e_row6_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row4_col3, #T_3719e_row6_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #7093f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row4_col5, #T_3719e_row9_col5, #T_3719e_row14_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #6f92f3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row4_col7, #T_3719e_row5_col5, #T_3719e_row8_col7, #T_3719e_row10_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #aac7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row4_col8, #T_3719e_row4_col9, #T_3719e_row8_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row4_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row4_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f4c6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row5_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row5_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row5_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row5_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #516ddb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row5_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a385;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row5_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row6_col4, #T_3719e_row7_col4, #T_3719e_row8_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f7a889;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row6_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row6_col8, #T_3719e_row14_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row6_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #e36c55;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row6_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row7_col3, #T_3719e_row8_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row7_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row7_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row7_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f59f80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row7_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row7_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #cedaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row8_col3, #T_3719e_row14_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row8_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f18f71;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row8_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row8_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row9_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row9_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f39475;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row9_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row9_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #f1ccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row9_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row10_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #6b8df0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row10_col5, #T_3719e_row12_col5 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row10_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b79b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row10_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row10_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #d5dbe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row10_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #6384eb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row11_col3, #T_3719e_row14_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row11_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row11_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row11_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b3cdfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row11_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row12_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row12_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row13_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row13_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row13_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row13_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row13_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3719e_row14_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #f7af91;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3719e_row14_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3719e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3719e_level0_col0\" class=\"col_heading level0 col0\" >total_train_samples</th>\n",
       "      <th id=\"T_3719e_level0_col1\" class=\"col_heading level0 col1\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_3719e_level0_col2\" class=\"col_heading level0 col2\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_3719e_level0_col3\" class=\"col_heading level0 col3\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_3719e_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_3719e_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_3719e_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_3719e_level0_col7\" class=\"col_heading level0 col7\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_3719e_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_3719e_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_3719e_level0_col10\" class=\"col_heading level0 col10\" >Average</th>\n",
       "      <th id=\"T_3719e_level0_col11\" class=\"col_heading level0 col11\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3719e_row0_col0\" class=\"data row0 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row0_col1\" class=\"data row0 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row0_col2\" class=\"data row0 col2\" >{'flan_v2': 100000}</td>\n",
       "      <td id=\"T_3719e_row0_col3\" class=\"data row0 col3\" >23.00</td>\n",
       "      <td id=\"T_3719e_row0_col4\" class=\"data row0 col4\" >3.50</td>\n",
       "      <td id=\"T_3719e_row0_col5\" class=\"data row0 col5\" >3.00</td>\n",
       "      <td id=\"T_3719e_row0_col6\" class=\"data row0 col6\" >29.76</td>\n",
       "      <td id=\"T_3719e_row0_col7\" class=\"data row0 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row0_col8\" class=\"data row0 col8\" >1.82</td>\n",
       "      <td id=\"T_3719e_row0_col9\" class=\"data row0 col9\" >8.84</td>\n",
       "      <td id=\"T_3719e_row0_col10\" class=\"data row0 col10\" >9.99</td>\n",
       "      <td id=\"T_3719e_row0_col11\" class=\"data row0 col11\" >-9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3719e_row1_col0\" class=\"data row1 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row1_col1\" class=\"data row1 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row1_col2\" class=\"data row1 col2\" >{'self_instruct': 100000}</td>\n",
       "      <td id=\"T_3719e_row1_col3\" class=\"data row1 col3\" >23.56</td>\n",
       "      <td id=\"T_3719e_row1_col4\" class=\"data row1 col4\" >4.00</td>\n",
       "      <td id=\"T_3719e_row1_col5\" class=\"data row1 col5\" >2.50</td>\n",
       "      <td id=\"T_3719e_row1_col6\" class=\"data row1 col6\" >29.59</td>\n",
       "      <td id=\"T_3719e_row1_col7\" class=\"data row1 col7\" >1.22</td>\n",
       "      <td id=\"T_3719e_row1_col8\" class=\"data row1 col8\" >1.41</td>\n",
       "      <td id=\"T_3719e_row1_col9\" class=\"data row1 col9\" >6.14</td>\n",
       "      <td id=\"T_3719e_row1_col10\" class=\"data row1 col10\" >9.77</td>\n",
       "      <td id=\"T_3719e_row1_col11\" class=\"data row1 col11\" >-10.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3719e_row2_col0\" class=\"data row2 col0\" >99999</td>\n",
       "      <td id=\"T_3719e_row2_col1\" class=\"data row2 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row2_col2\" class=\"data row2 col2\" >{'cot': 40031, 'dolly': 6009, 'flan_v2': 40031, 'oasst1': 13928}</td>\n",
       "      <td id=\"T_3719e_row2_col3\" class=\"data row2 col3\" >23.02</td>\n",
       "      <td id=\"T_3719e_row2_col4\" class=\"data row2 col4\" >2.50</td>\n",
       "      <td id=\"T_3719e_row2_col5\" class=\"data row2 col5\" >2.50</td>\n",
       "      <td id=\"T_3719e_row2_col6\" class=\"data row2 col6\" >29.83</td>\n",
       "      <td id=\"T_3719e_row2_col7\" class=\"data row2 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row2_col8\" class=\"data row2 col8\" >2.16</td>\n",
       "      <td id=\"T_3719e_row2_col9\" class=\"data row2 col9\" >7.92</td>\n",
       "      <td id=\"T_3719e_row2_col10\" class=\"data row2 col10\" >9.70</td>\n",
       "      <td id=\"T_3719e_row2_col11\" class=\"data row2 col11\" >-12.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3719e_row3_col0\" class=\"data row3 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row3_col1\" class=\"data row3 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row3_col2\" class=\"data row3 col2\" >{'super_ni': 100000}</td>\n",
       "      <td id=\"T_3719e_row3_col3\" class=\"data row3 col3\" >23.15</td>\n",
       "      <td id=\"T_3719e_row3_col4\" class=\"data row3 col4\" >2.50</td>\n",
       "      <td id=\"T_3719e_row3_col5\" class=\"data row3 col5\" >2.50</td>\n",
       "      <td id=\"T_3719e_row3_col6\" class=\"data row3 col6\" >29.26</td>\n",
       "      <td id=\"T_3719e_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row3_col8\" class=\"data row3 col8\" >1.88</td>\n",
       "      <td id=\"T_3719e_row3_col9\" class=\"data row3 col9\" >7.12</td>\n",
       "      <td id=\"T_3719e_row3_col10\" class=\"data row3 col10\" >9.49</td>\n",
       "      <td id=\"T_3719e_row3_col11\" class=\"data row3 col11\" >-13.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3719e_row4_col0\" class=\"data row4 col0\" >99996</td>\n",
       "      <td id=\"T_3719e_row4_col1\" class=\"data row4 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row4_col2\" class=\"data row4 col2\" >{'baize': 8333, 'code_alpaca': 8333, 'cot': 8333, 'dolly': 8333, 'flan_v2': 8333, 'gpt4_alpaca': 8333, 'oasst1': 8333, 'self_instruct': 8333, 'sharegpt': 8333, 'stanford_alpaca': 8333, 'super_ni': 8333, 'unnatural_instructions': 8333}</td>\n",
       "      <td id=\"T_3719e_row4_col3\" class=\"data row4 col3\" >22.94</td>\n",
       "      <td id=\"T_3719e_row4_col4\" class=\"data row4 col4\" >3.50</td>\n",
       "      <td id=\"T_3719e_row4_col5\" class=\"data row4 col5\" >0.50</td>\n",
       "      <td id=\"T_3719e_row4_col6\" class=\"data row4 col6\" >30.39</td>\n",
       "      <td id=\"T_3719e_row4_col7\" class=\"data row4 col7\" >0.61</td>\n",
       "      <td id=\"T_3719e_row4_col8\" class=\"data row4 col8\" >1.77</td>\n",
       "      <td id=\"T_3719e_row4_col9\" class=\"data row4 col9\" >5.95</td>\n",
       "      <td id=\"T_3719e_row4_col10\" class=\"data row4 col10\" >9.38</td>\n",
       "      <td id=\"T_3719e_row4_col11\" class=\"data row4 col11\" >-14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3719e_row5_col0\" class=\"data row5 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row5_col1\" class=\"data row5 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row5_col2\" class=\"data row5 col2\" >{'unnatural_instructions': 100000}</td>\n",
       "      <td id=\"T_3719e_row5_col3\" class=\"data row5 col3\" >23.20</td>\n",
       "      <td id=\"T_3719e_row5_col4\" class=\"data row5 col4\" >3.50</td>\n",
       "      <td id=\"T_3719e_row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
       "      <td id=\"T_3719e_row5_col6\" class=\"data row5 col6\" >29.13</td>\n",
       "      <td id=\"T_3719e_row5_col7\" class=\"data row5 col7\" >1.83</td>\n",
       "      <td id=\"T_3719e_row5_col8\" class=\"data row5 col8\" >1.73</td>\n",
       "      <td id=\"T_3719e_row5_col9\" class=\"data row5 col9\" >4.10</td>\n",
       "      <td id=\"T_3719e_row5_col10\" class=\"data row5 col10\" >9.21</td>\n",
       "      <td id=\"T_3719e_row5_col11\" class=\"data row5 col11\" >-14.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3719e_row6_col0\" class=\"data row6 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row6_col1\" class=\"data row6 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row6_col2\" class=\"data row6 col2\" >{'cot': 25000, 'dolly': 25000, 'flan_v2': 25000, 'oasst1': 25000}</td>\n",
       "      <td id=\"T_3719e_row6_col3\" class=\"data row6 col3\" >22.94</td>\n",
       "      <td id=\"T_3719e_row6_col4\" class=\"data row6 col4\" >3.00</td>\n",
       "      <td id=\"T_3719e_row6_col5\" class=\"data row6 col5\" >2.50</td>\n",
       "      <td id=\"T_3719e_row6_col6\" class=\"data row6 col6\" >29.98</td>\n",
       "      <td id=\"T_3719e_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row6_col8\" class=\"data row6 col8\" >1.45</td>\n",
       "      <td id=\"T_3719e_row6_col9\" class=\"data row6 col9\" >7.09</td>\n",
       "      <td id=\"T_3719e_row6_col10\" class=\"data row6 col10\" >9.56</td>\n",
       "      <td id=\"T_3719e_row6_col11\" class=\"data row6 col11\" >-15.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3719e_row7_col0\" class=\"data row7 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row7_col1\" class=\"data row7 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row7_col2\" class=\"data row7 col2\" >{'cot': 100000}</td>\n",
       "      <td id=\"T_3719e_row7_col3\" class=\"data row7 col3\" >23.02</td>\n",
       "      <td id=\"T_3719e_row7_col4\" class=\"data row7 col4\" >3.00</td>\n",
       "      <td id=\"T_3719e_row7_col5\" class=\"data row7 col5\" >0.00</td>\n",
       "      <td id=\"T_3719e_row7_col6\" class=\"data row7 col6\" >27.56</td>\n",
       "      <td id=\"T_3719e_row7_col7\" class=\"data row7 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row7_col8\" class=\"data row7 col8\" >1.93</td>\n",
       "      <td id=\"T_3719e_row7_col9\" class=\"data row7 col9\" >7.50</td>\n",
       "      <td id=\"T_3719e_row7_col10\" class=\"data row7 col10\" >9.00</td>\n",
       "      <td id=\"T_3719e_row7_col11\" class=\"data row7 col11\" >-16.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3719e_row8_col0\" class=\"data row8 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row8_col1\" class=\"data row8 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row8_col2\" class=\"data row8 col2\" >{'stanford_alpaca': 100000}</td>\n",
       "      <td id=\"T_3719e_row8_col3\" class=\"data row8 col3\" >22.99</td>\n",
       "      <td id=\"T_3719e_row8_col4\" class=\"data row8 col4\" >3.00</td>\n",
       "      <td id=\"T_3719e_row8_col5\" class=\"data row8 col5\" >2.50</td>\n",
       "      <td id=\"T_3719e_row8_col6\" class=\"data row8 col6\" >27.05</td>\n",
       "      <td id=\"T_3719e_row8_col7\" class=\"data row8 col7\" >0.61</td>\n",
       "      <td id=\"T_3719e_row8_col8\" class=\"data row8 col8\" >1.62</td>\n",
       "      <td id=\"T_3719e_row8_col9\" class=\"data row8 col9\" >5.11</td>\n",
       "      <td id=\"T_3719e_row8_col10\" class=\"data row8 col10\" >8.98</td>\n",
       "      <td id=\"T_3719e_row8_col11\" class=\"data row8 col11\" >-16.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3719e_row9_col0\" class=\"data row9 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row9_col1\" class=\"data row9 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row9_col2\" class=\"data row9 col2\" >{'code_alpaca': 100000}</td>\n",
       "      <td id=\"T_3719e_row9_col3\" class=\"data row9 col3\" >22.97</td>\n",
       "      <td id=\"T_3719e_row9_col4\" class=\"data row9 col4\" >3.50</td>\n",
       "      <td id=\"T_3719e_row9_col5\" class=\"data row9 col5\" >0.50</td>\n",
       "      <td id=\"T_3719e_row9_col6\" class=\"data row9 col6\" >26.88</td>\n",
       "      <td id=\"T_3719e_row9_col7\" class=\"data row9 col7\" >1.22</td>\n",
       "      <td id=\"T_3719e_row9_col8\" class=\"data row9 col8\" >1.38</td>\n",
       "      <td id=\"T_3719e_row9_col9\" class=\"data row9 col9\" >5.53</td>\n",
       "      <td id=\"T_3719e_row9_col10\" class=\"data row9 col10\" >8.85</td>\n",
       "      <td id=\"T_3719e_row9_col11\" class=\"data row9 col11\" >-17.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_3719e_row10_col0\" class=\"data row10 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row10_col1\" class=\"data row10 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row10_col2\" class=\"data row10 col2\" >{'baize': 100000}</td>\n",
       "      <td id=\"T_3719e_row10_col3\" class=\"data row10 col3\" >22.92</td>\n",
       "      <td id=\"T_3719e_row10_col4\" class=\"data row10 col4\" >3.50</td>\n",
       "      <td id=\"T_3719e_row10_col5\" class=\"data row10 col5\" >1.50</td>\n",
       "      <td id=\"T_3719e_row10_col6\" class=\"data row10 col6\" >25.49</td>\n",
       "      <td id=\"T_3719e_row10_col7\" class=\"data row10 col7\" >0.61</td>\n",
       "      <td id=\"T_3719e_row10_col8\" class=\"data row10 col8\" >1.33</td>\n",
       "      <td id=\"T_3719e_row10_col9\" class=\"data row10 col9\" >4.14</td>\n",
       "      <td id=\"T_3719e_row10_col10\" class=\"data row10 col10\" >8.50</td>\n",
       "      <td id=\"T_3719e_row10_col11\" class=\"data row10 col11\" >-19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_3719e_row11_col0\" class=\"data row11 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row11_col1\" class=\"data row11 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row11_col2\" class=\"data row11 col2\" >{'sharegpt': 100000}</td>\n",
       "      <td id=\"T_3719e_row11_col3\" class=\"data row11 col3\" >23.05</td>\n",
       "      <td id=\"T_3719e_row11_col4\" class=\"data row11 col4\" >0.50</td>\n",
       "      <td id=\"T_3719e_row11_col5\" class=\"data row11 col5\" >2.00</td>\n",
       "      <td id=\"T_3719e_row11_col6\" class=\"data row11 col6\" >26.32</td>\n",
       "      <td id=\"T_3719e_row11_col7\" class=\"data row11 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row11_col8\" class=\"data row11 col8\" >1.73</td>\n",
       "      <td id=\"T_3719e_row11_col9\" class=\"data row11 col9\" >3.70</td>\n",
       "      <td id=\"T_3719e_row11_col10\" class=\"data row11 col10\" >8.19</td>\n",
       "      <td id=\"T_3719e_row11_col11\" class=\"data row11 col11\" >-20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_3719e_row12_col0\" class=\"data row12 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row12_col1\" class=\"data row12 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row12_col2\" class=\"data row12 col2\" >{'dolly': 100000}</td>\n",
       "      <td id=\"T_3719e_row12_col3\" class=\"data row12 col3\" >22.81</td>\n",
       "      <td id=\"T_3719e_row12_col4\" class=\"data row12 col4\" >2.50</td>\n",
       "      <td id=\"T_3719e_row12_col5\" class=\"data row12 col5\" >1.50</td>\n",
       "      <td id=\"T_3719e_row12_col6\" class=\"data row12 col6\" >15.48</td>\n",
       "      <td id=\"T_3719e_row12_col7\" class=\"data row12 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row12_col8\" class=\"data row12 col8\" >2.35</td>\n",
       "      <td id=\"T_3719e_row12_col9\" class=\"data row12 col9\" >5.57</td>\n",
       "      <td id=\"T_3719e_row12_col10\" class=\"data row12 col10\" >7.17</td>\n",
       "      <td id=\"T_3719e_row12_col11\" class=\"data row12 col11\" >-20.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_3719e_row13_col0\" class=\"data row13 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row13_col1\" class=\"data row13 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row13_col2\" class=\"data row13 col2\" >{'oasst1': 100000}</td>\n",
       "      <td id=\"T_3719e_row13_col3\" class=\"data row13 col3\" >22.97</td>\n",
       "      <td id=\"T_3719e_row13_col4\" class=\"data row13 col4\" >2.00</td>\n",
       "      <td id=\"T_3719e_row13_col5\" class=\"data row13 col5\" >0.00</td>\n",
       "      <td id=\"T_3719e_row13_col6\" class=\"data row13 col6\" >24.00</td>\n",
       "      <td id=\"T_3719e_row13_col7\" class=\"data row13 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row13_col8\" class=\"data row13 col8\" >1.80</td>\n",
       "      <td id=\"T_3719e_row13_col9\" class=\"data row13 col9\" >6.33</td>\n",
       "      <td id=\"T_3719e_row13_col10\" class=\"data row13 col10\" >8.16</td>\n",
       "      <td id=\"T_3719e_row13_col11\" class=\"data row13 col11\" >-21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3719e_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_3719e_row14_col0\" class=\"data row14 col0\" >100000</td>\n",
       "      <td id=\"T_3719e_row14_col1\" class=\"data row14 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_3719e_row14_col2\" class=\"data row14 col2\" >{'gpt4_alpaca': 100000}</td>\n",
       "      <td id=\"T_3719e_row14_col3\" class=\"data row14 col3\" >23.05</td>\n",
       "      <td id=\"T_3719e_row14_col4\" class=\"data row14 col4\" >1.50</td>\n",
       "      <td id=\"T_3719e_row14_col5\" class=\"data row14 col5\" >0.50</td>\n",
       "      <td id=\"T_3719e_row14_col6\" class=\"data row14 col6\" >25.81</td>\n",
       "      <td id=\"T_3719e_row14_col7\" class=\"data row14 col7\" >0.00</td>\n",
       "      <td id=\"T_3719e_row14_col8\" class=\"data row14 col8\" >1.57</td>\n",
       "      <td id=\"T_3719e_row14_col9\" class=\"data row14 col9\" >4.29</td>\n",
       "      <td id=\"T_3719e_row14_col10\" class=\"data row14 col10\" >8.10</td>\n",
       "      <td id=\"T_3719e_row14_col11\" class=\"data row14 col11\" >-21.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff6f0b6230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d7060_row0_col0, #T_d7060_row0_col1, #T_d7060_row0_col2, #T_d7060_row1_col0, #T_d7060_row1_col1, #T_d7060_row1_col2, #T_d7060_row2_col0, #T_d7060_row2_col1, #T_d7060_row2_col2, #T_d7060_row3_col0, #T_d7060_row3_col1, #T_d7060_row3_col2, #T_d7060_row4_col0, #T_d7060_row4_col1, #T_d7060_row4_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d7060_row0_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row0_col4, #T_d7060_row4_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row0_col5, #T_d7060_row1_col3, #T_d7060_row1_col5, #T_d7060_row1_col7, #T_d7060_row2_col7, #T_d7060_row3_col4, #T_d7060_row3_col8, #T_d7060_row4_col5, #T_d7060_row4_col6, #T_d7060_row4_col7, #T_d7060_row4_col9, #T_d7060_row4_col10, #T_d7060_row4_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row0_col7 {\n",
       "  text-align: left;\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row0_col9, #T_d7060_row0_col10, #T_d7060_row0_col11, #T_d7060_row1_col4, #T_d7060_row1_col8, #T_d7060_row2_col5, #T_d7060_row2_col6, #T_d7060_row3_col7, #T_d7060_row4_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row1_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #cf453c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row1_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row1_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b70d28;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row2_col4 {\n",
       "  text-align: left;\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row2_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f7b194;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row2_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row2_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f59d7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row3_col3 {\n",
       "  text-align: left;\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row3_col5, #T_d7060_row3_col9 {\n",
       "  text-align: left;\n",
       "  background-color: #f49a7b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row3_col6 {\n",
       "  text-align: left;\n",
       "  background-color: #d95847;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row3_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #ca3b37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d7060_row3_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d7060_row4_col8 {\n",
       "  text-align: left;\n",
       "  background-color: #4c66d6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d7060\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d7060_level0_col0\" class=\"col_heading level0 col0\" >total_train_samples</th>\n",
       "      <th id=\"T_d7060_level0_col1\" class=\"col_heading level0 col1\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_d7060_level0_col2\" class=\"col_heading level0 col2\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_d7060_level0_col3\" class=\"col_heading level0 col3\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_d7060_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_d7060_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_d7060_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_d7060_level0_col7\" class=\"col_heading level0 col7\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_d7060_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_d7060_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_d7060_level0_col10\" class=\"col_heading level0 col10\" >Average</th>\n",
       "      <th id=\"T_d7060_level0_col11\" class=\"col_heading level0 col11\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d7060_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d7060_row0_col0\" class=\"data row0 col0\" >200128</td>\n",
       "      <td id=\"T_d7060_row0_col1\" class=\"data row0 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_d7060_row0_col2\" class=\"data row0 col2\" >{'cot': 22540, 'dolly': 2790, 'flan_v2': 174520, 'oasst1': 278}</td>\n",
       "      <td id=\"T_d7060_row0_col3\" class=\"data row0 col3\" >23.12</td>\n",
       "      <td id=\"T_d7060_row0_col4\" class=\"data row0 col4\" >4.00</td>\n",
       "      <td id=\"T_d7060_row0_col5\" class=\"data row0 col5\" >1.50</td>\n",
       "      <td id=\"T_d7060_row0_col6\" class=\"data row0 col6\" >29.68</td>\n",
       "      <td id=\"T_d7060_row0_col7\" class=\"data row0 col7\" >0.61</td>\n",
       "      <td id=\"T_d7060_row0_col8\" class=\"data row0 col8\" >1.54</td>\n",
       "      <td id=\"T_d7060_row0_col9\" class=\"data row0 col9\" >8.90</td>\n",
       "      <td id=\"T_d7060_row0_col10\" class=\"data row0 col10\" >9.91</td>\n",
       "      <td id=\"T_d7060_row0_col11\" class=\"data row0 col11\" >-9.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d7060_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d7060_row1_col0\" class=\"data row1 col0\" >200000</td>\n",
       "      <td id=\"T_d7060_row1_col1\" class=\"data row1 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_d7060_row1_col2\" class=\"data row1 col2\" >{'cot': 50000, 'dolly': 50000, 'flan_v2': 50000, 'oasst1': 50000}</td>\n",
       "      <td id=\"T_d7060_row1_col3\" class=\"data row1 col3\" >22.94</td>\n",
       "      <td id=\"T_d7060_row1_col4\" class=\"data row1 col4\" >5.00</td>\n",
       "      <td id=\"T_d7060_row1_col5\" class=\"data row1 col5\" >1.50</td>\n",
       "      <td id=\"T_d7060_row1_col6\" class=\"data row1 col6\" >29.97</td>\n",
       "      <td id=\"T_d7060_row1_col7\" class=\"data row1 col7\" >0.00</td>\n",
       "      <td id=\"T_d7060_row1_col8\" class=\"data row1 col8\" >1.88</td>\n",
       "      <td id=\"T_d7060_row1_col9\" class=\"data row1 col9\" >7.96</td>\n",
       "      <td id=\"T_d7060_row1_col10\" class=\"data row1 col10\" >9.89</td>\n",
       "      <td id=\"T_d7060_row1_col11\" class=\"data row1 col11\" >-11.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d7060_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d7060_row2_col0\" class=\"data row2 col0\" >200804</td>\n",
       "      <td id=\"T_d7060_row2_col1\" class=\"data row2 col1\" >results/baselines/gpt2-medium</td>\n",
       "      <td id=\"T_d7060_row2_col2\" class=\"data row2 col2\" >{'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}</td>\n",
       "      <td id=\"T_d7060_row2_col3\" class=\"data row2 col3\" >23.05</td>\n",
       "      <td id=\"T_d7060_row2_col4\" class=\"data row2 col4\" >3.00</td>\n",
       "      <td id=\"T_d7060_row2_col5\" class=\"data row2 col5\" >3.50</td>\n",
       "      <td id=\"T_d7060_row2_col6\" class=\"data row2 col6\" >30.42</td>\n",
       "      <td id=\"T_d7060_row2_col7\" class=\"data row2 col7\" >0.00</td>\n",
       "      <td id=\"T_d7060_row2_col8\" class=\"data row2 col8\" >1.40</td>\n",
       "      <td id=\"T_d7060_row2_col9\" class=\"data row2 col9\" >7.71</td>\n",
       "      <td id=\"T_d7060_row2_col10\" class=\"data row2 col10\" >9.87</td>\n",
       "      <td id=\"T_d7060_row2_col11\" class=\"data row2 col11\" >-11.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d7060_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d7060_row3_col0\" class=\"data row3 col0\" >199992</td>\n",
       "      <td id=\"T_d7060_row3_col1\" class=\"data row3 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_d7060_row3_col2\" class=\"data row3 col2\" >{'baize': 16666, 'code_alpaca': 16666, 'cot': 16666, 'dolly': 16666, 'flan_v2': 16666, 'gpt4_alpaca': 16666, 'oasst1': 16666, 'self_instruct': 16666, 'sharegpt': 16666, 'stanford_alpaca': 16666, 'super_ni': 16666, 'unnatural_instructions': 16666}</td>\n",
       "      <td id=\"T_d7060_row3_col3\" class=\"data row3 col3\" >23.02</td>\n",
       "      <td id=\"T_d7060_row3_col4\" class=\"data row3 col4\" >2.50</td>\n",
       "      <td id=\"T_d7060_row3_col5\" class=\"data row3 col5\" >3.00</td>\n",
       "      <td id=\"T_d7060_row3_col6\" class=\"data row3 col6\" >29.76</td>\n",
       "      <td id=\"T_d7060_row3_col7\" class=\"data row3 col7\" >1.22</td>\n",
       "      <td id=\"T_d7060_row3_col8\" class=\"data row3 col8\" >1.37</td>\n",
       "      <td id=\"T_d7060_row3_col9\" class=\"data row3 col9\" >7.95</td>\n",
       "      <td id=\"T_d7060_row3_col10\" class=\"data row3 col10\" >9.83</td>\n",
       "      <td id=\"T_d7060_row3_col11\" class=\"data row3 col11\" >-12.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d7060_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d7060_row4_col0\" class=\"data row4 col0\" >200000</td>\n",
       "      <td id=\"T_d7060_row4_col1\" class=\"data row4 col1\" >gpt2-medium</td>\n",
       "      <td id=\"T_d7060_row4_col2\" class=\"data row4 col2\" >{}</td>\n",
       "      <td id=\"T_d7060_row4_col3\" class=\"data row4 col3\" >23.72</td>\n",
       "      <td id=\"T_d7060_row4_col4\" class=\"data row4 col4\" >4.00</td>\n",
       "      <td id=\"T_d7060_row4_col5\" class=\"data row4 col5\" >1.50</td>\n",
       "      <td id=\"T_d7060_row4_col6\" class=\"data row4 col6\" >24.46</td>\n",
       "      <td id=\"T_d7060_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_d7060_row4_col8\" class=\"data row4 col8\" >1.40</td>\n",
       "      <td id=\"T_d7060_row4_col9\" class=\"data row4 col9\" >5.11</td>\n",
       "      <td id=\"T_d7060_row4_col10\" class=\"data row4 col10\" >8.60</td>\n",
       "      <td id=\"T_d7060_row4_col11\" class=\"data row4 col11\" >-17.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff6f0b6230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1e2cd_row0_col0, #T_1e2cd_row0_col1, #T_1e2cd_row0_col2, #T_1e2cd_row1_col0, #T_1e2cd_row1_col1, #T_1e2cd_row1_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1e2cd_row0_col3, #T_1e2cd_row0_col6, #T_1e2cd_row0_col9, #T_1e2cd_row1_col5, #T_1e2cd_row1_col7, #T_1e2cd_row1_col8, #T_1e2cd_row1_col10 {\n",
       "  text-align: left;\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1e2cd_row0_col4, #T_1e2cd_row0_col5, #T_1e2cd_row0_col7, #T_1e2cd_row0_col8, #T_1e2cd_row0_col10, #T_1e2cd_row0_col11, #T_1e2cd_row1_col3, #T_1e2cd_row1_col4, #T_1e2cd_row1_col6, #T_1e2cd_row1_col9, #T_1e2cd_row1_col11 {\n",
       "  text-align: left;\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1e2cd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1e2cd_level0_col0\" class=\"col_heading level0 col0\" >total_train_samples</th>\n",
       "      <th id=\"T_1e2cd_level0_col1\" class=\"col_heading level0 col1\" >model_args.model_name_or_path</th>\n",
       "      <th id=\"T_1e2cd_level0_col2\" class=\"col_heading level0 col2\" >data_args.subsample_mixture</th>\n",
       "      <th id=\"T_1e2cd_level0_col3\" class=\"col_heading level0 col3\" >MMLU/0-shot</th>\n",
       "      <th id=\"T_1e2cd_level0_col4\" class=\"col_heading level0 col4\" >GSM/Direct</th>\n",
       "      <th id=\"T_1e2cd_level0_col5\" class=\"col_heading level0 col5\" >GSM/CoT</th>\n",
       "      <th id=\"T_1e2cd_level0_col6\" class=\"col_heading level0 col6\" >BBH/Direct</th>\n",
       "      <th id=\"T_1e2cd_level0_col7\" class=\"col_heading level0 col7\" >Codex-Eval/Pass@1</th>\n",
       "      <th id=\"T_1e2cd_level0_col8\" class=\"col_heading level0 col8\" >TydiQA/CB</th>\n",
       "      <th id=\"T_1e2cd_level0_col9\" class=\"col_heading level0 col9\" >TydiQA/GP</th>\n",
       "      <th id=\"T_1e2cd_level0_col10\" class=\"col_heading level0 col10\" >Average</th>\n",
       "      <th id=\"T_1e2cd_level0_col11\" class=\"col_heading level0 col11\" >ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1e2cd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1e2cd_row0_col0\" class=\"data row0 col0\" >200804</td>\n",
       "      <td id=\"T_1e2cd_row0_col1\" class=\"data row0 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_1e2cd_row0_col2\" class=\"data row0 col2\" >{'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}</td>\n",
       "      <td id=\"T_1e2cd_row0_col3\" class=\"data row0 col3\" >44.60</td>\n",
       "      <td id=\"T_1e2cd_row0_col4\" class=\"data row0 col4\" >3.50</td>\n",
       "      <td id=\"T_1e2cd_row0_col5\" class=\"data row0 col5\" >14.00</td>\n",
       "      <td id=\"T_1e2cd_row0_col6\" class=\"data row0 col6\" >37.15</td>\n",
       "      <td id=\"T_1e2cd_row0_col7\" class=\"data row0 col7\" >1.83</td>\n",
       "      <td id=\"T_1e2cd_row0_col8\" class=\"data row0 col8\" >9.93</td>\n",
       "      <td id=\"T_1e2cd_row0_col9\" class=\"data row0 col9\" >44.36</td>\n",
       "      <td id=\"T_1e2cd_row0_col10\" class=\"data row0 col10\" >22.20</td>\n",
       "      <td id=\"T_1e2cd_row0_col11\" class=\"data row0 col11\" >-2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1e2cd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1e2cd_row1_col0\" class=\"data row1 col0\" >200000</td>\n",
       "      <td id=\"T_1e2cd_row1_col1\" class=\"data row1 col1\" >results/baselines/huggyllama/llama-7b</td>\n",
       "      <td id=\"T_1e2cd_row1_col2\" class=\"data row1 col2\" >{}</td>\n",
       "      <td id=\"T_1e2cd_row1_col3\" class=\"data row1 col3\" >43.55</td>\n",
       "      <td id=\"T_1e2cd_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
       "      <td id=\"T_1e2cd_row1_col5\" class=\"data row1 col5\" >29.00</td>\n",
       "      <td id=\"T_1e2cd_row1_col6\" class=\"data row1 col6\" >36.11</td>\n",
       "      <td id=\"T_1e2cd_row1_col7\" class=\"data row1 col7\" >10.37</td>\n",
       "      <td id=\"T_1e2cd_row1_col8\" class=\"data row1 col8\" >10.37</td>\n",
       "      <td id=\"T_1e2cd_row1_col9\" class=\"data row1 col9\" >43.48</td>\n",
       "      <td id=\"T_1e2cd_row1_col10\" class=\"data row1 col10\" >28.81</td>\n",
       "      <td id=\"T_1e2cd_row1_col11\" class=\"data row1 col11\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff6ef48700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import dict_iterated_getitem, pd_sort_rows_by_avg_ranking\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "class EvalResults:\n",
    "    \n",
    "    def __init__(self, save_dir, run_name=None):\n",
    "        self.save_dir = save_dir\n",
    "        self.eval_dir = os.path.join(self.save_dir, 'eval')\n",
    "        self.run_name = run_name if run_name else self.save_dir\n",
    "        \n",
    "    def get_ft_args(self):\n",
    "        ft_args_path = os.path.join(self.save_dir, 'ft_args.json')\n",
    "        if os.path.isfile(ft_args_path):\n",
    "            with open(ft_args_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {}\n",
    "        \n",
    "    def get_result_df(self, chat_fmt=None, ft_args_fields=None):\n",
    "        if os.path.islink(self.save_dir) or not os.path.isdir(self.eval_dir):\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        task_names = os.listdir(self.eval_dir)\n",
    "\n",
    "        dfs = []\n",
    "        for task_name in task_names:\n",
    "            task_save_dir = os.path.join(self.eval_dir, task_name)\n",
    "            metrics_file = os.path.join(task_save_dir, 'metrics.json')\n",
    "            if not os.path.exists(metrics_file):\n",
    "                continue\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            if 'mmlu' in task_name:\n",
    "                for k, v in metrics['subcat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['subcat_acc']\n",
    "                for k, v in metrics['cat_acc'].items():\n",
    "                    metrics[k] = v\n",
    "                del metrics['cat_acc']\n",
    "            if task_name.startswith('tydiqa'):\n",
    "                metrics['average_f1'] = metrics['average']['f1']\n",
    "            columns = [f'{task_name}/{k}' for k in metrics.keys()]\n",
    "            # columns = pd.MultiIndex.from_tuples(columns)\n",
    "            data = list(metrics.values())\n",
    "\n",
    "            df = pd.DataFrame([data], columns=columns)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "        mapper = {\n",
    "#             'mmlu/average_acc': 'MMLU/0-shot',\n",
    "#             'mmlu_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'mmlu_s=0/average_acc': 'MMLU/0-shot',\n",
    "            'mmlu_s=0_chatfmt/average_acc': 'MMLU/0-shot_chatfmt',\n",
    "            'mmlu_s=5/average_acc': 'MMLU/5-shot',\n",
    "            'mmlu_s=5_chatfmt/average_acc': 'MMLU/t-shot_chatfmt',\n",
    "            # gsm\n",
    "#             'gsm/exact_match': 'GSM/CoT',\n",
    "#             'gsm_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            'gsm_s=8': 'GSM/Direct',\n",
    "            'gsm_s=8_chatfmt/exact_match': 'GSM/Direct_chatfmt',\n",
    "            'gsm_s=8_cot': 'GSM/CoT',\n",
    "            'gsm_s=8_cot_chatfmt/exact_match': 'GSM/CoT_chatfmt',\n",
    "            # bbh\n",
    "            'bbh_s=3/average_exact_match': 'BBH/Direct',\n",
    "            'bbh_s=3_chatfmt/average_exact_match': 'BBH/Direct_chatfmt',\n",
    "#             'bbh_s=3_cot/average_exact_match': 'BBH/CoT',\n",
    "#             'bbh_s=3_cot_chatfmt/average_exact_match': 'BBH/CoT_chatfmt',\n",
    "            # humaneval\n",
    "            'humaneval/pass@1': 'Codex-Eval/Pass@1',\n",
    "            'humaneval_chatfmt/pass@1': 'Codex-Eval/Pass@1_chatfmt',\n",
    "            # tydiqa\n",
    "#             'tydiqa_cb/average_f1': 'TydiQA/CB',\n",
    "#             'tydiqa_cb_chatfmt/average_f1': 'TydiQA/CB_chatfmt',\n",
    "#             'tydiqa_gp/average_f1': 'TydiQA/GP',\n",
    "#             'tydiqa_gp_chatfmt/average_f1': 'TydiQA/GP_chatfmt',\n",
    "            'tydiqa_s=1_cb/average_f1': 'TydiQA/CB',\n",
    "            'tydiqa_s=1_cb_chatfmt/average_f1': 'TydiQA/CB_chatfmt',\n",
    "            'tydiqa_s=1_gp/average_f1': 'TydiQA/GP',\n",
    "            'tydiqa_s=1_gp_chatfmt/average_f1': 'TydiQA/GP_chatfmt',\n",
    "        }\n",
    "        cols = [col for col in mapper.keys() if col in df]\n",
    "        if chat_fmt is not None:\n",
    "            if chat_fmt:\n",
    "                cols = [col for col in cols if 'chatfmt' in col]\n",
    "            else:\n",
    "                cols = [col for col in cols if 'chatfmt' not in col]\n",
    "        for col in cols:\n",
    "            if 'tydiqa' not in col:\n",
    "                df[col] = df[col]*100\n",
    "        df = df[cols]\n",
    "        df = df.rename(columns=mapper)\n",
    "        if chat_fmt is not None:\n",
    "            df.columns = [x.replace('_chatfmt', '') for x in df.columns]\n",
    "        else:\n",
    "            cols = [x.split('_') for x in df.columns]\n",
    "            cols = [x+[''] if len(x)==1 else x for x in cols]\n",
    "            df.columns = pd.MultiIndex.from_tuples(cols)\n",
    "            \n",
    "        df.insert(len(df.columns), 'Average', df.mean(axis=1))\n",
    "        \n",
    "        # append extra fields to the table.\n",
    "        if ft_args_fields is not None:\n",
    "            ft_args = r.get_ft_args()\n",
    "            for k in ft_args_fields[::-1]:\n",
    "                try:\n",
    "                    v = dict_iterated_getitem(ft_args, k)\n",
    "                except:\n",
    "                    v = None\n",
    "                df.insert(0, k, [v])\n",
    "        return df\n",
    "\n",
    "# get_last_checkpoint(v)\n",
    "save_dirs = []\n",
    "save_dirs += [\n",
    "#     ('gpt2', '../results/baselines/gpt2'),\n",
    "    ('gpt2m', '../results/baselines/gpt2-medium'),\n",
    "    ('llama-7b', '../results/baselines/huggyllama/llama-7b/'),\n",
    "#     ('llama2-7b', '../results/baselines/NousResearch/Llama-2-7b-hf/'),\n",
    "#     ('llama2-7b+humanmix', '../results/llama2-7b_humanmix'),\n",
    "]\n",
    "# datasets = [\n",
    "#     'super_ni', 'cot', 'flan_v2', 'dolly', 'oasst1',\n",
    "#     'gpt4_alpaca', 'stanford_alpaca', 'code_alpaca', 'baize', 'self_instruct', \n",
    "#     'sharegpt', 'humanmix', # 'unnatural_instructions',\n",
    "#     'dolly:oasst1', 'cot:flanv2'\n",
    "# ]\n",
    "\n",
    "# save_dirs += [(f'llama-7b+{x}', f'../results/llama-7b_{x}') for x in datasets]\n",
    "save_dirs += [\n",
    "   ('llama-7b_humanmix', '../results/ft1/llama-7b_humanmix'),\n",
    "]\n",
    "\n",
    "exp_dir = '../results/ft2'\n",
    "save_dirs += [(os.path.basename(x), x) for x in \n",
    "              [os.path.join(exp_dir, x) for x in os.listdir(exp_dir)]]\n",
    "ft_args_fields = ['model_args.model_name_or_path',\n",
    "                  'data_args.subsample_mixture']\n",
    "\n",
    "dfs = []\n",
    "for model_name, save_dir in save_dirs:\n",
    "    if finished_training(save_dir) and not os.path.islink(save_dir):\n",
    "        r = EvalResults(save_dir, model_name)\n",
    "        df = r.get_result_df(chat_fmt=True, ft_args_fields=ft_args_fields)\n",
    "        dfs.append(df)\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df = pd_sort_rows_by_avg_ranking(df); df['ranking'] = -df['ranking']\n",
    "sort_value_col, sort_value_col_ascending = 'Average', False\n",
    "sort_value_col, sort_value_col_ascending = 'ranking', False\n",
    "df = df.sort_values(sort_value_col, ascending=sort_value_col_ascending)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "if exp_dir.endswith('ft2'):\n",
    "    for model_name_contain in ['gpt2', 'llama']:\n",
    "        for total_train_samples in [10000, 50000, 100000, 200000]:\n",
    "            dfc = df.copy()\n",
    "            dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "                lambda d: sum(list(d.values())) if d else 200000))\n",
    "            dfc = dfc[dfc['total_train_samples'].apply(\n",
    "                lambda x: total_train_samples-1000<x<total_train_samples+1000)]\n",
    "            dfc = dfc[dfc['model_args.model_name_or_path'].apply(\n",
    "                lambda x: model_name_contain in x)]\n",
    "            dfc['total_train_samples'] = dfc['total_train_samples'].astype(str)\n",
    "            dfc = dfc.reset_index(drop=True)\n",
    "            if len(dfc):\n",
    "                display(dfc\n",
    "                        .style\n",
    "                        .set_properties(**{'text-align': 'left'})\n",
    "                        .background_gradient(cmap ='coolwarm')\n",
    "                        .format(precision=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9719269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'results/ft1'\n",
    "\n",
    "# d = {\n",
    "#     'bbh_s=0': 'bbh_s=3',\n",
    "#     'gsm': 'gsm_s=8_cot',\n",
    "#     'mmlu': 'mmlu_s=0',\n",
    "#     'tydiqa_cb': 'tydiqa_s=1_cb',\n",
    "#     'tydiqa_gp': 'tydiqa_s=1_gp',\n",
    "# }\n",
    "\n",
    "# d.update({k+'_chatfmt': v+'_chatfmt' for k,v in d.items()})\n",
    "\n",
    "# for subdir in os.listdir(exp_dir):    \n",
    "#     for task_name_src, task_name_tgt in d.items():\n",
    "#         path_src = os.path.join(exp_dir, subdir, 'eval', task_name_src)\n",
    "#         path_tgt = os.path.join(exp_dir, subdir, 'eval', task_name_tgt)\n",
    "#         if os.path.isdir(path_src):\n",
    "# #             os.rename(path_src, path_tgt)\n",
    "#             print(path_src)\n",
    "#             print(path_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27138820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_train_samples</th>\n",
       "      <th>model_args.model_name_or_path</th>\n",
       "      <th>data_args.subsample_mixture</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/Direct</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "      <th>TydiQA/CB</th>\n",
       "      <th>TydiQA/GP</th>\n",
       "      <th>Average</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200804</td>\n",
       "      <td>results/baselines/huggyllama/llama-7b</td>\n",
       "      <td>{'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}</td>\n",
       "      <td>44.601909</td>\n",
       "      <td>3.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.147236</td>\n",
       "      <td>1.829268</td>\n",
       "      <td>9.934316</td>\n",
       "      <td>44.361673</td>\n",
       "      <td>22.196343</td>\n",
       "      <td>-2.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200128</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 22540, 'dolly': 2790, 'flan_v2': 174520, 'oasst1': 278}</td>\n",
       "      <td>23.123487</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>29.677548</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.543347</td>\n",
       "      <td>8.898066</td>\n",
       "      <td>9.907458</td>\n",
       "      <td>-9.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'flan_v2': 100000}</td>\n",
       "      <td>23.002421</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.757914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.821590</td>\n",
       "      <td>8.841241</td>\n",
       "      <td>9.989024</td>\n",
       "      <td>-9.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'self_instruct': 100000}</td>\n",
       "      <td>23.557898</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.590479</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>1.410211</td>\n",
       "      <td>6.140904</td>\n",
       "      <td>9.774144</td>\n",
       "      <td>-10.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 50000, 'dolly': 50000, 'flan_v2': 50000, 'oasst1': 50000}</td>\n",
       "      <td>22.938328</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>29.973845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875094</td>\n",
       "      <td>7.962404</td>\n",
       "      <td>9.892810</td>\n",
       "      <td>-11.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200804</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}</td>\n",
       "      <td>23.045150</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>30.422551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.395340</td>\n",
       "      <td>7.712177</td>\n",
       "      <td>9.867888</td>\n",
       "      <td>-11.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 12500, 'dolly': 12500, 'flan_v2': 12500, 'oasst1': 12500}</td>\n",
       "      <td>22.952571</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.284956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.896922</td>\n",
       "      <td>7.082380</td>\n",
       "      <td>9.745261</td>\n",
       "      <td>-11.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>199992</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'baize': 16666, 'code_alpaca': 16666, 'cot': 16666, 'dolly': 16666, 'flan_v2': 16666, 'gpt4_alpaca': 16666, 'oasst1': 16666, 'self_instruct': 16666, 'sharegpt': 16666, 'stanford_alpaca': 16666, 'super_ni': 16666, 'unnatural_instructions': 16666}</td>\n",
       "      <td>23.016664</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.758117</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>1.367148</td>\n",
       "      <td>7.950251</td>\n",
       "      <td>9.830242</td>\n",
       "      <td>-12.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>99999</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 40031, 'dolly': 6009, 'flan_v2': 40031, 'oasst1': 13928}</td>\n",
       "      <td>23.023786</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.827929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.158432</td>\n",
       "      <td>7.918454</td>\n",
       "      <td>9.704086</td>\n",
       "      <td>-12.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'super_ni': 100000}</td>\n",
       "      <td>23.151973</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.258770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.877706</td>\n",
       "      <td>7.116692</td>\n",
       "      <td>9.486449</td>\n",
       "      <td>-13.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>99996</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'baize': 8333, 'code_alpaca': 8333, 'cot': 8333, 'dolly': 8333, 'flan_v2': 8333, 'gpt4_alpaca': 8333, 'oasst1': 8333, 'self_instruct': 8333, 'sharegpt': 8333, 'stanford_alpaca': 8333, 'super_ni': 8333, 'unnatural_instructions': 8333}</td>\n",
       "      <td>22.938328</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.391050</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.773780</td>\n",
       "      <td>5.948287</td>\n",
       "      <td>9.380172</td>\n",
       "      <td>-14.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'unnatural_instructions': 100000}</td>\n",
       "      <td>23.201823</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.127942</td>\n",
       "      <td>1.829268</td>\n",
       "      <td>1.730040</td>\n",
       "      <td>4.095138</td>\n",
       "      <td>9.212030</td>\n",
       "      <td>-14.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9998</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 4878, 'dolly': 73, 'flan_v2': 4878, 'oasst1': 169}</td>\n",
       "      <td>22.945449</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>28.108541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083539</td>\n",
       "      <td>6.646854</td>\n",
       "      <td>9.469198</td>\n",
       "      <td>-15.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 25000, 'dolly': 25000, 'flan_v2': 25000, 'oasst1': 25000}</td>\n",
       "      <td>22.938328</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>29.980136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.445112</td>\n",
       "      <td>7.088936</td>\n",
       "      <td>9.564645</td>\n",
       "      <td>-15.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49998</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 24392, 'dolly': 366, 'flan_v2': 24392, 'oasst1': 848}</td>\n",
       "      <td>22.895599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.028842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.448538</td>\n",
       "      <td>6.055629</td>\n",
       "      <td>9.489801</td>\n",
       "      <td>-15.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50031</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 5635, 'dolly': 697, 'flan_v2': 43630, 'oasst1': 69}</td>\n",
       "      <td>22.852870</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.429958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.456560</td>\n",
       "      <td>8.441289</td>\n",
       "      <td>9.668668</td>\n",
       "      <td>-15.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10005</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 1127, 'dolly': 139, 'flan_v2': 8726, 'oasst1': 13}</td>\n",
       "      <td>23.159094</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>29.467971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922864</td>\n",
       "      <td>4.975758</td>\n",
       "      <td>9.289384</td>\n",
       "      <td>-15.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'cot': 100000}</td>\n",
       "      <td>23.016664</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.558459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.931833</td>\n",
       "      <td>7.495874</td>\n",
       "      <td>9.000404</td>\n",
       "      <td>-16.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'stanford_alpaca': 100000}</td>\n",
       "      <td>22.988178</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>27.052148</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.615986</td>\n",
       "      <td>5.113901</td>\n",
       "      <td>8.982853</td>\n",
       "      <td>-16.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{}</td>\n",
       "      <td>23.721692</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>24.459354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.398436</td>\n",
       "      <td>5.114215</td>\n",
       "      <td>8.599100</td>\n",
       "      <td>-17.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'code_alpaca': 100000}</td>\n",
       "      <td>22.966814</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.876242</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>1.383378</td>\n",
       "      <td>5.525014</td>\n",
       "      <td>8.852994</td>\n",
       "      <td>-17.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10000</td>\n",
       "      <td>results/baselines/gpt2-medium</td>\n",
       "      <td>{'cot': 2500, 'dolly': 2500, 'flan_v2': 2500, 'oasst1': 2500}</td>\n",
       "      <td>22.852870</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.637130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.946636</td>\n",
       "      <td>5.612540</td>\n",
       "      <td>8.435597</td>\n",
       "      <td>-19.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'baize': 100000}</td>\n",
       "      <td>22.924085</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>25.485221</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>1.329486</td>\n",
       "      <td>4.139038</td>\n",
       "      <td>8.498227</td>\n",
       "      <td>-19.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'sharegpt': 100000}</td>\n",
       "      <td>23.052272</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.323305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.732119</td>\n",
       "      <td>3.702220</td>\n",
       "      <td>8.187131</td>\n",
       "      <td>-19.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'dolly': 100000}</td>\n",
       "      <td>22.810141</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>15.476953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.347582</td>\n",
       "      <td>5.569227</td>\n",
       "      <td>7.171986</td>\n",
       "      <td>-20.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'oasst1': 100000}</td>\n",
       "      <td>22.973935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.000967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.797507</td>\n",
       "      <td>6.326418</td>\n",
       "      <td>8.156975</td>\n",
       "      <td>-20.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100000</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>{'gpt4_alpaca': 100000}</td>\n",
       "      <td>23.052272</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.812740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.571938</td>\n",
       "      <td>4.294121</td>\n",
       "      <td>8.104439</td>\n",
       "      <td>-21.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>200000</td>\n",
       "      <td>huggyllama/llama-7b</td>\n",
       "      <td>{}</td>\n",
       "      <td>32.459764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.268293</td>\n",
       "      <td>9.765382</td>\n",
       "      <td>37.489078</td>\n",
       "      <td>18.996503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    total_train_samples          model_args.model_name_or_path  \\\n",
       "0                200804  results/baselines/huggyllama/llama-7b   \n",
       "1                200128          results/baselines/gpt2-medium   \n",
       "2                100000                            gpt2-medium   \n",
       "3                100000                            gpt2-medium   \n",
       "4                200000                            gpt2-medium   \n",
       "5                200804          results/baselines/gpt2-medium   \n",
       "6                 50000          results/baselines/gpt2-medium   \n",
       "7                199992                            gpt2-medium   \n",
       "8                 99999                            gpt2-medium   \n",
       "9                100000                            gpt2-medium   \n",
       "10                99996                            gpt2-medium   \n",
       "11               100000                            gpt2-medium   \n",
       "12                 9998          results/baselines/gpt2-medium   \n",
       "13               100000                            gpt2-medium   \n",
       "14                49998          results/baselines/gpt2-medium   \n",
       "15                50031          results/baselines/gpt2-medium   \n",
       "16                10005          results/baselines/gpt2-medium   \n",
       "17               100000                            gpt2-medium   \n",
       "18               100000                            gpt2-medium   \n",
       "19               200000                            gpt2-medium   \n",
       "20               100000                            gpt2-medium   \n",
       "21                10000          results/baselines/gpt2-medium   \n",
       "22               100000                            gpt2-medium   \n",
       "23               100000                            gpt2-medium   \n",
       "24               100000                            gpt2-medium   \n",
       "25               100000                            gpt2-medium   \n",
       "26               100000                            gpt2-medium   \n",
       "27               200000                    huggyllama/llama-7b   \n",
       "\n",
       "                                                                                                                                                                                                                               data_args.subsample_mixture  \\\n",
       "0                                                                                                                                                                                          {'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}   \n",
       "1                                                                                                                                                                                          {'cot': 22540, 'dolly': 2790, 'flan_v2': 174520, 'oasst1': 278}   \n",
       "2                                                                                                                                                                                                                                      {'flan_v2': 100000}   \n",
       "3                                                                                                                                                                                                                                {'self_instruct': 100000}   \n",
       "4                                                                                                                                                                                        {'cot': 50000, 'dolly': 50000, 'flan_v2': 50000, 'oasst1': 50000}   \n",
       "5                                                                                                                                                                                          {'cot': 15356, 'dolly': 894, 'flan_v2': 182740, 'oasst1': 1814}   \n",
       "6                                                                                                                                                                                        {'cot': 12500, 'dolly': 12500, 'flan_v2': 12500, 'oasst1': 12500}   \n",
       "7   {'baize': 16666, 'code_alpaca': 16666, 'cot': 16666, 'dolly': 16666, 'flan_v2': 16666, 'gpt4_alpaca': 16666, 'oasst1': 16666, 'self_instruct': 16666, 'sharegpt': 16666, 'stanford_alpaca': 16666, 'super_ni': 16666, 'unnatural_instructions': 16666}   \n",
       "8                                                                                                                                                                                         {'cot': 40031, 'dolly': 6009, 'flan_v2': 40031, 'oasst1': 13928}   \n",
       "9                                                                                                                                                                                                                                     {'super_ni': 100000}   \n",
       "10              {'baize': 8333, 'code_alpaca': 8333, 'cot': 8333, 'dolly': 8333, 'flan_v2': 8333, 'gpt4_alpaca': 8333, 'oasst1': 8333, 'self_instruct': 8333, 'sharegpt': 8333, 'stanford_alpaca': 8333, 'super_ni': 8333, 'unnatural_instructions': 8333}   \n",
       "11                                                                                                                                                                                                                      {'unnatural_instructions': 100000}   \n",
       "12                                                                                                                                                                                              {'cot': 4878, 'dolly': 73, 'flan_v2': 4878, 'oasst1': 169}   \n",
       "13                                                                                                                                                                                       {'cot': 25000, 'dolly': 25000, 'flan_v2': 25000, 'oasst1': 25000}   \n",
       "14                                                                                                                                                                                           {'cot': 24392, 'dolly': 366, 'flan_v2': 24392, 'oasst1': 848}   \n",
       "15                                                                                                                                                                                             {'cot': 5635, 'dolly': 697, 'flan_v2': 43630, 'oasst1': 69}   \n",
       "16                                                                                                                                                                                              {'cot': 1127, 'dolly': 139, 'flan_v2': 8726, 'oasst1': 13}   \n",
       "17                                                                                                                                                                                                                                         {'cot': 100000}   \n",
       "18                                                                                                                                                                                                                             {'stanford_alpaca': 100000}   \n",
       "19                                                                                                                                                                                                                                                      {}   \n",
       "20                                                                                                                                                                                                                                 {'code_alpaca': 100000}   \n",
       "21                                                                                                                                                                                           {'cot': 2500, 'dolly': 2500, 'flan_v2': 2500, 'oasst1': 2500}   \n",
       "22                                                                                                                                                                                                                                       {'baize': 100000}   \n",
       "23                                                                                                                                                                                                                                    {'sharegpt': 100000}   \n",
       "24                                                                                                                                                                                                                                       {'dolly': 100000}   \n",
       "25                                                                                                                                                                                                                                      {'oasst1': 100000}   \n",
       "26                                                                                                                                                                                                                                 {'gpt4_alpaca': 100000}   \n",
       "27                                                                                                                                                                                                                                                      {}   \n",
       "\n",
       "    MMLU/0-shot  GSM/Direct  GSM/CoT  BBH/Direct  Codex-Eval/Pass@1  \\\n",
       "0     44.601909         3.5     14.0   37.147236           1.829268   \n",
       "1     23.123487         4.0      1.5   29.677548           0.609756   \n",
       "2     23.002421         3.5      3.0   29.757914           0.000000   \n",
       "3     23.557898         4.0      2.5   29.590479           1.219512   \n",
       "4     22.938328         5.0      1.5   29.973845           0.000000   \n",
       "5     23.045150         3.0      3.5   30.422551           0.000000   \n",
       "6     22.952571         3.0      3.0   30.284956           0.000000   \n",
       "7     23.016664         2.5      3.0   29.758117           1.219512   \n",
       "8     23.023786         2.5      2.5   29.827929           0.000000   \n",
       "9     23.151973         2.5      2.5   29.258770           0.000000   \n",
       "10    22.938328         3.5      0.5   30.391050           0.609756   \n",
       "11    23.201823         3.5      1.0   29.127942           1.829268   \n",
       "12    22.945449         4.0      3.5   28.108541           0.000000   \n",
       "13    22.938328         3.0      2.5   29.980136           0.000000   \n",
       "14    22.895599         3.0      3.0   30.028842           0.000000   \n",
       "15    22.852870         2.5      3.0   29.429958           0.000000   \n",
       "16    23.159094         3.0      3.5   29.467971           0.000000   \n",
       "17    23.016664         3.0      0.0   27.558459           0.000000   \n",
       "18    22.988178         3.0      2.5   27.052148           0.609756   \n",
       "19    23.721692         4.0      1.5   24.459354           0.000000   \n",
       "20    22.966814         3.5      0.5   26.876242           1.219512   \n",
       "21    22.852870         3.0      1.0   24.637130           0.000000   \n",
       "22    22.924085         3.5      1.5   25.485221           0.609756   \n",
       "23    23.052272         0.5      2.0   26.323305           0.000000   \n",
       "24    22.810141         2.5      1.5   15.476953           0.000000   \n",
       "25    22.973935         2.0      0.0   24.000967           0.000000   \n",
       "26    23.052272         1.5      0.5   25.812740           0.000000   \n",
       "27    32.459764         NaN     11.0         NaN           4.268293   \n",
       "\n",
       "    TydiQA/CB  TydiQA/GP    Average  ranking  \n",
       "0    9.934316  44.361673  22.196343  -2.1250  \n",
       "1    1.543347   8.898066   9.907458  -9.2500  \n",
       "2    1.821590   8.841241   9.989024  -9.7500  \n",
       "3    1.410211   6.140904   9.774144 -10.3125  \n",
       "4    1.875094   7.962404   9.892810 -11.0625  \n",
       "5    1.395340   7.712177   9.867888 -11.2500  \n",
       "6    1.896922   7.082380   9.745261 -11.8750  \n",
       "7    1.367148   7.950251   9.830242 -12.0625  \n",
       "8    2.158432   7.918454   9.704086 -12.0625  \n",
       "9    1.877706   7.116692   9.486449 -13.6875  \n",
       "10   1.773780   5.948287   9.380172 -14.1250  \n",
       "11   1.730040   4.095138   9.212030 -14.1875  \n",
       "12   1.083539   6.646854   9.469198 -15.0000  \n",
       "13   1.445112   7.088936   9.564645 -15.0000  \n",
       "14   1.448538   6.055629   9.489801 -15.2500  \n",
       "15   1.456560   8.441289   9.668668 -15.5000  \n",
       "16   0.922864   4.975758   9.289384 -15.8750  \n",
       "17   1.931833   7.495874   9.000404 -16.1250  \n",
       "18   1.615986   5.113901   8.982853 -16.2500  \n",
       "19   1.398436   5.114215   8.599100 -17.0000  \n",
       "20   1.383378   5.525014   8.852994 -17.8125  \n",
       "21   1.946636   5.612540   8.435597 -19.5000  \n",
       "22   1.329486   4.139038   8.498227 -19.7500  \n",
       "23   1.732119   3.702220   8.187131 -19.8750  \n",
       "24   2.347582   5.569227   7.171986 -20.8125  \n",
       "25   1.797507   6.326418   8.156975 -20.8750  \n",
       "26   1.571938   4.294121   8.104439 -21.2500  \n",
       "27   9.765382  37.489078  18.996503      NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfc = df.copy()\n",
    "dfc.insert(0, 'total_train_samples',  dfc['data_args.subsample_mixture'].apply(\n",
    "    lambda d: sum(list(d.values())) if d else 200000))\n",
    "# dfc[dfc['total_train_samples'].apply(\n",
    "#     lambda x: total_train_samples-500<x<total_train_samples+500)]\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fad6edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6ec921210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3618: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:3619: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fff9c6f3250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:383\u001b[0m, in \u001b[0;36mStyler._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mHooks into Jupyter notebook rich display system, which calls _repr_html_ by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mdefault if an object is returned at the end of a cell.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyler.render.repr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1308\u001b[0m, in \u001b[0;36mStyler.to_html\u001b[0;34m(self, buf, table_uuid, table_attributes, sparse_index, sparse_columns, bold_headers, caption, max_rows, max_columns, encoding, doctype_html, exclude_styles, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     obj\u001b[38;5;241m.\u001b[39mset_caption(caption)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Build HTML string..\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_styles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_styles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyler.render.encoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoctype_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctype_html\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(\n\u001b[1;32m   1320\u001b[0m     html, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39m(encoding \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1321\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:205\u001b[0m, in \u001b[0;36mStylerRenderer._render_html\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    195\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Renders the ``Styler`` including all applied styles to HTML.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Generates a dict with necessary kwargs passed to jinja2 template.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&nbsp;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     d\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md,\n\u001b[1;32m    209\u001b[0m         html_table_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_table,\n\u001b[1;32m    210\u001b[0m         html_style_tpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_html_style,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:162\u001b[0m, in \u001b[0;36mStylerRenderer._render\u001b[0;34m(self, sparse_index, sparse_columns, max_rows, max_cols, blank)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     sparse_index: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     blank: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m ):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Computes and applies styles and then generates the general render dicts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Also extends the `ctx` and `ctx_index` attributes with those of concatenated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    stylers for use within `_translate_latex`\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m     ctx_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style_render.py:257\u001b[0m, in \u001b[0;36mStylerRenderer._compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_todo:\n\u001b[0;32m--> 257\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1693\u001b[0m, in \u001b[0;36mStyler._apply\u001b[0;34m(self, func, axis, subset, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1691\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1692\u001b[0m     )\n\u001b[0;32m-> 1693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/io/formats/style.py:1505\u001b[0m, in \u001b[0;36mStyler._update_ctx\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03mUpdate the state of the ``Styler`` for data cells.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    matter.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Styler.apply` and `.applymap` are not compatible \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-unique index or columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m attrs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1511\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(cn)\n",
      "\u001b[0;31mKeyError\u001b[0m: '`Styler.apply` and `.applymap` are not compatible with non-unique index or columns.'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff6ec19eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dfc = df.copy()\n",
    "dfc.columns = [x.split('_')[0] for x in dfc.columns]\n",
    "def get_dataset(x):\n",
    "    x = x.split('+')\n",
    "    if len(x) == 1:\n",
    "        return ''\n",
    "    else:\n",
    "        d = x[1]\n",
    "        d = d.replace('_', '')\n",
    "        return d\n",
    "dfc['Dataset'] = dfc['Model'].apply(get_dataset)\n",
    "order_list = ['',\n",
    " 'superni', 'cot', 'flanv2', 'dolly', 'oasst1',\n",
    " 'selfinstruct', 'unnaturalinstructions', 'stanfordalpaca', 'codealpaca', 'gpt4alpaca',\n",
    " 'baize', 'sharegpt', 'humanmix', 'h+gptmix']\n",
    "dfc['order'] = dfc['Dataset'].map({v: i for i, v in enumerate(order_list)})\n",
    "dfc = dfc.sort_values('order')\n",
    "dfc = dfc.drop(columns=['order', 'Dataset'])\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama-7b' in x and ':' not in x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "\n",
    "display(dfc[dfc['Model'].apply(\n",
    "            lambda x: 'llama-7b' in x and (\n",
    "                ':' in x or any(c in x for c in ['dolly', 'oasst1', 'cot', 'flan'])\n",
    "                or 'humanmix' in x\n",
    "            )\n",
    "        )]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n",
    "\n",
    "display(dfc[dfc['Model'].apply(lambda x: 'llama2-7b' in x or 'llama-7b'==x)]\n",
    "        .style\n",
    "        .background_gradient(cmap ='coolwarm')\n",
    "        .format(precision=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00ba1a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU/0-shot</th>\n",
       "      <th>GSM/CoT</th>\n",
       "      <th>BBH/Direct</th>\n",
       "      <th>Codex-Eval/Pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b</td>\n",
       "      <td>32.459764</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.970313</td>\n",
       "      <td>5.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-7b+dolly</td>\n",
       "      <td>37.231164</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30.603476</td>\n",
       "      <td>11.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-7b+oasst1</td>\n",
       "      <td>34.147557</td>\n",
       "      <td>7.5</td>\n",
       "      <td>29.692361</td>\n",
       "      <td>2.439024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-7b+dolly:oasst1</td>\n",
       "      <td>37.900584</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.150571</td>\n",
       "      <td>9.146341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  MMLU/0-shot  GSM/CoT  BBH/Direct  Codex-Eval/Pass@1\n",
       "0                llama-7b    32.459764     11.0   32.970313           5.487805\n",
       "4          llama-7b+dolly    37.231164     13.0   30.603476          11.585366\n",
       "5         llama-7b+oasst1    34.147557      7.5   29.692361           2.439024\n",
       "14  llama-7b+dolly:oasst1    37.900584      7.0   30.150571           9.146341"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0588857",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima datƒÉ √Æn istoria Uniunii Europene, la drepturile persoanelor care apar≈£in acestor minoritƒÉ≈£i ≈üi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n",
    "{\"dataset\": \"flan_v2\", \"id\": \"flan_v2_2\", \"messages\": [{\"role\": \"user\", \"content\": \"Tratatul de la Lisabona nu face inutil referire, pentru prima dat\\u0103 \\u00een istoria Uniunii Europene, la drepturile persoanelor care apar\\u0163in acestor minorit\\u0103\\u0163i \\u015fi la valorile proprii acestora.\\n\\nWhich language is this?\\n\"}, {\"role\": \"assistant\", \"content\": \"Romanian\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7702c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.1f}'.format):\n",
    "    display(df[['Model']+[x for x in df.columns if 'chatfmt' in x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82eac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:0.3f}'.format):\n",
    "    display(df[[x for x in df.columns if 'chatfmt' not in x]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "models = []\n",
    "models += ['t5-small', 't5-base', 't5-large', 't5-3b', 't5-11b']\n",
    "models += ['huggyllama/llama-7b']\n",
    "save_dirs = [f'../results/baselines/{x}/eval/gsm/' for x in models]\n",
    "\n",
    "data = []\n",
    "for model, save_dir in zip(models, save_dirs):\n",
    "    logfile_path = glob.glob(os.path.join(save_dir, '*.out'))[0]\n",
    "    out = get_run_statistics(logfile_path)\n",
    "    with open(os.path.join(save_dir, 'metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    data.append((model, out['cpu_time']/60/60, out['avg_mem'], out['max_mem'], metrics['exact_match']))\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "columns = ['name', 'cpu_time (hr)', 'avg_mem', 'max_mem', 'exact_match']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
