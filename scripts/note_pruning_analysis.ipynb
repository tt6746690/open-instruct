{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78835b0",
   "metadata": {},
   "source": [
    "\n",
    "Goal\n",
    "- verify if there is correlation between P_LM with output token length\n",
    "- analyze difference/similarity between sorted indices with different `sort_by`. see if they correlate with each other, e.g., first k item overlap\n",
    "- visualize text statistics w.r.t. data subsets obtaind from pruning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47917b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65846ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d37dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm_output(dataset):\n",
    "    lm_output_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/llama-7b_outputs'\n",
    "    save_path = os.path.join(lm_output_dir, f'{dataset}.pkl')\n",
    "    with open(save_path, 'rb') as f:\n",
    "        output = pickle.load(f)\n",
    "    output['log_probs'] = np.nan_to_num(output['log_probs'], nan=np.nanmean(output['log_probs']))\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_dataset(dataset):\n",
    "    processed_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed'\n",
    "    if 'tulu' in dataset:\n",
    "        train_file = os.path.join(processed_dir, 'tulu', f'{dataset}.jsonl')\n",
    "    else:\n",
    "        train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "    data_files = {'train': train_file}\n",
    "    ds = load_dataset('json', data_files=data_files)['train']\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def get_dataset_token_lengths(dataset, model_name_or_path, inds=None):\n",
    "    from open_instruct.finetune_trainer import encode_with_messages_format\n",
    "\n",
    "    ds = get_dataset(dataset)\n",
    "    if inds is not None: ds = ds.select(inds)\n",
    "    encode_fn = partial(encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "    ds = ds.map(encode_fn, batched=False, num_proc=16)\n",
    "    ds.set_format(type='np')\n",
    "\n",
    "    def count_token_lengths(d):\n",
    "        x = d['labels']\n",
    "        input_len = x[x==-100].shape[0]\n",
    "        output_len = x.shape[0] - input_len\n",
    "        return {'input_len': input_len, 'output_len': output_len}\n",
    "\n",
    "    ds = ds.map(count_token_lengths, num_proc=16)\n",
    "    return {'input_len': ds['input_len'], 'output_len': ds['output_len']}\n",
    "\n",
    "\n",
    "def get_sorted_inds(dataset, sort_by):\n",
    "    inds_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b'\n",
    "    save_path = os.path.join(inds_dir, dataset, f'{sort_by}.pkl')\n",
    "    with open(save_path, 'rb') as f:\n",
    "        output = pickle.load(f)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'flan_v2'\n",
    "model_name_or_path='../results/baselines/huggyllama/llama-7b'\n",
    "\n",
    "dataset_list = ['baize', 'code_alpaca', 'cot', \n",
    "                'dolly', 'flan_v2', 'gpt4_alpaca', \n",
    "                'lima', 'oasst1', 'open_orca', \n",
    "                'self_instruct', 'sharegpt', 'stanford_alpaca', \n",
    "                'super_ni', 'unnatural_instructions', 'wizardlm',\n",
    "                'tulu_v1_human_mix'\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8eb22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subsample_size = 2000\n",
    "\n",
    "\n",
    "w = 5\n",
    "fig, axs = plt.subplots(6,3, figsize=(w*3,w*6))\n",
    "\n",
    "\n",
    "for axi, dataset in enumerate(dataset_list[:100]):\n",
    "    \n",
    "    ## get the information\n",
    "\n",
    "    lm_output = get_lm_output(dataset)\n",
    "    T = lm_output['text_embeddings']\n",
    "    logP = lm_output['log_probs']\n",
    "\n",
    "    if subsample_size:\n",
    "        np.random.seed(0)\n",
    "        inds = np.random.randint(0, T.shape[0], subsample_size)\n",
    "        T = T[inds]\n",
    "        logP = logP[inds]\n",
    "\n",
    "    token_lengths = get_dataset_token_lengths(dataset, model_name_or_path, inds=inds if subsample_size else None)\n",
    "    input_len = token_lengths['input_len']\n",
    "    output_len = token_lengths['output_len']\n",
    "    \n",
    "    ## plot the information \n",
    "    ax = axs.flatten()[axi]\n",
    "\n",
    "    ys = np.exp(logP)\n",
    "\n",
    "    for label, xs in [\n",
    "            ('input_len', input_len),\n",
    "            ('output_len', output_len),\n",
    "        ]:\n",
    "        ax.scatter(xs, ys, label=label, alpha=.2)\n",
    "\n",
    "\n",
    "    ax.legend(loc='upper center')\n",
    "    eps = .001\n",
    "\n",
    "    ax.set_ylim(np.quantile(ys, eps), np.quantile(ys, 1-eps))\n",
    "    xs_for_xlim = np.maximum(input_len, output_len)\n",
    "    ax.set_xlim(np.quantile(xs_for_xlim, eps), np.quantile(xs_for_xlim, 1-eps))\n",
    "    \n",
    "    # ax.set_xlabel('token lengths', fontsize=20)\n",
    "    if axi%3 == 0:\n",
    "        ax.set_ylabel('$p_{LM}$', fontsize=25)\n",
    "    ax.set_title(dataset, fontsize=25)\n",
    "    \n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52320e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f03e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
