{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd29b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "Sun Nov  5 15:24:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   45C    P0   126W / 300W |   7591MiB / 32510MiB |     94%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    97W / 300W |   7591MiB / 32510MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   52C    P0   300W / 300W |   7591MiB / 32510MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   45C    P0   238W / 300W |   7591MiB / 32510MiB |     91%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2348625      C   ...da3/envs/lavis/bin/python     7589MiB |\n",
      "|    1   N/A  N/A   2348601      C   ...da3/envs/lavis/bin/python     7589MiB |\n",
      "|    2   N/A  N/A   2348616      C   ...da3/envs/lavis/bin/python     7589MiB |\n",
      "|    3   N/A  N/A   2348633      C   ...da3/envs/lavis/bin/python     7589MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[0]\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240bf446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from alpaca_eval import evaluate as alpaca_farm_evaluate\n",
    "from eval.utils import query_openai_chat_model, query_openai_model, generate_completions, dynamic_import_function, load_hf_lm_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ef69d0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(reference_path='alpaca_eval_data', save_dir='../results/ft1_ep=1/llama-7b_tuluv1m/eval/alpaca_farm/', model_name_or_path='../results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3', tokenizer_name_or_path=None, use_slow_tokenizer=False, openai_engine=None, eval_batch_size=5, load_in_8bit=False, gptq=False, use_chat_format=True, chat_formatting_function='eval.templates.create_prompt_with_tulu_chat_format', use_vllm=False, annotators_config='chatgpt', max_num_examples=None)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--reference_path\", type=str, default=\"data/eval/alpaca_farm/davinci_003_outputs_2048_token.json\", help=\"Path to the reference outputs. Alpaca_eval leaderboard use davinci_003 to generate the reference outputs, but they limit the max_tokens to 300. Here regenerated reference outputs with max_tokens=2048.\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/alpaca_farm\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"If specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"If specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--use_slow_tokenizer\", action=\"store_true\", help=\"If given, we will use the slow tokenizer.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"If specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"Batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"Load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, we will use the chat format for the prompts.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--use_vllm\", action=\"store_true\", help=\"If given, we will use vLLM to generate the predictions - much faster.\")\n",
    "parser.add_argument(\"--annotators_config\", type=str, default=\"alpaca_eval_gpt4_0314\")\n",
    "parser.add_argument(\"--max_num_examples\", type=int, default=None, help=\"maximum number of examples to evaluate.\")\n",
    "\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "model_name_or_path = '../results/ft1_ep=1/llama-7b_tuluv1m'\n",
    "save_dir = os.path.join(model_name_or_path, 'eval/alpaca_farm/')\n",
    "model_name_or_path = '../results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3'\n",
    "\n",
    "# eval_batch_size <=5 without oom for llama-7b gen 2048 new tokens\n",
    "\n",
    "#     --use_chat_format \\\n",
    "#     --max_num_examples 50 \\\n",
    "#     --max_num_examples 5 \\\n",
    "cmd = f\"\"\"\n",
    "    --reference_path alpaca_eval_data \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --save_dir {save_dir} \\\n",
    "    --eval_batch_size 5 \\\n",
    "    --use_chat_format \\\n",
    "    --annotators_config chatgpt \\\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5aebfcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading data and model...\n",
      "WARNING:datasets.builder:Found cached dataset alpaca_farm (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc3c105d0284d68a215259ab3710129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random.seed(42)\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "if args.reference_path != 'alpaca_eval_data':\n",
    "    raise NotImplementedError('Only support alpaca_eval_data for now. Need to work with `max_num_examples` in case I want to use other reference generation.')\n",
    "\n",
    "logging.info(\"loading data and model...\")\n",
    "alpaca_eval_data = datasets.load_dataset(\"tatsu-lab/alpaca_farm\", \"alpaca_farm_evaluation\")[\"eval\"]\n",
    "\n",
    "if args.max_num_examples and len(alpaca_eval_data) > args.max_num_examples:\n",
    "    inds = random.sample(range(len(alpaca_eval_data)), args.max_num_examples)\n",
    "    alpaca_eval_data = alpaca_eval_data.select(inds)\n",
    "\n",
    "prompts = []\n",
    "chat_formatting_function = dynamic_import_function(args.chat_formatting_function) if args.use_chat_format else None\n",
    "for example in alpaca_eval_data:\n",
    "    prompt = example[\"instruction\"] + \"\\n\\n\" + example[\"input\"] if example[\"input\"] != \"\" else example[\"instruction\"]\n",
    "    if args.use_chat_format:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = chat_formatting_function(messages, add_bos=False)\n",
    "    prompts.append(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f55e4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9139\n"
     ]
    }
   ],
   "source": [
    "# def format_prompt_template(x):\n",
    "#     return {\n",
    "#         'prompt': \n",
    "#     }\n",
    "\n",
    "\n",
    "# ds = alpaca_eval_data.map(format_prompt_template)\n",
    "# ds\n",
    "p = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv/eval/alpacafarm_ann=chatgpt_chatfmt/mistral-7b_ultrachat200k_beforesplitlongconv-greedy-long-output.json'\n",
    "import json\n",
    "with open(p, 'rb') as f:\n",
    "    anns = [json.loads(l) for l in f]\n",
    "\n",
    "prompts = []\n",
    "for output_1, output_2, instr, inp in zip(\n",
    "    [x['output'] for x in anns],\n",
    "    ds['output'],\n",
    "    ds['instruction'],\n",
    "    ds['input']):\n",
    "    \n",
    "    s = prompt_template.format(\n",
    "        output_1=output_1, \n",
    "        output_2=output_2, \n",
    "        instruction=instr + \"\\n\\n\" + inp if inp != \"\" else instr)\n",
    "    prompts.append(s)\n",
    "    \n",
    "print(max(len(x) for x in prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ee1adb9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "o = tokenizer(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "57b98b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2617"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in o.input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "359b5397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful instruction-following assistant.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here's an example:\n",
      "\n",
      "# Example:\n",
      "## Instruction:\n",
      "Give a description of the following job: \"ophthalmologist\"\n",
      "\n",
      "## Output (a):\n",
      "An ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\n",
      "\n",
      "## Output (b):\n",
      "An ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\n",
      "\n",
      "## Which is best, Output (a) or Output (b)?\n",
      "Output (a)\n",
      "\n",
      "Here the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\n",
      "\n",
      "# Task:\n",
      "Now is the real task, do not explain your answer, just say Output (a) or Output (b).\n",
      "\n",
      "## Instruction:\n",
      "How did US states get their names?\n",
      "\n",
      "## Output (a):\n",
      "The names of US states have a variety of origins, including:\n",
      "\n",
      "1. Native American languages: Many state names come from Native American languages, such as California, which comes from the Spanish word for \"the place of the California Indians.\"\n",
      "\n",
      "2. Colonial history: Some state names come from the colonial history of the United States, such as Virginia, named after Queen Elizabeth I.\n",
      "\n",
      "3. Geography: Some state names come from geographical features, such as Nevada, which means \"snow-capped\" in Spanish.\n",
      "\n",
      "4. European explorers: Many state names come from European explorers, such as Florida, named after the Spanish explorer Juan Ponce de León.\n",
      "\n",
      "5. Invented names: Some state names were simply invented, such as Colorado, which means \"colored red\" in Spanish.\n",
      "\n",
      "## Output (b):\n",
      "Most US states were named after either Native American tribes, geographical features, or historical figures. For example, the state of Florida was named after the Spanish explorer Ponce de Leon, and the state of Texas was named after the Caddo word “tejas” meaning friends or allies.\n",
      "\n",
      "## Which is best, Output (a) or Output (b)?\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(prompts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "37485da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4b6d0a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=args.model_name_or_path,\n",
    "    tokenizer_name_or_path=args.tokenizer_name_or_path if args.tokenizer_name_or_path is not None else args.model_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "    gptq_model=args.gptq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21f6ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(alpaca_eval_data['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b80f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(x) for x in out.input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f70b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating the annotator from `chatgpt`.\n",
      "INFO:root:Saving annotations to `/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "# args.annotators_config\n",
    "\n",
    "import pandas as pd\n",
    "from alpaca_eval import annotators\n",
    "\n",
    "\n",
    "class MyAnnotator(annotators.PairwiseAnnotator):\n",
    "    \n",
    "    \n",
    "\n",
    "    ### Public methods ###\n",
    "    def __call__(self, df_to_annotate: pd.DataFrame, **decoding_kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Annotates the given examples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_to_annotate : pd.DataFrame\n",
    "            Examples to annotate\n",
    "\n",
    "        decoding_kwargs :\n",
    "            Additional arguments to pass to `fn_completions`.\n",
    "        \"\"\"\n",
    "        df_to_annotate = df_to_annotate.copy()  # avoid in place modifications\n",
    "\n",
    "        if df_to_annotate.empty:\n",
    "            df_to_annotate[self.annotation_column] = []\n",
    "            return df_to_annotate\n",
    "\n",
    "        df_to_annotate = self._preprocess(df_to_annotate)\n",
    "\n",
    "        # prompts and completions here will not be the same length as the dataframe due to batching\n",
    "        prompts, df_to_annotate = self._make_prompts(df_to_annotate)\n",
    "\n",
    "        completions = self.fn_completions(prompts=prompts, **self.completions_kwargs, **decoding_kwargs)\n",
    "\n",
    "        annotations_to_save, completions_to_save = self._parse_completions(completions=completions[\"completions\"])\n",
    "        df_to_annotate[self.annotation_column] = annotations_to_save\n",
    "        if self.completion_column is not None:\n",
    "            df_to_annotate[self.completion_column] = completions_to_save\n",
    "\n",
    "        for k, v in completions.items():\n",
    "            if k != \"completions\":\n",
    "                if len(df_to_annotate[self.annotation_column]) == len(v) * self.batch_size:\n",
    "                    v = [el for el in v for _ in range(self.batch_size)]\n",
    "                df_to_annotate[k] = v\n",
    "                if \"per_example\" in k:\n",
    "                    df_to_annotate[k] = df_to_annotate[k] / self.batch_size\n",
    "\n",
    "        df_annotated = self._postprocess(df_to_annotate)\n",
    "\n",
    "        return df_annotated\n",
    "\n",
    "\n",
    "    \n",
    "import alpaca_eval\n",
    "Annotator = MyAnnotator\n",
    "\n",
    "model_outputs=model_results\n",
    "reference_outputs=args.reference_path if args.reference_path != 'alpaca_eval_data' else alpaca_eval_data\n",
    "annotators_config=args.annotators_config\n",
    "model_outputs = alpaca_eval.utils.load_or_convert_to_dataframe(model_outputs)\n",
    "reference_outputs = alpaca_eval.utils.load_or_convert_to_dataframe(reference_outputs)    \n",
    "\n",
    "# MyAnnotator()\n",
    "\n",
    "model_outputs = model_outputs[:1]\n",
    "reference_outputs = reference_outputs[:1]\n",
    "\n",
    "annotator_kwargs = dict()\n",
    "annotator = Annotator(annotators_config=annotators_config, **annotator_kwargs)\n",
    "# annotations = annotator.annotate_head2head(\n",
    "#     outputs_1=reference_outputs, outputs_2=model_outputs, **annotation_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1f45ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful instruction-following assistant.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here's an example:\n",
      "\n",
      "# Example:\n",
      "## Instruction:\n",
      "Give a description of the following job: \"ophthalmologist\"\n",
      "\n",
      "## Output (a):\n",
      "An ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\n",
      "\n",
      "## Output (b):\n",
      "An ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\n",
      "\n",
      "## Which is best, Output (a) or Output (b)?\n",
      "Output (a)\n",
      "\n",
      "Here the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\n",
      "\n",
      "# Task:\n",
      "Now is the real task, do not explain your answer, just say Output (a) or Output (b).\n",
      "\n",
      "## Instruction:\n",
      "{instruction}\n",
      "\n",
      "## Output (a):\n",
      "{output_1}\n",
      "\n",
      "## Output (b):\n",
      "{output_2}\n",
      "\n",
      "## Which is best, Output (a) or Output (b)?\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2be9f702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output_1</th>\n",
       "      <th>generator_1</th>\n",
       "      <th>sample_mode_1</th>\n",
       "      <th>dataset</th>\n",
       "      <th>datasplit</th>\n",
       "      <th>output_2</th>\n",
       "      <th>generator_2</th>\n",
       "      <th>sample_mode_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of some famous actors that ...</td>\n",
       "      <td></td>\n",
       "      <td>Some famous actors that started their careers ...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>temp=0.7,top_p=1.0,max_new_tokens=300</td>\n",
       "      <td>helpful_base</td>\n",
       "      <td>eval</td>\n",
       "      <td>Some famous actors that started their careers ...</td>\n",
       "      <td>llama-7b_tuluv1m-greedy-long</td>\n",
       "      <td>temp=0,max_new_tokens=2048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction input  \\\n",
       "0  What are the names of some famous actors that ...         \n",
       "\n",
       "                                            output_1       generator_1  \\\n",
       "0  Some famous actors that started their careers ...  text-davinci-003   \n",
       "\n",
       "                           sample_mode_1       dataset datasplit  \\\n",
       "0  temp=0.7,top_p=1.0,max_new_tokens=300  helpful_base      eval   \n",
       "\n",
       "                                            output_2  \\\n",
       "0  Some famous actors that started their careers ...   \n",
       "\n",
       "                    generator_2               sample_mode_2  \n",
       "0  llama-7b_tuluv1m-greedy-long  temp=0,max_new_tokens=2048  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/main.py#L133\n",
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/pairwise_evaluator.py#L155\n",
    "self = annotator\n",
    "keys_to_merge = None\n",
    "outputs_1 = reference_outputs\n",
    "outputs_2 = model_outputs\n",
    "\n",
    "if keys_to_merge is None:\n",
    "    keys_to_merge = self.input_keys\n",
    "\n",
    "keys_to_merge = list(keys_to_merge)\n",
    "\n",
    "\n",
    "outputs_1 = alpaca_eval.utils.convert_to_dataframe(outputs_1)\n",
    "outputs_2 = alpaca_eval.utils.convert_to_dataframe(outputs_2)\n",
    "\n",
    "\n",
    "# find all the columns that are in both\n",
    "other_same_cols = [k for k in outputs_1.columns if k in outputs_2 and k not in (keys_to_merge + [\"output\"])]\n",
    "\n",
    "df_to_annotate = pd.merge(\n",
    "    outputs_1,\n",
    "    outputs_2,\n",
    "    on=keys_to_merge,\n",
    "    suffixes=(\"_1\", \"_2\"),\n",
    ")\n",
    "\n",
    "for c in other_same_cols:\n",
    "    # if the columns are the same, we can drop the _2\n",
    "    if df_to_annotate[c + \"_1\"].equals(df_to_annotate[c + \"_2\"]):\n",
    "        df_to_annotate = df_to_annotate.drop(columns=c + \"_2\").rename(columns={c + \"_1\": c})\n",
    "\n",
    "# if you are taking the cartesian product, you can have undesired duplicates\n",
    "df_to_annotate = df_to_annotate.drop_duplicates()\n",
    "\n",
    "if not (len(outputs_1) == len(outputs_2) == len(df_to_annotate)):\n",
    "    logging.warning(\n",
    "        f\"The length of outputs before and after merge are not the same. We have len(outputs_1)==\"\n",
    "        f\"{len(outputs_1)}, len(outputs_2)=={len(outputs_2)}, and len(df_annotated)=={len(df_to_annotate)}.\"\n",
    "        f\" This means that there are missing examples or duplicates. We are taking a SQL inner join.\"\n",
    "    )\n",
    "\n",
    "df_to_annotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a53a79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/base.py#L135\n",
    "# out = self.__call__(df_to_annotate, **decoding_kwargs)\n",
    "\n",
    "to_annotate = df_to_annotate\n",
    "chunksize = 128\n",
    "decoding_kwargs = dict()\n",
    "\n",
    "\n",
    "# note: not ideal potentially doing a lot of dataframe copies. But given that they should be small, ~ok\n",
    "df_to_annotate = alpaca_eval.utils.convert_to_dataframe(to_annotate)\n",
    "# make sure primary keys are strings\n",
    "for c in self.primary_keys:\n",
    "    df_to_annotate[c] = df_to_annotate[c].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "all_annotated = []\n",
    "for df_chunk in alpaca_eval.utils.dataframe_chunk_generator(df_to_annotate, chunksize, tqdm_desc=\"Annotation chunk\"):\n",
    "    curr_df_to_annotate = self._preprocess(df_chunk)\n",
    "#     df_annotated = self._annotate(curr_df_to_annotate, **decoding_kwargs)\n",
    "#     annotated = self._postprocess_and_store_(df_annotated, df_chunk)\n",
    "#     all_annotated.extend(annotated)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "675f98a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/base.py#L245\n",
    "self.annotators['chatgpt']\n",
    "# curr_df_to_annotate = curr_df_to_annotate.drop(columns=['annotator', 'preference', 'price_per_example', 'time_per_example'])\n",
    "curr_df_to_annotate.loc[0,'annotator'] = 'chatgpt'\n",
    "\n",
    "\n",
    "curr_idcs = curr_df_to_annotate[self.annotator_column] == 'chatgpt'\n",
    "# if self.annotation_key in curr_df_to_annotate.columns:\n",
    "#     curr_idcs &= curr_df_to_annotate[self.annotation_key].isna()\n",
    "\n",
    "\n",
    "# # actual annotation\n",
    "# curr_annotated = self.annotators['chatgpt'](\n",
    "#     curr_df_to_annotate.loc[curr_idcs, self.available_fields_to_format],\n",
    "#     **decoding_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ad9523f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output_1</th>\n",
       "      <th>output_2</th>\n",
       "      <th>annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of some famous actors that ...</td>\n",
       "      <td>Some famous actors that started their careers ...</td>\n",
       "      <td>Some famous actors that started their careers ...</td>\n",
       "      <td>chatgpt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  What are the names of some famous actors that ...   \n",
       "\n",
       "                                            output_1  \\\n",
       "0  Some famous actors that started their careers ...   \n",
       "\n",
       "                                            output_2 annotator  \n",
       "0  Some famous actors that started their careers ...   chatgpt  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_df_to_annotate.loc[curr_idcs, self.available_fields_to_format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "369530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/base.py#L547\n",
    "# SingleAnnotator\n",
    "df_to_annotate = curr_df_to_annotate.loc[curr_idcs, self.available_fields_to_format]\n",
    "\n",
    "\n",
    "df_to_annotate = df_to_annotate.copy()  # avoid in place modifications\n",
    "\n",
    "# if df_to_annotate.empty:\n",
    "#     df_to_annotate[self.annotation_column] = []\n",
    "#     return df_to_annotate\n",
    "\n",
    "df_to_annotate = self.annotators['chatgpt']._preprocess(df_to_annotate)\n",
    "\n",
    "# prompts and completions here will not be the same length as the dataframe due to batching\n",
    "prompts, df_to_annotate = self.annotators['chatgpt']._make_prompts(df_to_annotate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e537649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_template = self.annotators['chatgpt'].prompt_template\n",
    "batch_size=self.annotators['chatgpt'].batch_size\n",
    "prompts, _ = alpaca_eval.utils.make_prompts(df=df_to_annotate, template=prompt_template, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "67ca3cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'instruction': 1, 'output_1': 1, 'output_2': 1})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/utils.py#L94\n",
    "df = df_to_annotate\n",
    "template = prompt_template\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "text_to_format = re.findall(r\"{([^ \\s]+?)}\", template)\n",
    "n_occurrences = Counter(text_to_format)\n",
    "n_occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f15aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fe991e4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Filtered')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/urllib3/connectionpool.py:711\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_new_proxy_conn \u001b[38;5;129;01mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/urllib3/connectionpool.py:1007\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     conn\u001b[38;5;241m.\u001b[39mtls_in_tls_required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/urllib3/connection.py:374\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Calls self._set_hostport(), so self.host is\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# self._tunnel_host below.\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Mark this connection as not reusable\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/http/client.py:925\u001b[0m, in \u001b[0;36mHTTPConnection._tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTunnel connection failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: Tunnel connection failed: 403 Filtered",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/requests/adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 440\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Filtered')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# encoding = tiktoken.get_encoding(\"cl100k_base\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tiktoken/model.py:97\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoding_for_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Encoding:\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the encoding used by a model.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Raises a KeyError if the model name is not recognised.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding_name_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tiktoken/registry.py:73\u001b[0m, in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Plugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     72\u001b[0m constructor \u001b[38;5;241m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[0;32m---> 73\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoding(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     74\u001b[0m ENCODINGS[encoding_name] \u001b[38;5;241m=\u001b[39m enc\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m enc\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tiktoken_ext/openai_public.py:64\u001b[0m, in \u001b[0;36mcl100k_base\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcl100k_base\u001b[39m():\n\u001b[0;32m---> 64\u001b[0m     mergeable_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     special_tokens \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     68\u001b[0m         ENDOFTEXT: \u001b[38;5;241m100257\u001b[39m,\n\u001b[1;32m     69\u001b[0m         FIM_PREFIX: \u001b[38;5;241m100258\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m         ENDOFPROMPT: \u001b[38;5;241m100276\u001b[39m,\n\u001b[1;32m     73\u001b[0m     }\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpat_str\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m(?i:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md)|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,3}| ?[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]+[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]*|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(?!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS)|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergeable_ranks\u001b[39m\u001b[38;5;124m\"\u001b[39m: mergeable_ranks,\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens,\n\u001b[1;32m     79\u001b[0m     }\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tiktoken/load.py:116\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    118\u001b[0m         base64\u001b[38;5;241m.\u001b[39mb64decode(token): \u001b[38;5;28mint\u001b[39m(rank)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token, rank \u001b[38;5;129;01min\u001b[39;00m (line\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m line)\n\u001b[1;32m    120\u001b[0m     }\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tiktoken/load.py:48\u001b[0m, in \u001b[0;36mread_file_cached\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 48\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m tmp_filename \u001b[38;5;241m=\u001b[39m cache_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tiktoken/load.py:24\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/requests/sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    648\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/requests/adapters.py:513\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Filtered')))"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79f9e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Completions:   0%|          | 0/5 [00:00<?, ?it/s]/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Generating Completions: 100%|██████████| 5/5 [01:51<00:00, 22.39s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts[:5],\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    batch_size=args.eval_batch_size if args.eval_batch_size else 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f666ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(outputs)):\n",
    "    print(i, )\n",
    "    prompt_len = len(tokenizer(prompts[i], add_special_tokens=False)['input_ids'])\n",
    "    print(f'---------- user (length={prompt_len}) ----------')\n",
    "    print(prompts[i])\n",
    "    output_len = len(tokenizer(outputs[i], add_special_tokens=True)['input_ids'])\n",
    "    print(f'---------- assistant (length={output_len}) ----------')\n",
    "    print(outputs[i])\n",
    "    print('\\n\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "632508b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = os.path.basename(os.path.normpath(args.model_name_or_path)) if args.model_name_or_path is not None else args.openai_engine\n",
    "model_results = []\n",
    "with open(os.path.join(args.save_dir, f\"{model_name}-greedy-long-output.json\"), \"w\") as fout:\n",
    "    for example, output in zip(alpaca_eval_data, outputs):\n",
    "        example[\"output\"] = output\n",
    "        example[\"generator\"] = f\"{model_name}-greedy-long\"\n",
    "        example[\"sample_mode\"] = \"temp=0,max_new_tokens=2048\"\n",
    "        fout.write(json.dumps(example) + \"\\n\")\n",
    "        model_results.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8a4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_leaderboard, annotations = alpaca_farm_evaluate(\n",
    "    model_outputs=model_results,\n",
    "    reference_outputs=args.reference_path if args.reference_path != 'alpaca_eval_data' else alpaca_eval_data,\n",
    "    annotators_config=args.annotators_config,\n",
    "    output_path=args.save_dir,\n",
    "    is_return_instead_of_print=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183bdd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# prices = [x['price_per_example'] for x in annotations]\n",
    "# print(f'Price (per-example / total) = {np.mean(prices):.4f} / {np.sum(prices):.2f}')\n",
    "\n",
    "# times = [x['time_per_example'] for x in annotations]\n",
    "# print(f'Time  (per-example / total) = {np.mean(times):.4f} / {np.sum(times):.2f}')\n",
    "\n",
    "# df_leaderboard['avg_output_tok_length'] = np.mean(\n",
    "#     [len(tokenizer(x['output'])['input_ids']) for x in model_results])\n",
    "# df_leaderboard['price'] = np.sum(prices)\n",
    "\n",
    "# df_leaderboard.insert(0, 'model', df_leaderboard.index)\n",
    "df_leaderboard.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(df_leaderboard.to_string(float_format=\"%.2f\"))\n",
    "display(df_leaderboard)\n",
    "# save to json\n",
    "\n",
    "with open(os.path.join(args.save_dir, f\"metrics.json\"), \"w\") as fout:\n",
    "    json.dump(df_leaderboard.iloc[0].to_dict(), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b058c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
