{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd29b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Wed Oct 25 15:48:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    71W / 300W |  14426MiB / 32510MiB |     81%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    37W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    37W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    51W / 300W |    373MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1662739      C   python                           7211MiB |\n",
      "|    0   N/A  N/A   1663825      C   python                           7211MiB |\n",
      "|    3   N/A  N/A   1669392      C   ...8.8/build/bin/./mdrun_mpi      371MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[0]\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240bf446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-25 15:48:52,450] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from alpaca_eval import evaluate as alpaca_farm_evaluate\n",
    "from eval.utils import query_openai_chat_model, query_openai_model, generate_completions, dynamic_import_function, load_hf_lm_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66646ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# response = openai.ChatCompletion.create(\n",
    "#   model=\"gpt-3.5-turbo-0301\",\n",
    "#   messages=[{'role': 'user', 'content': 'Who is the president of the US right now.'}],\n",
    "#   temperature=0,\n",
    "#   max_tokens=256\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef69d0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(reference_path='alpaca_eval_data', save_dir='../results/ft1_ep=1/llama-7b_tuluv1m/eval/alpaca_farm/', model_name_or_path='../results/ft1_ep=1/llama-7b_tuluv1m', tokenizer_name_or_path=None, openai_engine=None, eval_batch_size=5, load_in_8bit=False, gptq=False, use_chat_format=True, chat_formatting_function='eval.templates.create_prompt_with_tulu_chat_format', use_vllm=False, annotators_config='chatgpt', max_num_examples=5)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--reference_path\", type=str, default=\"data/eval/alpaca_farm/davinci_003_outputs_2048_token.json\", help=\"Path to the reference outputs. Alpaca_eval leaderboard use davinci_003 to generate the reference outputs, but they limit the max_tokens to 300. Here regenerated reference outputs with max_tokens=2048.\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/alpaca_farm\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"If specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"If specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"If specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"Batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"Load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, we will use the chat format for the prompts.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--use_vllm\", action=\"store_true\", help=\"If given, we will use vLLM to generate the predictions - much faster.\")\n",
    "parser.add_argument(\"--annotators_config\", type=str, default=\"alpaca_eval_gpt4_0314\")\n",
    "parser.add_argument(\"--max_num_examples\", type=int, default=None, help=\"maximum number of examples to evaluate.\")\n",
    "\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "model_name_or_path = '../results/ft1_ep=1/llama-7b_tuluv1m'\n",
    "save_dir = os.path.join(model_name_or_path, 'eval/alpaca_farm/')\n",
    "\n",
    "# eval_batch_size <=5 without oom for llama-7b gen 2048 new tokens\n",
    "\n",
    "#     --use_chat_format \\\n",
    "#     --max_num_examples 50 \\\n",
    "cmd = f\"\"\"\n",
    "    --reference_path alpaca_eval_data \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --save_dir {save_dir} \\\n",
    "    --eval_batch_size 5 \\\n",
    "    --use_chat_format \\\n",
    "    --annotators_config chatgpt \\\n",
    "    --max_num_examples 5 \\\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5aebfcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading data and model...\n",
      "WARNING:datasets.builder:Found cached dataset alpaca_farm (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a481ff82cd404ebe9e48136980693258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random.seed(42)\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "if args.reference_path != 'alpaca_eval_data':\n",
    "    raise NotImplementedError('Only support alpaca_eval_data for now. Need to work with `max_num_examples` in case I want to use other reference generation.')\n",
    "\n",
    "logging.info(\"loading data and model...\")\n",
    "alpaca_eval_data = datasets.load_dataset(\"tatsu-lab/alpaca_farm\", \"alpaca_farm_evaluation\")[\"eval\"]\n",
    "\n",
    "if args.max_num_examples and len(alpaca_eval_data) > args.max_num_examples:\n",
    "    inds = random.sample(range(len(alpaca_eval_data)), args.max_num_examples)\n",
    "    alpaca_eval_data = alpaca_eval_data.select(inds)\n",
    "\n",
    "prompts = []\n",
    "chat_formatting_function = dynamic_import_function(args.chat_formatting_function) if args.use_chat_format else None\n",
    "for example in alpaca_eval_data:\n",
    "    prompt = example[\"instruction\"] + \"\\n\\n\" + example[\"input\"] if example[\"input\"] != \"\" else example[\"instruction\"]\n",
    "    if args.use_chat_format:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = chat_formatting_function(messages, add_bos=False)\n",
    "    prompts.append(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6d0a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=args.model_name_or_path,\n",
    "    tokenizer_name_or_path=args.tokenizer_name_or_path if args.tokenizer_name_or_path is not None else args.model_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "    gptq_model=args.gptq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f9e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Completions:   0%|          | 0/5 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Generating Completions: 100%|██████████| 5/5 [02:07<00:00, 25.48s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts[:5],\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    batch_size=args.eval_batch_size if args.eval_batch_size else 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31f666ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(outputs)):\n",
    "    print(i, )\n",
    "    prompt_len = len(tokenizer(prompts[i], add_special_tokens=False)['input_ids'])\n",
    "    print(f'---------- user (length={prompt_len}) ----------')\n",
    "    print(prompts[i])\n",
    "    output_len = len(tokenizer(outputs[i], add_special_tokens=True)['input_ids'])\n",
    "    print(f'---------- assistant (length={output_len}) ----------')\n",
    "    print(outputs[i])\n",
    "    print('\\n\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "632508b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = os.path.basename(os.path.normpath(args.model_name_or_path)) if args.model_name_or_path is not None else args.openai_engine\n",
    "model_results = []\n",
    "with open(os.path.join(args.save_dir, f\"{model_name}-greedy-long-output.json\"), \"w\") as fout:\n",
    "    for example, output in zip(alpaca_eval_data, outputs):\n",
    "        example[\"output\"] = output\n",
    "        example[\"generator\"] = f\"{model_name}-greedy-long\"\n",
    "        example[\"sample_mode\"] = \"temp=0,max_new_tokens=2048\"\n",
    "        fout.write(json.dumps(example) + \"\\n\")\n",
    "        model_results.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd3fde1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:precomputed_leaderboard = 'auto'. But we have found no corresponding leaderboard\n",
      "INFO:root:Evaluating the llama-7b_tuluv1m-greedy-long outputs.\n",
      "INFO:root:Creating the annotator from `chatgpt`.\n",
      "INFO:root:Saving annotations to `/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt/annotations_seed0_configs.json`.\n",
      "WARNING:root:The length of outputs before and after merge are not the same. We have len(outputs_1)==805, len(outputs_2)==5, and len(df_annotated)==5. This means that there are missing examples or duplicates. We are taking a SQL inner join.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]INFO:root:Annotating 5 examples with chatgpt\n",
      "INFO:root:Using `openai_completions` on 5 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'temperature': 0}. num_procs=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "prompt_batches:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "prompt_batches: 100%|██████████| 5/5 [00:00<00:00,  7.46it/s]\u001b[A\n",
      "INFO:root:Completed 5 examples in 0.9 seconds.\n",
      "INFO:root:Saving all annotations to /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "INFO:root:Saving all results to ../results/ft1_ep=1/llama-7b_tuluv1m/eval/alpaca_farm\n",
      "INFO:root:Not saving the result to the cached leaderboard because precomputed_leaderboard is not a path but <class 'NoneType'>.\n"
     ]
    }
   ],
   "source": [
    "df_leaderboard, annotations = alpaca_farm_evaluate(\n",
    "    model_outputs=model_results,\n",
    "    reference_outputs=args.reference_path if args.reference_path != 'alpaca_eval_data' else alpaca_eval_data,\n",
    "    annotators_config=args.annotators_config,\n",
    "    output_path=args.save_dir,\n",
    "    is_return_instead_of_print=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "62dfb959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win_rate</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>n_wins</th>\n",
       "      <th>n_wins_base</th>\n",
       "      <th>n_draws</th>\n",
       "      <th>n_total</th>\n",
       "      <th>mode</th>\n",
       "      <th>avg_length</th>\n",
       "      <th>avg_output_tok_length</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-7b_tuluv1m-greedy-long</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.494897</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>community</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>694.4</td>\n",
       "      <td>0.008982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  win_rate  standard_error  n_wins  \\\n",
       "0  llama-7b_tuluv1m-greedy-long      60.0       24.494897     3.0   \n",
       "\n",
       "   n_wins_base  n_draws  n_total       mode  avg_length  \\\n",
       "0          2.0      0.0      5.0  community      2238.0   \n",
       "\n",
       "   avg_output_tok_length     price  \n",
       "0                  694.4  0.008982  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# prices = [x['price_per_example'] for x in annotations]\n",
    "# print(f'Price (per-example / total) = {np.mean(prices):.4f} / {np.sum(prices):.2f}')\n",
    "\n",
    "# times = [x['time_per_example'] for x in annotations]\n",
    "# print(f'Time  (per-example / total) = {np.mean(times):.4f} / {np.sum(times):.2f}')\n",
    "\n",
    "# df_leaderboard['avg_output_tok_length'] = np.mean(\n",
    "#     [len(tokenizer(x['output'])['input_ids']) for x in model_results])\n",
    "# df_leaderboard['price'] = np.sum(prices)\n",
    "\n",
    "# df_leaderboard.insert(0, 'model', df_leaderboard.index)\n",
    "df_leaderboard.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(df_leaderboard.to_string(float_format=\"%.2f\"))\n",
    "display(df_leaderboard)\n",
    "# save to json\n",
    "\n",
    "with open(os.path.join(args.save_dir, f\"metrics.json\"), \"w\") as fout:\n",
    "    json.dump(df_leaderboard.iloc[0].to_dict(), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4b262ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win_rate</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>n_wins</th>\n",
       "      <th>n_wins_base</th>\n",
       "      <th>n_draws</th>\n",
       "      <th>n_total</th>\n",
       "      <th>mode</th>\n",
       "      <th>avg_length</th>\n",
       "      <th>avg_output_tok_length</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama-7b_tuluv1m-greedy-long</th>\n",
       "      <td>llama-7b_tuluv1m-greedy-long</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.494897</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>community</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>694.4</td>\n",
       "      <td>0.008982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     model  win_rate  \\\n",
       "llama-7b_tuluv1m-greedy-long  llama-7b_tuluv1m-greedy-long      60.0   \n",
       "\n",
       "                              standard_error  n_wins  n_wins_base  n_draws  \\\n",
       "llama-7b_tuluv1m-greedy-long       24.494897     3.0          2.0      0.0   \n",
       "\n",
       "                              n_total       mode  avg_length  \\\n",
       "llama-7b_tuluv1m-greedy-long      5.0  community      2238.0   \n",
       "\n",
       "                              avg_output_tok_length     price  \n",
       "llama-7b_tuluv1m-greedy-long                  694.4  0.008982  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
