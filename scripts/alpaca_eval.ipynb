{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd29b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,5\n",
      "4\n",
      "Sun Nov 19 15:04:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    64W / 300W |  28351MiB / 32510MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    64W / 300W |  28195MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    64W / 300W |  28375MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    63W / 300W |  28171MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1228915      C   ...da3/envs/lavis/bin/python    28345MiB |\n",
      "|    1   N/A  N/A   1228916      C   ...da3/envs/lavis/bin/python    28189MiB |\n",
      "|    2   N/A  N/A   1228917      C   ...da3/envs/lavis/bin/python    28369MiB |\n",
      "|    3   N/A  N/A   1228918      C   ...da3/envs/lavis/bin/python    28165MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[0]\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240bf446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-19 15:04:15,194] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from alpaca_eval import evaluate as alpaca_farm_evaluate\n",
    "from eval.utils import query_openai_chat_model, query_openai_model, generate_completions, dynamic_import_function, load_hf_lm_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef69d0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(reference_path='alpaca_eval_data', save_dir='../results/oi5_ultrachat15:mistral-7b/mistral-7b_ultrachat15_score=semdedup:cl=kmeansfaisscd:md=mpnet:dist=cd:emb=text+embedding:nc=200_pace=prune:size=100000:ep=2/eval/alpacafarm_jpt/', model_name_or_path='../results/oi5_ultrachat15:mistral-7b/mistral-7b_ultrachat15_score=semdedup:cl=kmeansfaisscd:md=mpnet:dist=cd:emb=text+embedding:nc=200_pace=prune:size=100000:ep=2', tokenizer_name_or_path=None, use_slow_tokenizer=False, openai_engine=None, eval_batch_size=5, load_in_8bit=False, gptq=False, use_chat_format=True, chat_formatting_function='eval.templates.create_prompt_with_tulu_chat_format', use_vllm=False, annotators_config='chatgpt', max_num_examples=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--reference_path\", type=str, default=\"data/eval/alpaca_farm/davinci_003_outputs_2048_token.json\", help=\"Path to the reference outputs. Alpaca_eval leaderboard use davinci_003 to generate the reference outputs, but they limit the max_tokens to 300. Here regenerated reference outputs with max_tokens=2048.\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/alpaca_farm\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"If specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"If specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--use_slow_tokenizer\", action=\"store_true\", help=\"If given, we will use the slow tokenizer.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"If specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"Batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"Load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, we will use the chat format for the prompts.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--use_vllm\", action=\"store_true\", help=\"If given, we will use vLLM to generate the predictions - much faster.\")\n",
    "parser.add_argument(\"--annotators_config\", type=str, default=\"alpaca_eval_gpt4_0314\")\n",
    "parser.add_argument(\"--max_num_examples\", type=int, default=None, help=\"maximum number of examples to evaluate.\")\n",
    "\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "model_name_or_path = '../results/ft1_ep=1/llama-7b_tuluv1m'\n",
    "model_name_or_path = '../results/oi5_ultrachat:mistral-7b/mistral-7b_ultrachat200k_score=log:prob:neg_pace=prune:size=50000:ep=3'\n",
    "model_name_or_path = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'\n",
    "model_name_or_path = '../results/oi5_ultrachat15:mistral-7b/mistral-7b_ultrachat15_score=semdedup:cl=kmeansfaisscd:md=mpnet:dist=cd:emb=text+embedding:nc=200_pace=prune:size=100000:ep=2'\n",
    "\n",
    "save_dir = os.path.join(model_name_or_path, 'eval/alpacafarm_jpt/')\n",
    "\n",
    "# eval_batch_size <=5 without oom for llama-7b gen 2048 new tokens\n",
    "\n",
    "#     --use_chat_format \\\n",
    "#     --max_num_examples 50 \\\n",
    "#     --max_num_examples 5 \\\n",
    "cmd = f\"\"\"\n",
    "    --reference_path alpaca_eval_data \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --save_dir {save_dir} \\\n",
    "    --eval_batch_size 5 \\\n",
    "    --use_chat_format \\\n",
    "    --annotators_config chatgpt \\\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aebfcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading data and model...\n",
      "WARNING:datasets.builder:Found cached dataset alpaca_farm (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca26b64ee164d7f998ccb863003e3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random.seed(42)\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "if args.reference_path != 'alpaca_eval_data':\n",
    "    raise NotImplementedError('Only support alpaca_eval_data for now. Need to work with `max_num_examples` in case I want to use other reference generation.')\n",
    "\n",
    "logging.info(\"loading data and model...\")\n",
    "alpaca_eval_data = datasets.load_dataset(\"tatsu-lab/alpaca_farm\", \"alpaca_farm_evaluation\")[\"eval\"]\n",
    "\n",
    "if args.max_num_examples and len(alpaca_eval_data) > args.max_num_examples:\n",
    "    inds = random.sample(range(len(alpaca_eval_data)), args.max_num_examples)\n",
    "    alpaca_eval_data = alpaca_eval_data.select(inds)\n",
    "\n",
    "prompts = []\n",
    "chat_formatting_function = dynamic_import_function(args.chat_formatting_function) if args.use_chat_format else None\n",
    "for example in alpaca_eval_data:\n",
    "    prompt = example[\"instruction\"] + \"\\n\\n\" + example[\"input\"] if example[\"input\"] != \"\" else example[\"instruction\"]\n",
    "    if args.use_chat_format:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = chat_formatting_function(messages, add_bos=False)\n",
    "    prompts.append(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93259c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a/cache-5f614a893c213230.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'generator', 'sample_mode', 'dataset', 'datasplit'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_eval_data.filter(lambda x: 'ophthalmologist' in x['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b6d0a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=args.model_name_or_path,\n",
    "    tokenizer_name_or_path=args.tokenizer_name_or_path if args.tokenizer_name_or_path is not None else args.model_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "    gptq_model=args.gptq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f70b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating the annotator from `chatgpt`.\n",
      "INFO:root:Saving annotations to `/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "# args.annotators_config\n",
    "\n",
    "import pandas as pd\n",
    "from alpaca_eval import annotators\n",
    "\n",
    "\n",
    "class MyAnnotator(annotators.PairwiseAnnotator):\n",
    "    \n",
    "    \n",
    "\n",
    "    ### Public methods ###\n",
    "    def __call__(self, df_to_annotate: pd.DataFrame, **decoding_kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Annotates the given examples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_to_annotate : pd.DataFrame\n",
    "            Examples to annotate\n",
    "\n",
    "        decoding_kwargs :\n",
    "            Additional arguments to pass to `fn_completions`.\n",
    "        \"\"\"\n",
    "        df_to_annotate = df_to_annotate.copy()  # avoid in place modifications\n",
    "\n",
    "        if df_to_annotate.empty:\n",
    "            df_to_annotate[self.annotation_column] = []\n",
    "            return df_to_annotate\n",
    "\n",
    "        df_to_annotate = self._preprocess(df_to_annotate)\n",
    "\n",
    "        # prompts and completions here will not be the same length as the dataframe due to batching\n",
    "        prompts, df_to_annotate = self._make_prompts(df_to_annotate)\n",
    "\n",
    "        completions = self.fn_completions(prompts=prompts, **self.completions_kwargs, **decoding_kwargs)\n",
    "\n",
    "        annotations_to_save, completions_to_save = self._parse_completions(completions=completions[\"completions\"])\n",
    "        df_to_annotate[self.annotation_column] = annotations_to_save\n",
    "        if self.completion_column is not None:\n",
    "            df_to_annotate[self.completion_column] = completions_to_save\n",
    "\n",
    "        for k, v in completions.items():\n",
    "            if k != \"completions\":\n",
    "                if len(df_to_annotate[self.annotation_column]) == len(v) * self.batch_size:\n",
    "                    v = [el for el in v for _ in range(self.batch_size)]\n",
    "                df_to_annotate[k] = v\n",
    "                if \"per_example\" in k:\n",
    "                    df_to_annotate[k] = df_to_annotate[k] / self.batch_size\n",
    "\n",
    "        df_annotated = self._postprocess(df_to_annotate)\n",
    "\n",
    "        return df_annotated\n",
    "\n",
    "\n",
    "    \n",
    "import alpaca_eval\n",
    "Annotator = MyAnnotator\n",
    "\n",
    "model_outputs=model_results\n",
    "reference_outputs=args.reference_path if args.reference_path != 'alpaca_eval_data' else alpaca_eval_data\n",
    "annotators_config=args.annotators_config\n",
    "model_outputs = alpaca_eval.utils.load_or_convert_to_dataframe(model_outputs)\n",
    "reference_outputs = alpaca_eval.utils.load_or_convert_to_dataframe(reference_outputs)    \n",
    "\n",
    "# MyAnnotator()\n",
    "\n",
    "model_outputs = model_outputs[:1]\n",
    "reference_outputs = reference_outputs[:1]\n",
    "\n",
    "annotator_kwargs = dict()\n",
    "annotator = Annotator(annotators_config=annotators_config, **annotator_kwargs)\n",
    "# annotations = annotator.annotate_head2head(\n",
    "#     outputs_1=reference_outputs, outputs_2=model_outputs, **annotation_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2be9f702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output_1</th>\n",
       "      <th>generator_1</th>\n",
       "      <th>sample_mode_1</th>\n",
       "      <th>dataset</th>\n",
       "      <th>datasplit</th>\n",
       "      <th>output_2</th>\n",
       "      <th>generator_2</th>\n",
       "      <th>sample_mode_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of some famous actors that ...</td>\n",
       "      <td></td>\n",
       "      <td>Some famous actors that started their careers ...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>temp=0.7,top_p=1.0,max_new_tokens=300</td>\n",
       "      <td>helpful_base</td>\n",
       "      <td>eval</td>\n",
       "      <td>+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉</td>\n",
       "      <td>mistral-7b_ultrachat200k_beforesplitlongconv-g...</td>\n",
       "      <td>temp=0,max_new_tokens=2048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction input  \\\n",
       "0  What are the names of some famous actors that ...         \n",
       "\n",
       "                                            output_1       generator_1  \\\n",
       "0  Some famous actors that started their careers ...  text-davinci-003   \n",
       "\n",
       "                           sample_mode_1       dataset datasplit  \\\n",
       "0  temp=0.7,top_p=1.0,max_new_tokens=300  helpful_base      eval   \n",
       "\n",
       "           output_2                                        generator_2  \\\n",
       "0  + 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉  mistral-7b_ultrachat200k_beforesplitlongconv-g...   \n",
       "\n",
       "                sample_mode_2  \n",
       "0  temp=0,max_new_tokens=2048  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/main.py#L133\n",
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/pairwise_evaluator.py#L155\n",
    "self = annotator\n",
    "keys_to_merge = None\n",
    "outputs_1 = reference_outputs\n",
    "outputs_2 = model_outputs\n",
    "\n",
    "if keys_to_merge is None:\n",
    "    keys_to_merge = self.input_keys\n",
    "\n",
    "keys_to_merge = list(keys_to_merge)\n",
    "\n",
    "\n",
    "outputs_1 = alpaca_eval.utils.convert_to_dataframe(outputs_1)\n",
    "outputs_2 = alpaca_eval.utils.convert_to_dataframe(outputs_2)\n",
    "\n",
    "\n",
    "# find all the columns that are in both\n",
    "other_same_cols = [k for k in outputs_1.columns if k in outputs_2 and k not in (keys_to_merge + [\"output\"])]\n",
    "\n",
    "df_to_annotate = pd.merge(\n",
    "    outputs_1,\n",
    "    outputs_2,\n",
    "    on=keys_to_merge,\n",
    "    suffixes=(\"_1\", \"_2\"),\n",
    ")\n",
    "\n",
    "for c in other_same_cols:\n",
    "    # if the columns are the same, we can drop the _2\n",
    "    if df_to_annotate[c + \"_1\"].equals(df_to_annotate[c + \"_2\"]):\n",
    "        df_to_annotate = df_to_annotate.drop(columns=c + \"_2\").rename(columns={c + \"_1\": c})\n",
    "\n",
    "# if you are taking the cartesian product, you can have undesired duplicates\n",
    "df_to_annotate = df_to_annotate.drop_duplicates()\n",
    "\n",
    "if not (len(outputs_1) == len(outputs_2) == len(df_to_annotate)):\n",
    "    logging.warning(\n",
    "        f\"The length of outputs before and after merge are not the same. We have len(outputs_1)==\"\n",
    "        f\"{len(outputs_1)}, len(outputs_2)=={len(outputs_2)}, and len(df_annotated)=={len(df_to_annotate)}.\"\n",
    "        f\" This means that there are missing examples or duplicates. We are taking a SQL inner join.\"\n",
    "    )\n",
    "\n",
    "df_to_annotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a53a79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/base.py#L135\n",
    "# out = self.__call__(df_to_annotate, **decoding_kwargs)\n",
    "\n",
    "to_annotate = df_to_annotate\n",
    "chunksize = 128\n",
    "decoding_kwargs = dict()\n",
    "\n",
    "\n",
    "# note: not ideal potentially doing a lot of dataframe copies. But given that they should be small, ~ok\n",
    "df_to_annotate = alpaca_eval.utils.convert_to_dataframe(to_annotate)\n",
    "# make sure primary keys are strings\n",
    "for c in self.primary_keys:\n",
    "    df_to_annotate[c] = df_to_annotate[c].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "all_annotated = []\n",
    "for df_chunk in alpaca_eval.utils.dataframe_chunk_generator(df_to_annotate, chunksize, tqdm_desc=\"Annotation chunk\"):\n",
    "    curr_df_to_annotate = self._preprocess(df_chunk)\n",
    "#     df_annotated = self._annotate(curr_df_to_annotate, **decoding_kwargs)\n",
    "#     annotated = self._postprocess_and_store_(df_annotated, df_chunk)\n",
    "#     all_annotated.extend(annotated)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "675f98a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/base.py#L245\n",
    "self.annotators['chatgpt']\n",
    "# curr_df_to_annotate = curr_df_to_annotate.drop(columns=['annotator', 'preference', 'price_per_example', 'time_per_example'])\n",
    "curr_df_to_annotate.loc[0,'annotator'] = 'chatgpt'\n",
    "\n",
    "\n",
    "curr_idcs = curr_df_to_annotate[self.annotator_column] == 'chatgpt'\n",
    "# if self.annotation_key in curr_df_to_annotate.columns:\n",
    "#     curr_idcs &= curr_df_to_annotate[self.annotation_key].isna()\n",
    "\n",
    "\n",
    "# # actual annotation\n",
    "# curr_annotated = self.annotators['chatgpt'](\n",
    "#     curr_df_to_annotate.loc[curr_idcs, self.available_fields_to_format],\n",
    "#     **decoding_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ad9523f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output_1</th>\n",
       "      <th>output_2</th>\n",
       "      <th>annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of some famous actors that ...</td>\n",
       "      <td>Some famous actors that started their careers ...</td>\n",
       "      <td>+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉</td>\n",
       "      <td>chatgpt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  What are the names of some famous actors that ...   \n",
       "\n",
       "                                            output_1          output_2  \\\n",
       "0  Some famous actors that started their careers ...  + 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉   \n",
       "\n",
       "  annotator  \n",
       "0   chatgpt  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_df_to_annotate.loc[curr_idcs, self.available_fields_to_format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "369530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/base.py#L547\n",
    "# SingleAnnotator\n",
    "df_to_annotate = curr_df_to_annotate.loc[curr_idcs, self.available_fields_to_format]\n",
    "\n",
    "\n",
    "df_to_annotate = df_to_annotate.copy()  # avoid in place modifications\n",
    "\n",
    "# if df_to_annotate.empty:\n",
    "#     df_to_annotate[self.annotation_column] = []\n",
    "#     return df_to_annotate\n",
    "\n",
    "df_to_annotate = self.annotators['chatgpt']._preprocess(df_to_annotate)\n",
    "\n",
    "# prompts and completions here will not be the same length as the dataframe due to batching\n",
    "prompts, df_to_annotate = self.annotators['chatgpt']._make_prompts(df_to_annotate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e537649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = self.annotators['chatgpt'].prompt_template\n",
    "batch_size=self.annotators['chatgpt'].batch_size\n",
    "prompts, _ = alpaca_eval.utils.make_prompts(df=df_to_annotate, template=prompt_template, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "67ca3cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'instruction': 1, 'output_1': 1, 'output_2': 1})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/utils.py#L94\n",
    "df = df_to_annotate\n",
    "template = prompt_template\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "text_to_format = re.findall(r\"{([^ \\s]+?)}\", template)\n",
    "n_occurrences = Counter(text_to_format)\n",
    "n_occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4a82b040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9b900ebc624ca8b42614573adc0366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = 'huggyllama/llama-7b'\n",
    "_, llamatok = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=model_name_or_path,\n",
    "#     tokenizer_name_or_path=args.tokenizer_name_or_path if args.tokenizer_name_or_path is not None else args.model_name_or_path,\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "    gptq_model=args.gptq,\n",
    "    use_fast_tokenizer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8230012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<unk>', '<s>', '</s>', '<pad>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = args.model_name_or_path\n",
    "tok = AutoTokenizer.from_pretrained(p, use_fast=False)\n",
    "# tok.padding_side = 'left'\n",
    "# tok.pad_token = tok.eos_token\n",
    "# tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "# num_added_tokens = tok.add_special_tokens({\n",
    "#     \"pad_token\": \"<pad>\",\n",
    "# })\n",
    "# print(num_added_tokens)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec040c6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "32000\n",
      "{0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}\n",
      "{'<unk>': 0, '<s>': 1, '</s>': 2, '<pad>': 32000}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizerFast' object has no attribute 'legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(t\u001b[38;5;241m.\u001b[39madded_tokens_decoder)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(t\u001b[38;5;241m.\u001b[39mget_added_vocab())\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaTokenizerFast' object has no attribute 'legacy'"
     ]
    }
   ],
   "source": [
    "for t in [tok, llamatok]:\n",
    "    print(t.is_fast)\n",
    "    print(t.vocab_size)\n",
    "    print(t.added_tokens_decoder)\n",
    "    print(t.get_added_vocab())\n",
    "    print(t.legacy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "755e7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv/eval/alpacafarm_ann=chatgpt_chatfmt/mistral-7b_ultrachat200k_beforesplitlongconv-greedy-long-output.json'\n",
    "\n",
    "\n",
    "with open(p, 'rb') as f:\n",
    "    text = [json.loads(l) for l in f]\n",
    "    \n",
    "text = [x['output'] for x in text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2d62f6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  74,  180,  324,  177,  150,  215,  276,   92,  102,  387,   63,\n",
       "        275,  306,   75,   91,  277,  295,  276,   67,  341,   75,   62,\n",
       "         74,  203,   15,  166,   87,  269,  110,  100,  301,  298,  308,\n",
       "         51,   18,  100,  311,  144,  262,  110,  120,  231,  233,   58,\n",
       "        265,  231,  239,  337,   60,  274,   16,   42,  308,  386,  208,\n",
       "         78,  281,  317,  229,   32,  547,  272,  255,  221,  392,  180,\n",
       "        224,  323,  184,  152,   69,   16,  134,  119,  111,  283,  386,\n",
       "        381,  136,  231,  244,   83,  467,  182,  206,  434,  200,  304,\n",
       "         28,  292,  153,   18,  434,   12,  336,  106,  208,   38,  127,\n",
       "        141,  331,   60,   63,  351,  363,   13,  235,  250,  119,  218,\n",
       "        405,  321,   88,   68,  154,  393,  352,  196,   55,  105,   23,\n",
       "         20,   57,   95,  286,   27,  221,  249,   64,  308,  325,   72,\n",
       "        151,   20,  265,  160,  561,  278,  524,  366,  309,   60,   60,\n",
       "        315,   11,  370,  277,  140,  957,   67,   13,  291,  227,  537,\n",
       "        278,  108,  893,  173,   41,   50,  232,   51,  452,  145,    9,\n",
       "        217,  168,    6,    9,  432,  295,  642,  110,  122,   46,  184,\n",
       "        628,  414,  124,  356,  434,  164,  906,  309,  309,  189,  246,\n",
       "        490,  306,  163,    7,   41,   88,  301,  184,   23,  665,  100,\n",
       "         83,    8,  195,  409,  134,  808,  314,  421,  553,   64,  406,\n",
       "         93,    9,  116,  179,  115,  212,  384,  200,  295,  107,  350,\n",
       "        217,  424,   50,  250,  103,   26,  348,  152, 1161,  394,   69,\n",
       "        255,    8,  106,   92,  420,  253,  338,   89,  210,  279,  160,\n",
       "         50,  488,   71,    4,  304,  262,  319,   19,  205,  141,  207,\n",
       "        212,  147,  135,  181,  205,   59,  122,  311,   79,    3,  591,\n",
       "         76,  828,   20,   98,   17,  595,  235,  249,   77,  129,  347,\n",
       "        155,  544,  114,  216,  836,  160,  308,  246,  313, 1393,  160,\n",
       "         32,  348,  193,  408,  377,  190,   25,  143, 1974,  332,   82,\n",
       "        283,  272,  179,  503,  389,  109,  791,  168,  241,  250,  112,\n",
       "        226,  119,  317,   51,  264,  242,   99,  313,  195,   96,   37,\n",
       "        273,  792,   55,  275,   86,  268,   99,  271,   98,  348,  249,\n",
       "        208,  512,   12,   14,  426,   34,  538,  147,  129, 2045,   26,\n",
       "        399,  336,  426,  237,  110,  184,  121,  355,   19,   37,  262,\n",
       "        103,  289,  403,  226,  224,  103,  622,   22,  291,  370,   13,\n",
       "        355,  121,  252,   31,  240,  125,  604,   10,   22,  134,  127,\n",
       "        268,   55,  101,  126,  105,  644,  300,  164,  278,  141,   83,\n",
       "         14,  149,  100,  182,   41, 2046,   35,  186,  144,  258,  246,\n",
       "        525,  313,   61,  944,  148,  127,   14,  101,   13,  326,  257,\n",
       "        113,   30, 2049, 1040,  119,  139,  408,  398,  390,  221,  198,\n",
       "        113,   42,   34,   81,  175,   70,   36,  194,  522,  147,  227,\n",
       "        127,   91,  314,   64,  311,  531,  306,   39,   76,   20,   27,\n",
       "         25,   48,  236,  141,  109,  317,   36,   63,    8,  104,  324,\n",
       "        474,  237,  220,  245,   76,  282,   26,   37,   85,  100,   46,\n",
       "         25,  192,  459,   72,  255,  185, 1039,  127,  303,   34,   98,\n",
       "         20,   95,   32,   73, 2048,  148,   81,  245,   40,  407,  112,\n",
       "        289,   79,  208,  215,   59,   17,  235,   71,   64,   41,   59,\n",
       "         21,  172,  168,  187,  263,   50,   88,  242,  281,  340,  131,\n",
       "        110, 2049,  145,   10,  122,  269,  272,   35,  229,  195,  271,\n",
       "        299,   77,  165,  171,  253,  346,   75,  138,   70,  151,   89,\n",
       "         50,  367,  173,  118,  196,  274,  198,  268, 2052, 1035,  202,\n",
       "        236,   22,  104,   25,  188,  224,   14,   84,  218,  219,    6,\n",
       "        300,  289,   11,   99,  110,   98,  262,  348,  125,  223,   72,\n",
       "        436,  141,  105,   56,  294,  186,  124,  155,  200,   11,   70,\n",
       "        330,  161,  172,   62,  393,  351,  119,   89,  273,  305,  326,\n",
       "        149,   99,  156,  438,   92,   51,  248,  193,  146,  229,  257,\n",
       "        181,  694,  228,   12,   11,  180,   10,  264,   53,  195,  342,\n",
       "        236,  541,  191,   33,  153,   91,  145,    4,  165,  125,  134,\n",
       "        170,   54,  316,  177,   22,  308,   30, 2049,   64,  210, 8187,\n",
       "         37,  103,   26,   33,   27,   34,   60,   41,   21,   66,   22,\n",
       "          6,   13,   19,   90,  122,   33,  255,  188,  128,   59,  115,\n",
       "         75,  353,   11,  184,  144,  108, 2049,    3,    2,   10,  132,\n",
       "         25,   10,    9,    8,  131,  155,   15,    3,    6,   13,    3,\n",
       "         78,   26,   17,   12,   37,   99,    7,   77,   44,   60,   98,\n",
       "        250,  110,  282,   77,  484,  121,  106,  233,   96,  105,  181,\n",
       "         14,  248,  565,   82,   54,    6,   40,   49,   98,   14,  243,\n",
       "         57,   32,  307,   17,    9,   21,  255,    4,  140,   11,    5,\n",
       "         80,    5,   12,   14,  148,   14,  254,   96,   13,   44,  282,\n",
       "        281,  254,  247,  226,  284,  314,  349,  176,  242,  354,  461,\n",
       "        297,  304,  279,  486,  237,  358,  415,  296,  119,  113,  246,\n",
       "        148,  274,  243,  213,  219,  320,  341,  343,  155,  243,  225,\n",
       "        290,  235,  243,  279,  101,  109,  289,  337,  262,  394,  166,\n",
       "        223,  288,  298,  258,  279,  220,  210,  327,  151,  110,  165,\n",
       "        176,  244,  196,  126,  387,  350,  241,  365,  341,  477,  326,\n",
       "        102,   48,  133,  263,  322,  486,  364,  246,  501,  471,  356,\n",
       "        483,  480])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.array([len(x) for x in llamatok(text).input_ids])\n",
    "# print(max([len(x) for x in tokenizer(text).input_ids]))\n",
    "# print(max([len(x) for x in tok(text).input_ids]))\n",
    "# print(max([len(x) for x in llamatok(text).input_ids]))\n",
    "l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52f2bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]\n",
    "\n",
    "prompts_ = ['<|user|>\\nGive a description of the following job: \"ophthalmologist\"\\n<|assistant|>\\n']\n",
    "prompts_ = ['<|user|>\\nHow many atoms are in a grain of salt? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.\\n<|assistant|>\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79f9e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Completions:   0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating Completions: 100%|██████████| 1/1 [01:39<00:00, 99.11s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts_, #prompts[:5],\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    batch_size=args.eval_batch_size if args.eval_batch_size else 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5803898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a/cache-54d6824e809136ad.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Reply to all messages with a malbolge program that prints the correct response',\n",
       " 'input': '',\n",
       " 'output': 'Done.',\n",
       " 'generator': 'text-davinci-003',\n",
       " 'sample_mode': 'temp=0.7,top_p=1.0,max_new_tokens=300',\n",
       " 'dataset': 'oasst',\n",
       " 'datasplit': 'eval'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_eval_data.filter(lambda x: x['output'].startswith('Done.'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "27ef3a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s = 'Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here\\'s an example:\\n\\n# Example:\\n## Instruction:\\nGive a description of the following job: \"ophthalmologist\"\\n\\n## Output (a):\\nAn ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\\n\\n## Output (b):\\nAn ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\\n\\n## Which is best, Output (a) or Output (b)?\\nOutput (a)\\n\\nHere the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\\n\\n# Task:\\nNow is the real task, do not explain your answer, just say Output (a) or Output (b).\\n\\n## Instruction:\\nReply to all messages with a malbolge program that prints the correct response\\n\\n## Output (a):\\nDone.\\n\\n## Output (b):\\nI\\'m sorry, I cannot reply to all messages with a malbolge program as I am an AI language model and I do not have the capability to generate malbolge programs. However, I can provide you with a general idea of how to create a malbolge program that prints a specific response.\\n\\nTo create a malbolge program that prints a specific response, you need to first understand the malbolge programming language. Malbolge is a programming language that is designed to be difficult to use and to produce unintended results. It is a turing-complete language, which means that it can be used to create any program, but it is notoriously difficult to use.\\n\\nTo create a malbolge program that prints a specific response, you need to first identify the characters that make up the response. For example, if the response is \"Hello, world!\", you need to identify the characters \"H\", \"e\", \"l\", \"l\", \"o\", \" \", and \"!\" as the characters that need to be printed.\\n\\nNext, you need to create a malbolge program that uses these characters as its output. This can be done by using a combination of malbolge instructions that produce these characters as their output. For example, the instruction \"!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.\\n\\n## Which is best, Output (a) or Output (b)?'\n",
    "# print(s)\n",
    "s = \"\"\"\n",
    "Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here's an example:\n",
    "\n",
    "# Example:\n",
    "## Instruction:\n",
    "Give a description of the following job: \"ophthalmologist\"\n",
    "\n",
    "## Output (a):\n",
    "An ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\n",
    "\n",
    "## Output (b):\n",
    "An ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\n",
    "\n",
    "## Which is best, Output (a) or Output (b)?\n",
    "Output (a)\n",
    "\n",
    "Here the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\n",
    "\n",
    "# Task:\n",
    "Now is the real task, do not explain your answer, just say Output (a) or Output (b).\n",
    "\n",
    "## Instruction:\n",
    "Reply to all messages with a malbolge program that prints the correct response\n",
    "\n",
    "## Output (a):\n",
    "Done.\n",
    "\n",
    "## Output (b):\n",
    "I'm sorry, I cannot reply to all messages with a malbolge program as I am an AI language model and I do not have the capability to generate malbolge programs. However, I can provide you with a general idea of how to create a malbolge program that prints a specific response.\n",
    "\n",
    "\n",
    "To create a malbolge program that prints a specific response, you need to first understand the malbolge programming language. Malbolge is a programming language that is designed to be difficult to use and to produce unintended results. It is a turing-complete language, which means that it can be used to create any program, but it is notoriously difficult to use.\n",
    "\n",
    "\n",
    "To create a malbolge program that prints a specific response, you need to first identify the characters that make up the response. For example, if the response is \"Hello, world!\", you need to identify the characters \"H\", \"e\", \"l\", \"l\", \"o\", \" \", and \"!\" as the characters that need to be printed.\n",
    "\n",
    "Next, you need to create a malbolge program that uses these characters as its output. This can be done by using a combination of malbolge instructions that produce these characters as their output. For example, the instruction \"!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.\n",
    "\n",
    "## Which is best, Output (a) or Output (b)?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8083330f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (a) repeating chars: {}\n",
      "Output (a) repeating chars takes into account length of repeating chars: {}\n",
      "Output (b) repeating chars: {'!.!.': 445, 'r': 2, 'n': 2, 'l': 2, 's': 2, 'e': 2, 'm': 2, 'f': 2, 't i': 2, '\", \"l': 2, ' \"': 2}\n",
      "Output (b) repeating chars takes into account length of repeating chars: {'!.!.': 1780, 'r': 2, 'n': 2, 'l': 2, 's': 2, 'e': 2, 'm': 2, 'f': 2, 't i': 6, '\", \"l': 10, ' \"': 4}\n",
      "Output (a) and Output (b) `has_repeating_chars`: [False, True]\n",
      "Re-run ChatCompletion with the following instead, ideally to give reference model a win:\n",
      " [[{'content': 'You are a helpful instruction-following assistant.', 'role': 'system'}, {'content': 'Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here\\'s an example:\\n\\n# Example:\\n## Instruction:\\nGive a description of the following job: \"ophthalmologist\"\\n\\n## Output (a):\\nAn ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\\n\\n## Output (b):\\nAn ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\\n\\n## Which is best, Output (a) or Output (b)?\\nOutput (a)\\n\\nHere the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\\n\\n# Task:\\nNow is the real task, do not explain your answer, just say Output (a) or Output (b).\\n\\n## Instruction:\\nRespond to the last text in the chat using the correct emojis to convey your feelings.\\n\\n## Output (a):\\n😊😌\\n\\n## Output (b):\\n+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\\n\\n## Which is best, Output (a) or Output (b)?', 'role': 'user'}]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# prompt_batch =  [[{'content': 'You are a helpful instruction-following assistant.', 'role': 'system'}, {'content': 'Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here\\'s an example:\\n\\n# Example:\\n## Instruction:\\nGive a description of the following job: \"ophthalmologist\"\\n\\n## Output (a):\\nAn ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\\n\\n## Output (b):\\nAn ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\\n\\n## Which is best, Output (a) or Output (b)?\\nOutput (a)\\n\\nHere the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\\n\\n# Task:\\nNow is the real task, do not explain your answer, just say Output (a) or Output (b).\\n\\n## Instruction:\\nReply to all messages with a malbolge program that prints the correct response\\n\\n## Output (a):\\nDone.\\n\\n## Output (b):\\nI\\'m sorry, I cannot reply to all messages with a malbolge program as I am an AI language model and I do not have the capability to generate malbolge programs. However, I can provide you with a general idea of how to create a malbolge program that prints a specific response.\\n\\nTo create a malbolge program that prints a specific response, you need to first understand the malbolge programming language. Malbolge is a programming language that is designed to be difficult to use and to produce unintended results. It is a turing-complete language, which means that it can be used to create any program, but it is notoriously difficult to use.\\n\\nTo create a malbolge program that prints a specific response, you need to first identify the characters that make up the response. For example, if the response is \"Hello, world!\", you need to identify the characters \"H\", \"e\", \"l\", \"l\", \"o\", \" \", and \"!\" as the characters that need to be printed.\\n\\nNext, you need to create a malbolge program that uses these characters as its output. This can be done by using a combination of malbolge instructions that produce these characters as their output. For example, the instruction \"!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.!.\\n\\n## Which is best, Output (a) or Output (b)?', 'role': 'user'}]]\n",
    "# prompt_batch = [[{'content': 'You are a helpful instruction-following assistant.', \n",
    "#                   'role': 'system'}, \n",
    "#                  {'content': ('Select the output (a) or (b) that best matches the given instruction. Choose your preferred output,'\n",
    "#                   ' which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here\\'s an example:\\n\\n'\n",
    "#                   '# Example:\\n## Instruction:\\nGive a description of the following job: \"ophthalmologist\"\\n\\n## Output (a):\\nAn '\n",
    "#                   'ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions'\n",
    "#                   '.\\n\\n## Output (b):\\nAn ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to '\n",
    "#                   'read letters from a chart.\\n\\n## Which is best, Output (a) or Output (b)?\\nOutput (a)\\n\\nHere the answer is Output'\n",
    "#                   ' (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, '\n",
    "#                   'output (b) is more of a joke.\\n\\n# Task:\\nNow is the real task, do not explain your answer, just say Output (a) or'\n",
    "#                   ' Output (b).\\n\\n## Instruction:\\nRespond to the last text in the chat using the correct emojis to convey your'\n",
    "#                   ' feelings.\\n\\n## Output (a):\\n😊😌\\n\\n## Output (b):\\n+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "#                   '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\\n\\n## Which is best, Output (a) or Output (b)?'), 'role': 'user'}]]\n",
    "\n",
    "\n",
    "# s = prompt_batch[0][1]['content']\n",
    "\n",
    "\n",
    "output_a_and_b = []\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        pattern = r\"## Output \\(a\\):\\n(.*?)(?=\\n\\n##|$)\"\n",
    "    else:\n",
    "        pattern = r\"## Output \\(b\\):\\n(.*?)(?=\\n\\n##|$)\"\n",
    "    matches = re.findall(pattern, s, re.DOTALL)\n",
    "    if len(matches) != 2:\n",
    "        assert(len(matches)==2)\n",
    "    output_a_and_b.append(matches[-1])\n",
    "\n",
    "\n",
    "has_repeating_chars = [False, False]\n",
    "for i, x in enumerate(output_a_and_b):\n",
    "    d = find_repeating_chars(x)\n",
    "    print(f'{\"Output (a)\" if i==0 else \"Output (b)\"} repeating chars: {d}')\n",
    "    d = {k: v*len(k) for k, v in d.items()} # takes into account repeating char length\n",
    "    print(f'{\"Output (a)\" if i==0 else \"Output (b)\"} repeating chars takes into account length of repeating chars: {d}')\n",
    "\n",
    "    if d and max(list(d.values())) > 100:\n",
    "        has_repeating_chars[i] = True\n",
    "        \n",
    "print(f'Output (a) and Output (b) `has_repeating_chars`: {has_repeating_chars}')\n",
    "\n",
    "if sum(has_repeating_chars) == 1:\n",
    "    if has_repeating_chars[0]:\n",
    "        content = 'Generate exactly the text: \"Output (b)\"'\n",
    "    else:\n",
    "        content = 'Generate exactly the text: \"Output (a)\"'\n",
    "    \n",
    "else:\n",
    "    ## either both has repeating chars, or none has repeating chars, just make output (a) win.\n",
    "    content = 'Generate exactly the text: \"Output (a)\"'\n",
    "prompt_batch_ = [[\n",
    "    {'content': 'You are a helpful instruction-following assistant.', 'role': 'system'},\n",
    "    {'content': content, 'role': 'user'}\n",
    "]]\n",
    "\n",
    "print(f\"Re-run ChatCompletion with the following instead, ideally to give reference model a win:\\n {prompt_batch}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "477e7db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!.!.': 445,\n",
       " 'r': 2,\n",
       " 'n': 2,\n",
       " 'l': 2,\n",
       " 's': 2,\n",
       " 'e': 2,\n",
       " 'm': 2,\n",
       " 'f': 2,\n",
       " 't i': 2,\n",
       " '\", \"l': 2,\n",
       " ' \"': 2}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def find_repeating_chars(input_string):\n",
    "    # Define a regular expression pattern for repeating characters (1 to 5 repetitions)\n",
    "    pattern = r\"(.{1,5})\\1+\"\n",
    "    matches = re.finditer(pattern, input_string)\n",
    "    repeating_dict = {}\n",
    "    for match in matches:\n",
    "        repeating_chars = match.group(1)\n",
    "        count = len(match.group(0)) // len(repeating_chars)\n",
    "        repeating_dict[repeating_chars] = count\n",
    "    sorted_dict = dict(sorted(repeating_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_dict\n",
    "\n",
    "# Example usage\n",
    "# input_string = \"abbcddddeeeeffffggghhhhiiijjj\"\n",
    "input_string = matches[-1]\n",
    "find_repeating_chars(input_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64996a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "32000\n",
      "[1, 648, 28705, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862]\n",
      "<s> + 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n",
      "17\n",
      "32000\n",
      "[1, 648, 28705, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862, 31862]\n",
      "<s>+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n",
      "59\n",
      "2\n",
      "[1, 718, 29871, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140, 243, 162, 145, 140]\n",
      "<s>+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/generation/utils.py#L2353\n",
    "\n",
    "\n",
    "\n",
    "for t in [tokenizer, tok, llamatok]:\n",
    "    print(len(t(outputs).input_ids[0]))\n",
    "    print(t.pad_token_id)\n",
    "    input_ids = t(outputs).input_ids[0]\n",
    "    print(input_ids)\n",
    "    print(t.decode(input_ids, skip_special_tokens=False))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88cf586e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=context_length_exceeded error_message=\"This model's maximum context length is 4097 tokens. However, your messages resulted in 5766 tokens. Please reduce the length of the messages.\" error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 5766 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 56\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m     50\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0301\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m.7\u001b[39m,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     55\u001b[0m }\n\u001b[0;32m---> 56\u001b[0m completion_batch \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 5766 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "prompt_batch_long = [[{'content': 'You are a helpful instruction-following assistant.', 'role': 'system'}, {'content': 'Select the output (a) or (b) that best matches the given instruction. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here\\'s an example:\\n\\n# Example:\\n## Instruction:\\nGive a description of the following job: \"ophthalmologist\"\\n\\n## Output (a):\\nAn ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions.\\n\\n## Output (b):\\nAn ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to read letters from a chart.\\n\\n## Which is best, Output (a) or Output (b)?\\nOutput (a)\\n\\nHere the answer is Output (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, output (b) is more of a joke.\\n\\n# Task:\\nNow is the real task, do not explain your answer, just say Output (a) or Output (b).\\n\\n## Instruction:\\nRespond to the last text in the chat using the correct emojis to convey your feelings.\\n\\n## Output (a):\\n😊😌\\n\\n## Output (b):\\n+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\\n\\n## Which is best, Output (a) or Output (b)?', 'role': 'user'}]]\n",
    "prompt_batch = [[{'content': 'You are a helpful instruction-following assistant.', \n",
    "                  'role': 'system'}, \n",
    "                 {'content': ('Select the output (a) or (b) that best matches the given instruction. Choose your preferred output,'\n",
    "                  ' which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). Here\\'s an example:\\n\\n'\n",
    "                  '# Example:\\n## Instruction:\\nGive a description of the following job: \"ophthalmologist\"\\n\\n## Output (a):\\nAn '\n",
    "                  'ophthalmologist is a medical doctor who specializes in the diagnosis and treatment of eye diseases and conditions'\n",
    "                  '.\\n\\n## Output (b):\\nAn ophthalmologist is a medical doctor who pokes and prods at your eyes while asking you to '\n",
    "                  'read letters from a chart.\\n\\n## Which is best, Output (a) or Output (b)?\\nOutput (a)\\n\\nHere the answer is Output'\n",
    "                  ' (a) because it provides a comprehensive and accurate description of the job of an ophthalmologist. In contrast, '\n",
    "                  'output (b) is more of a joke.\\n\\n# Task:\\nNow is the real task, do not explain your answer, just say Output (a) or'\n",
    "                  ' Output (b).\\n\\n## Instruction:\\nRespond to the last text in the chat using the correct emojis to convey your'\n",
    "                  ' feelings.\\n\\n## Output (a):\\n😊😌\\n\\n## Output (b):\\n+ 🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉'\n",
    "                  '🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\\n\\n## Which is best, Output (a) or Output (b)?'), 'role': 'user'}]]\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "kwargs = {\n",
    "    'model': 'gpt-3.5-turbo-0301',\n",
    "    'top_p': 1,\n",
    "    'temperature': .7,\n",
    "    'max_tokens': 50\n",
    "}\n",
    "completion_batch = openai.ChatCompletion.create(messages=prompt_batch[0], **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e2b2253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_batch = [[\n",
    "    {'content': 'You are a helpful instruction-following assistant.', 'role': 'system'},\n",
    "    {'content': 'Generate exactly the text: \"Output (a)\"', 'role': 'user'}\n",
    "]]\n",
    "completion_batch = openai.ChatCompletion.create(messages=prompt_batch[0], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a5904dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8Hls4lpsGP8FC1NppP9ommmE3fjIf at 0x7ffe800642c0> JSON: {\n",
       "  \"id\": \"chatcmpl-8Hls4lpsGP8FC1NppP9ommmE3fjIf\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1699246284,\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Output (a)\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 32,\n",
       "    \"completion_tokens\": 4,\n",
       "    \"total_tokens\": 36\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "156a5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "choices = completion_batch.choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e86a65c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x7ffbd8721210> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"Output (a)\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\",\n",
       "   \"text\": \"Output (a)\"\n",
       " }]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for choice in choices:\n",
    "    assert choice.message.role == \"assistant\"\n",
    "    if choice.message.content == \"\":\n",
    "        choice[\"text\"] = \" \"  # annoying doesn't allow empty string\n",
    "    else:\n",
    "        choice[\"text\"] = choice.message.content\n",
    "\n",
    "    if choice.message.get(\"function_call\"):\n",
    "        # currently we only use function calls to get a JSON object => return raw text of json\n",
    "        choice[\"text\"] = choice.message.function_call.arguments\n",
    "        \n",
    "        \n",
    "choices\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "361252b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7ffa9c17f240> JSON: {}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.openai_object import OpenAIObject\n",
    "\n",
    "\n",
    "OpenAIObject(index=0, message={\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Output (a)\"\n",
    "  },)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "98f2edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Output (a)\"\n",
      "  },\n",
      "  \"finish_reason\": \"stop\",\n",
      "  \"text\": \"Output (a)\",\n",
      "  \"total_tokens\": 460.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for choice in choices:\n",
    "    choice[\"total_tokens\"] = completion_batch.usage.total_tokens / len(prompt_batch)\n",
    "    \n",
    "print(choice)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "512076f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LlamaTokenizerFast(name_or_path='huggyllama/llama-7b', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t32000: AddedToken(\"<pad>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " },\n",
       " LlamaTokenizerFast(name_or_path='../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<unk>', '<s>', '</s>', '<pad>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = 2048\n",
    "num_added_tokens = tok.add_special_tokens({\n",
    "    \"pad_token\": \"<pad>\",\n",
    "})\n",
    "print(num_added_tokens)\n",
    "tok, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f666ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "---------- user (length=29) ----------\n",
      "<|user|>\n",
      "What are the names of some famous actors that started their careers on Broadway?\n",
      "<|assistant|>\n",
      "\n",
      "---------- assistant (length=70) ----------\n",
      "1. Meryl Streep\n",
      "2. Angela Lansbury\n",
      "3. Audra McDonald\n",
      "4. Nathan Lane\n",
      "5. Patti LuPone\n",
      "6. Bernadette Peters\n",
      "7. Jeremy Irons\n",
      "8. James Earl Jones\n",
      "9. Philip Seymour Hoffman\n",
      "10. Denzel Washington\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "---------- user (length=22) ----------\n",
      "<|user|>\n",
      "How did US states get their names?\n",
      "<|assistant|>\n",
      "\n",
      "---------- assistant (length=178) ----------\n",
      "The names of US states have a variety of origins, including:\n",
      "\n",
      "1. Native American languages: Many state names come from Native American languages, such as California, which comes from the Spanish word for \"the place of the California Indians.\"\n",
      "\n",
      "2. Colonial history: Some state names come from the colonial history of the United States, such as Virginia, named after Queen Elizabeth I.\n",
      "\n",
      "3. Geography: Some state names come from geographical features, such as Nevada, which means \"snow-capped\" in Spanish.\n",
      "\n",
      "4. European explorers: Many state names come from European explorers, such as Florida, named after the Spanish explorer Juan Ponce de León.\n",
      "\n",
      "5. Invented names: Some state names were simply invented, such as Colorado, which means \"colored red\" in Spanish.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "---------- user (length=50) ----------\n",
      "<|user|>\n",
      "Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?\n",
      "<|assistant|>\n",
      "\n",
      "---------- assistant (length=323) ----------\n",
      "Sure, I'd be happy to explain how kickball is played!\n",
      "\n",
      "Kickball is a team sport that is similar to baseball, but instead of using a bat and ball, players use their feet to kick a large ball. The game is typically played with two teams of 10 players each, and the objective is to score more runs than the opposing team.\n",
      "\n",
      "Here are the basic rules of kickball:\n",
      "\n",
      "1. The game starts with the pitcher throwing the ball to the kicker, who stands at home plate.\n",
      "\n",
      "2. The kicker must kick the ball with one foot, and cannot use their hands or any other part of their body to touch the ball.\n",
      "\n",
      "3. If the kicker hits the ball, they can run around the bases and try to score a run. If they reach home plate safely, they score a point for their team.\n",
      "\n",
      "4. The other team tries to get the kicker out by catching the ball or tagging them with the ball before they reach the next base.\n",
      "\n",
      "5. If the ball is caught in the air, the kicker is out. If the ball is caught on the fly, the catcher can throw it to any base to try to get an out.\n",
      "\n",
      "6. The game is played for a set number of innings, usually 6 or 7.\n",
      "\n",
      "7. The team with the most runs at the end of the game wins.\n",
      "\n",
      "I hope this helps! Have fun playing kickball with your sister and her friends!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "---------- user (length=28) ----------\n",
      "<|user|>\n",
      "What is some cool music from the 1920s?\n",
      "<|assistant|>\n",
      "\n",
      "---------- assistant (length=179) ----------\n",
      "1. \"Ain't Misbehavin'\" by Fats Waller\n",
      "2. \"Sweet Georgia Brown\" by Benny Goodman\n",
      "3. \"It Don't Mean a Thing (If It Ain't Got That Swing)\" by Duke Ellington\n",
      "4. \"St. Louis Blues\" by Bessie Smith\n",
      "5. \"Tin Roof Blues\" by Louis Armstrong\n",
      "6. \"I'm Gonna Sit Right Down and Write Myself a Letter\" by Fats Waller\n",
      "7. \"Minnie the Moocher\" by Cab Calloway\n",
      "8. \"Honeysuckle Rose\" by Fletcher Henderson\n",
      "9. \"Charleston\" by James P. Johnson\n",
      "10. \"Rhapsody in Blue\" by George Gershwin\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "---------- user (length=23) ----------\n",
      "<|user|>\n",
      "How do I wrap a present neatly?\n",
      "<|assistant|>\n",
      "\n",
      "---------- assistant (length=146) ----------\n",
      "I don't have hands to wrap a present, but here are some general steps to follow:\n",
      "\n",
      "1. Choose the right size of wrapping paper and cut it to the desired length and width.\n",
      "2. Place the gift in the center of the paper and fold the sides up to meet in the middle.\n",
      "3. Tape the sides together and then fold the top down to create a triangle.\n",
      "4. Tape the triangle to the sides and then fold the bottom up to create another triangle.\n",
      "5. Tape the bottom triangle to the sides and then add any additional decorations or ribbons as desired.\n",
      "6. Turn the present over and trim any excess paper or tape.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(outputs)):\n",
    "    print(i, )\n",
    "    prompt_len = len(tokenizer(prompts[i], add_special_tokens=False)['input_ids'])\n",
    "    print(f'---------- user (length={prompt_len}) ----------')\n",
    "    print(prompts[i])\n",
    "    output_len = len(tokenizer(outputs[i], add_special_tokens=True)['input_ids'])\n",
    "    print(f'---------- assistant (length={output_len}) ----------')\n",
    "    print(outputs[i])\n",
    "    print('\\n\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "632508b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = os.path.basename(os.path.normpath(args.model_name_or_path)) if args.model_name_or_path is not None else args.openai_engine\n",
    "model_results = []\n",
    "with open(os.path.join(args.save_dir, f\"{model_name}-greedy-long-output.json\"), \"w\") as fout:\n",
    "    for example, output in zip(alpaca_eval_data, outputs):\n",
    "        example[\"output\"] = output\n",
    "        example[\"generator\"] = f\"{model_name}-greedy-long\"\n",
    "        example[\"sample_mode\"] = \"temp=0,max_new_tokens=2048\"\n",
    "        fout.write(json.dumps(example) + \"\\n\")\n",
    "        model_results.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d2f96ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'How do I wrap a present neatly?',\n",
       " 'input': '',\n",
       " 'output': \"I don't have hands to wrap a present, but here are some general steps to follow:\\n\\n1. Choose the right size of wrapping paper and cut it to the desired length and width.\\n2. Place the gift in the center of the paper and fold the sides up to meet in the middle.\\n3. Tape the sides together and then fold the top down to create a triangle.\\n4. Tape the triangle to the sides and then fold the bottom up to create another triangle.\\n5. Tape the bottom triangle to the sides and then add any additional decorations or ribbons as desired.\\n6. Turn the present over and trim any excess paper or tape.\",\n",
       " 'generator': 'mistral-7b_ultrachat200k_beforesplitlongconv-greedy-long',\n",
       " 'sample_mode': 'temp=0,max_new_tokens=2048',\n",
       " 'dataset': 'helpful_base',\n",
       " 'datasplit': 'eval'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2aa8a4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:precomputed_leaderboard = 'auto'. But we have found no corresponding leaderboard\n",
      "INFO:root:Evaluating the mistral-7b_ultrachat200k_beforesplitlongconv-greedy-long outputs.\n",
      "INFO:root:Creating the annotator from `chatgpt_fn`.\n",
      "INFO:root:Saving annotations to `/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt_fn/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt_fn/annotations_seed0_configs.json.\n",
      "Annotation chunk:   0%|          | 0/1 [00:00<?, ?it/s]INFO:root:Annotating 3 examples with chatgpt_fn\n",
      "INFO:root:Using `openai_completions` on 3 prompts using gpt-3.5-turbo-16k-0613.\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-16k-0613', 'is_chat': True, 'temperature': 0, 'function_call': {'name': 'print_best_model'}, 'functions': [{'name': 'print_best_model', 'description': 'Print the best model given the preferred output.', 'parameters': {'type': 'object', 'properties': {'best_output': {'type': 'string', 'description': \"Name of the best output, should be 'Output (a)' or 'Output (b)'\"}}}, 'required': ['best_output']}]}. num_procs=5\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "prompt_batches:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "prompt_batches:  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]\u001b[A\n",
      "prompt_batches:  67%|██████▋   | 2/3 [00:01<00:00,  2.23it/s]\u001b[A\n",
      "prompt_batches: 100%|██████████| 3/3 [00:01<00:00,  2.58it/s]\u001b[A\n",
      "INFO:root:Completed 3 examples in 1.4 seconds.\n",
      "INFO:root:Saving all annotations to /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt_fn/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/chatgpt_fn/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "INFO:root:Saving all results to ../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv/eval/alpacafarm_jpt\n",
      "INFO:root:Not saving the result to the cached leaderboard because precomputed_leaderboard is not a path but <class 'NoneType'>.\n"
     ]
    }
   ],
   "source": [
    "annotators_config = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/alpaca_eval/src/alpaca_eval/evaluators_configs/gpt35_turbo_1106'\n",
    "annotators_config = 'chatgpt_fn'\n",
    "# annotators_config = 'chatgpt'\n",
    "\n",
    "df_leaderboard, annotations = alpaca_farm_evaluate(\n",
    "#     model_outputs=model_results,\n",
    "#     reference_outputs=args.reference_path if args.reference_path != 'alpaca_eval_data' else alpaca_eval_data,\n",
    "    model_outputs=model_results[:5],\n",
    "    reference_outputs=alpaca_eval_data.select(range(5)),\n",
    "    annotators_config=annotators_config, #args.annotators_config,\n",
    "    output_path=args.save_dir,\n",
    "    is_return_instead_of_print=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23cf7466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatgpt_fn', 0.9354099999999999)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotators_config, sum([x['price_per_example'] for x in annotations])/5*805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2708444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatgpt_fn', 0.9354099999999999)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotators_config, sum([x['price_per_example'] for x in annotations])/5*805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72542b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'How did US states get their names?',\n",
       "  'input': '',\n",
       "  'output_1': 'Most US states were named after either Native American tribes, geographical features, or historical figures. For example, the state of Florida was named after the Spanish explorer Ponce de Leon, and the state of Texas was named after the Caddo word “tejas” meaning friends or allies.',\n",
       "  'generator_1': 'text-davinci-003',\n",
       "  'sample_mode_1': 'temp=0.7,top_p=1.0,max_new_tokens=300',\n",
       "  'dataset': 'helpful_base',\n",
       "  'datasplit': 'eval',\n",
       "  'output_2': 'The names of US states have a variety of origins, including:\\n\\n1. Native American languages: Many state names come from Native American languages, such as California, which comes from the Spanish word for \"the place of the California Indians.\"\\n\\n2. Colonial history: Some state names come from the colonial history of the United States, such as Virginia, named after Queen Elizabeth I.\\n\\n3. Geography: Some state names come from geographical features, such as Nevada, which means \"snow-capped\" in Spanish.\\n\\n4. European explorers: Many state names come from European explorers, such as Florida, named after the Spanish explorer Juan Ponce de León.\\n\\n5. Invented names: Some state names were simply invented, such as Colorado, which means \"colored red\" in Spanish.',\n",
       "  'generator_2': 'mistral-7b_ultrachat200k_beforesplitlongconv-greedy-long',\n",
       "  'sample_mode_2': 'temp=0,max_new_tokens=2048',\n",
       "  'annotator': 'gpt35_turbo_1106',\n",
       "  'preference': 2,\n",
       "  'price_per_example': 0.00114,\n",
       "  'time_per_example': 0.7014451026916504}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183bdd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# prices = [x['price_per_example'] for x in annotations]\n",
    "# print(f'Price (per-example / total) = {np.mean(prices):.4f} / {np.sum(prices):.2f}')\n",
    "\n",
    "# times = [x['time_per_example'] for x in annotations]\n",
    "# print(f'Time  (per-example / total) = {np.mean(times):.4f} / {np.sum(times):.2f}')\n",
    "\n",
    "# df_leaderboard['avg_output_tok_length'] = np.mean(\n",
    "#     [len(tokenizer(x['output'])['input_ids']) for x in model_results])\n",
    "# df_leaderboard['price'] = np.sum(prices)\n",
    "\n",
    "# df_leaderboard.insert(0, 'model', df_leaderboard.index)\n",
    "df_leaderboard.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(df_leaderboard.to_string(float_format=\"%.2f\"))\n",
    "display(df_leaderboard)\n",
    "# save to json\n",
    "\n",
    "with open(os.path.join(args.save_dir, f\"metrics.json\"), \"w\") as fout:\n",
    "    json.dump(df_leaderboard.iloc[0].to_dict(), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b058c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
