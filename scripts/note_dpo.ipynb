{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc757f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Wed Jan 10 00:21:55 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   36C    P0   125W / 300W |  16773MiB / 32510MiB |     59%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   38C    P0   141W / 300W |  16407MiB / 32510MiB |     58%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   37C    P0   130W / 300W |  16787MiB / 32510MiB |     57%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   39C    P0   146W / 300W |  16491MiB / 32510MiB |     58%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2112360      C   python                          16771MiB |\n",
      "|    3   N/A  N/A   2109331      C   python                          16405MiB |\n",
      "|    4   N/A  N/A   2108336      C   python                          16785MiB |\n",
      "|    5   N/A  N/A   2113271      C   python                          16489MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook, jpt_parse_args, jpt_setup; jpt_setup()\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084de686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-10 00:21:58,841] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pyarrow # add before datasets/torch\n",
    "import datasets\n",
    "import torch\n",
    "from functools import partial\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import deepspeed\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    CodeLlamaTokenizerFast,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    GPT2Tokenizer,\n",
    "    OPTForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from open_instruct.dpo_utils import dpo_loss, concatenated_forward, DataCollatorForSeq2SeqDPO\n",
    "\n",
    "import sys\n",
    "sys.path.append('../open_instruct/')\n",
    "from dpo_tune import (encode_with_messages_format, \n",
    "                      save_with_accelerate, \n",
    "                      prepare_deepspeed,\n",
    "                      parse_args,\n",
    "                     )\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34afc29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model_name_or_path results/baselines/EleutherAI/pythia-70m \\\n",
      "--tokenizer_name results/baselines/EleutherAI/pythia-70m \\\n",
      "--gradient_checkpointing \\\n",
      "--train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl \\\n",
      "--max_seq_length 2048 \\\n",
      "--preprocessing_num_workers 32 \\\n",
      "--per_device_train_batch_size 1 \\\n",
      "--gradient_accumulation_steps 32 \\\n",
      "--learning_rate 5e-7 \\\n",
      "--lr_scheduler_type linear \\\n",
      "--warmup_ratio 0.1 \\\n",
      "--weight_decay 0. \\\n",
      "--num_train_epochs 3 \\\n",
      "--with_tracking \\\n",
      "--checkpointing_steps epoch \\\n",
      "--low_cpu_mem_usage \\\n",
      "--logging_steps 1 \\\n",
      "--output_dir results/jpt_pythia-70m_ultrafeedback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(dataset_name=None, dataset_config_name=None, train_file='data/processed/ultrafeedback/ultrafeedback_data.jsonl', model_name_or_path='results/baselines/EleutherAI/pythia-70m', config_name=None, use_lora=False, lora_rank=64, lora_alpha=16, lora_dropout=0.1, use_flash_attn=False, tokenizer_name='results/baselines/EleutherAI/pythia-70m', use_slow_tokenizer=False, max_seq_length=2048, per_device_train_batch_size=1, learning_rate=5e-07, weight_decay=0.0, num_train_epochs=3, max_train_steps=None, gradient_accumulation_steps=32, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.1, output_dir='results/jpt_pythia-70m_ultrafeedback', seed=None, preprocessing_num_workers=32, overwrite_cache=False, checkpointing_steps='epoch', logging_steps=1, resume_from_checkpoint=None, with_tracking=True, report_to='all', low_cpu_mem_usage=True, gradient_checkpointing=True, use_qlora=False, clip_grad_norm=-1, use_8bit_optimizer=False, beta=0.1, use_paged_optimizer=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_from_checkpoint = None\n",
    "use_fast_tokenizer = True\n",
    "gradient_checkpointing = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; max_seq_length = 2048; abbr_model_name = 'llama-7b'\n",
    "model_name_or_path = hf_models_dir+'EleutherAI/pythia-70m'; max_seq_length = 2048; abbr_model_name = 'pythia-70m'\n",
    "# model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama7b+sharegptv2ep2'\n",
    "\n",
    "\n",
    "train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; abbr_train_file = 'ultrafeedback'\n",
    "\n",
    "\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    --preprocessing_num_workers 32 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --with_tracking \\\n",
    "    --checkpointing_steps epoch \\\n",
    "    {'--resume_from_checkpoint '+str(resume_from_checkpoint) if resume_from_checkpoint else ''} \\\n",
    "    --low_cpu_mem_usage\n",
    "    --logging_steps 1 \\\n",
    "    --output_dir {output_dir}\n",
    "\n",
    "\"\"\"\n",
    "print(' \\\\\\n'.join([x.strip() for x in re.split(r'\\s{3,}', cmd.strip())]))\n",
    "\n",
    "args = parse_args(cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c5ad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 00:22:13 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
    "# in the environment\n",
    "accelerator_log_kwargs = {}\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
    "    accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0127ce2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 00:22:14 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-9a630b550f133bce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d7d617b19f490a926ee5c16edbe130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        args.dataset_name,\n",
    "        args.dataset_config_name,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train_prefs\"] = args.train_file\n",
    "    raw_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        **dataset_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b6a9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/baselines/EleutherAI/pythia-70m/config.json\n",
      "Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"results/baselines/EleutherAI/pythia-70m\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported by this script.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a81fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file results/baselines/EleutherAI/pythia-70m/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-70m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading weights file results/baselines/EleutherAI/pythia-70m/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-70m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    if args.model_name_or_path:\n",
    "        if args.use_qlora:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "            device_index = accelerator.local_process_index\n",
    "            device_map = {\"\": device_index} # force data-parallel training.\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                config=config,\n",
    "                load_in_4bit=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                use_flash_attention_2=True if args.use_flash_attn else False,\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                config=config,\n",
    "                low_cpu_mem_usage=args.low_cpu_mem_usage,\n",
    "                use_flash_attention_2=True if args.use_flash_attn else False,\n",
    "            )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c86ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file results/baselines/EleutherAI/pythia-70m/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-70m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading weights file results/baselines/EleutherAI/pythia-70m/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-70m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model()\n",
    "if not args.use_lora:\n",
    "    reference_model = load_model()\n",
    "else:\n",
    "    reference_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66776f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map \n",
    "if isinstance(tokenizer, (LlamaTokenizer, LlamaTokenizerFast, CodeLlamaTokenizerFast)):\n",
    "    from transformers import AddedToken\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": AddedToken(\"<s>\", normalized=False, special=True),\n",
    "        \"eos_token\": AddedToken(\"</s>\", normalized=False, special=True),\n",
    "        \"unk_token\": AddedToken(\"<unk>\", normalized=False, special=True),\n",
    "        \"pad_token\": AddedToken(\"<pad>\", normalized=False, special=True),\n",
    "    })\n",
    "    ## wpq: for `huggyllama`/`NousResearch/Llama-2-7b-hf`, `LlamaTokenizerFast` tokenizer config not properly implemented and cannot tokenize special tokens like eos_token corretly. \n",
    "    # Need the following workaround. More details: https://github.com/huggingface/transformers/issues/23833\n",
    "    def check_tokenizer_pad_properly(tokenizer):\n",
    "        tokenizer_handles_pad_ok = True\n",
    "        for s, s_tokenized in [\n",
    "            (\"Hi<s>Hey</s>sir<unk>what<pad><pad>\", \n",
    "            ['▁Hi', '<s>', '▁Hey', '</s>', '▁sir', '<unk>', '▁what', '<pad>', '<pad>']),\n",
    "        ]:\n",
    "            if tokenizer.tokenize(s, add_special_tokens=False)!=s_tokenized:\n",
    "                tokenizer_handles_pad_ok = False\n",
    "        return tokenizer_handles_pad_ok\n",
    "    if not check_tokenizer_pad_properly(tokenizer):\n",
    "        if os.path.isdir(args.model_name_or_path):\n",
    "            tmp_tok_path = os.path.join(\n",
    "                os.path.dirname(args.model_name_or_path),\n",
    "                os.path.basename(args.model_name_or_path)+'_fixtok')\n",
    "            if not os.path.isdir(tmp_tok_path):\n",
    "                tokenizer.save_pretrained(tmp_tok_path)\n",
    "        else:\n",
    "            from secrets import token_hex\n",
    "            tmp_tok_path = f'/tmp/wpq_tok_{token_hex(16)}'\n",
    "            tokenizer.save_pretrained(tmp_tok_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tmp_tok_path, use_fast=not args.use_slow_tokenizer)\n",
    "    assert(check_tokenizer_pad_properly(tokenizer))\n",
    "elif isinstance(tokenizer, GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, GPT2Tokenizer) and isinstance(model, OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({'unk_token': '<unk>'})\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if not args.use_lora:\n",
    "        reference_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a09509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.use_lora:\n",
    "    if args.use_qlora:\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
    "\n",
    "    logger.info(\"Initializing LORA model...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        r=args.lora_rank, \n",
    "        lora_alpha=args.lora_alpha, \n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"v_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "\n",
    "# wpq: `use_cache=True` is incompatible with gradient checkpointing\n",
    "model.config.use_cache = True if not args.gradient_checkpointing else False\n",
    "if args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8f1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing the datasets.\n",
    "if \"prompt\" in raw_datasets[\"train_prefs\"].column_names and \"completion\" in raw_datasets[\"train_prefs\"].column_names:\n",
    "    raise ValueError(\"Sorry, prompt-completion format is not supported for DPO training.\")\n",
    "elif \"chosen\" in raw_datasets[\"train_prefs\"].column_names and \"rejected\" in raw_datasets[\"train_prefs\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"You need to have 'chosen' and 'rejected in your column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ca3144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 00:23:00 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-9a630b550f133bce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-58145d8286ae1dc4_*_of_00032.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = raw_datasets[\"train_prefs\"].map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=[name for name in raw_datasets[\"train_prefs\"].column_names if name not in [\"chosen_input_ids\", \"chosen_labels\", \"chosen_attention_mask\", \"rejected_input_ids\", \"rejected_labels\", \"rejected_attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "    lm_datasets.set_format(type=\"pt\")\n",
    "    # wpq: if enforce max_seq_length properly, below should do nothing, so just remove.\n",
    "    # # our thresholding mighta meant some examples have no labels, remove.\n",
    "    # lm_datasets = lm_datasets.filter(lambda example: (example['chosen_labels'] != -100).any(), num_proc=8)\n",
    "    # lm_datasets = lm_datasets.filter(lambda example: (example['rejected_labels'] != -100).any(), num_proc=8)\n",
    "\n",
    "train_dataset = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fc459e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen:\n",
      "<s> <|user|>\n",
      "People watch a soccer player in uniform make a kick on sand.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "yes\n",
      "\n",
      "\n",
      "I doubt the duck is talking to a hamster while a boy watches.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "Question followed by answer: Test for natural language inference.\n",
      "Premise: \"Guy looking out at beautiful scenery.\"\n",
      "Hypothesis: \"The guy is looking at the ground.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "No all cloth and papers are used to communicate with an alien race.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Just because Festival-goers have a fun time it does not mean it was a concert.\n",
      "The question and answer are below.\n",
      "Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Kids cannot be in front of and walking away from the same location.\n",
      "The question and answer are below.\n",
      "<|assistant|>\n",
      "Given the sentence \"Children playing hide-and-seek at park but some are in front of and others are walking away from the location.\" is it true that \"Some kids are neither hiding nor seeking.\"?\n",
      "\n",
      "No\n",
      "Confidence: 90%</s>\n",
      "--------------------------------------------------------------------------------\n",
      "rejected:\n",
      "<s> <|user|>\n",
      "People watch a soccer player in uniform make a kick on sand.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "yes\n",
      "\n",
      "\n",
      "I doubt the duck is talking to a hamster while a boy watches.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "Question followed by answer: Test for natural language inference.\n",
      "Premise: \"Guy looking out at beautiful scenery.\"\n",
      "Hypothesis: \"The guy is looking at the ground.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "No all cloth and papers are used to communicate with an alien race.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Just because Festival-goers have a fun time it does not mean it was a concert.\n",
      "The question and answer are below.\n",
      "Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Kids cannot be in front of and walking away from the same location.\n",
      "The question and answer are below.\n",
      "<|assistant|>\n",
      "Here are the answers to your questions:\n",
      "\n",
      "1. Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "\n",
      "Yes. The sentence states that the boy is kicking a soccer ball, which implies that there is an activity happening. The fact that the sentence also states that the boy is wearing black shorts and a shirt suggests that this is a formal event, such as a soccer game. Therefore, it is reasonable to conclude that there are other people watching the boy play soccer.\n",
      "\n",
      "2. Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "\n",
      "No. The sentence simply states that a boy is watching a duck. There is no information about what the duck is doing, so we cannot conclude that it is talking to a hamster.\n",
      "\n",
      "3. If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "\n",
      "The question could be: \"What is the guy doing?\" or \"What is the guy looking at?\" The answer to the first question could be that the guy is looking out the window, while the answer to the second question could be that the guy is looking at the beautiful scenery.\n",
      "\n",
      "4. Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "\n",
      "It is not possible to tell for sure. The sentence does not provide enough information to make a definitive conclusion. The child could be trying to communicate with an alien race, but it is also possible that they are doing something else, such as waving to someone or playing a game.\n",
      "\n",
      "5. Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "\n",
      "It is not possible to tell for sure. The premise states that festival-goers are having a fun time with their children. However, it does not state that the festival is a concert. Therefore, it is possible that the festival-goers are having a fun time at some other event, such as a fair or a picnic.\n",
      "\n",
      "6. Kids cannot be in front of and walking away from the same location.\n",
      "\n",
      "This is true. If a kid is in front of a location, then they are facing the location. If they are walking away from the location, then they are turned away from the location. Therefore, it is not possible for a kid to be in front of and walking away from the same location at the same time.</s>\n"
     ]
    }
   ],
   "source": [
    "example = train_dataset[2]\n",
    "\n",
    "print('chosen:')\n",
    "print(tokenizer.decode(example['chosen_input_ids']))\n",
    "print('----'*20)\n",
    "print('rejected:')\n",
    "print(tokenizer.decode(example['rejected_input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df8907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    collate_fn=DataCollatorForSeq2SeqDPO(tokenizer=tokenizer, model=model, padding=\"longest\"),\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b149a9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen_input_ids\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            181\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_labels\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            181\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_attention_mask\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            181\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_input_ids\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            306\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_labels\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            306\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_attention_mask\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            306\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "import json\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in batch.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4430931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "if args.use_qlora or args.use_paged_optimizer:\n",
    "    from bitsandbytes.optim import AdamW\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        optim_bits=8 if args.use_8bit_optimizer else 32,\n",
    "        is_paged=True\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "298a2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataloader):  60506\n",
      "args.gradient_accumulation_steps:  32\n",
      "args.max_train_steps:  5673\n",
      "overrode_max_train_steps:  True\n",
      "num_training_steps_for_scheduler:  5673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7ffeda8411b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# Note: the current accelerator.step() calls the .step() of the real scheduler for the `num_processes` times. This is because they assume \n",
    "# the user initialize the scheduler with the entire training set. In the case of data parallel training, each process only\n",
    "# sees a subset (1/num_processes) of the training set. So each time the process needs to update the lr multiple times so that the total \n",
    "# number of updates in the end matches the num_training_steps here.\n",
    "# Here we need to set the num_training_steps to either using the entire training set (when epochs is specified) or we need to multiply the \n",
    "# num_training_steps by num_processes so that the total number of updates matches the num_training_steps.\n",
    "num_training_steps_for_scheduler = args.max_train_steps if overrode_max_train_steps else args.max_train_steps * accelerator.num_processes\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps_for_scheduler,\n",
    "    num_warmup_steps=int(num_training_steps_for_scheduler * args.warmup_ratio),\n",
    ")\n",
    "\n",
    "print('len(train_dataloader): ', len(train_dataloader))\n",
    "print('args.gradient_accumulation_steps: ', args.gradient_accumulation_steps)\n",
    "print('args.max_train_steps: ', args.max_train_steps)\n",
    "print('overrode_max_train_steps: ', overrode_max_train_steps)\n",
    "print('num_training_steps_for_scheduler: ', num_training_steps_for_scheduler)\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8664b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare everything with `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "if not jpt_in_notebook():\n",
    "    if not args.use_lora:\n",
    "        reference_model = prepare_deepspeed(accelerator, reference_model)\n",
    "else:\n",
    "    reference_model = reference_model.to('cuda')\n",
    "    reference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4121859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 00:23:56 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"mitibm\", experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5f065fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 00:24:11 - INFO - __main__ - ***** Running training *****\n",
      "01/10/2024 00:24:11 - INFO - __main__ -   Num examples = 60506\n",
      "01/10/2024 00:24:11 - INFO - __main__ -   Num Epochs = 3\n",
      "01/10/2024 00:24:11 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "01/10/2024 00:24:11 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "01/10/2024 00:24:11 - INFO - __main__ -   Gradient Accumulation steps = 32\n",
      "01/10/2024 00:24:11 - INFO - __main__ -   Total optimization steps = 5673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb86710c1c1413c88bbcfb584d4626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e3ad603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "        checkpoint_path = args.resume_from_checkpoint\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "        dirs.sort(key=os.path.getctime)\n",
    "        path = dirs[\n",
    "            -1\n",
    "        ]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "        checkpoint_path = path\n",
    "        path = os.path.basename(checkpoint_path)\n",
    "\n",
    "    accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "    accelerator.load_state(path)\n",
    "    # Extract `epoch_{i}` or `step_{i}`\n",
    "    training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "    if \"epoch\" in training_difference:\n",
    "        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "        resume_step = None\n",
    "        completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "    else:\n",
    "        # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "        resume_step = (\n",
    "            int(training_difference.replace(\"step_\", \"\"))\n",
    "            * args.gradient_accumulation_steps\n",
    "        )\n",
    "        starting_epoch = resume_step // len(train_dataloader)\n",
    "        completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "        resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "progress_bar.update(completed_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a99d4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 00:26:38 - INFO - __main__ -   Step: 1, LR: 8.818342151675484e-10, Loss: 0.6931474804878235\n",
      "01/10/2024 00:26:41 - INFO - __main__ -   Step: 2, LR: 1.7636684303350969e-09, Loss: 0.6931474804878235\n",
      "01/10/2024 00:26:45 - INFO - __main__ -   Step: 3, LR: 2.645502645502645e-09, Loss: 0.6934130787849426\n",
      "01/10/2024 00:26:47 - INFO - __main__ -   Step: 4, LR: 3.5273368606701937e-09, Loss: 0.6932925581932068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We keep track of the loss at each logged step\u001b[39;00m\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 31\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# clip gradient norm. don't do this with deepspeed\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mclip_grad_norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    if (\n",
    "        args.resume_from_checkpoint\n",
    "        and epoch == starting_epoch\n",
    "        and resume_step is not None\n",
    "    ):\n",
    "        # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "        active_dataloader = accelerator.skip_first_batches(\n",
    "            train_dataloader, resume_step\n",
    "        )\n",
    "    else:\n",
    "        active_dataloader = train_dataloader\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        # dpo forward pass & loss\n",
    "        with accelerator.accumulate(model):\n",
    "            policy_chosen_logps, policy_rejected_logps = concatenated_forward(model, batch)\n",
    "            with torch.no_grad():\n",
    "                if args.use_lora:\n",
    "                    with accelerator.unwrap_model(model).disable_adapter():\n",
    "                        reference_chosen_logps, reference_rejected_logps = concatenated_forward(model, batch)\n",
    "                else:\n",
    "                    reference_chosen_logps, reference_rejected_logps = concatenated_forward(reference_model, batch)\n",
    "            losses, _, _ = dpo_loss(\n",
    "                policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, beta=args.beta)\n",
    "            # TODO: metric logging          \n",
    "            loss = losses.mean()\n",
    "            # We keep track of the loss at each logged step\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            # clip gradient norm. don't do this with deepspeed\n",
    "            if accelerator.sync_gradients and args.clip_grad_norm > 0:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()       \n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            if args.logging_steps and completed_steps % args.logging_steps == 0:\n",
    "                avg_loss = accelerator.gather(total_loss).mean().item() / args.gradient_accumulation_steps / args.logging_steps\n",
    "                logger.info(f\"  Step: {completed_steps}, LR: {lr_scheduler.get_last_lr()[0]}, Loss: {avg_loss}\")\n",
    "                if args.with_tracking:\n",
    "                    accelerator.log(\n",
    "                        {\n",
    "                            \"learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "                            \"train_loss\": avg_loss,\n",
    "                            \"step\": completed_steps,\n",
    "                        },\n",
    "                        step=completed_steps,\n",
    "                    )\n",
    "                total_loss = 0\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                if completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    if args.output_dir is not None:\n",
    "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    save_with_accelerate(accelerator, model, tokenizer, output_dir, args)\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        save_with_accelerate(accelerator, model, tokenizer, output_dir, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0004a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_chosen_logps, policy_rejected_logps = concatenated_forward(model, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80672b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1108.4854], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([-1251.1152], device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_chosen_logps, policy_rejected_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab886125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = batch['chosen_input_ids']\n",
    "attention_mask = batch['chosen_attention_mask']\n",
    "labels=batch['chosen_labels']\n",
    "\n",
    "chosen_output = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "752de046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5102], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-1108.4854], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor(1.5102, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from dpo_utils import _get_batch_logps\n",
    "\n",
    "# (bsz, seq_len, vocab)\n",
    "all_logps = _get_batch_logps(chosen_output.logits, labels, average_log_prob=True)\n",
    "print(all_logps)\n",
    "all_logps = _get_batch_logps(chosen_output.logits, labels, average_log_prob=False)\n",
    "print(all_logps)\n",
    "print(chosen_output['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52919caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1108.4854], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chosen_output['loss'] * (batch['chosen_labels'][:, 1:] != 100).sum(-1)\n",
    "\n",
    "\n",
    "# labels.shape, chosen_output.logits.shape\n",
    "\n",
    "chosen_output['loss'] * (labels[:, 1:] != -100).sum(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39e29905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(759, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29e3d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpo_utils import concatenated_inputs\n",
    "\n",
    "concatenated_batch = concatenated_inputs(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54d8425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"concatenated_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"concatenated_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"concatenated_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in batch.items()}, indent=4))\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in concatenated_batch.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0258897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3264.91552734375 -480.8586120605469\n",
      "-3264.8359375 -480.8492431640625\n",
      "0.6966567635536194\n"
     ]
    }
   ],
   "source": [
    "print(policy_chosen_logps.item(), policy_rejected_logps.item())\n",
    "print(reference_chosen_logps.item(), reference_rejected_logps.item())\n",
    "print(losses.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8767b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-3264.8359], device='cuda:0'), tensor([-480.8492], device='cuda:0'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc1c0c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['chosen_input_ids'].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
