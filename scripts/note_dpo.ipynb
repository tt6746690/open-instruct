{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1898d5c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw_train/openai_summarization\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/openai_summarization\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mconvert_openai_summarization_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/reformat_datasets.py:814\u001b[0m, in \u001b[0;36mconvert_openai_summarization_data\u001b[0;34m(data_dir, output_dir)\u001b[0m\n\u001b[1;32m    806\u001b[0m             fout\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(example) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source_filename, target_filename \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    809\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_summarize_from_feedback_train.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    810\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_summarization_train_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    811\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_summarize_from_feedback_validation.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    812\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_summarization_vadlidation_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    813\u001b[0m ]:\n\u001b[0;32m--> 814\u001b[0m     \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/reformat_datasets.py:772\u001b[0m, in \u001b[0;36mconvert_openai_summarization_data.<locals>.convert\u001b[0;34m(source_filename, target_filename)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(source_filename, target_filename):\n\u001b[1;32m    771\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, source_filename))    \n\u001b[0;32m--> 772\u001b[0m     examples \u001b[38;5;241m=\u001b[39m [row\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_unique_ids_subset\u001b[39m(L):\n\u001b[1;32m    774\u001b[0m         random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/open_instruct/reformat_datasets.py:772\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(source_filename, target_filename):\n\u001b[1;32m    771\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, source_filename))    \n\u001b[0;32m--> 772\u001b[0m     examples \u001b[38;5;241m=\u001b[39m [\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_unique_ids_subset\u001b[39m(L):\n\u001b[1;32m    774\u001b[0m         random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/core/series.py:1897\u001b[0m, in \u001b[0;36mSeries.to_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1894\u001b[0m into_c \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mstandardize_mapping(into)\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minto_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaybe_box_native\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;66;03m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;66;03m# indexer return native python type\u001b[39;00m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/core/series.py:1897\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1894\u001b[0m into_c \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mstandardize_mapping(into)\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c((k, \u001b[43mmaybe_box_native\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;66;03m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;66;03m# indexer return native python type\u001b[39;00m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:182\u001b[0m, in \u001b[0;36mmaybe_box_native\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    177\u001b[0m         value \u001b[38;5;241m=\u001b[39m Timedelta(value)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_box_native\u001b[39m(value: Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NAType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NAType:\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    If passed a scalar cast the scalar to a python native type.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    scalar or Series\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float(value):\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"float\" has incompatible type\u001b[39;00m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;66;03m# \"Union[Union[str, int, float, bool], Union[Any, Timestamp, Timedelta, Any]]\";\u001b[39;00m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# expected \"Union[SupportsFloat, _SupportsIndex, str]\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from open_instruct.reformat_datasets import convert_openai_summarization_data\n",
    "data_dir = 'data/raw_train/openai_summarization'\n",
    "output_dir = 'data/processed/openai_summarization'\n",
    "convert_openai_summarization_data(data_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5b8920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/openai_summarization/json/default-b1d2a6a5a765787c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'id', 'chosen', 'rejected'],\n",
       "    num_rows: 14767\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from note_pruning_analysis import get_dataset\n",
    "\n",
    "ds = get_dataset('openai_summarization')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8650562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering openai_summarization to max_seq_length=2048...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/14767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/14767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/14767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=64):   0%|          | 0/14767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter_examples_by_numtoks] Filter to <=2048, the number of examples 14767 -> 14767 examples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from note_pruning_analysis import filter_json_by_numtoks\n",
    "dataset = 'openai_summarization'\n",
    "target_filename = \"openai_summarization_train_data.jsonl\"\n",
    "print(f\"Filtering {dataset} to max_seq_length=2048...\")\n",
    "filepath = os.path.join(output_dir, target_filename)\n",
    "filter_json_by_numtoks(filepath, max_seq_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94eebee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = 'data/raw_train/openai_summarization'\n",
    "output_dir = 'data/processed/openai_summarization'\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dataset = 'openai_summarization'\n",
    "\n",
    "source_filename = \"openai_summarize_from_feedback_train.parquet\"\n",
    "target_filename = \"openai_summarization_train_data.jsonl\"\n",
    "\n",
    "\n",
    "df = pd.read_parquet(os.path.join(data_dir, source_filename))    \n",
    "examples = [row.to_dict() for _, row in df.iterrows()]\n",
    "def get_unique_ids_subset(L):\n",
    "    random.seed(0)\n",
    "    ids = set()\n",
    "    S = []\n",
    "    random.shuffle(L)\n",
    "    for x in L:\n",
    "        if x[\"info\"][\"id\"] not in ids:\n",
    "            ids.add(x[\"info\"][\"id\"])\n",
    "            S.append(x)\n",
    "    return S\n",
    "examples = get_unique_ids_subset(examples)\n",
    "def convert_example_to_messages(example):\n",
    "    post = example[\"info\"][\"post\"]\n",
    "    choice = example[\"choice\"]\n",
    "    answer_chosen = example[\"summaries\"][choice][\"text\"]\n",
    "    answer_rejected = example[\"summaries\"][1-choice][\"text\"]\n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"id\": dataset+'_'+example[\"info\"][\"id\"],\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": post},\n",
    "            {\"role\": \"assistant\", \"content\": answer_chosen},\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": post},\n",
    "            {\"role\": \"assistant\", \"content\": answer_rejected},\n",
    "        ]\n",
    "    }\n",
    "examples = [convert_example_to_messages(x) for x in examples]\n",
    "output_path = os.path.join(output_dir, target_filename)\n",
    "with open(output_path, 'w') as fout:\n",
    "    for idx, example in enumerate(examples):\n",
    "        fout.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8012c070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'openai_summarization',\n",
       " 'id': 'openai_summarization_t3_gt8y1',\n",
       " 'chosen': [{'role': 'user',\n",
       "   'content': \"I posted this in r/pets, but no one is responding, so I thought I'd wander over to r/askreddit and give it a shot.\\n\\nOn an ill-conceived notion today, I adopted a cat. We have 2 dogs at home (a tame but barky Collie and a playful German-Shepard/Chow mix). I didn't know how they would react to a cat, but it turns out not great. They told me at the shelter the cat was good with dogs but it MOST CERTAINLY is not. It hisses, arches up, and runs away.\\nIt has since found it's way into the basement and we have decided if we can get it to work out, we will call it Base. But in the meantime, I'm wondering if anyone has any advice on how to get them to interact. I'm mostly worried the cat will simply become a basement dweller and never leave (we put it's food/water/litter down there). We don't have cages for the dogs, so we can't put the dogs in a kennel as is suggested in many posts on this topic online.\\nI really really like this cat and don't want to have to take it back tomorrow. My dogs are not being mean, they simply don't know cats very well yet. Any advice would be great.\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \" I adopted a cat; can't get the dogs to interact with it well.\"}],\n",
       " 'rejected': [{'role': 'user',\n",
       "   'content': \"I posted this in r/pets, but no one is responding, so I thought I'd wander over to r/askreddit and give it a shot.\\n\\nOn an ill-conceived notion today, I adopted a cat. We have 2 dogs at home (a tame but barky Collie and a playful German-Shepard/Chow mix). I didn't know how they would react to a cat, but it turns out not great. They told me at the shelter the cat was good with dogs but it MOST CERTAINLY is not. It hisses, arches up, and runs away.\\nIt has since found it's way into the basement and we have decided if we can get it to work out, we will call it Base. But in the meantime, I'm wondering if anyone has any advice on how to get them to interact. I'm mostly worried the cat will simply become a basement dweller and never leave (we put it's food/water/litter down there). We don't have cages for the dogs, so we can't put the dogs in a kennel as is suggested in many posts on this topic online.\\nI really really like this cat and don't want to have to take it back tomorrow. My dogs are not being mean, they simply don't know cats very well yet. Any advice would be great.\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': ' I adopted a cat, am worried it will become a basement dweller and never leave. Any advice?\\n\\nEDIT: Grammar.'}]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bbab326b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf13d3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'ref', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('ref', 'sup1', 0),\n",
       " ('sup1', 'ref', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ('sup1', 'sup1', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = examples[5]\n",
    "L = [tuple(x['policy'] for x in example['summaries']) + (example['choice'],) for example in examples]\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff5485b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bbf9a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'openai_summarization',\n",
       " 'id': 'openai_summarization_t3_34xale',\n",
       " 'chosen': [{'role': 'user',\n",
       "   'content': \"My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\\n\\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \\n\\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': ' Mum thought I was going to road trip with my boyfriend. I cancelled the flight. Mum is annoyed because she thought she would be traveling with me. I am fine with that.'}],\n",
       " 'rejected': [{'role': 'user',\n",
       "   'content': \"My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\\n\\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \\n\\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': ' Mum is mad at me for not flying on my own trip to meet my boyfriend.'}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = examples[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07850852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Explain nuclear fusion like I am five[SEP] Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.[SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "# rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_name)\n",
    "question, ansgwer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt')\n",
    "# score = rank_model(**inputs).logits[0].cpu().detach()\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc757f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Fri Jan 12 14:42:31 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    52W / 300W |   1137MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    54W / 300W |    359MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   46C    P0   264W / 300W |   1341MiB / 32510MiB |     73%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   49C    P0   271W / 300W |   1341MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    659484      C   .../open-instruct/bin/python     1135MiB |\n",
      "|    2   N/A  N/A    657489      C   ...8.8/build/bin/./mdrun_mpi      357MiB |\n",
      "|    4   N/A  N/A    658517      C   python                           1339MiB |\n",
      "|    5   N/A  N/A    631244      C   python                           1339MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook, jpt_parse_args, jpt_setup; jpt_setup()\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084de686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-12 14:42:33,205] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pyarrow # add before datasets/torch\n",
    "import datasets\n",
    "import torch\n",
    "from functools import partial\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.auto import tqdm\n",
    "import deepspeed\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    CodeLlamaTokenizerFast,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    GPT2Tokenizer,\n",
    "    OPTForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from open_instruct.dpo_utils import dpo_loss, concatenated_forward, DataCollatorForSeq2SeqDPO\n",
    "\n",
    "import sys\n",
    "sys.path.append('../open_instruct/')\n",
    "from dpo_tune import (encode_with_messages_format, \n",
    "                      save_with_accelerate, \n",
    "                      prepare_deepspeed,\n",
    "                      parse_args,\n",
    "                      clean_checkpoints,\n",
    "                     )\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34afc29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model_name_or_path results/oi2/llama-7b_sharegptv2_ep=2 \\\n",
      "--tokenizer_name results/oi2/llama-7b_sharegptv2_ep=2 \\\n",
      "--gradient_checkpointing \\\n",
      "--train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl \\\n",
      "--max_seq_length 2048 \\\n",
      "--dataloader_sampler SequentialSampler \\\n",
      "--preprocessing_num_workers 32 \\\n",
      "--per_device_train_batch_size 1 \\\n",
      "--gradient_accumulation_steps 32 \\\n",
      "--learning_rate 5e-7 \\\n",
      "--lr_scheduler_type linear \\\n",
      "--warmup_ratio 0.1 \\\n",
      "--weight_decay 0. \\\n",
      "--num_train_epochs 1 \\\n",
      "--with_tracking \\\n",
      "--report_to tensorboard \\\n",
      "--checkpointing_steps 3 \\\n",
      "--max_train_steps 10 \\\n",
      "--resume_from_checkpoint \\\n",
      "--low_cpu_mem_usage \\\n",
      "--logging_steps 1 \\\n",
      "--output_dir results/jpt_llama7b+sharegptv2ep2_ultrafeedback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(dataset_name=None, dataset_config_name=None, train_file='data/processed/ultrafeedback/ultrafeedback_data.jsonl', model_name_or_path='results/oi2/llama-7b_sharegptv2_ep=2', config_name=None, use_lora=False, lora_rank=64, lora_alpha=16, lora_dropout=0.1, use_flash_attn=False, tokenizer_name='results/oi2/llama-7b_sharegptv2_ep=2', use_slow_tokenizer=False, max_seq_length=2048, per_device_train_batch_size=1, learning_rate=5e-07, weight_decay=0.0, num_train_epochs=1, max_train_steps=10, gradient_accumulation_steps=32, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.1, output_dir='results/jpt_llama7b+sharegptv2ep2_ultrafeedback', seed=None, preprocessing_num_workers=32, overwrite_cache=False, checkpointing_steps='3', logging_steps=1, resume_from_checkpoint=True, with_tracking=True, report_to='tensorboard', low_cpu_mem_usage=True, gradient_checkpointing=True, use_qlora=False, clip_grad_norm=-1, use_8bit_optimizer=False, beta=0.1, use_paged_optimizer=False, subsample_inds_file=None, dataloader_sampler='SequentialSampler')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_from_checkpoint = True\n",
    "use_fast_tokenizer = True\n",
    "gradient_checkpointing = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "max_train_steps = 10\n",
    "checkpointing_steps = 3\n",
    "dataloader_sampler = 'RandomSampler'\n",
    "\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; max_seq_length = 2048; abbr_model_name = 'llama-7b'\n",
    "# model_name_or_path = hf_models_dir+'EleutherAI/pythia-70m'; max_seq_length = 2048; abbr_model_name = 'pythia-70m'\n",
    "model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama7b+sharegptv2ep2'\n",
    "\n",
    "train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; abbr_train_file = 'ultrafeedback'\n",
    "subsample_inds_file = ''\n",
    "dataloader_sampler = 'SequentialSampler'\n",
    "\n",
    "\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--subsample_inds_file '+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "    --dataloader_sampler {dataloader_sampler} \\\n",
    "    --preprocessing_num_workers 32 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    --checkpointing_steps {checkpointing_steps} \\\n",
    "    {'--max_train_steps '+str(max_train_steps) if max_train_steps else ''} \\\n",
    "    {'--resume_from_checkpoint' if resume_from_checkpoint else ''} \\\n",
    "    --low_cpu_mem_usage\n",
    "    --logging_steps 1 \\\n",
    "    --output_dir {output_dir}\n",
    "\n",
    "\"\"\"\n",
    "print(' \\\\\\n'.join([x.strip() for x in re.split(r'\\s{3,}', cmd.strip())]))\n",
    "\n",
    "args = parse_args(cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c5ad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 14:42:41 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
    "# in the environment\n",
    "accelerator_log_kwargs = {}\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
    "    accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0127ce2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/12/2024 14:42:41 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-9a630b550f133bce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08e9f1bb5b1470f881c6a635d8b7ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        args.dataset_name,\n",
    "        args.dataset_config_name,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train_prefs\"] = args.train_file\n",
    "    raw_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        **dataset_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6a9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/oi2/llama-7b_sharegptv2_ep=2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"results/oi2/llama-7b_sharegptv2_ep=2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported by this script.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a81fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    if args.model_name_or_path:\n",
    "        if args.use_qlora:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "            device_index = accelerator.local_process_index\n",
    "            device_map = {\"\": device_index} # force data-parallel training.\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                config=config,\n",
    "                load_in_4bit=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                use_flash_attention_2=True if args.use_flash_attn else False,\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                config=config,\n",
    "                low_cpu_mem_usage=args.low_cpu_mem_usage,\n",
    "                use_flash_attention_2=True if args.use_flash_attn else False,\n",
    "            )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c86ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file results/oi2/llama-7b_sharegptv2_ep=2/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/oi2/llama-7b_sharegptv2_ep=2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/oi2/llama-7b_sharegptv2_ep=2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file results/oi2/llama-7b_sharegptv2_ep=2/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at results/oi2/llama-7b_sharegptv2_ep=2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file results/oi2/llama-7b_sharegptv2_ep=2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model()\n",
    "if not args.use_lora:\n",
    "    reference_model = load_model()\n",
    "else:\n",
    "    reference_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66776f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map \n",
    "if isinstance(tokenizer, (LlamaTokenizer, LlamaTokenizerFast, CodeLlamaTokenizerFast)):\n",
    "    from transformers import AddedToken\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": AddedToken(\"<s>\", normalized=False, special=True),\n",
    "        \"eos_token\": AddedToken(\"</s>\", normalized=False, special=True),\n",
    "        \"unk_token\": AddedToken(\"<unk>\", normalized=False, special=True),\n",
    "        \"pad_token\": AddedToken(\"<pad>\", normalized=False, special=True),\n",
    "    })\n",
    "    ## wpq: for `huggyllama`/`NousResearch/Llama-2-7b-hf`, `LlamaTokenizerFast` tokenizer config not properly implemented and cannot tokenize special tokens like eos_token corretly. \n",
    "    # Need the following workaround. More details: https://github.com/huggingface/transformers/issues/23833\n",
    "    def check_tokenizer_pad_properly(tokenizer):\n",
    "        tokenizer_handles_pad_ok = True\n",
    "        for s, s_tokenized in [\n",
    "            (\"Hi<s>Hey</s>sir<unk>what<pad><pad>\", \n",
    "            ['Hi', '<s>', 'Hey', '</s>', 'sir', '<unk>', 'what', '<pad>', '<pad>']),\n",
    "        ]:\n",
    "            if tokenizer.tokenize(s, add_special_tokens=False)!=s_tokenized:\n",
    "                tokenizer_handles_pad_ok = False\n",
    "        return tokenizer_handles_pad_ok\n",
    "    if not check_tokenizer_pad_properly(tokenizer):\n",
    "        if os.path.isdir(args.model_name_or_path):\n",
    "            tmp_tok_path = os.path.join(\n",
    "                os.path.dirname(args.model_name_or_path),\n",
    "                os.path.basename(args.model_name_or_path)+'_fixtok')\n",
    "            if not os.path.isdir(tmp_tok_path):\n",
    "                tokenizer.save_pretrained(tmp_tok_path)\n",
    "        else:\n",
    "            from secrets import token_hex\n",
    "            tmp_tok_path = f'/tmp/wpq_tok_{token_hex(16)}'\n",
    "            tokenizer.save_pretrained(tmp_tok_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tmp_tok_path, use_fast=not args.use_slow_tokenizer)\n",
    "    assert(check_tokenizer_pad_properly(tokenizer))\n",
    "elif isinstance(tokenizer, GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, GPT2Tokenizer) and isinstance(model, OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({'unk_token': '<unk>'})\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if not args.use_lora:\n",
    "        reference_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7a09509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.use_lora:\n",
    "    if args.use_qlora:\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
    "\n",
    "    logger.info(\"Initializing LORA model...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        r=args.lora_rank, \n",
    "        lora_alpha=args.lora_alpha, \n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"v_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "\n",
    "# wpq: `use_cache=True` is incompatible with gradient checkpointing\n",
    "model.config.use_cache = True if not args.gradient_checkpointing else False\n",
    "if args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd8f1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing the datasets.\n",
    "if \"prompt\" in raw_datasets[\"train_prefs\"].column_names and \"completion\" in raw_datasets[\"train_prefs\"].column_names:\n",
    "    raise ValueError(\"Sorry, prompt-completion format is not supported for DPO training.\")\n",
    "elif \"chosen\" in raw_datasets[\"train_prefs\"].column_names and \"rejected\" in raw_datasets[\"train_prefs\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"You need to have 'chosen' and 'rejected in your column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca3144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=32):   0%|          | 0/60506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = raw_datasets[\"train_prefs\"].map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=[name for name in raw_datasets[\"train_prefs\"].column_names if name not in [\"chosen_input_ids\", \"chosen_labels\", \"chosen_attention_mask\", \"rejected_input_ids\", \"rejected_labels\", \"rejected_attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "    lm_datasets.set_format(type=\"pt\")\n",
    "    # wpq: if enforce max_seq_length properly, below should do nothing, so just remove.\n",
    "    # # our thresholding mighta meant some examples have no labels, remove.\n",
    "    # lm_datasets = lm_datasets.filter(lambda example: (example['chosen_labels'] != -100).any(), num_proc=8)\n",
    "    # lm_datasets = lm_datasets.filter(lambda example: (example['rejected_labels'] != -100).any(), num_proc=8)\n",
    "\n",
    "train_dataset = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa518f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen_input_ids', 'chosen_labels', 'chosen_attention_mask', 'rejected_input_ids', 'rejected_labels', 'rejected_attention_mask'],\n",
       "    num_rows: 60506\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if args.subsample_inds_file is not None:\n",
    "    logger.info(f'Subsample dataset according to indices: {args.subsample_inds_file}')\n",
    "    with open(args.subsample_inds_file, 'rb') as f:\n",
    "        inds = pickle.load(f)['inds']\n",
    "    logger.info(f'subsample_inds_file has {len(inds)} indices.')\n",
    "    train_dataset = train_dataset.select(inds)\n",
    "    \n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc459e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen:\n",
      "<|user|>\n",
      "People watch a soccer player in uniform make a kick on sand.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "yes\n",
      "\n",
      "\n",
      "I doubt the duck is talking to a hamster while a boy watches.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "Question followed by answer: Test for natural language inference.\n",
      "Premise: \"Guy looking out at beautiful scenery.\"\n",
      "Hypothesis: \"The guy is looking at the ground.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "No all cloth and papers are used to communicate with an alien race.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Just because Festival-goers have a fun time it does not mean it was a concert.\n",
      "The question and answer are below.\n",
      "Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Kids cannot be in front of and walking away from the same location.\n",
      "The question and answer are below.\n",
      "<|assistant|>\n",
      "Given the sentence \"Children playing hide-and-seek at park but some are in front of and others are walking away from the location.\" is it true that \"Some kids are neither hiding nor seeking.\"?\n",
      "\n",
      "No\n",
      "Confidence: 90%<|endoftext|>\n",
      "--------------------------------------------------------------------------------\n",
      "rejected:\n",
      "<|user|>\n",
      "People watch a soccer player in uniform make a kick on sand.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "yes\n",
      "\n",
      "\n",
      "I doubt the duck is talking to a hamster while a boy watches.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "Question followed by answer: Test for natural language inference.\n",
      "Premise: \"Guy looking out at beautiful scenery.\"\n",
      "Hypothesis: \"The guy is looking at the ground.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "No all cloth and papers are used to communicate with an alien race.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Just because Festival-goers have a fun time it does not mean it was a concert.\n",
      "The question and answer are below.\n",
      "Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Kids cannot be in front of and walking away from the same location.\n",
      "The question and answer are below.\n",
      "<|assistant|>\n",
      "Here are the answers to your questions:\n",
      "\n",
      "1. Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "\n",
      "Yes. The sentence states that the boy is kicking a soccer ball, which implies that there is an activity happening. The fact that the sentence also states that the boy is wearing black shorts and a shirt suggests that this is a formal event, such as a soccer game. Therefore, it is reasonable to conclude that there are other people watching the boy play soccer.\n",
      "\n",
      "2. Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "\n",
      "No. The sentence simply states that a boy is watching a duck. There is no information about what the duck is doing, so we cannot conclude that it is talking to a hamster.\n",
      "\n",
      "3. If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "\n",
      "The question could be: \"What is the guy doing?\" or \"What is the guy looking at?\" The answer to the first question could be that the guy is looking out the window, while the answer to the second question could be that the guy is looking at the beautiful scenery.\n",
      "\n",
      "4. Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "\n",
      "It is not possible to tell for sure. The sentence does not provide enough information to make a definitive conclusion. The child could be trying to communicate with an alien race, but it is also possible that they are doing something else, such as waving to someone or playing a game.\n",
      "\n",
      "5. Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "\n",
      "It is not possible to tell for sure. The premise states that festival-goers are having a fun time with their children. However, it does not state that the festival is a concert. Therefore, it is possible that the festival-goers are having a fun time at some other event, such as a fair or a picnic.\n",
      "\n",
      "6. Kids cannot be in front of and walking away from the same location.\n",
      "\n",
      "This is true. If a kid is in front of a location, then they are facing the location. If they are walking away from the location, then they are turned away from the location. Therefore, it is not possible for a kid to be in front of and walking away from the same location at the same time.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "example = train_dataset[2]\n",
    "\n",
    "print('chosen:')\n",
    "print(tokenizer.decode(example['chosen_input_ids']))\n",
    "print('----'*20)\n",
    "print('rejected:')\n",
    "print(tokenizer.decode(example['rejected_input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6df8907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wpq: add option to change sampler to allow for non-shuffled ordering of data\n",
    "if args.dataloader_sampler == 'RandomSampler':\n",
    "    sampler = RandomSampler(train_dataset)\n",
    "elif args.dataloader_sampler == 'SequentialSampler':\n",
    "    sampler = SequentialSampler(train_dataset)\n",
    "else:\n",
    "    raise ValueError(f'{args.dataloader_sampler} not supported.')\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=DataCollatorForSeq2SeqDPO(tokenizer=tokenizer, model=model, padding=\"longest\"),\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    sampler=sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51ffc9eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<|user|>\n",
      "Write a 1,000-word op-ed piece in a forma\n",
      "<|user|>\n",
      "Write a 1,000-word op-ed piece in a forma\n",
      "1\n",
      "<|user|>\n",
      "Using a range of physical activities and \n",
      "<|user|>\n",
      "Using a range of physical activities and \n",
      "2\n",
      "<|user|>\n",
      "People watch a soccer player in uniform m\n",
      "<|user|>\n",
      "People watch a soccer player in uniform m\n",
      "3\n",
      "<|user|>\n",
      "What is the advantage of separable filter\n",
      "<|user|>\n",
      "What is the advantage of separable filter\n"
     ]
    }
   ],
   "source": [
    "loader = iter(train_dataloader)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(i)\n",
    "    print(tokenizer.decode(batch['chosen_input_ids'][0])[:50])\n",
    "    print(tokenizer.decode(train_dataset[i]['chosen_input_ids'])[:50])\n",
    "    \n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b149a9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen_input_ids\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            1006\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_labels\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            1006\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_attention_mask\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            1006\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_input_ids\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            985\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_labels\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            985\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_attention_mask\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            985\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "import json\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in batch.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4430931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "if args.use_qlora or args.use_paged_optimizer:\n",
    "    from bitsandbytes.optim import AdamW\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        optim_bits=8 if args.use_8bit_optimizer else 32,\n",
    "        is_paged=True\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "298a2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  num_update_steps_per_epoch = 1891\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  overrode_max_train_steps = False\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  args.max_train_steps = 10\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  num_training_steps_for_scheduler = 10\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  accelerator.num_processes = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7ffec6895450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# Note: the current accelerator.step() calls the .step() of the real scheduler for the `num_processes` times. This is because they assume \n",
    "# the user initialize the scheduler with the entire training set. In the case of data parallel training, each process only\n",
    "# sees a subset (1/num_processes) of the training set. So each time the process needs to update the lr multiple times so that the total \n",
    "# number of updates in the end matches the num_training_steps here.\n",
    "# Here we need to set the num_training_steps to either using the entire training set (when epochs is specified) or we need to multiply the \n",
    "# num_training_steps by num_processes so that the total number of updates matches the num_training_steps.\n",
    "num_training_steps_for_scheduler = args.max_train_steps if overrode_max_train_steps else args.max_train_steps * accelerator.num_processes\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps_for_scheduler,\n",
    "    num_warmup_steps=int(num_training_steps_for_scheduler * args.warmup_ratio),\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(f\"[wpq]  num_update_steps_per_epoch = {num_update_steps_per_epoch}\")\n",
    "logger.info(f\"[wpq]  overrode_max_train_steps = {overrode_max_train_steps}\")\n",
    "logger.info(f\"[wpq]  args.max_train_steps = {args.max_train_steps}\")\n",
    "logger.info(f\"[wpq]  num_training_steps_for_scheduler = {num_training_steps_for_scheduler}\")\n",
    "logger.info(f\"[wpq]  accelerator.num_processes = {accelerator.num_processes}\")\n",
    "\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8664b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare everything with `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "if not jpt_in_notebook():\n",
    "    if not args.use_lora:\n",
    "        reference_model = prepare_deepspeed(accelerator, reference_model)\n",
    "else:\n",
    "    reference_model = reference_model.to('cuda')\n",
    "    reference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1799a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4121859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"mitibm\", experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f065fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 15:21:08 - INFO - __main__ - ***** Running training *****\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Num examples = 60506\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Num Epochs = 1\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Gradient Accumulation steps = 32\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bfb82b0f574b4b87580fe189903419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d73c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 15:21:08 - INFO - accelerate.accelerator - Loading states from results/jpt_pythia-70m_ultrafeedback/step_3\n",
      "01/10/2024 15:21:08 - INFO - accelerate.checkpointing - All model weights loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.checkpointing - All optimizer states loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.checkpointing - All scheduler states loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.checkpointing - All random states loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.accelerator - Loading in 0 custom states\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from checkpoint: results/jpt_pythia-70m_ultrafeedback/step_3\n"
     ]
    }
   ],
   "source": [
    "# wpq: instead of using `from_pretrained`, use accelerate's `load_state` to save model weights, optimizer state etc. reference: https://github.com/huggingface/accelerate/blob/v0.25.0/examples/by_feature/deepspeed_with_config_support.py\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    dirs = glob.glob(os.path.join(args.output_dir, 'step_*')) + \\\n",
    "        glob.glob(os.path.join(args.output_dir, 'epoch_*'))\n",
    "    dirs = [x for x in dirs if os.path.isdir(x)]\n",
    "    dirs.sort(key=os.path.getctime) # Sorts folders by date modified\n",
    "    if len(dirs) == 0:\n",
    "        args.resume_from_checkpoint = False\n",
    "        logger.info(f\"No checkpoint found in {args.output_dir}. Training from scratch.\")\n",
    "    else:\n",
    "        checkpoint_path = dirs[-1]\n",
    "        accelerator.load_state(checkpoint_path)\n",
    "        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "        # Extract `epoch_{i}` or `step_{i}`\n",
    "        training_difference = os.path.splitext(os.path.basename(checkpoint_path))[0]\n",
    "\n",
    "        if \"epoch\" in training_difference:\n",
    "            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "            resume_step = None\n",
    "            completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "        else:\n",
    "            # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "            resume_step = (\n",
    "                int(training_difference.replace(\"step_\", \"\"))\n",
    "                * args.gradient_accumulation_steps\n",
    "            )\n",
    "            starting_epoch = resume_step // len(train_dataloader)\n",
    "            completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "            resume_step -= starting_epoch * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a99d4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    if (\n",
    "        args.resume_from_checkpoint\n",
    "        and epoch == starting_epoch\n",
    "        and resume_step is not None\n",
    "    ):\n",
    "        # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "        active_dataloader = accelerator.skip_first_batches(\n",
    "            train_dataloader, resume_step\n",
    "        )\n",
    "    else:\n",
    "        active_dataloader = train_dataloader\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        logps = defaultdict(int)\n",
    "        \n",
    "        # dpo forward pass & loss\n",
    "        with accelerator.accumulate(model):\n",
    "            policy_chosen_logps, policy_rejected_logps = concatenated_forward(model, batch)\n",
    "            with torch.no_grad():\n",
    "                if args.use_lora:\n",
    "                    with accelerator.unwrap_model(model).disable_adapter():\n",
    "                        reference_chosen_logps, reference_rejected_logps = concatenated_forward(model, batch)\n",
    "                else:\n",
    "                    reference_chosen_logps, reference_rejected_logps = concatenated_forward(reference_model, batch)\n",
    "            losses, _, _ = dpo_loss(\n",
    "                policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, beta=args.beta)\n",
    "            # TODO: metric logging          \n",
    "            loss = losses.mean()\n",
    "            # We keep track of the loss at each logged step\n",
    "            total_loss += loss.detach().float()\n",
    "            logps['logps_policy_chosen'] += policy_chosen_logps.detach().float()\n",
    "            logps['logps_policy_rejected'] += policy_rejected_logps.detach().float()\n",
    "            logps['logps_reference_chosen'] += reference_chosen_logps.detach().float()\n",
    "            logps['logps_reference_rejected'] += reference_rejected_logps.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            # clip gradient norm. don't do this with deepspeed\n",
    "            if accelerator.sync_gradients and args.clip_grad_norm > 0:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            if args.logging_steps and completed_steps % args.logging_steps == 0:\n",
    "                avg_loss = accelerator.gather(total_loss).mean().item() / args.gradient_accumulation_steps / args.logging_steps\n",
    "                for k in list(logps.keys()):\n",
    "                    logps[k] = accelerator.gather(logps[k]).mean().item() / args.gradient_accumulation_steps                \n",
    "                logger.info(f\"  Step: {completed_steps}, LR: {lr_scheduler.get_last_lr()[0]}, Loss: {avg_loss}\")\n",
    "                if args.with_tracking:\n",
    "                    accelerator.log(\n",
    "                        {\n",
    "                            \"learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "                            \"train_loss\": avg_loss,\n",
    "                            \"step\": completed_steps,\n",
    "                            'logps_policy_chosen': logps['logps_policy_chosen'],\n",
    "                            'logps_policy_rejected': logps['logps_policy_rejected'],\n",
    "                            'logps_reference_chosen': logps['logps_reference_chosen'],\n",
    "                            'logps_reference_rejected': logps['logps_reference_rejected'],\n",
    "                        },\n",
    "                        step=completed_steps,\n",
    "                    )\n",
    "                total_loss = 0\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                if completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    if args.output_dir is not None:\n",
    "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    accelerator.save_state(output_dir)\n",
    "                    clean_checkpoints(args.output_dir, keep_n=1)\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "        break\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        saccelerator.save_state(output_dir)\n",
    "        clean_checkpoints(args.output_dir, keep_n=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d05f4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0004a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in results/jpt_pythia-70m_ultrafeedback/tokenizer_config.json\n",
      "Special tokens file saved in results/jpt_pythia-70m_ultrafeedback/special_tokens_map.json\n",
      "Configuration saved in results/jpt_pythia-70m_ultrafeedback/config.json\n",
      "Configuration saved in results/jpt_pythia-70m_ultrafeedback/generation_config.json\n",
      "Model weights saved in results/jpt_pythia-70m_ultrafeedback/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "save_with_accelerate(accelerator, model, tokenizer, args.output_dir, args)\n",
    "clean_checkpoints(args.output_dir, keep_n=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf526bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3]\n",
    "l[:len(l)-1]\n",
    "\n",
    "l[len(l)-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab886125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = batch['chosen_input_ids']\n",
    "attention_mask = batch['chosen_attention_mask']\n",
    "labels=batch['chosen_labels']\n",
    "\n",
    "chosen_output = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "752de046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5102], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-1108.4854], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor(1.5102, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from dpo_utils import _get_batch_logps\n",
    "\n",
    "# (bsz, seq_len, vocab)\n",
    "all_logps = _get_batch_logps(chosen_output.logits, labels, average_log_prob=True)\n",
    "print(all_logps)\n",
    "all_logps = _get_batch_logps(chosen_output.logits, labels, average_log_prob=False)\n",
    "print(all_logps)\n",
    "print(chosen_output['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52919caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1108.4854], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chosen_output['loss'] * (batch['chosen_labels'][:, 1:] != 100).sum(-1)\n",
    "\n",
    "\n",
    "# labels.shape, chosen_output.logits.shape\n",
    "\n",
    "chosen_output['loss'] * (labels[:, 1:] != -100).sum(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39e29905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(759, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29e3d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpo_utils import concatenated_inputs\n",
    "\n",
    "concatenated_batch = concatenated_inputs(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54d8425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"concatenated_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"concatenated_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"concatenated_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in batch.items()}, indent=4))\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in concatenated_batch.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0258897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3264.91552734375 -480.8586120605469\n",
      "-3264.8359375 -480.8492431640625\n",
      "0.6966567635536194\n"
     ]
    }
   ],
   "source": [
    "print(policy_chosen_logps.item(), policy_rejected_logps.item())\n",
    "print(reference_chosen_logps.item(), reference_rejected_logps.item())\n",
    "print(losses.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8767b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-3264.8359], device='cuda:0'), tensor([-480.8492], device='cuda:0'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc1c0c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['chosen_input_ids'].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
