{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc757f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Thu Jan 11 15:26:15 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 300W |   8717MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    62W / 300W |   8717MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    69W / 300W |   7587MiB / 32510MiB |     65%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    52W / 300W |   8719MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A    417006      C   ...nda3/envs/goal/bin/python     8715MiB |\n",
      "|    3   N/A  N/A    416995      C   ...nda3/envs/goal/bin/python     8715MiB |\n",
      "|    4   N/A  N/A    417017      C   ...nda3/envs/goal/bin/python     7585MiB |\n",
      "|    5   N/A  N/A    417053      C   ...nda3/envs/goal/bin/python     8717MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook, jpt_parse_args, jpt_setup; jpt_setup()\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "084de686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pyarrow # add before datasets/torch\n",
    "import datasets\n",
    "import torch\n",
    "from functools import partial\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.auto import tqdm\n",
    "import deepspeed\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    CodeLlamaTokenizerFast,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    GPT2Tokenizer,\n",
    "    OPTForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from open_instruct.dpo_utils import dpo_loss, concatenated_forward, DataCollatorForSeq2SeqDPO\n",
    "\n",
    "import sys\n",
    "sys.path.append('../open_instruct/')\n",
    "from dpo_tune import (encode_with_messages_format, \n",
    "                      save_with_accelerate, \n",
    "                      prepare_deepspeed,\n",
    "                      parse_args,\n",
    "                      clean_checkpoints,\n",
    "                     )\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34afc29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--model_name_or_path results/baselines/EleutherAI/pythia-70m \\\n",
      "--tokenizer_name results/baselines/EleutherAI/pythia-70m \\\n",
      "--gradient_checkpointing \\\n",
      "--train_file data/processed/ultrafeedback/ultrafeedback_data.jsonl \\\n",
      "--max_seq_length 2048 \\\n",
      "--dataloader_sampler SequentialSampler \\\n",
      "--preprocessing_num_workers 32 \\\n",
      "--per_device_train_batch_size 1 \\\n",
      "--gradient_accumulation_steps 32 \\\n",
      "--learning_rate 5e-7 \\\n",
      "--lr_scheduler_type linear \\\n",
      "--warmup_ratio 0.1 \\\n",
      "--weight_decay 0. \\\n",
      "--num_train_epochs 1 \\\n",
      "--with_tracking \\\n",
      "--report_to tensorboard \\\n",
      "--checkpointing_steps 3 \\\n",
      "--max_train_steps 10 \\\n",
      "--resume_from_checkpoint \\\n",
      "--low_cpu_mem_usage \\\n",
      "--logging_steps 1 \\\n",
      "--output_dir results/jpt_pythia-70m_ultrafeedback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(dataset_name=None, dataset_config_name=None, train_file='data/processed/ultrafeedback/ultrafeedback_data.jsonl', model_name_or_path='results/baselines/EleutherAI/pythia-70m', config_name=None, use_lora=False, lora_rank=64, lora_alpha=16, lora_dropout=0.1, use_flash_attn=False, tokenizer_name='results/baselines/EleutherAI/pythia-70m', use_slow_tokenizer=False, max_seq_length=2048, per_device_train_batch_size=1, learning_rate=5e-07, weight_decay=0.0, num_train_epochs=1, max_train_steps=10, gradient_accumulation_steps=32, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.1, output_dir='results/jpt_pythia-70m_ultrafeedback', seed=None, preprocessing_num_workers=32, overwrite_cache=False, checkpointing_steps='3', logging_steps=1, resume_from_checkpoint=True, with_tracking=True, report_to='tensorboard', low_cpu_mem_usage=True, gradient_checkpointing=True, use_qlora=False, clip_grad_norm=-1, use_8bit_optimizer=False, beta=0.1, use_paged_optimizer=False, subsample_inds_file=None, dataloader_sampler='SequentialSampler')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_from_checkpoint = True\n",
    "use_fast_tokenizer = True\n",
    "gradient_checkpointing = True\n",
    "hf_models_dir = 'results/baselines/'\n",
    "max_train_steps = 10\n",
    "checkpointing_steps = 3\n",
    "dataloader_sampler = 'RandomSampler'\n",
    "\n",
    "\n",
    "# model_name_or_path = hf_models_dir+'huggyllama/llama-7b'; max_seq_length = 2048; abbr_model_name = 'llama-7b'\n",
    "model_name_or_path = hf_models_dir+'EleutherAI/pythia-70m'; max_seq_length = 2048; abbr_model_name = 'pythia-70m'\n",
    "# model_name_or_path = 'results/oi2/llama-7b_sharegptv2_ep=2'; max_seq_length = 2048; abbr_model_name = 'llama7b+sharegptv2ep2'\n",
    "\n",
    "train_file = 'data/processed/ultrafeedback/ultrafeedback_data.jsonl'; abbr_train_file = 'ultrafeedback'\n",
    "subsample_inds_file = ''\n",
    "dataloader_sampler = 'SequentialSampler'\n",
    "\n",
    "\n",
    "output_dirname = f\"{abbr_model_name}_{abbr_train_file}\"\n",
    "output_dirname = 'jpt_'+output_dirname\n",
    "output_dir = os.path.join('results', output_dirname)\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    {'--use_slow_tokenizer' if not  use_fast_tokenizer else ''} \\\n",
    "    {'--gradient_checkpointing' if gradient_checkpointing  else ''} \\\n",
    "    --train_file {train_file} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    {'--subsample_inds_file '+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "    --dataloader_sampler {dataloader_sampler} \\\n",
    "    --preprocessing_num_workers 32 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --learning_rate 5e-7 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --weight_decay 0. \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --with_tracking \\\n",
    "    --report_to tensorboard \\\n",
    "    --checkpointing_steps {checkpointing_steps} \\\n",
    "    {'--max_train_steps '+str(max_train_steps) if max_train_steps else ''} \\\n",
    "    {'--resume_from_checkpoint' if resume_from_checkpoint else ''} \\\n",
    "    --low_cpu_mem_usage\n",
    "    --logging_steps 1 \\\n",
    "    --output_dir {output_dir}\n",
    "\n",
    "\"\"\"\n",
    "print(' \\\\\\n'.join([x.strip() for x in re.split(r'\\s{3,}', cmd.strip())]))\n",
    "\n",
    "args = parse_args(cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c5ad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2024 16:25:07 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
    "# in the environment\n",
    "accelerator_log_kwargs = {}\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
    "    accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0127ce2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2024 16:25:08 - WARNING - datasets.builder - Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-9a630b550f133bce/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f14b75c7c4bcdbacdda4a530a1eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        args.dataset_name,\n",
    "        args.dataset_config_name,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train_prefs\"] = args.train_file\n",
    "    raw_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        **dataset_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b6a9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/baselines/EleutherAI/pythia-70m/config.json\n",
      "Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"results/baselines/EleutherAI/pythia-70m\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported by this script.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a81fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    if args.model_name_or_path:\n",
    "        if args.use_qlora:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "            device_index = accelerator.local_process_index\n",
    "            device_map = {\"\": device_index} # force data-parallel training.\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                config=config,\n",
    "                load_in_4bit=True,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                use_flash_attention_2=True if args.use_flash_attn else False,\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                config=config,\n",
    "                low_cpu_mem_usage=args.low_cpu_mem_usage,\n",
    "                use_flash_attention_2=True if args.use_flash_attn else False,\n",
    "            )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0c86ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file results/baselines/EleutherAI/pythia-70m/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-70m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading weights file results/baselines/EleutherAI/pythia-70m/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at results/baselines/EleutherAI/pythia-70m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model()\n",
    "if not args.use_lora:\n",
    "    reference_model = load_model()\n",
    "else:\n",
    "    reference_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66776f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map \n",
    "if isinstance(tokenizer, (LlamaTokenizer, LlamaTokenizerFast, CodeLlamaTokenizerFast)):\n",
    "    from transformers import AddedToken\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": AddedToken(\"<s>\", normalized=False, special=True),\n",
    "        \"eos_token\": AddedToken(\"</s>\", normalized=False, special=True),\n",
    "        \"unk_token\": AddedToken(\"<unk>\", normalized=False, special=True),\n",
    "        \"pad_token\": AddedToken(\"<pad>\", normalized=False, special=True),\n",
    "    })\n",
    "    ## wpq: for `huggyllama`/`NousResearch/Llama-2-7b-hf`, `LlamaTokenizerFast` tokenizer config not properly implemented and cannot tokenize special tokens like eos_token corretly. \n",
    "    # Need the following workaround. More details: https://github.com/huggingface/transformers/issues/23833\n",
    "    def check_tokenizer_pad_properly(tokenizer):\n",
    "        tokenizer_handles_pad_ok = True\n",
    "        for s, s_tokenized in [\n",
    "            (\"Hi<s>Hey</s>sir<unk>what<pad><pad>\", \n",
    "            ['笆？i', '<s>', '笆？ey', '</s>', '笆《ir', '<unk>', '笆『hat', '<pad>', '<pad>']),\n",
    "        ]:\n",
    "            if tokenizer.tokenize(s, add_special_tokens=False)!=s_tokenized:\n",
    "                tokenizer_handles_pad_ok = False\n",
    "        return tokenizer_handles_pad_ok\n",
    "    if not check_tokenizer_pad_properly(tokenizer):\n",
    "        if os.path.isdir(args.model_name_or_path):\n",
    "            tmp_tok_path = os.path.join(\n",
    "                os.path.dirname(args.model_name_or_path),\n",
    "                os.path.basename(args.model_name_or_path)+'_fixtok')\n",
    "            if not os.path.isdir(tmp_tok_path):\n",
    "                tokenizer.save_pretrained(tmp_tok_path)\n",
    "        else:\n",
    "            from secrets import token_hex\n",
    "            tmp_tok_path = f'/tmp/wpq_tok_{token_hex(16)}'\n",
    "            tokenizer.save_pretrained(tmp_tok_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tmp_tok_path, use_fast=not args.use_slow_tokenizer)\n",
    "    assert(check_tokenizer_pad_properly(tokenizer))\n",
    "elif isinstance(tokenizer, GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, GPT2Tokenizer) and isinstance(model, OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({'unk_token': '<unk>'})\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if not args.use_lora:\n",
    "        reference_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7a09509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.use_lora:\n",
    "    if args.use_qlora:\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
    "\n",
    "    logger.info(\"Initializing LORA model...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        r=args.lora_rank, \n",
    "        lora_alpha=args.lora_alpha, \n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"v_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "\n",
    "# wpq: `use_cache=True` is incompatible with gradient checkpointing\n",
    "model.config.use_cache = True if not args.gradient_checkpointing else False\n",
    "if args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd8f1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing the datasets.\n",
    "if \"prompt\" in raw_datasets[\"train_prefs\"].column_names and \"completion\" in raw_datasets[\"train_prefs\"].column_names:\n",
    "    raise ValueError(\"Sorry, prompt-completion format is not supported for DPO training.\")\n",
    "elif \"chosen\" in raw_datasets[\"train_prefs\"].column_names and \"rejected\" in raw_datasets[\"train_prefs\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"You need to have 'chosen' and 'rejected in your column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca3144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=32):   0%|          | 0/60506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = raw_datasets[\"train_prefs\"].map(\n",
    "        encode_function,\n",
    "        batched=False,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=[name for name in raw_datasets[\"train_prefs\"].column_names if name not in [\"chosen_input_ids\", \"chosen_labels\", \"chosen_attention_mask\", \"rejected_input_ids\", \"rejected_labels\", \"rejected_attention_mask\"]],\n",
    "        desc=\"Tokenizing and reformatting instruction data\",\n",
    "    )\n",
    "    lm_datasets.set_format(type=\"pt\")\n",
    "    # wpq: if enforce max_seq_length properly, below should do nothing, so just remove.\n",
    "    # # our thresholding mighta meant some examples have no labels, remove.\n",
    "    # lm_datasets = lm_datasets.filter(lambda example: (example['chosen_labels'] != -100).any(), num_proc=8)\n",
    "    # lm_datasets = lm_datasets.filter(lambda example: (example['rejected_labels'] != -100).any(), num_proc=8)\n",
    "\n",
    "train_dataset = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa518f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen_input_ids', 'chosen_labels', 'chosen_attention_mask', 'rejected_input_ids', 'rejected_labels', 'rejected_attention_mask'],\n",
       "    num_rows: 60506\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if args.subsample_inds_file is not None:\n",
    "    logger.info(f'Subsample dataset according to indices: {args.subsample_inds_file}')\n",
    "    with open(args.subsample_inds_file, 'rb') as f:\n",
    "        inds = pickle.load(f)['inds']\n",
    "    logger.info(f'subsample_inds_file has {len(inds)} indices.')\n",
    "    train_dataset = train_dataset.select(inds)\n",
    "    \n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc459e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen:\n",
      "<|user|>\n",
      "People watch a soccer player in uniform make a kick on sand.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "yes\n",
      "\n",
      "\n",
      "I doubt the duck is talking to a hamster while a boy watches.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "Question followed by answer: Test for natural language inference.\n",
      "Premise: \"Guy looking out at beautiful scenery.\"\n",
      "Hypothesis: \"The guy is looking at the ground.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "No all cloth and papers are used to communicate with an alien race.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Just because Festival-goers have a fun time it does not mean it was a concert.\n",
      "The question and answer are below.\n",
      "Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Kids cannot be in front of and walking away from the same location.\n",
      "The question and answer are below.\n",
      "<|assistant|>\n",
      "Given the sentence \"Children playing hide-and-seek at park but some are in front of and others are walking away from the location.\" is it true that \"Some kids are neither hiding nor seeking.\"?\n",
      "\n",
      "No\n",
      "Confidence: 90%<|endoftext|>\n",
      "--------------------------------------------------------------------------------\n",
      "rejected:\n",
      "<|user|>\n",
      "People watch a soccer player in uniform make a kick on sand.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "yes\n",
      "\n",
      "\n",
      "I doubt the duck is talking to a hamster while a boy watches.\n",
      "The question and answer are below.\n",
      "Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "Question followed by answer: Test for natural language inference.\n",
      "Premise: \"Guy looking out at beautiful scenery.\"\n",
      "Hypothesis: \"The guy is looking at the ground.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "no\n",
      "\n",
      "\n",
      "No all cloth and papers are used to communicate with an alien race.\n",
      "The question and answer are below.\n",
      "Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Just because Festival-goers have a fun time it does not mean it was a concert.\n",
      "The question and answer are below.\n",
      "Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "it is not possible to tell\n",
      "\n",
      "\n",
      "Kids cannot be in front of and walking away from the same location.\n",
      "The question and answer are below.\n",
      "<|assistant|>\n",
      "Here are the answers to your questions:\n",
      "\n",
      "1. Given the sentence \"A black boy dressed in black shots and shirt kicks a soccer ball on sand.\" is it true that \"While others watch.\"?\n",
      "\n",
      "Yes. The sentence states that the boy is kicking a soccer ball, which implies that there is an activity happening. The fact that the sentence also states that the boy is wearing black shorts and a shirt suggests that this is a formal event, such as a soccer game. Therefore, it is reasonable to conclude that there are other people watching the boy play soccer.\n",
      "\n",
      "2. Can we conclude from \"A little boy at a lake watching a duck.\" that \"The duck is talking to the hamster.\"?\n",
      "\n",
      "No. The sentence simply states that a boy is watching a duck. There is no information about what the duck is doing, so we cannot conclude that it is talking to a hamster.\n",
      "\n",
      "3. If you are looking out the ground you can't see the beautiful scenery.. So what could be the question?\n",
      "\n",
      "The question could be: \"What is the guy doing?\" or \"What is the guy looking at?\" The answer to the first question could be that the guy is looking out the window, while the answer to the second question could be that the guy is looking at the beautiful scenery.\n",
      "\n",
      "4. Given the sentence \"A child stands on a hill above a village holding a piece of cloth and papers.\" is it true that \"The child is trying to communicate with an alien race.\"?\n",
      "\n",
      "It is not possible to tell for sure. The sentence does not provide enough information to make a definitive conclusion. The child could be trying to communicate with an alien race, but it is also possible that they are doing something else, such as waving to someone or playing a game.\n",
      "\n",
      "5. Premise: \"Festival-goers have a fun time with their children.\"\n",
      "Hypothesis: \"The concert was closed.\"\n",
      "Do we know that the hypothesis entailed by the premise?\n",
      "\n",
      "It is not possible to tell for sure. The premise states that festival-goers are having a fun time with their children. However, it does not state that the festival is a concert. Therefore, it is possible that the festival-goers are having a fun time at some other event, such as a fair or a picnic.\n",
      "\n",
      "6. Kids cannot be in front of and walking away from the same location.\n",
      "\n",
      "This is true. If a kid is in front of a location, then they are facing the location. If they are walking away from the location, then they are turned away from the location. Therefore, it is not possible for a kid to be in front of and walking away from the same location at the same time.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "example = train_dataset[2]\n",
    "\n",
    "print('chosen:')\n",
    "print(tokenizer.decode(example['chosen_input_ids']))\n",
    "print('----'*20)\n",
    "print('rejected:')\n",
    "print(tokenizer.decode(example['rejected_input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6df8907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wpq: add option to change sampler to allow for non-shuffled ordering of data\n",
    "if args.dataloader_sampler == 'RandomSampler':\n",
    "    sampler = RandomSampler(train_dataset)\n",
    "elif args.dataloader_sampler == 'SequentialSampler':\n",
    "    sampler = SequentialSampler(train_dataset)\n",
    "else:\n",
    "    raise ValueError(f'{args.dataloader_sampler} not supported.')\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=DataCollatorForSeq2SeqDPO(tokenizer=tokenizer, model=model, padding=\"longest\"),\n",
    "    batch_size=args.per_device_train_batch_size,\n",
    "    sampler=sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51ffc9eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<|user|>\n",
      "Write a 1,000-word op-ed piece in a forma\n",
      "<|user|>\n",
      "Write a 1,000-word op-ed piece in a forma\n",
      "1\n",
      "<|user|>\n",
      "Using a range of physical activities and \n",
      "<|user|>\n",
      "Using a range of physical activities and \n",
      "2\n",
      "<|user|>\n",
      "People watch a soccer player in uniform m\n",
      "<|user|>\n",
      "People watch a soccer player in uniform m\n",
      "3\n",
      "<|user|>\n",
      "What is the advantage of separable filter\n",
      "<|user|>\n",
      "What is the advantage of separable filter\n"
     ]
    }
   ],
   "source": [
    "loader = iter(train_dataloader)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(i)\n",
    "    print(tokenizer.decode(batch['chosen_input_ids'][0])[:50])\n",
    "    print(tokenizer.decode(train_dataset[i]['chosen_input_ids'])[:50])\n",
    "    \n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b149a9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen_input_ids\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            1006\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_labels\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            1006\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_attention_mask\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            1006\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_input_ids\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            985\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_labels\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            985\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_attention_mask\": {\n",
      "        \"device\": \"cpu\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            985\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "import json\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in batch.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4430931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "if args.use_qlora or args.use_paged_optimizer:\n",
    "    from bitsandbytes.optim import AdamW\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        optim_bits=8 if args.use_8bit_optimizer else 32,\n",
    "        is_paged=True\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "298a2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  num_update_steps_per_epoch = 1891\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  overrode_max_train_steps = False\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  args.max_train_steps = 10\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  num_training_steps_for_scheduler = 10\n",
      "01/10/2024 15:21:06 - INFO - __main__ - [wpq]  accelerator.num_processes = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7ffec6895450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# Note: the current accelerator.step() calls the .step() of the real scheduler for the `num_processes` times. This is because they assume \n",
    "# the user initialize the scheduler with the entire training set. In the case of data parallel training, each process only\n",
    "# sees a subset (1/num_processes) of the training set. So each time the process needs to update the lr multiple times so that the total \n",
    "# number of updates in the end matches the num_training_steps here.\n",
    "# Here we need to set the num_training_steps to either using the entire training set (when epochs is specified) or we need to multiply the \n",
    "# num_training_steps by num_processes so that the total number of updates matches the num_training_steps.\n",
    "num_training_steps_for_scheduler = args.max_train_steps if overrode_max_train_steps else args.max_train_steps * accelerator.num_processes\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps_for_scheduler,\n",
    "    num_warmup_steps=int(num_training_steps_for_scheduler * args.warmup_ratio),\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(f\"[wpq]  num_update_steps_per_epoch = {num_update_steps_per_epoch}\")\n",
    "logger.info(f\"[wpq]  overrode_max_train_steps = {overrode_max_train_steps}\")\n",
    "logger.info(f\"[wpq]  args.max_train_steps = {args.max_train_steps}\")\n",
    "logger.info(f\"[wpq]  num_training_steps_for_scheduler = {num_training_steps_for_scheduler}\")\n",
    "logger.info(f\"[wpq]  accelerator.num_processes = {accelerator.num_processes}\")\n",
    "\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8664b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare everything with `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "if not jpt_in_notebook():\n",
    "    if not args.use_lora:\n",
    "        reference_model = prepare_deepspeed(accelerator, reference_model)\n",
    "else:\n",
    "    reference_model = reference_model.to('cuda')\n",
    "    reference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1799a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4121859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"mitibm\", experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f065fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 15:21:08 - INFO - __main__ - ***** Running training *****\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Num examples = 60506\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Num Epochs = 1\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Gradient Accumulation steps = 32\n",
      "01/10/2024 15:21:08 - INFO - __main__ -   Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bfb82b0f574b4b87580fe189903419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d73c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/10/2024 15:21:08 - INFO - accelerate.accelerator - Loading states from results/jpt_pythia-70m_ultrafeedback/step_3\n",
      "01/10/2024 15:21:08 - INFO - accelerate.checkpointing - All model weights loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.checkpointing - All optimizer states loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.checkpointing - All scheduler states loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.checkpointing - All random states loaded successfully\n",
      "01/10/2024 15:21:09 - INFO - accelerate.accelerator - Loading in 0 custom states\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from checkpoint: results/jpt_pythia-70m_ultrafeedback/step_3\n"
     ]
    }
   ],
   "source": [
    "# wpq: instead of using `from_pretrained`, use accelerate's `load_state` to save model weights, optimizer state etc. reference: https://github.com/huggingface/accelerate/blob/v0.25.0/examples/by_feature/deepspeed_with_config_support.py\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    dirs = glob.glob(os.path.join(args.output_dir, 'step_*')) + \\\n",
    "        glob.glob(os.path.join(args.output_dir, 'epoch_*'))\n",
    "    dirs = [x for x in dirs if os.path.isdir(x)]\n",
    "    dirs.sort(key=os.path.getctime) # Sorts folders by date modified\n",
    "    if len(dirs) == 0:\n",
    "        args.resume_from_checkpoint = False\n",
    "        logger.info(f\"No checkpoint found in {args.output_dir}. Training from scratch.\")\n",
    "    else:\n",
    "        checkpoint_path = dirs[-1]\n",
    "        accelerator.load_state(checkpoint_path)\n",
    "        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "        # Extract `epoch_{i}` or `step_{i}`\n",
    "        training_difference = os.path.splitext(os.path.basename(checkpoint_path))[0]\n",
    "\n",
    "        if \"epoch\" in training_difference:\n",
    "            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "            resume_step = None\n",
    "            completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "        else:\n",
    "            # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "            resume_step = (\n",
    "                int(training_difference.replace(\"step_\", \"\"))\n",
    "                * args.gradient_accumulation_steps\n",
    "            )\n",
    "            starting_epoch = resume_step // len(train_dataloader)\n",
    "            completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "            resume_step -= starting_epoch * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a99d4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    if (\n",
    "        args.resume_from_checkpoint\n",
    "        and epoch == starting_epoch\n",
    "        and resume_step is not None\n",
    "    ):\n",
    "        # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "        active_dataloader = accelerator.skip_first_batches(\n",
    "            train_dataloader, resume_step\n",
    "        )\n",
    "    else:\n",
    "        active_dataloader = train_dataloader\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        logps = defaultdict(int)\n",
    "        \n",
    "        # dpo forward pass & loss\n",
    "        with accelerator.accumulate(model):\n",
    "            policy_chosen_logps, policy_rejected_logps = concatenated_forward(model, batch)\n",
    "            with torch.no_grad():\n",
    "                if args.use_lora:\n",
    "                    with accelerator.unwrap_model(model).disable_adapter():\n",
    "                        reference_chosen_logps, reference_rejected_logps = concatenated_forward(model, batch)\n",
    "                else:\n",
    "                    reference_chosen_logps, reference_rejected_logps = concatenated_forward(reference_model, batch)\n",
    "            losses, _, _ = dpo_loss(\n",
    "                policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, beta=args.beta)\n",
    "            # TODO: metric logging          \n",
    "            loss = losses.mean()\n",
    "            # We keep track of the loss at each logged step\n",
    "            total_loss += loss.detach().float()\n",
    "            logps['logps_policy_chosen'] += policy_chosen_logps.detach().float()\n",
    "            logps['logps_policy_rejected'] += policy_rejected_logps.detach().float()\n",
    "            logps['logps_reference_chosen'] += reference_chosen_logps.detach().float()\n",
    "            logps['logps_reference_rejected'] += reference_rejected_logps.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            # clip gradient norm. don't do this with deepspeed\n",
    "            if accelerator.sync_gradients and args.clip_grad_norm > 0:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            if args.logging_steps and completed_steps % args.logging_steps == 0:\n",
    "                avg_loss = accelerator.gather(total_loss).mean().item() / args.gradient_accumulation_steps / args.logging_steps\n",
    "                for k in list(logps.keys()):\n",
    "                    logps[k] = accelerator.gather(logps[k]).mean().item() / args.gradient_accumulation_steps                \n",
    "                logger.info(f\"  Step: {completed_steps}, LR: {lr_scheduler.get_last_lr()[0]}, Loss: {avg_loss}\")\n",
    "                if args.with_tracking:\n",
    "                    accelerator.log(\n",
    "                        {\n",
    "                            \"learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "                            \"train_loss\": avg_loss,\n",
    "                            \"step\": completed_steps,\n",
    "                            'logps_policy_chosen': logps['logps_policy_chosen'],\n",
    "                            'logps_policy_rejected': logps['logps_policy_rejected'],\n",
    "                            'logps_reference_chosen': logps['logps_reference_chosen'],\n",
    "                            'logps_reference_rejected': logps['logps_reference_rejected'],\n",
    "                        },\n",
    "                        step=completed_steps,\n",
    "                    )\n",
    "                total_loss = 0\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                if completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    if args.output_dir is not None:\n",
    "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    accelerator.save_state(output_dir)\n",
    "                    clean_checkpoints(args.output_dir, keep_n=1)\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "        break\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        saccelerator.save_state(output_dir)\n",
    "        clean_checkpoints(args.output_dir, keep_n=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d05f4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0004a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in results/jpt_pythia-70m_ultrafeedback/tokenizer_config.json\n",
      "Special tokens file saved in results/jpt_pythia-70m_ultrafeedback/special_tokens_map.json\n",
      "Configuration saved in results/jpt_pythia-70m_ultrafeedback/config.json\n",
      "Configuration saved in results/jpt_pythia-70m_ultrafeedback/generation_config.json\n",
      "Model weights saved in results/jpt_pythia-70m_ultrafeedback/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "save_with_accelerate(accelerator, model, tokenizer, args.output_dir, args)\n",
    "clean_checkpoints(args.output_dir, keep_n=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf526bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3]\n",
    "l[:len(l)-1]\n",
    "\n",
    "l[len(l)-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab886125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = batch['chosen_input_ids']\n",
    "attention_mask = batch['chosen_attention_mask']\n",
    "labels=batch['chosen_labels']\n",
    "\n",
    "chosen_output = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "752de046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5102], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([-1108.4854], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor(1.5102, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from dpo_utils import _get_batch_logps\n",
    "\n",
    "# (bsz, seq_len, vocab)\n",
    "all_logps = _get_batch_logps(chosen_output.logits, labels, average_log_prob=True)\n",
    "print(all_logps)\n",
    "all_logps = _get_batch_logps(chosen_output.logits, labels, average_log_prob=False)\n",
    "print(all_logps)\n",
    "print(chosen_output['loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52919caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1108.4854], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chosen_output['loss'] * (batch['chosen_labels'][:, 1:] != 100).sum(-1)\n",
    "\n",
    "\n",
    "# labels.shape, chosen_output.logits.shape\n",
    "\n",
    "chosen_output['loss'] * (labels[:, 1:] != -100).sum(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39e29905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(759, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29e3d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpo_utils import concatenated_inputs\n",
    "\n",
    "concatenated_batch = concatenated_inputs(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54d8425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chosen_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"chosen_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    },\n",
      "    \"rejected_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            1,\n",
      "            170\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"concatenated_input_ids\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"concatenated_labels\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    },\n",
      "    \"concatenated_attention_mask\": {\n",
      "        \"device\": \"cuda:0\",\n",
      "        \"shape\": [\n",
      "            2,\n",
      "            180\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in batch.items()}, indent=4))\n",
    "print(json.dumps({k: {'device': str(v.device), 'shape': list(v.shape)} for k, v in concatenated_batch.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0258897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3264.91552734375 -480.8586120605469\n",
      "-3264.8359375 -480.8492431640625\n",
      "0.6966567635536194\n"
     ]
    }
   ],
   "source": [
    "print(policy_chosen_logps.item(), policy_rejected_logps.item())\n",
    "print(reference_chosen_logps.item(), reference_rejected_logps.item())\n",
    "print(losses.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8767b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-3264.8359], device='cuda:0'), tensor([-480.8492], device='cuda:0'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc1c0c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['chosen_input_ids'].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
