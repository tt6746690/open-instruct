{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49217907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 18 01:34:41 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:C4:00.0 Off |                    0 |\r\n",
      "| N/A   33C    P0              62W / 400W |      4MiB / 81920MiB |      0%   E. Process |\r\n",
      "|                                         |                      |             Disabled |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e0a074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9f45cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a3503f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shlex \n",
    "import re\n",
    "\n",
    "import os\n",
    "from note_pruning_analysis import open_instruct_dir\n",
    "\n",
    "\n",
    "def multiline_to_singleline(cmd):\n",
    "    cmd = cmd.strip()\n",
    "    cmd = re.sub(r'\\\\(?![$])', '', cmd) # replace all '\\' but not '\\$'\n",
    "    cmd = cmd.replace('\\n', '')\n",
    "    cmd = re.sub(' +', ' ', cmd)\n",
    "    cmd = cmd.strip()\n",
    "    return cmd\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "\n",
    "torch_dtype = 'bfloat16'\n",
    "devices = os.environ['CUDA_VISIBLE_DEVICES']\n",
    "model_name_or_path = '/dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10'\n",
    "\n",
    "task_name = 'mtbench_ann=gpt:3.5:turbo:1106'\n",
    "\n",
    "\n",
    "\n",
    "match = re.search(r'ann=([^_]+)', task_name)\n",
    "judge_model = match.group(1).replace(':', '-')\n",
    "from fastchat.model.model_adapter import OPENAI_MODEL_LIST\n",
    "if not judge_model in OPENAI_MODEL_LIST: # \"gpt-4\", \"gpt-3.5-turbo-1106\", \"gpt-4-turbo\"\n",
    "    raise ValueError('fastchat does not support the judge model.')\n",
    "save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_id = 'tulu' # for using tulu's template\n",
    "fastchat_mtbench_data_dir = os.path.normpath(\n",
    "    os.path.join(open_instruct_dir, '../FastChat/fastchat/llm_judge/data'))\n",
    "question_file = os.path.join(fastchat_mtbench_data_dir, 'mt_bench/question.jsonl')\n",
    "answer_file = os.path.join(save_dir, 'model_answer.jsonl')\n",
    "rating_file = os.path.join(save_dir, f'{judge_model}_single.jsonl')\n",
    "\n",
    "question_begin, question_end = (0, 1) if test_run else (None, None)\n",
    "question_begin = 0; question_end = 1\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    python -m fastchat.llm_judge.gen_model_answer \\\n",
    "        --model-path {model_name_or_path} \\\n",
    "        --model-id {model_id} \\\n",
    "        --bench-name mt_bench \\\n",
    "        --question-file {question_file} \\\n",
    "        {'--question-begin '+str(question_begin) if question_begin else ''} \\\n",
    "        {'--question-end '+str(question_end) if question_end else ''} \\\n",
    "        --max-new-token 2048 \\\n",
    "        --answer-file {answer_file} \\\n",
    "        --dtype {torch_dtype} \\\n",
    "    && \\\n",
    "    python -m fastchat.llm_judge.gen_judgment \\\n",
    "        --bench-name mt_bench \\\n",
    "        --judge-file {os.path.join(fastchat_mtbench_data_dir, 'judge_prompts.jsonl')} \\\n",
    "        --judge-model {judge_model} \\\n",
    "        --mode single \\\n",
    "        --question-file {question_file} \\\n",
    "        --answer-dir {save_dir} \\\n",
    "        --ref-answer-dir {os.path.join(fastchat_mtbench_data_dir, 'mt_bench/reference_answer')} \\\n",
    "        --output-file {rating_file} \\\n",
    "        --first-n 1 \\\n",
    "    && \\\n",
    "    python -m fastchat.llm_judge.show_result \\\n",
    "        --bench-name mt_bench \\\n",
    "        --input-file {rating_file} \\\n",
    "        --mode single \\\n",
    "        --save-to-json\n",
    "\"\"\"\n",
    "# cmd = 'echo hi && echo foo'\n",
    "cmd = multiline_to_singleline(cmd)\n",
    "# cmd = shlex.split(cmd)\n",
    "cmds = [cmd]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e331999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4f76bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_mtbench.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[-1]\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d08f4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\r\n",
      "set -x\r\n",
      "CUDA_VISIBLE_DEVICES=0 python -m fastchat.llm_judge.gen_model_answer --model-path /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10 --model-id tulu --bench-name mt_bench --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --question-end 1 --max-new-token 2048 --answer-file /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/model_answer.jsonl --dtype bfloat16 && python -m fastchat.llm_judge.gen_judgment --bench-name mt_bench --judge-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl --judge-model gpt-3.5-turbo-1106 --mode single --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --answer-dir /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106 --ref-answer-dir /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer --output-file /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/gpt-3.5-turbo-1106_single.jsonl --first-n 1 && python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/gpt-3.5-turbo-1106_single.jsonl --mode single --save-to-json"
     ]
    }
   ],
   "source": [
    "!cat eval_mtbench.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44d88d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/gpt-3.5-turbo-1106_single.jsonl --mode single --save-to-json\n",
      "Mode: single\n",
      "Input file: /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/gpt-3.5-turbo-1106_single.jsonl\n",
      "\n",
      "########## First turn ##########\n",
      "                   score\n",
      "model        turn       \n",
      "model_answer 1       7.0\n",
      "\n",
      "########## Second turn ##########\n",
      "                   score\n",
      "model        turn       \n",
      "model_answer 2       8.0\n",
      "\n",
      "########## Average ##########\n",
      "              score\n",
      "model              \n",
      "model_answer    7.5\n",
      "Input file: /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/gpt-3.5-turbo-1106_single.jsonl\n",
      "Save metrics to \n",
      "\t/dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/metrics.json\n"
     ]
    }
   ],
   "source": [
    "!bash eval_mtbench.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "68f06c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106/gpt-3.5-turbo-1106_single.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model</th>\n",
       "      <th>judge</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>judgment</th>\n",
       "      <th>score</th>\n",
       "      <th>turn</th>\n",
       "      <th>tstamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>model_answer</td>\n",
       "      <td>[gpt-3.5-turbo-1106, single-v1]</td>\n",
       "      <td>[Instruction]\\nPlease act as an impartial judg...</td>\n",
       "      <td>The response provides an engaging travel blog ...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.705565e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>model_answer</td>\n",
       "      <td>[gpt-3.5-turbo-1106, single-v1-multi-turn]</td>\n",
       "      <td>&lt;|The Start of Assistant A's Conversation with...</td>\n",
       "      <td>The response provided by Assistant A is well-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.705565e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id         model                                       judge  \\\n",
       "0           81  model_answer             [gpt-3.5-turbo-1106, single-v1]   \n",
       "1           81  model_answer  [gpt-3.5-turbo-1106, single-v1-multi-turn]   \n",
       "\n",
       "                                         user_prompt  \\\n",
       "0  [Instruction]\\nPlease act as an impartial judg...   \n",
       "1  <|The Start of Assistant A's Conversation with...   \n",
       "\n",
       "                                            judgment  score  turn  \\\n",
       "0  The response provides an engaging travel blog ...      7     1   \n",
       "1  The response provided by Assistant A is well-s...      8     2   \n",
       "\n",
       "         tstamp  \n",
       "0  1.705565e+09  \n",
       "1  1.705565e+09  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_file = rating_file\n",
    "\n",
    "\n",
    "print(f\"Input file: {input_file}\")\n",
    "df_all = pd.read_json(input_file, lines=True)\n",
    "df = df_all[[\"model\", \"score\", \"turn\"]]\n",
    "df = df[df[\"score\"] != -1]\n",
    "df_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13a91966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_answer</th>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score\n",
       "model              \n",
       "model_answer    7.5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"model\", \"score\"]].groupby([\"model\"]).mean().iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60c5568b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(rating_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c88a3a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 02:06:58,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Output to data/mt_bench/model_answer/tulu.jsonl\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:39<00:00, 13.04s/it]\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 10/10 [01:42<00:00, 10.30s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "!cd ../../FastChat/fastchat/llm_judge && CUDA_VISIBLE_DEVICES=0 python gen_model_answer.py \\\n",
    "    --model-path /dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3 \\\n",
    "    --model-id tulu \\\n",
    "    --question-begin 0 \\\n",
    "    --question-end 10 \\\n",
    "    --max-new-token 2048 \\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5960bf5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 01:36:43,721] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Stats:\n",
      "{\n",
      "    \"bench_name\": \"mt_bench\",\n",
      "    \"mode\": \"single\",\n",
      "    \"judge\": \"gpt-4\",\n",
      "    \"baseline\": null,\n",
      "    \"model_list\": [\n",
      "        \"tulu\"\n",
      "    ],\n",
      "    \"total_num_questions\": 1,\n",
      "    \"total_num_matches\": 2,\n",
      "    \"output_path\": \"data/mt_bench/model_judgment/gpt-4_single.jsonl\"\n",
      "}\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]question: 81, turn: 1, model: tulu, score: 5, judge: ('gpt-4', 'single-v1')\n",
      " 50%|██████████████████████▌                      | 1/2 [00:15<00:15, 15.89s/it]question: 81, turn: 2, model: tulu, score: 1, judge: ('gpt-4', 'single-v1-multi-turn')\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:24<00:00, 12.12s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd ../../FastChat/fastchat/llm_judge && \\\n",
    "    python gen_judgment.py --model-list tulu --parallel 1 --first-n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "badc207d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'hi'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastchat.llm_judge.common import Judge\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "model = 'gpt-3.5-turbo-1106'\n",
    "judge = Judge(\n",
    "        model, judge_prompts[\"single-v1-multi-turn\"], multi_turn=True\n",
    "    )\n",
    "\n",
    "conv = get_conversation_template(model)\n",
    "conv.append_message('user', 'hi')\n",
    "conv.to_openai_api_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034e3e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "\n",
      "[Question]\n",
      "Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n",
      "\n",
      "[The Start of Assistant's Answer]\n",
      "Hawaii is a magical place full of natural beauty and unique cultural experiences. From lush rainforests and verdant peaks to pristine beaches and thriving coral reefs, the islands of Hawaii are a wonder to behold. Travelers can explore the island's diverse ecosystems, learn about the local history and culture, and take in the breathtaking views that make Hawaii such a special place. Here are some of the must-see attractions on the islands of Hawaii, and some of the unique cultural experiences visitors should be sure to check out: \n",
      "\n",
      "1. Visit the iconic Mauna Kea Volcano, a dormant volcano that is one of the tallest mountains in the world. Take a hike up the mountain to explore the lush landscape, or simply admire the views of the Pacific Ocean from the summit.\n",
      "\n",
      "2. Take a trip to the famous Waikiki Beach and explore the beautiful beaches and bustling shops.\n",
      "\n",
      "3. Visit the historic Kauai Lighthouse and take in the stunning views of the Pacific Ocean.\n",
      "\n",
      "4. Visit the famous I\n",
      "[The End of Assistant's Answer]\n"
     ]
    }
   ],
   "source": [
    "print(\"[Instruction]\\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\\n\\n[Question]\\nCompose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\\n\\n[The Start of Assistant's Answer]\\nHawaii is a magical place full of natural beauty and unique cultural experiences. From lush rainforests and verdant peaks to pristine beaches and thriving coral reefs, the islands of Hawaii are a wonder to behold. Travelers can explore the island's diverse ecosystems, learn about the local history and culture, and take in the breathtaking views that make Hawaii such a special place. Here are some of the must-see attractions on the islands of Hawaii, and some of the unique cultural experiences visitors should be sure to check out: \\n\\n1. Visit the iconic Mauna Kea Volcano, a dormant volcano that is one of the tallest mountains in the world. Take a hike up the mountain to explore the lush landscape, or simply admire the views of the Pacific Ocean from the summit.\\n\\n2. Take a trip to the famous Waikiki Beach and explore the beautiful beaches and bustling shops.\\n\\n3. Visit the historic Kauai Lighthouse and take in the stunning views of the Pacific Ocean.\\n\\n4. Visit the famous I\\n[The End of Assistant's Answer]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185e403d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|The Start of Assistant A's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n",
      "\n",
      "### Assistant A:\n",
      "Hawaii is a magical place full of natural beauty and unique cultural experiences. From lush rainforests and verdant peaks to pristine beaches and thriving coral reefs, the islands of Hawaii are a wonder to behold. Travelers can explore the island's diverse ecosystems, learn about the local history and culture, and take in the breathtaking views that make Hawaii such a special place. Here are some of the must-see attractions on the islands of Hawaii, and some of the unique cultural experiences visitors should be sure to check out: \n",
      "\n",
      "1. Visit the iconic Mauna Kea Volcano, a dormant volcano that is one of the tallest mountains in the world. Take a hike up the mountain to explore the lush landscape, or simply admire the views of the Pacific Ocean from the summit.\n",
      "\n",
      "2. Take a trip to the famous Waikiki Beach and explore the beautiful beaches and bustling shops.\n",
      "\n",
      "3. Visit the historic Kauai Lighthouse and take in the stunning views of the Pacific Ocean.\n",
      "\n",
      "4. Visit the famous I\n",
      "\n",
      "### User:\n",
      "Rewrite your previous response. Start every sentence with the letter A.\n",
      "\n",
      "### Assistant A:\n",
      "A Hawaii is a magical place full of natural beauty and unique cultural experiences. From lush rainforests and verdant peaks to pristine beaches and thriving coral reefs, the islands of Hawaii are a wonder to behold. Travelers can explore the island's diverse ecosystems, learn about the local history and culture, and take in the breathtaking views that make Hawaii such a special place. Here are some of the must-see attractions on the islands of Hawaii, and some of the unique cultural experiences visitors should be sure to check out: \n",
      "\n",
      "1. Visit the iconic Mauna Kea Volcano, a dormant volcano that is one of the tallest mountains in the world. Take a hike up the mountain to explore the lush landscape, or simply admire the views of the Pacific Ocean from the summit.\n",
      "\n",
      "2. Take a trip to the famous Waikiki Beach and explore the beautiful beaches and bustling shops.\n",
      "\n",
      "3. Visit the historic Kauai Lighthouse and take in the stunning views of the Pacific Ocean.\n",
      "\n",
      "4. Visit the famous Iol\n",
      "\n",
      "<|The End of Assistant A's Conversation with User|>\n"
     ]
    }
   ],
   "source": [
    "print(\"<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\nCompose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\\n\\n### Assistant A:\\nHawaii is a magical place full of natural beauty and unique cultural experiences. From lush rainforests and verdant peaks to pristine beaches and thriving coral reefs, the islands of Hawaii are a wonder to behold. Travelers can explore the island's diverse ecosystems, learn about the local history and culture, and take in the breathtaking views that make Hawaii such a special place. Here are some of the must-see attractions on the islands of Hawaii, and some of the unique cultural experiences visitors should be sure to check out: \\n\\n1. Visit the iconic Mauna Kea Volcano, a dormant volcano that is one of the tallest mountains in the world. Take a hike up the mountain to explore the lush landscape, or simply admire the views of the Pacific Ocean from the summit.\\n\\n2. Take a trip to the famous Waikiki Beach and explore the beautiful beaches and bustling shops.\\n\\n3. Visit the historic Kauai Lighthouse and take in the stunning views of the Pacific Ocean.\\n\\n4. Visit the famous I\\n\\n### User:\\nRewrite your previous response. Start every sentence with the letter A.\\n\\n### Assistant A:\\nA Hawaii is a magical place full of natural beauty and unique cultural experiences. From lush rainforests and verdant peaks to pristine beaches and thriving coral reefs, the islands of Hawaii are a wonder to behold. Travelers can explore the island's diverse ecosystems, learn about the local history and culture, and take in the breathtaking views that make Hawaii such a special place. Here are some of the must-see attractions on the islands of Hawaii, and some of the unique cultural experiences visitors should be sure to check out: \\n\\n1. Visit the iconic Mauna Kea Volcano, a dormant volcano that is one of the tallest mountains in the world. Take a hike up the mountain to explore the lush landscape, or simply admire the views of the Pacific Ocean from the summit.\\n\\n2. Take a trip to the famous Waikiki Beach and explore the beautiful beaches and bustling shops.\\n\\n3. Visit the historic Kauai Lighthouse and take in the stunning views of the Pacific Ocean.\\n\\n4. Visit the famous Iol\\n\\n<|The End of Assistant A's Conversation with User|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d86f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 01:42:45,418] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pair-v2': {'name': 'pair-v2',\n",
       "  'type': 'pairwise',\n",
       "  'system_prompt': 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.',\n",
       "  'prompt_template': \"[User Question]\\n{question}\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\",\n",
       "  'description': 'Prompt for general questions',\n",
       "  'category': 'general',\n",
       "  'output_format': '[[A]]'},\n",
       " 'pair-v2-multi-turn': {'name': 'pair-v2-multi-turn',\n",
       "  'type': 'pairwise',\n",
       "  'system_prompt': 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.',\n",
       "  'prompt_template': \"<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant A:\\n{answer_a_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant A:\\n{answer_a_2}\\n\\n<|The End of Assistant A's Conversation with User|>\\n\\n\\n<|The Start of Assistant B's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant B:\\n{answer_b_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant B:\\n{answer_b_2}\\n\\n<|The End of Assistant B's Conversation with User|>\",\n",
       "  'description': 'Prompt for multi-turn general questions',\n",
       "  'category': 'general',\n",
       "  'output_format': '[[A]]'},\n",
       " 'pair-math-v1': {'name': 'pair-math-v1',\n",
       "  'type': 'pairwise',\n",
       "  'system_prompt': 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A\\'s answer, and assistant B\\'s answer. Your job is to evaluate which assistant\\'s answer is better. Begin your evaluation by comparing both assistants\\' answers with the reference answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.',\n",
       "  'prompt_template': \"[User Question]\\n{question}\\n\\n[The Start of Reference Answer]\\n{ref_answer_1}\\n[The End of Reference Answer]\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\",\n",
       "  'description': 'Prompt for math questions',\n",
       "  'category': 'math',\n",
       "  'output_format': '[[A]]'},\n",
       " 'pair-math-v1-multi-turn': {'name': 'pair-math-v1-multi-turn',\n",
       "  'type': 'pairwise',\n",
       "  'system_prompt': 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. Your evaluation should consider correctness and helpfulness. You will be given reference answers, the assistant A\\'s answers, the assistant B\\'s answers. Your job is to determine which assistant provides correct and helpful answers to the second user question. Begin your evaluation by comparing both assistants\\' answers with the reference answers. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.',\n",
       "  'prompt_template': \"<|The Start of Reference Answer|>\\n\\n### User:\\n{question_1}\\n\\n### Reference answer:\\n{ref_answer_1}\\n\\n### User:\\n{question_2}\\n\\n### Reference answer:\\n{ref_answer_2}\\n\\n<|The End of Reference Answer|>\\n\\n\\n<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant A:\\n{answer_a_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant A:\\n{answer_a_2}\\n\\n<|The End of Assistant A's Conversation with User|>\\n\\n\\n<|The Start of Assistant B's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant B:\\n{answer_b_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant B:\\n{answer_b_2}\\n\\n<|The End of Assistant B's Conversation with User|>\",\n",
       "  'description': 'Prompt for multi-turn general questions',\n",
       "  'category': 'general',\n",
       "  'output_format': '[[A]]'},\n",
       " 'single-v1': {'name': 'single-v1',\n",
       "  'type': 'single',\n",
       "  'system_prompt': 'You are a helpful assistant.',\n",
       "  'prompt_template': '[Instruction]\\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant\\'s Answer]\\n{answer}\\n[The End of Assistant\\'s Answer]',\n",
       "  'description': 'Prompt for general questions',\n",
       "  'category': 'general',\n",
       "  'output_format': '[[rating]]'},\n",
       " 'single-math-v1': {'name': 'single-math-v1',\n",
       "  'type': 'single',\n",
       "  'system_prompt': 'You are a helpful assistant.',\n",
       "  'prompt_template': '[Instruction]\\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant\\'s answer. Begin your evaluation by comparing the assistant\\'s answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\n\\n[Question]\\n{question}\\n\\n[The Start of Reference Answer]\\n{ref_answer_1}\\n[The End of Reference Answer]\\n\\n[The Start of Assistant\\'s Answer]\\n{answer}\\n[The End of Assistant\\'s Answer]',\n",
       "  'description': 'Prompt for general questions',\n",
       "  'category': 'math',\n",
       "  'output_format': '[[rating]]'},\n",
       " 'single-v1-multi-turn': {'name': 'single-v1-multi-turn',\n",
       "  'type': 'single',\n",
       "  'system_prompt': 'Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You evaluation should focus on the assistant\\'s answer to the second user question. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\n\\n',\n",
       "  'prompt_template': \"<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant A:\\n{answer_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant A:\\n{answer_2}\\n\\n<|The End of Assistant A's Conversation with User|>\",\n",
       "  'description': 'Prompt for general questions',\n",
       "  'category': 'general',\n",
       "  'output_format': '[[rating]]'},\n",
       " 'single-math-v1-multi-turn': {'name': 'single-math-v1-multi-turn',\n",
       "  'type': 'single',\n",
       "  'system_prompt': 'Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant\\'s answer. You evaluation should focus on the assistant\\'s answer to the second question. Begin your evaluation by comparing the assistant\\'s answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\n\\n',\n",
       "  'prompt_template': \"<|The Start of Reference Answer|>\\n\\n### User:\\n{question_1}\\n\\n### Reference answer:\\n{ref_answer_1}\\n\\n### User:\\n{question_2}\\n\\n### Reference answer:\\n{ref_answer_2}\\n\\n<|The End of Reference Answer|>\\n\\n\\n<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant A:\\n{answer_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant A:\\n{answer_2}\\n\\n<|The End of Assistant A's Conversation with User|>\",\n",
       "  'description': 'Prompt for general questions',\n",
       "  'category': 'math',\n",
       "  'output_format': '[[rating]]'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastchat.llm_judge.common import load_judge_prompts\n",
    "\n",
    "judge_file = '/dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl'\n",
    "judge_prompts = load_judge_prompts(judge_file)\n",
    "judge_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8636e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|The Start of Assistant A's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "{question_1}\n",
      "\n",
      "### Assistant A:\n",
      "{answer_1}\n",
      "\n",
      "### User:\n",
      "{question_2}\n",
      "\n",
      "### Assistant A:\n",
      "{answer_2}\n",
      "\n",
      "<|The End of Assistant A's Conversation with User|>\n"
     ]
    }
   ],
   "source": [
    "print(judge_prompts['single-v1-multi-turn']['prompt_template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92c3078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/data-pruning/wpq/github/mitibm2023/external/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26c6d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from note_pruning_analysis import open_instruct_dir\n",
    "\n",
    "\n",
    "question_file = os.path.normpath(os.path.join(open_instruct_dir, '../FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl'))\n",
    "question_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae870397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     24\u001b[0m answer_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m'\u001b[39m: model_path,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m'\u001b[39m: model_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m }\n\u001b[0;32m---> 44\u001b[0m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[k] \u001b[38;5;241m=\u001b[39m v\n",
      "File \u001b[0;32m/dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/gen_model_answer.py:55\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(model_path, model_id, question_file, question_begin, question_end, answer_file, max_new_token, num_choices, num_gpus_per_model, num_gpus_total, max_gpu_memory, dtype, revision)\u001b[0m\n\u001b[1;32m     52\u001b[0m ans_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(questions), chunk_size):\n\u001b[1;32m     54\u001b[0m     ans_handles\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 55\u001b[0m         \u001b[43mget_answers_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43manswer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_choices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_gpus_per_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_gpu_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_ray:\n\u001b[1;32m     70\u001b[0m     ray\u001b[38;5;241m.\u001b[39mget(ans_handles)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/gen_model_answer.py:86\u001b[0m, in \u001b[0;36mget_model_answers\u001b[0;34m(model_path, model_id, questions, answer_file, max_new_token, num_choices, num_gpus_per_model, max_gpu_memory, dtype, revision)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_answers\u001b[39m(\n\u001b[1;32m     75\u001b[0m     model_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     revision,\n\u001b[1;32m     85\u001b[0m ):\n\u001b[0;32m---> 86\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus_per_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_gpu_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gpu_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offloading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tqdm(questions):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m question[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m temperature_config:\n",
      "File \u001b[0;32m/dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/model/model_adapter.py:362\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, device, num_gpus, max_gpu_memory, dtype, load_8bit, cpu_offloading, gptq_config, awq_config, exllama_config, xft_config, revision, debug)\u001b[0m\n\u001b[1;32m    355\u001b[0m     model \u001b[38;5;241m=\u001b[39m ipex\u001b[38;5;241m.\u001b[39moptimize(model, dtype\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_gpus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu_offloading) \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    365\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39moptimize(model, dtype\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/dccstor/data-pruning/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from fastchat.llm_judge.common import load_questions, temperature_config\n",
    "from fastchat.model import load_model, get_conversation_template\n",
    "from fastchat.llm_judge.gen_model_answer import get_model_answers, run_eval\n",
    "\n",
    "\n",
    "\n",
    "# model_id = 'llama-7b'\n",
    "# model_path = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/huggyllama/llama-7b'\n",
    "model_id = 'tulu'\n",
    "model_id = 'zephyr-7b'\n",
    "model_path = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/results/baselines/HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "\n",
    "model_id = 'tulu'\n",
    "model_path = '/dccstor/data-pruning/results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=30000:ep=3'\n",
    "\n",
    "\n",
    "torch_dtype = 'float16'\n",
    "torch_dtype = 'bfloat16'\n",
    "# question_file = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl'\n",
    "\n",
    "\n",
    "answer_file = f'{model_id}.jsonl'\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    'model_path': model_path,\n",
    "    'model_id': model_id,\n",
    "    'question_file': question_file,\n",
    "    'question_begin': 0,\n",
    "    'question_end': 1,\n",
    "    'answer_file': answer_file,\n",
    "    'max_new_token': 256,\n",
    "    'num_choices': 1,\n",
    "    'num_gpus_per_model': 1,\n",
    "    'num_gpus_total': 1,\n",
    "    'max_gpu_memory': None,\n",
    "    'dtype': getattr(torch, torch_dtype),\n",
    "    'revision': 'main',\n",
    "}\n",
    "\n",
    "\n",
    "run_eval(**kwargs)\n",
    "\n",
    "for k, v in kwargs.items():\n",
    "    globals()[k] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c28763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# questions = load_questions(question_file, question_begin, question_end)\n",
    "questions = load_questions(question_file, None, None)\n",
    "# # random shuffle the questions to balance the loading\n",
    "# random.shuffle(questions)\n",
    "questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95eb043a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'writing': 10,\n",
       "         'roleplay': 10,\n",
       "         'reasoning': 10,\n",
       "         'math': 10,\n",
       "         'coding': 10,\n",
       "         'extraction': 10,\n",
       "         'stem': 10,\n",
       "         'humanities': 10})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([x['category'] for x in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf17619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the question file into `num_gpus` files\n",
    "assert num_gpus_total % num_gpus_per_model == 0\n",
    "use_ray = num_gpus_total // num_gpus_per_model > 1\n",
    "\n",
    "if use_ray:\n",
    "    get_answers_func = ray.remote(num_gpus=num_gpus_per_model)(\n",
    "        get_model_answers\n",
    "    ).remote\n",
    "else:\n",
    "    get_answers_func = get_model_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ea600d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = len(questions) // (num_gpus_total // num_gpus_per_model)\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c3756d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351d48c240934c219171ee852c9b5d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 31.75 GiB total capacity; 1002.02 MiB already allocated; 441.88 MiB free; 1020.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus_per_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gpu_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gpu_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu_offloading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/FastChat/fastchat/model/model_adapter.py:362\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, device, num_gpus, max_gpu_memory, dtype, load_8bit, cpu_offloading, gptq_config, awq_config, exllama_config, xft_config, revision, debug)\u001b[0m\n\u001b[1;32m    355\u001b[0m     model \u001b[38;5;241m=\u001b[39m ipex\u001b[38;5;241m.\u001b[39moptimize(model, dtype\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_gpus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu_offloading) \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    365\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39moptimize(model, dtype\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 31.75 GiB total capacity; 1002.02 MiB already allocated; 441.88 MiB free; 1020.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    model_path,\n",
    "    revision=revision,\n",
    "    device=\"cuda\",\n",
    "    num_gpus=num_gpus_per_model,\n",
    "    max_gpu_memory=max_gpu_memory,\n",
    "    dtype=dtype,\n",
    "    load_8bit=False,\n",
    "    cpu_offloading=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    if question[\"category\"] in temperature_config:\n",
    "        temperature = temperature_config[question[\"category\"]]\n",
    "    else:\n",
    "        temperature = 0.7\n",
    "\n",
    "    choices = []\n",
    "    for i in range(num_choices):\n",
    "        torch.manual_seed(i)\n",
    "        conv = get_conversation_template(model_id)\n",
    "        turns = []\n",
    "        for j in range(len(question[\"turns\"])):\n",
    "            qs = question[\"turns\"][j]\n",
    "            conv.append_message(conv.roles[0], qs)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            input_ids = tokenizer([prompt]).input_ids\n",
    "\n",
    "            if temperature < 1e-4:\n",
    "                do_sample = False\n",
    "            else:\n",
    "                do_sample = True\n",
    "\n",
    "            # some models may error out when generating long outputs\n",
    "            try:\n",
    "                output_ids = model.generate(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    do_sample=do_sample,\n",
    "                    temperature=temperature,\n",
    "                    max_new_tokens=max_new_token,\n",
    "                )\n",
    "                if model.config.is_encoder_decoder:\n",
    "                    output_ids = output_ids[0]\n",
    "                else:\n",
    "                    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "\n",
    "                # be consistent with the template's stop_token_ids\n",
    "                if conv.stop_token_ids:\n",
    "                    stop_token_ids_index = [\n",
    "                        i\n",
    "                        for i, id in enumerate(output_ids)\n",
    "                        if id in conv.stop_token_ids\n",
    "                    ]\n",
    "                    if len(stop_token_ids_index) > 0:\n",
    "                        output_ids = output_ids[: stop_token_ids_index[0]]\n",
    "\n",
    "                output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "                if conv.stop_str and isinstance(conv.stop_str, list):\n",
    "                    stop_str_indices = sorted(\n",
    "                        [\n",
    "                            output.find(stop_str)\n",
    "                            for stop_str in conv.stop_str\n",
    "                            if output.find(stop_str) > 0\n",
    "                        ]\n",
    "                    )\n",
    "                    if len(stop_str_indices) > 0:\n",
    "                        output = output[: stop_str_indices[0]]\n",
    "                elif conv.stop_str and output.find(conv.stop_str) > 0:\n",
    "                    output = output[: output.find(conv.stop_str)]\n",
    "\n",
    "                for special_token in tokenizer.special_tokens_map.values():\n",
    "                    if isinstance(special_token, list):\n",
    "                        for special_tok in special_token:\n",
    "                            output = output.replace(special_tok, \"\")\n",
    "                    else:\n",
    "                        output = output.replace(special_token, \"\")\n",
    "                output = output.strip()\n",
    "\n",
    "                if conv.name == \"xgen\" and output.startswith(\"Assistant:\"):\n",
    "                    output = output.replace(\"Assistant:\", \"\", 1).strip()\n",
    "            except RuntimeError as e:\n",
    "                print(\"ERROR question ID: \", question[\"question_id\"])\n",
    "                output = \"ERROR\"\n",
    "\n",
    "            conv.update_last_message(output)\n",
    "            turns.append(output)\n",
    "\n",
    "        choices.append({\"index\": i, \"turns\": turns})\n",
    "\n",
    "#     # Dump answers\n",
    "#     os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n",
    "#     with open(os.path.expanduser(answer_file), \"a\") as fout:\n",
    "#         ans_json = {\n",
    "#             \"question_id\": question[\"question_id\"],\n",
    "#             \"answer_id\": shortuuid.uuid(),\n",
    "#             \"model_id\": model_id,\n",
    "#             \"choices\": choices,\n",
    "#             \"tstamp\": time.time(),\n",
    "#         }\n",
    "#         fout.write(json.dumps(ans_json) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
