{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf2dd69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/minoconda3_x86/envs/open-instruct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4\n",
      "4\n",
      "Tue Nov 21 00:20:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    56W / 300W |   4960MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    66W / 300W |   2974MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    71W / 300W |   2958MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    69W / 300W |   1272MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    42W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    68W / 300W |   2954MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   42C    P0   161W / 300W |   2978MiB / 32510MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   45C    P0   144W / 300W |   2960MiB / 32510MiB |     97%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     15256      C   .../open-instruct/bin/python     4957MiB |\n",
      "|    1   N/A  N/A     43523      C   ...r-3.1.2-linux-x64/blender     2971MiB |\n",
      "|    2   N/A  N/A     43570      C   ...r-3.1.2-linux-x64/blender     2955MiB |\n",
      "|    3   N/A  N/A     38901      C   ...r-3.1.2-linux-x64/blender     1269MiB |\n",
      "|    5   N/A  N/A     61085      C   ...r-3.1.2-linux-x64/blender     2951MiB |\n",
      "|    6   N/A  N/A     24626      C   ...r-3.1.2-linux-x64/blender     2975MiB |\n",
      "|    7   N/A  N/A     43440      C   ...r-3.1.2-linux-x64/blender     2957MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1]\n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf384bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from rosemary import jpt_in_notebook\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "# model_name = 'llama-7b'; encode_fn_type = 'sft'; md = 'llama7b'\n",
    "# model_name = 'llama-7b+lora:r=256:a=256'; encode_fn_type = 'sft'; md = 'llama7b'\n",
    "model_name = 'mistral-7b+lora:r=256:a=256'; encode_fn_type = 'sft'; md = 'mistral7b'\n",
    "# model_name = 'all-mpnet-base-v2'; encode_fn_type = 'input'; md = 'mpnet'\n",
    "# model_name = 'bge-large-en-v1.5'; encode_fn_type = 'input'; md = 'bge'\n",
    "\n",
    "# nc_list = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "nc_list = [100, 500, 1000]\n",
    "nc_list = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "sort_by_list = [\n",
    "    f'semdedup_cl=kmeansfaisscd_md={md}_dist=cd_emb={emb}_nc={nc}'\n",
    "    for nc in nc_list\n",
    "    for emb in ['text+embedding', 'grad+rp+loraB'] # 'grad+rp+loraB' \n",
    "]\n",
    "\n",
    "# sort_by_list = [\n",
    "#     'random_s=0', 'random_s=1', 'random_s=2',\n",
    "#     'log_prob', 'logit_margin', 'el2n_agg=mean',\n",
    "#     'grad_loraB_l2n', 'numtoks'\n",
    "# ]\n",
    "# sort_by_list += [\n",
    "# #     'kmeansl2_emb=grad+rp+loraB_nc=300',\n",
    "# #     'kmeansl2_emb=grad+rp+loraB_nc=1000',\n",
    "#     'kmeansl2_emb=grad+rp+loraB_nc=3000',\n",
    "# #     'kmeansl2_emb=text+embedding_nc=300',\n",
    "#     'kmeansl2_emb=text+embedding_nc=1000',\n",
    "# #     'kmeansl2_emb=text+embedding_nc=3000',\n",
    "# ]\n",
    "# sort_by_list = ['numtoks']\n",
    "\n",
    "# sort_by_list += [\n",
    "#     'dppmap_emb=grad+rp+loraB_k=Kcos', \n",
    "#     'dppmap_emb=text+embedding_k=Kcos', \n",
    "#     'dppmap_emb=grad+rp+loraB_k=Kcosp', \n",
    "#     'dppmap_emb=text+embedding_k=Kcosp',\n",
    "#     'dppmap_emb=grad+rp+loraB_k=Kcos1np', \n",
    "#     'dppmap_emb=text+embedding_k=Kcos1np',\n",
    "# ]\n",
    "# dataset_list = ['lima']\n",
    "# dataset_list = ['flan2022_1m']\n",
    "# dataset_list = ['tulu_v1_mix']\n",
    "# dataset_list = ['ultrachat']\n",
    "dataset_list = ['ultrachat15']\n",
    "# dataset_list = ['wizardlm']\n",
    "# dataset_list = ['sharegpt']\n",
    "\n",
    "\n",
    "# sort_by_list = [\n",
    "#  'log_prob',\n",
    "#  'el2n_agg=mean',\n",
    "#  'el2n_agg=l2n',\n",
    "#  'logit_margin',\n",
    "# ]\n",
    "# if 'lora' in model_name:\n",
    "#     sort_by_list += ['grad_loraB_l2n']\n",
    "# else:\n",
    "#     sort_by_list += ['grad_all_l2n', 'grad_qkv_l2n', 'grad_mlp_l2n', 'grad_last_l2n',]\n",
    "# sort_by_list = ['kmeansl2_emb=grad+rp+loraB_nc=30',\n",
    "#                 'kmeansl2_emb=text+embedding_nc=30']\n",
    "# dataset_list = ['lima']\n",
    "\n",
    "\n",
    "# model_name = 'pythia-1b-deduped'\n",
    "# model_name = 'pythia-1b-deduped+lora:r=256:a=256'\n",
    "# dataset_list = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1']\n",
    "# # sort_by_list = ['random_s=0', \n",
    "# #                 'log_prob', 'logit_margin', 'el2n_agg=mean', 'el2n_agg=l2n', \n",
    "# #                 'kmeansl2_nc=3000', 'kmeanscd_nc=3000',\n",
    "# #                 'grad_loraB_l2n',\n",
    "# #                 'grad_all_l2n', 'grad_qkv_l2n', 'grad_mlp_l2n', 'grad_last_l2n',\n",
    "# #                ]\n",
    "# sort_by_list = ['grad_loraB_l2n']\n",
    "\n",
    "from note_pruning_analysis import data_inds_dir\n",
    "\n",
    "options_list = itertools.product(dataset_list, sort_by_list)\n",
    "\n",
    "print('test_run =',test_run)\n",
    "cmds = []\n",
    "for dataset, sort_by in options_list:\n",
    "    save_dir = os.path.join(data_inds_dir, model_name, dataset)\n",
    "    cmd = f\"\"\"\n",
    "     python note_pruning.py \\\n",
    "        --dataset {dataset} \\\n",
    "        --sort_by {sort_by} \\\n",
    "        --model_name {model_name} \\\n",
    "        --encode_fn_type {encode_fn_type} \\\n",
    "        --save_dir {save_dir} \\\n",
    "    \"\"\".strip()\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'prune.{dataset}.{sort_by}', \n",
    "        nodes=1,\n",
    "        num_cpus=64, # 32\n",
    "        cpu_mem=256, # 128\n",
    "        num_gpus=1,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "        \n",
    "print('#cmds: ', len(cmds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a7b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('note_pruning_run_cmds.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES']\n",
    "    devices = 1\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363aa87",
   "metadata": {},
   "source": [
    "### Generate curriculum from pre-computed scores (via `note_pruning.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25a293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from note_curriculum import (\n",
    "    get_curriculum_scores,\n",
    "    get_curriculum,\n",
    "    generate_curriculum,\n",
    "    generate_curriculum_forall_scoring_fn,\n",
    "    scores_path_to_attrs,\n",
    "    np_random_choice_maximize_noreplacement,\n",
    "    plt_curriculum,\n",
    ")\n",
    "from note_pruning_analysis import assets_dir\n",
    "\n",
    "# model_name = 'llama-7b'; dataset = 'tulu_v1_mix'; M = 150_000\n",
    "# model_name = 'llama-7b'; dataset = 'sharegpt'; M = 150_000\n",
    "\n",
    "## mistral+ultrachat\n",
    "# model_name = 'mistral-7b'; dataset = 'ultrachat200k'; M = 50_000\n",
    "# model_name = 'mistral-7b'; dataset = 'ultrachat15'; M = 100_000\n",
    "\n",
    "## semdedup\n",
    "# model_name = 'llama-7b'; dataset = 'wizardlm'; M = 100_000\n",
    "# model_name = 'all-mpnet-base-v2'; dataset = 'wizardlm'; M = 100_000\n",
    "# model_name = 'bge-large-en-v1.5'; dataset = 'wizardlm'; M = 100_000\n",
    "# model_name = 'all-mpnet-base-v2'; dataset = 'ultrachat15'; M = 100_000\n",
    "model_name = 'mistral-7b'; dataset = 'ultrachat15'; M = 100_000\n",
    "\n",
    "\n",
    "pacing_fn_list = [\n",
    "#     f'prune_size={M}_ep=1',\n",
    "    f'prune_size={M}_ep=2',\n",
    "#     f'prune_size={M}_ep=3',\n",
    "#     f'singlestep_size={M}_startingfrac=0.1',\n",
    "#     f'singlestep_size={M}_startingfrac=0.05',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "]\n",
    "\n",
    "output_list = generate_curriculum_forall_scoring_fn(\n",
    "    model_name, dataset, pacing_fn_list, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774da90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cdbf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from note_curriculum import get_curriculum_scores, generate_curriculum, plt_curriculum\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = 'mistral-7b'; dataset = 'ultrachat'; M =  50_000\n",
    "# model_name = 'llama-7b'; dataset = 'tulu_v1_mix'; M = 150_000\n",
    "\n",
    "\n",
    "paths = glob.glob('curriculum/*/*/*/scores.pkl')\n",
    "paths = [x for x in paths if 'llama' in x and 'tulu_v1_mix' in x and 'log_prob_neg' in x]\n",
    "path = paths[0]\n",
    "\n",
    "verbose = True\n",
    "print(path)\n",
    "pacing_fn = f'prune_size={M}_ep=3'\n",
    "# pacing_fn = f'singlestep_size={M}_startingfrac=0.1'\n",
    "# pacing_fn = f'singlestep_size={M}_startingfrac=0.2'\n",
    "# pacing_fn = f'singlestep_size={M}_startingfrac=0.3'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=2'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=1.5'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.2_inc=1.5'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=1.5'\n",
    "\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep=3',\n",
    "    f'singlestep_size={M}_startingfrac=0.05',\n",
    "#     f'singlestep_size={M}_startingfrac=0.1',\n",
    "#     f'singlestep_size={M}_startingfrac=0.2',\n",
    "#     f'singlestep_size={M}_startingfrac=0.3',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=1.5',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=2',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=3',\n",
    "    f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "    f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=2',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=3',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.25'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "nrows = len(pacing_fn_list)\n",
    "fig, axs = plt.subplots(nrows, 3, figsize=(15,3*nrows), sharey=False, gridspec_kw={'width_ratios': [2,.5,.5]})\n",
    "\n",
    "for i, pacing_fn in enumerate(pacing_fn_list):\n",
    "\n",
    "    plt_kwargs = generate_curriculum(path, pacing_fn, verbose=True, save_output=False)\n",
    "    output = plt_kwargs.pop('output')\n",
    "    plt_kwargs.update({'fig': fig, 'axs': axs[i]})\n",
    "    plt_curriculum(**plt_kwargs)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "save_plt = 0\n",
    "if save_plt:\n",
    "    model_name, dataset, scoring_fn = output['model_name'], output['dataset'], output['scoring_fn']\n",
    "    save_path = os.path.join(\n",
    "        assets_dir, f'note_curriculum_{model_name}:{dataset}:{scoring_fn}.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight', dpi=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82473d",
   "metadata": {},
   "source": [
    "### main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1891b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from note_pruning import (\n",
    "    save_to_pickle,\n",
    "    save_sorted_inds,\n",
    "    sort_kmeans_dist_to_cluster_centers,\n",
    "    sort_dpp_map,\n",
    "    save_prune_results,\n",
    "    sort_dpp_map_memefficient,\n",
    ")\n",
    "from note_pruning_analysis import get_lm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c45b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wizardlm all-mpnet-base-v2 dppmap_emb=text+embedding_k=Kcos\n",
      "data_inds/input/all-mpnet-base-v2/wizardlm\n"
     ]
    }
   ],
   "source": [
    "test_run = False\n",
    "dataset = 'tulu_v1_human_mix'\n",
    "dataset = 'tulu_v2_human_mix'\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'lima'\n",
    "dataset = 'flan2022_1m'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat200k'\n",
    "dataset = 'ultrachat15'\n",
    "dataset = 'wizardlm'\n",
    "\n",
    "# sort_by = 'random_s=0'\n",
    "# sort_by = 'kmeansl2_nc=3000'\n",
    "# sort_by = 'kmeanscd_nc=3000'\n",
    "# sort_by = 'log_prob'\n",
    "# sort_by = 'dppmap_k=Kcos'\n",
    "# sort_by = 'dppmap_k=Kcos1np'\n",
    "# sort_by = 'el2n'\n",
    "# sort_by = 'grad_norm'\n",
    "# sort_by = 'kmeansl2_emb=grad+rp+loraB_nc=3000'\n",
    "# sort_by = 'kmeansl2_emb=text+embedding_nc=3000'\n",
    "sort_by = 'dppmap_emb=text+embedding_k=Kcos'\n",
    "# sort_by = 'logit_margin'\n",
    "# rhov1: mistral-7b base-tuned(ultrachat200k_beforesplitlongconv)\n",
    "# sort_by = 'rhov1'\n",
    "# sort_by = 'numtoks'\n",
    "\n",
    "# used for generating model output.\n",
    "# model_name = 'llama-7b'; encode_fn_type = 'sft'\n",
    "# model_name = 'llama-7b_ft=hmv1'; encode_fn_type = 'sft'\n",
    "# model_name = 'llama-7b+lora:r=256:a=256'; encode_fn_type = 'sft'\n",
    "# model_name = 'mistral-7b+lora:r=256:a=256'; encode_fn_type = 'sft'\n",
    "model_name = 'all-mpnet-base-v2'; encode_fn_type = 'input'\n",
    "# model_name = 'bge-large-en-v1.5'; encode_fn_type = 'input'\n",
    "\n",
    "# model_name = 'mistral-7b+lora:r=256:a=256__rho__mistral-7b-ultrachat200k-v1+lora:r=256:a=256'\n",
    "\n",
    "\n",
    "save_dir = f\"data_inds/\"\n",
    "save_dir = os.path.join(save_dir, '' if encode_fn_type=='sft' else encode_fn_type, model_name, dataset)\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "\n",
    "print(dataset, model_name, sort_by)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f985baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143000\n"
     ]
    }
   ],
   "source": [
    "d = get_lm_output(dataset, model_name, encode_fn_type=encode_fn_type, return_text_embedding=True)\n",
    "if test_run:\n",
    "    d = {k: v[:1000] for k, v in d.items()}\n",
    "    \n",
    "# some entries are nan, impute with mean value.\n",
    "N = d['text_embedding'].shape[0]\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b84014",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = 'dppmap_emb=text+embedding_k=Kcos'\n",
    "# 'semdedup_cl=kmeansfaisscd_md=mistral7b_dist=cd_emb=text+embedding_nc=200',\n",
    "\n",
    "sort_by = 'dppmap_cl=kmeansfaisscd_md=mpnet_emb=text+embedding_nc=200'\n",
    "\n",
    "\n",
    "kwd = parse_kv_from_string(sort_by)\n",
    "\n",
    "\n",
    "match = re.search(r'k=(\\w+)', sort_by)\n",
    "kernel_type = match.group(1) if match else None\n",
    "match = re.search(r'emb=([^_]+)', sort_by)\n",
    "embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "    raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "md = re.search(r'md=([^_]+)', sort_by).group(1)\n",
    "if (md == 'mpnet' and model_name != 'all-mpnet-base-v2') or \\\n",
    "   (md == 'bge' and model_name != 'bge-large-en-v1.5') or \\\n",
    "   (md == 'llama7b' and not model_name.lower().startswith('llama-7b')) or \\\n",
    "   (md == 'mistral7b' and not model_name.lower().startswith('mistral-7b')):\n",
    "    raise ValueError(f'md={md} does not match with model_name={model_name}')\n",
    "print(kernel_type, embed_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match = re.search(r'k=(\\w+)', sort_by)\n",
    "kernel_type = match.group(1) if match else None\n",
    "match = re.search(r'emb=([^_]+)', sort_by)\n",
    "embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "    raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    md = re.search(r'md=([^_]+)', sort_by).group(1)\n",
    "    if (md == 'mpnet' and model_name != 'all-mpnet-base-v2') or \\\n",
    "       (md == 'bge' and model_name != 'bge-large-en-v1.5') or \\\n",
    "       (md == 'llama7b' and not model_name.lower().startswith('llama-7b')) or \\\n",
    "       (md == 'mistral7b' and not model_name.lower().startswith('mistral-7b')):\n",
    "        raise ValueError(f'md={md} does not match with model_name={model_name}')\n",
    "emb = d[embed_type]\n",
    "log_prob = d['log_prob']\n",
    "# inds = sort_dpp_map_memefficient(emb, log_prob, kernel_type=kernel_type, torch_compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862f6c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vmf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sort_by = 'dppmap_cl=kmeansfaisscd_md=mpnet_emb=text+embedding_nc=200'\n",
    "sort_by = 'dppmap_k=vmf_gamma=1.0_kmd=mpnet_kemb=text+embedding'\n",
    "\n",
    "\n",
    "from rosemary import parse_kv_from_string\n",
    "kvs = parse_kv_from_string(sort_by)\n",
    "\n",
    "## wpq: note since dppmap potentially rely on output of >1 model,\n",
    "#  set `model_name` to whatever model that generates the quality score.\n",
    "#  e.g., model_name=llama-7b, md=mpnet since use its embedding on instructions, \n",
    "#      and q=log_prob uses log probability from llama-7b\n",
    "# \n",
    "\n",
    "if kvs['k'] == 'vmf':\n",
    "    kernel_kwargs = {'gamma': kvs['gamma']}\n",
    "elif kvs['k'] == 'rbf':\n",
    "    kernel_kwargs = {'sigma': kvs['sigma']}\n",
    "else:\n",
    "    kernel_kwargs = {}\n",
    "kwargs = {\n",
    "    'dataset': dataset,\n",
    "    'kernel_type': kvs['k'],\n",
    "    'kernel_embed_model': kvs['kmd'],\n",
    "    'kernel_embed_type': re.sub(r'[+]', '_', kvs['kemb']) if 'kemb' in kvs else None,\n",
    "    'kernel_kwargs': kernel_kwargs,\n",
    "    'quality_score_type': 'log_prob', #kvs.get('q', None),\n",
    "    'quality_score_embed_model': kvs.get('qmd', None),\n",
    "}\n",
    "\n",
    "\n",
    "dataset = dataset\n",
    "kernel_type = kvs['k']\n",
    "kernel_embed_model = kvs['kmd']\n",
    "kernel_embed_type = re.sub(r'[+]', '_', kvs['kemb'])\n",
    "quality_score_type = 'log_prob' #kvs.get('q', None)\n",
    "quality_score_embed_model = 'llama7b' # kvs.get('qmd', None)\n",
    "kernel_kwargs = kwargs['kernel_kwargs']\n",
    "\n",
    "\n",
    "#     inds = sort_dpp_map_memefficient(emb, log_prob, kernel_type=kernel_type, torch_compile=False)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/fast-map-dpp\")\n",
    "from dpp import dpp_lazy\n",
    "\n",
    "from functools import partial\n",
    "from note_pruning_dpp import torch_vmf_kernel, torch_rbf_kernel\n",
    "\n",
    "\n",
    "def get_full_model_name(md):\n",
    "    if md == 'mpnet':\n",
    "        model_name = 'all-mpnet-base-v2'\n",
    "    elif md == 'bge':\n",
    "        model_name = 'bge-large-en-v1.5'\n",
    "    elif md == 'llama7b':\n",
    "        model_name = 'llama-7b+lora:r=256:a=256'\n",
    "    elif md == 'mistral7b':\n",
    "        model_name = 'mistral-7b+lora:r=256:a=256'\n",
    "    else:\n",
    "        raise ValueError(f'Dont know full name for model_name={md}')\n",
    "    return model_name\n",
    "\n",
    "if kernel_type not in ['ip', 'vmf', 'rbf']:\n",
    "    raise ValueError(f'kernel_type={kernel_type} not supported.')\n",
    "if kernel_embed_model not in ['mpnet', 'bge', 'llama7b', 'mistral7b']:\n",
    "    raise ValueError(f'kernel_embed_model={kernel_embed_model} not supported.')\n",
    "if kernel_embed_type not in ['text_embedding', 'grad_rp_loraB']:\n",
    "    raise ValueError(f'kernel_embed_type={kernel_embed_type} not supported.')\n",
    "    \n",
    "dk = get_lm_output(\n",
    "    dataset, \n",
    "    get_full_model_name(kernel_embed_model),\n",
    "    encode_fn_type='input' if kernel_embed_model in ['mpnet', 'bge'] else 'sft', \n",
    "    return_text_embedding=True)\n",
    "X = dk[kernel_embed_type]\n",
    "if any(x in kernel_embed_model for x in ['mpnet', 'bge']):\n",
    "    X = X / np.maximum(np.linalg.norm(X, axis=-1, keepdims=True), 1e-8) # possibly divide by zero.\n",
    "\n",
    "\n",
    "if quality_score_type is None:\n",
    "    class IdentityQuality:\n",
    "        def __getitem__(self, i):\n",
    "            return 1\n",
    "    Q = IdentityQuality()\n",
    "else:\n",
    "    dq = get_lm_output(\n",
    "        dataset, \n",
    "        get_full_model_name(quality_score_embed_model),\n",
    "        encode_fn_type='input' if quality_score_embed_model in ['mpnet', 'bge'] else 'sft', \n",
    "        return_text_embedding=True)\n",
    "    Q = dq[quality_score_type]\n",
    "    Q = Q.reshape(-1, 1)\n",
    "\n",
    "if not X.shape[0] == Q.shape[0]:\n",
    "    raise ValueError(f'X ({X.shape}) and Q (Q.shape) does not match')\n",
    "\n",
    "if kernel_type == 'vmf':\n",
    "    kernel_fn = partial(torch_vmf_kernel, **kernel_kwargs)\n",
    "elif kernel_type == 'rbf':\n",
    "    kernel_fn = partial(torch_rbf_kernel, **kernel_kwargs)\n",
    "else:\n",
    "    raise ValueError(f'kernel_type={kernel_type} not supported.')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N = X.shape[0]\n",
    "kernel_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4241a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function torch_vmf_kernel at 0x7f5751396b90>, gamma=1.0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dppmap() missing 1 required positional argument: 'M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     96\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 97\u001b[0m inds \u001b[38;5;241m=\u001b[39m \u001b[43mdppmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(inds)\n",
      "\u001b[0;31mTypeError\u001b[0m: dppmap() missing 1 required positional argument: 'M'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# (1.5*10**6 * 50_000 * 4) / 1024**3\n",
    "\n",
    "def dpp_np(kernel_matrix, max_length, epsilon=1E-10):\n",
    "    \"\"\"\n",
    "    Our proposed fast implementation of the greedy algorithm\n",
    "    :param kernel_matrix: 2-d array\n",
    "    :param max_length: positive int\n",
    "    :param epsilon: small positive scalar\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    item_size = kernel_matrix.shape[0]\n",
    "    cis = np.zeros((max_length, item_size))\n",
    "    di2s = np.copy(np.diag(kernel_matrix))\n",
    "    selected_items = list()\n",
    "    selected_item = np.argmax(di2s)\n",
    "    selected_items.append(selected_item)\n",
    "    while len(selected_items) < max_length:\n",
    "        if len(selected_items)%(max_length//10)==0:\n",
    "            print('fast_map_dpp iterations = ',len(selected_items))\n",
    "        k = len(selected_items) - 1\n",
    "        ci_optimal = cis[:k, selected_item]\n",
    "        di_optimal = math.sqrt(di2s[selected_item])\n",
    "        elements = kernel_matrix[selected_item, :]\n",
    "#         print(elements.shape, ci_optimal.shape, cis[:k, :].shape, di_optimal)\n",
    "#         print(ci_optimal, cis[:k,:], np.dot(ci_optimal, cis[:k,:]))\n",
    "#         print((elements - np.dot(ci_optimal, cis[:k, :])))\n",
    "        if k == 0:\n",
    "            eis = elements\n",
    "        else:\n",
    "            print(k, ci_optimal.shape, cis[:k, :].shape)\n",
    "            eis = (elements - np.dot(ci_optimal, cis[:k, :])) / di_optimal\n",
    "#         print(eis)\n",
    "        cis[k, :] = eis\n",
    "        di2s -= np.square(eis)\n",
    "        di2s[selected_item] = -np.inf\n",
    "        selected_item = np.argmax(di2s)\n",
    "        if di2s[selected_item] < epsilon:\n",
    "            print(di2s[selected_item])\n",
    "            break\n",
    "        selected_items.append(selected_item)\n",
    "    return selected_items\n",
    "\n",
    "\n",
    "\n",
    "def dppmap(Ki, Kdiag, M, epsilon=1e-10, verbose=True):\n",
    "    \"\"\"dpp map inference \n",
    "            - https://arxiv.org/pdf/1709.05135.pdf\n",
    "    \"\"\"\n",
    "    N = K.shape[0]\n",
    "    # print(f'Memory for cis: {M*N*4/(1024**3)} GB')\n",
    "    # (M, N)\n",
    "    cis = torch.zeros((M, N), dtype=torch.float32)\n",
    "    # (N,)\n",
    "    di2s = torch.diag(K)\n",
    "    # grows to at most length M\n",
    "    inds = []\n",
    "    j = torch.argmax(di2s).item()\n",
    "    inds.append(j)\n",
    "    while len(inds) < M:\n",
    "        if verbose:\n",
    "            if len(inds) % (M // 10) == 0:\n",
    "                print('fast_map_dpp iterations = ', len(inds))\n",
    "        k = len(inds) - 1\n",
    "        # (k,)\n",
    "        ci_optimal = cis[:k, j]\n",
    "        di_optimal = torch.sqrt(di2s[j])\n",
    "        Kj = K[j, :]\n",
    "        # (N,) - (k,)@(k,N) -> (N,)\n",
    "        eis = (Kj - ci_optimal@cis[:k, :]) / di_optimal\n",
    "        # update to k are updated.\n",
    "        cis[k, :] = eis\n",
    "        di2s -= torch.square(eis)\n",
    "        di2s[j] = -float('inf')\n",
    "        j = torch.argmax(di2s).item()\n",
    "\n",
    "        if di2s[j] < epsilon:\n",
    "            print(f'Stop on dᵢ^2 = {di2s[j]}')\n",
    "            break\n",
    "        inds.append(j)\n",
    "\n",
    "    return inds\n",
    "\n",
    "\n",
    "XX = X[:100]\n",
    "XX = torch.from_numpy(XX)\n",
    "\n",
    "kernel_fn = partial(torch_vmf_kernel, **kernel_kwargs)\n",
    "print(kernel_fn)\n",
    "\n",
    "\n",
    "K = kernel_fn(XX, XX)\n",
    "k = 10\n",
    "\n",
    "device = 'cuda'\n",
    "inds = dppmap(K, k)\n",
    "print(inds)\n",
    "\n",
    "\n",
    "# inds_np = dpp_np(K.numpy(), k)\n",
    "# print(inds_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9da416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6012, 0.6346, 0.7396, 0.6801, 0.6672, 0.6004, 0.6556, 0.7009,\n",
       "        0.7131, 0.6304, 0.6623, 0.6418, 0.6371, 0.7030, 0.6056, 0.6052, 0.6030,\n",
       "        0.6130, 0.6558, 0.6738, 0.6401, 0.6797, 0.6598, 0.6209, 0.6926, 0.6393,\n",
       "        0.6475, 0.6048, 0.6312, 0.7039, 0.6910, 0.6626, 0.6068, 0.6211, 0.6762,\n",
       "        0.6631, 0.6091, 0.6494, 0.6313, 0.6523, 0.6010, 0.6503, 0.6498, 0.6331,\n",
       "        0.6711, 0.6420, 0.6562, 0.6808, 0.6466, 0.6131, 0.6687, 0.6065, 0.6397,\n",
       "        0.7244, 0.6231, 0.6423, 0.6724, 0.6509, 0.6427, 0.6828, 0.6403, 0.6659,\n",
       "        0.6556, 0.6364, 0.6081, 0.6259, 0.6425, 0.6845, 0.6386, 0.6564, 0.6480,\n",
       "        0.6175, 0.6821, 0.6352, 0.6627, 0.6342, 0.6620, 0.6126, 0.6297, 0.6847,\n",
       "        0.6368, 0.6352, 0.6113, 0.6370, 0.6339, 0.7008, 0.6400, 0.6300, 0.6418,\n",
       "        0.6189, 0.6408, 0.6256, 0.6046, 0.6131, 0.6118, 0.6885, 0.6277, 0.6333,\n",
       "        0.6364])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "kernel_fn(XX[i], XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95e93533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15256/1656774835.py:7: RuntimeWarning: divide by zero encountered in divide\n",
      "  ys = xs/(2*(1-xs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5750198850>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def Ki(i):\n",
    "#     \"\"\"Returns i-th row of kernel matrix K\"\"\"\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "xs = np.linspace(0, 1, 100)\n",
    "ys = xs/(2*(1-xs))\n",
    "plt.plot(xs,ys)\n",
    "ys = np.exp(ys)\n",
    "plt.plot(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc907510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory for cis: 37.25290298461914 GB\n"
     ]
    }
   ],
   "source": [
    "# Cost of computing (given feature_dim=d, size of kernel=N)\n",
    "# - K[i,:]: O(dN)\n",
    "# - ci_optimal@cis[:k, :]: O(kN) for large k is very costly.\n",
    "\n",
    "print(f'Memory for cis: {50_000*200_000*4/(1024**3)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f22499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9456, 0.9343, 0.9641, 0.9337, 0.9126, 0.9139, 0.8069, 0.9716,\n",
       "         0.9713],\n",
       "        [0.9456, 1.0000, 0.9990, 0.9928, 0.8873, 0.8680, 0.9016, 0.9373, 0.8563,\n",
       "         0.9262],\n",
       "        [0.9343, 0.9990, 1.0000, 0.9868, 0.8864, 0.8686, 0.8861, 0.9426, 0.8424,\n",
       "         0.9104],\n",
       "        [0.9641, 0.9928, 0.9868, 1.0000, 0.8743, 0.8510, 0.9418, 0.9215, 0.8817,\n",
       "         0.9624],\n",
       "        [0.9337, 0.8873, 0.8864, 0.8743, 1.0000, 0.9982, 0.7503, 0.7303, 0.9349,\n",
       "         0.8365],\n",
       "        [0.9126, 0.8680, 0.8686, 0.8510, 0.9982, 1.0000, 0.7217, 0.7130, 0.9193,\n",
       "         0.8090],\n",
       "        [0.9139, 0.9016, 0.8861, 0.9418, 0.7503, 0.7217, 1.0000, 0.8617, 0.8498,\n",
       "         0.9809],\n",
       "        [0.8069, 0.9373, 0.9426, 0.9215, 0.7303, 0.7130, 0.8617, 1.0000, 0.6919,\n",
       "         0.8309],\n",
       "        [0.9716, 0.8563, 0.8424, 0.8817, 0.9349, 0.9193, 0.8498, 0.6919, 1.0000,\n",
       "         0.9270],\n",
       "        [0.9713, 0.9262, 0.9104, 0.9624, 0.8365, 0.8090, 0.9809, 0.8309, 0.9270,\n",
       "         1.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = torch.rand(10,3)\n",
    "X = X / np.maximum(np.linalg.norm(X, axis=-1, keepdims=True), 1e-8)\n",
    "kernel_fn(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8b6195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5879391729831696"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "n = 5_000\n",
    "a, b = torch.rand(n), torch.rand(n, 1_500_000)\n",
    "\n",
    "((a.numel() + b.numel())*4 / (1024**3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22705b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ((a.numel() + b.numel())*4 / (1024**3))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchunk_size\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunk_size' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2abff51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.48 s ± 118 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.27 s ± 78.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "79.2 ms ± 699 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def matmul_mem_efficient(a, b, device='cuda', split_dim=1, gpu_mem_budget=.1):\n",
    "    \"\"\"assume `b` is more memory intensive\n",
    "        put `a` into gpu memory by default.\n",
    "        and put chunks of `b` into memory. \"\"\"\n",
    "    num_chunks = math.ceil(((a.numel() + b.numel())*4 / (1024**3)) / gpu_mem_budget)\n",
    "    split_size = math.ceil(b.shape[split_dim] / num_chunks)\n",
    "    c = []\n",
    "    a = a.to(device)\n",
    "    b_split = torch.split(b, split_size, dim=split_dim)\n",
    "    for bi in b_split:\n",
    "        bi = bi.to(device,  non_blocking=True)\n",
    "        ci = a@bi\n",
    "        c.append(ci.to('cpu'))\n",
    "    c = torch.hstack(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def matmul_mem_efficient2(a, b, device='cuda', split_dim=1, gpu_mem_budget=.1):\n",
    "    \"\"\"assume `b` is more memory intensive\n",
    "        put `a` into gpu memory by default.\n",
    "        and put chunks of `b` into memory. \"\"\"\n",
    "    num_chunks = math.ceil(((a.numel() + b.numel())*4 / (1024**3)) / gpu_mem_budget)\n",
    "    split_size = math.ceil(b.shape[split_dim] / num_chunks)\n",
    "    c = []\n",
    "    a = a.to(device)\n",
    "    b_split = torch.split(b, split_size, dim=split_dim)\n",
    "    for bi in b_split:\n",
    "        bi = bi.to(device,  non_blocking=True)\n",
    "        ci = a@bi\n",
    "        c.append(ci.to('cpu'))\n",
    "    c = torch.hstack(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "a, b = torch.rand(1000), torch.rand(1000, 1_500_000)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "matmul_mem_efficient(a, b, gpu_mem_budget=5)\n",
    "print(time.time()-t0)\n",
    "\n",
    "t0 = time.time()\n",
    "a@b\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6161049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 s ± 13.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit matmul_mem_efficient(a, b, gpu_mem_budget=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "11fbd48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([100, 250000]),\n",
       " torch.Size([100, 250000]),\n",
       " torch.Size([100, 250000]),\n",
       " torch.Size([100, 250000]),\n",
       " torch.Size([100, 250000]),\n",
       " torch.Size([100, 250000])]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.shape[0] / (num_chunks+1)\n",
    "# b.shape[0] / (num_chunks)\n",
    "\n",
    "[x.shape for x in b_split]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dce7a5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logP = torch.from_numpy(logP).to('cuda')\n",
    "P = logP.exp()\n",
    "\n",
    "N = P.shape[0]\n",
    "\n",
    "if torch_compile:\n",
    "    X = X / np.linalg.norm(X, axis=-1, ord=2, keepdims=True)\n",
    "else:\n",
    "    X = torch.from_numpy(X).to('cuda')\n",
    "    X = torch.nn.functional.normalize(X, dim=-1)\n",
    "\n",
    "jitter = 1e-3\n",
    "\n",
    "def kernel_matrix_ith_row(i):\n",
    "    \"\"\"Returns i-th row of kernel matrix `K`\"\"\"\n",
    "    if kernel_type == 'Kcos':\n",
    "        Ki = X[i]@X.T\n",
    "    elif kernel_type == 'Kcosp':\n",
    "        Ki = P[i]*X[i]@X.T*P.reshape(1,N)\n",
    "    elif kernel_type == 'Kcos1np':\n",
    "        Ki = (1-P[i])*X[i]@X.T*(1-P.reshape(1,N))\n",
    "    else:\n",
    "        raise ValueError(f'kernel_type={kernel_type} not supported')\n",
    "    Ki = Ki.squeeze()\n",
    "    Ki[i] += jitter\n",
    "    if torch_compile:\n",
    "        return Ki\n",
    "    else:\n",
    "        return Ki.to('cpu').numpy()\n",
    "\n",
    "def kernel_matrix_diag(): \n",
    "    if kernel_type == 'Kcos':\n",
    "        Kdiag = (X*X).sum(-1)\n",
    "    elif kernel_type == 'Kcosp':\n",
    "        Kdiag = (X*X).sum(-1) * (P*P)\n",
    "    elif kernel_type == 'Kcos1np':\n",
    "        Kdiag = (X*X).sum(-1) * ((1-P)*(1-P))\n",
    "    else:\n",
    "        raise ValueError(f'kernel_type={kernel_type} not supported')\n",
    "    Kdiag = Kdiag.squeeze()\n",
    "    Kdiag += jitter\n",
    "    if torch_compile:\n",
    "        return Kdiag\n",
    "    else:\n",
    "        return Kdiag.to('cpu').numpy()\n",
    "\n",
    "max_length = min(50000, int(.3*N))\n",
    "if torch_compile:\n",
    "    dpp_lazy = torch.compile(dpp_lazy)\n",
    "    kernel_matrix_ith_row = torch.compile(kernel_matrix_ith_row)\n",
    "    kernel_matrix_diag = torch.compile(kernel_matrix_diag)\n",
    "inds = dpp_lazy(N, kernel_matrix_ith_row, kernel_matrix_diag, max_length, jitter)\n",
    "if len(inds) != N:\n",
    "    print(f'dpp map len(indices)={len(inds)} != {N} = N')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85d6b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling note_pruning_clustering.main with kwargs={\n",
      "    \"model_name\": \"all-mpnet-base-v2\",\n",
      "    \"dataset\": \"wizardlm\",\n",
      "    \"encode_fn_type\": \"input\",\n",
      "    \"clustering_fn\": \"cl=kmeansfaisscd_md=mpnet_dist=cd_emb=text+embedding_nc=200\",\n",
      "    \"embed_type\": \"text_embedding\",\n",
      "    \"normalize_embeddings\": true,\n",
      "    \"first_N\": null,\n",
      "    \"save_dir\": \"clustering/input/all-mpnet-base-v2/wizardlm/cl=kmeansfaisscd_md=mpnet_dist=cd_emb=text+embedding_nc=200\"\n",
      "}\n",
      "Sampling a subset of 51200 / 143000 for training\n",
      "Clustering 51200 points in 768D to 200 clusters, redo 1 times, 30 iterations\n",
      "  Preprocessing in 0.22 s\n",
      "  Iteration 29 (13.94 s, search 11.78 s): objective=28569.9 imbalance=1.112 nsplit=0       \n",
      "Apply SemDeDup to discard duplicates.\n",
      "brute force compute scores for semdedup. max step length = 911\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f792932",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the copy in `note_pruning.py` is most up to date\n",
    "\n",
    "pkl_extra = {}\n",
    "inds = None\n",
    "\n",
    "# sort_by = 'semdedup_cl=kmeansfaisscd_md=mpnet_dist=cd_emb=text+embedding_nc=200'\n",
    "\n",
    "t0 = time.time()\n",
    "if any(sort_by.startswith(x) for x in [\n",
    "        'log_prob', \n",
    "        'el2n',  # el2n_agg={l2n|mean}\n",
    "        'logit_margin', \n",
    "        'grad',  # grad_{loraB|qkv|all|last}_l2n\n",
    "    ]):\n",
    "    if sort_by not in d:\n",
    "        print(f'sort_by={sort_by} not in model output: ({dataset}, {model_name})')\n",
    "    S = np.nan_to_num(d[sort_by], nan=np.nanmean(d[sort_by])).squeeze()\n",
    "elif sort_by.startswith('random'):\n",
    "    match = re.search(r's=(\\d+)', sort_by)\n",
    "    seed = int(match.group(1))\n",
    "    np.random.seed(seed)\n",
    "    S = np.random.rand(N)\n",
    "    assert(S.shape == np.unique(S).shape)\n",
    "if sort_by.startswith('kmeans'):\n",
    "    dist_fn = 'l2' if sort_by.startswith('kmeansl2') else 'cd'\n",
    "    match = re.search(r'nc=(\\d+)', sort_by)\n",
    "    n_clusters = int(match.group(1)) if match else None\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    emb = d[embed_type]\n",
    "    print(f'Running kmeans(n_clusters={n_clusters}) {{ {embed_type} }} to compute {\"euclidean\" if dist_fn == \"l2\" else \"cosine\"} distance to cluster centers.')\n",
    "    S, kms = sort_kmeans_dist_to_cluster_centers(emb, n_clusters, dist_fn=dist_fn)\n",
    "    pkl_extra['kmeans'] = kms\n",
    "elif sort_by.startswith('semdedup'):\n",
    "    import note_pruning_clustering\n",
    "    from rosemary import parse_kv_from_string\n",
    "    kvs = parse_kv_from_string(sort_by)\n",
    "    md = kvs['md']\n",
    "    if (md == 'mpnet' and model_name != 'all-mpnet-base-v2') or \\\n",
    "       (md == 'bge' and model_name != 'bge-large-en-v1.5') or \\\n",
    "       (md == 'llama7b' and not model_name.lower().startswith('llama-7b')) or \\\n",
    "       (md == 'mistral7b' and not model_name.lower().startswith('mistral-7b')):\n",
    "        raise ValueError(f'md={md} does not match with model_name={model_name}')\n",
    "    clustering_fn = create_string_from_kv(\n",
    "        {k: v for k, v in kvs.items() if k in ['cl', 'nc', 'bsz', 'ms', 'emb']})\n",
    "    dist = kvs['dist']\n",
    "    assert(dist in ['cd', 'l2'])\n",
    "    embed_type = re.sub(r'[+]', '_', kvs['emb'])\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    save_dir_clustering = os.path.join('clustering', encode_fn_type, model_name, dataset, clustering_fn)\n",
    "    os.makedirs(save_dir_clustering, exist_ok=True)\n",
    "    # normalize embeddings to unit norm if the model that generated the embeddings does the \n",
    "    # same, e.g., mpnet, bge, or if using spherical kmeans clustering.\n",
    "    if any(x in model_name for x in ['mpnet', 'bge']) or 'kmeansfaisscd' in clustering_fn:\n",
    "        normalize_embeddings = True\n",
    "    else:\n",
    "        normalize_embeddings = False\n",
    "    kwargs = {\n",
    "        'model_name': model_name,\n",
    "        'dataset': dataset,\n",
    "        'encode_fn_type': encode_fn_type,\n",
    "        'clustering_fn': clustering_fn,\n",
    "        'embed_type': embed_type,\n",
    "        'normalize_embeddings': normalize_embeddings,\n",
    "        'first_N': None,\n",
    "        'save_dir': save_dir_clustering,\n",
    "    }\n",
    "    print(f'Calling note_pruning_clustering.main with kwargs={json.dumps(kwargs, indent=4)}')\n",
    "    X, Y, C = note_pruning_clustering.main(**kwargs)\n",
    "    print('Apply SemDeDup to discard duplicates.')\n",
    "    S = note_pruning_clustering.semdedup(X, Y, dist=dist, device='cuda')\n",
    "elif sort_by.startswith('dpp'):\n",
    "    match = re.search(r'k=(\\w+)', sort_by)\n",
    "    kernel_type = match.group(1) if match else None\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    emb = d[embed_type]\n",
    "    log_prob = d['log_prob']\n",
    "    inds = sort_dpp_map_memefficient(emb, log_prob, kernel_type=kernel_type, torch_compile=False)\n",
    "elif sort_by.startswith('rho'):\n",
    "    if sort_by == 'rhov1':\n",
    "        model_names = ['mistral-7b+lora:r=256:a=256',\n",
    "                       'mistral-7b-ultrachat200k-v1+lora:r=256:a=256']\n",
    "        assert(model_name == model_names[0])\n",
    "    else:\n",
    "        raise ValueError(f'sort_by={sort_by} not implemented.')\n",
    "    assert(len(model_names) == 2)\n",
    "    ds = []\n",
    "    for x in model_names:\n",
    "        ds.append(get_lm_output(dataset, x, return_text_embedding=False, fill_nan=False))\n",
    "    ks = [set(d.keys()) for d in ds]\n",
    "    ks = ks[0] & ks[1]\n",
    "    for k in ks:\n",
    "        S0 = ds[0][k]\n",
    "        S1 = ds[1][k]\n",
    "        # handle nan entries properly.\n",
    "        nan_mask = np.logical_or(np.isnan(S0), np.isnan(S1))\n",
    "        S = np.subtract(S0, S1)\n",
    "        S[nan_mask] = np.nan\n",
    "        S = S.squeeze()\n",
    "        save_prune_results(save_dir, None, S, {}, f'{sort_by}_{k}', model_name, dataset)\n",
    "elif sort_by.startswith('numtoks'):\n",
    "    from transformers import AutoTokenizer\n",
    "    from note_pruning_analysis import get_dataset_token_lengths\n",
    "    if 'llama' in model_name or 'mistral' in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/huggyllama/llama-7b',\n",
    "            use_fast=False, # use_fast sometimes cause error.\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Need to supply appropriate tokenizer to count token lengths,')\n",
    "    d = get_dataset_token_lengths(dataset, tokenizer)\n",
    "\n",
    "    d['total_len'] = d['input_len'] + d['output_len']\n",
    "    for k in ['input', 'output', 'total']:\n",
    "        S = d[f'{k}_len']\n",
    "        save_prune_results(save_dir, None, S, {}, f'{sort_by}_{k}', model_name, dataset)\n",
    "\n",
    "        \n",
    "t1 = time.time()\n",
    "print(f'Rank datapoints with {sort_by} took {t1-t0:.2f} seconds.')\n",
    "\n",
    "# from note_pruning import save_prune_results\n",
    "if not any(sort_by.startswith(x) for x in ['rho', 'numtoks']):\n",
    "    save_prune_results(save_dir, inds, S, pkl_extra, sort_by, model_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5b429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f483857",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'log_prob'\n",
    "t = 100\n",
    "S0 = ds[0][k].squeeze()[::t]\n",
    "S1 = ds[1][k].squeeze()[::t]\n",
    "# log prob neg\n",
    "S0, S1 = -S0, -S1\n",
    "# S = S0-S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0ece3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 3))\n",
    "xs = np.arange(len(S0))\n",
    "inds_sorted = np.argsort(S0)\n",
    "ax.plot(xs, S0[inds_sorted], label='S_base')\n",
    "ax.plot(xs, S1[inds_sorted], label='S_train')\n",
    "ax.plot(xs, S[inds_sorted], label='S_base-S_train')\n",
    "ax.legend()\n",
    "ax.set_title(f'{model_name}  {dataset}  sort_by={sort_by}', fontsize=20)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 3))\n",
    "xs = np.arange(len(S0))\n",
    "inds_sorted = np.argsort(S1)\n",
    "ax.plot(xs, S0[inds_sorted], label='S_base')\n",
    "ax.plot(xs, S1[inds_sorted], label='S_train')\n",
    "ax.plot(xs, S[inds_sorted], label='S_base-S_train')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 3))\n",
    "xs = np.arange(len(S0))\n",
    "inds_sorted = np.argsort(S)\n",
    "ax.plot(xs, S0[inds_sorted], label='S_base')\n",
    "ax.plot(xs, S1[inds_sorted], label='S_train')\n",
    "ax.plot(xs, S[inds_sorted], label='S_base-S_train')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ebd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec32921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8895c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea377f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
