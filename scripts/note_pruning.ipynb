{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf384bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_run = False\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.random_s=0\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by random_s=0 --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.random_s=1\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by random_s=1 --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.random_s=2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by random_s=2 --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.log_prob\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by log_prob --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.logit_margin\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by logit_margin --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.el2n_agg=mean\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by el2n_agg=mean --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"prune.ultrachat.grad_loraB_l2n\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 64,\n",
      "    \"cpu_mem\": 256,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "python note_pruning.py --dataset ultrachat --sort_by grad_loraB_l2n --lm_output_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/llama-7b+lora:r=256:a=256\n",
      "#cmds:  7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from rosemary import jpt_in_notebook\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "model_name = 'llama-7b'\n",
    "model_name = 'llama-7b+lora:r=256:a=256' # includes lora grad norm\n",
    "# model_name = 'llama-7b_ft=hmv1' # means llama-7b finetuned on tulu humanmix already.\n",
    "sort_by_list = [\n",
    "    'random_s=0', 'random_s=1', 'random_s=2',\n",
    "    'log_prob', 'logit_margin', 'el2n_agg=mean',\n",
    "    'grad_loraB_l2n',\n",
    "]\n",
    "sort_by_list += [\n",
    "#     'kmeansl2_emb=grad+rp+loraB_nc=300',\n",
    "    'kmeansl2_emb=grad+rp+loraB_nc=1000',\n",
    "    'kmeansl2_emb=grad+rp+loraB_nc=3000',\n",
    "#     'kmeansl2_emb=text+embedding_nc=300',\n",
    "    'kmeansl2_emb=text+embedding_nc=1000',\n",
    "    'kmeansl2_emb=text+embedding_nc=3000',\n",
    "]\n",
    "sort_by_list += [\n",
    "    'dppmap_emb=grad+rp+loraB_k=Kcos', \n",
    "    'dppmap_emb=text+embedding_k=Kcos', \n",
    "    'dppmap_emb=grad+rp+loraB_k=Kcosp', \n",
    "    'dppmap_emb=text+embedding_k=Kcosp',\n",
    "    'dppmap_emb=grad+rp+loraB_k=Kcos1np', \n",
    "    'dppmap_emb=text+embedding_k=Kcos1np',\n",
    "]\n",
    "# dataset_list = ['lima']\n",
    "# dataset_list = ['flan2022_1m']\n",
    "# dataset_list = ['tulu_v1_mix']\n",
    "dataset_list = ['ultrachat']\n",
    "\n",
    "# sort_by_list = [\n",
    "#  'log_prob',\n",
    "#  'el2n_agg=mean',\n",
    "#  'el2n_agg=l2n',\n",
    "#  'logit_margin',\n",
    "# ]\n",
    "# if 'lora' in model_name:\n",
    "#     sort_by_list += ['grad_loraB_l2n']\n",
    "# else:\n",
    "#     sort_by_list += ['grad_all_l2n', 'grad_qkv_l2n', 'grad_mlp_l2n', 'grad_last_l2n',]\n",
    "# sort_by_list = ['kmeansl2_emb=grad+rp+loraB_nc=30',\n",
    "#                 'kmeansl2_emb=text+embedding_nc=30']\n",
    "# dataset_list = ['lima']\n",
    "\n",
    "\n",
    "# model_name = 'pythia-1b-deduped'\n",
    "# model_name = 'pythia-1b-deduped+lora:r=256:a=256'\n",
    "# dataset_list = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1']\n",
    "# # sort_by_list = ['random_s=0', \n",
    "# #                 'log_prob', 'logit_margin', 'el2n_agg=mean', 'el2n_agg=l2n', \n",
    "# #                 'kmeansl2_nc=3000', 'kmeanscd_nc=3000',\n",
    "# #                 'grad_loraB_l2n',\n",
    "# #                 'grad_all_l2n', 'grad_qkv_l2n', 'grad_mlp_l2n', 'grad_last_l2n',\n",
    "# #                ]\n",
    "# sort_by_list = ['grad_loraB_l2n']\n",
    "\n",
    "from note_pruning_analysis import lm_output_dir, data_inds_dir\n",
    "save_dir = os.path.join(data_inds_dir, model_name)\n",
    "lm_output_dir = os.path.join(lm_output_dir, model_name)\n",
    "\n",
    "options_list = itertools.product(dataset_list, sort_by_list)\n",
    "\n",
    "print('test_run =',test_run)\n",
    "cmds = []\n",
    "for dataset, sort_by in options_list:\n",
    "    cmd = f\"\"\"\n",
    "     python note_pruning.py \\\n",
    "        --dataset {dataset} \\\n",
    "        --sort_by {sort_by} \\\n",
    "        --lm_output_dir {lm_output_dir} \\\n",
    "        --save_dir {save_dir} \\\n",
    "    \"\"\".strip()\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=os.path.join(save_dir, dataset))\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'prune.{dataset}.{sort_by}', \n",
    "        nodes=1,\n",
    "        num_cpus=64, # 32\n",
    "        cpu_mem=256, # 128\n",
    "        num_gpus=1,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "        \n",
    "print('#cmds: ', len(cmds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4869d38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin', 'grad_loraB_l2n', 'grad_rp_loraB'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = ('/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/'\n",
    "    'pythia-1b-deduped')\n",
    "\n",
    "\n",
    "dataset = 'lima'\n",
    "model_name = 'llama-7b+lora:r=256:a=256'\n",
    "\n",
    "from note_pruning_analysis import get_lm_output\n",
    "\n",
    "output = get_lm_output(dataset, model_name, return_text_embedding=False)\n",
    "output.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bf2dd69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Fri Oct 20 00:34:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   39C    P0   157W / 300W |   7019MiB / 32510MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    37W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    807425      C   ...8.8/build/bin/./mdrun_mpi      543MiB |\n",
      "|    0   N/A  N/A    808658      C   python                           3237MiB |\n",
      "|    0   N/A  N/A    808763      C   python                           3237MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[0] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9632c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/json/default-ddd1a699a14e41bf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4807e93448df45339564bc4145bfe2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c877470f878a4e81b337bfd418edde38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/json/default-ddd1a699a14e41bf/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'id', 'messages'],\n",
       "    num_rows: 160564\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from note_pruning_analysis import get_dataset\n",
    "\n",
    "ds = get_dataset('sharegpt', processed=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c1891b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from note_pruning import (\n",
    "    save_to_pickle,\n",
    "    save_sorted_inds,\n",
    "    sort_kmeans_dist_to_cluster_centers,\n",
    "    sort_dpp_map,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c45b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = False\n",
    "dataset = 'tulu_v1_human_mix'\n",
    "dataset = 'tulu_v2_human_mix'\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'lima'\n",
    "dataset = 'flan2022_1m'\n",
    "dataset = 'tulu_v1_mix'\n",
    "\n",
    "# sort_by = 'random_s=0'\n",
    "# sort_by = 'kmeansl2_nc=3000'\n",
    "# sort_by = 'kmeanscd_nc=3000'\n",
    "# sort_by = 'prob'\n",
    "# sort_by = 'dppmap_k=Kcos'\n",
    "# sort_by = 'dppmap_k=Kcos1np'\n",
    "# sort_by = 'el2n'\n",
    "# sort_by = 'grad_norm'\n",
    "# sort_by = 'kmeansl2_emb=grad+rp+loraB_nc=3000'\n",
    "# sort_by = 'kmeansl2_emb=text+embedding_nc=3000'\n",
    "sort_by = 'dppmap_emb=text+embedding_k=Kcos'\n",
    "\n",
    "\n",
    "# used for generating model output.\n",
    "# model_name = 'llama-7b'\n",
    "# model_name = 'llama-7b_ft=hmv1'\n",
    "model_name = 'llama-7b+lora:r=256:a=256'\n",
    "\n",
    "\n",
    "save_dir = f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/data_inds/{model_name}/\"\n",
    "lm_output_dir = f'/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/{model_name}'\n",
    "save_dir = os.path.join(save_dir, dataset)\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f985baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482145\n"
     ]
    }
   ],
   "source": [
    "dataset = 'tulu_v1_mix'\n",
    "save_path = os.path.join(lm_output_dir, f'{dataset}.pkl')\n",
    "with open(save_path, 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "if test_run:\n",
    "    d = {k: v[:10000] for k, v in d.items()}\n",
    "\n",
    "# some entries are nan, impute with mean value.\n",
    "N = d['text_embedding'].shape[0]\n",
    "log_prob = np.nan_to_num(d['log_prob'], nan=np.nanmean(d['log_prob'])).squeeze()\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f792932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1539/49999 [01:25<44:42, 18.07it/s]  \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## the copy in `note_pruning.py` is most up to date\n",
    "from note_pruning import sort_dpp_map_memefficient\n",
    "\n",
    "\n",
    "\n",
    "pkl_extra = {}\n",
    "\n",
    "t0 = time.time()\n",
    "if any(sort_by.startswith(x) for x in [\n",
    "        'log_prob', \n",
    "        'el2n',  # el2n_agg={l2n|mean}\n",
    "        'logit_margin', \n",
    "        'grad',  # grad_{loraB|qkv|all|last}_l2n\n",
    "    ]):\n",
    "    if sort_by not in d:\n",
    "        print(f'sort_by={sort_by} not in lm_output_dir={lm_output_dir}')\n",
    "    S = np.nan_to_num(d[sort_by], nan=np.nanmean(d[sort_by])).squeeze()\n",
    "elif sort_by.startswith('random'):\n",
    "    match = re.search(r's=(\\d+)', sort_by)\n",
    "    seed = int(match.group(1))\n",
    "    random.seed(seed)\n",
    "    inds = list(range(N))\n",
    "    random.shuffle(inds)\n",
    "if sort_by.startswith('kmeans'):\n",
    "    dist_fn = 'l2' if sort_by.startswith('kmeansl2') else 'cd'\n",
    "    match = re.search(r'nc=(\\d+)', sort_by)\n",
    "    n_clusters = int(match.group(1)) if match else None\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    emb = d[embed_type]\n",
    "    print(f'Running kmeans(n_clusters={n_clusters}) {{ {embed_type} }} to compute {\"euclidean\" if dist_fn == \"l2\" else \"cosine\"} distance to cluster centers.')\n",
    "    S, kms = sort_kmeans_dist_to_cluster_centers(emb, n_clusters, dist_fn=dist_fn)\n",
    "    pkl_extra['kmeans'] = kms\n",
    "elif sort_by.startswith('dpp'):\n",
    "    match = re.search(r'k=(\\w+)', sort_by)\n",
    "    kernel_type = match.group(1) if match else None\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    emb = d[embed_type]\n",
    "    inds = sort_dpp_map_memefficient(emb, log_prob, kernel_type=kernel_type, torch_compile=False)\n",
    "t1 = time.time()\n",
    "print(f'Rank datapoints with {sort_by} took {t1-t0:.2f} seconds.')\n",
    "\n",
    "#     if any(sort_by.startswith(x) for x in ['dpp', 'random']):\n",
    "#         output = {'inds': inds}\n",
    "#         if pkl_extra:\n",
    "#             output.update(pkl_extra)\n",
    "#         save_to_pickle(\n",
    "#             save_path=os.path.join(save_dir, f'{sort_by}.pkl'),\n",
    "#             output=output)\n",
    "#     else:\n",
    "#         save_sorted_inds(save_dir, S, sort_by, extra=pkl_extra, reverse=False)\n",
    "#         save_sorted_inds(save_dir, S, sort_by, extra=pkl_extra, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10000,4096).astype(np.float32)\n",
    "b = np.random.rand(10000,4096).astype(np.float32)\n",
    "%timeit np.sum(a*b,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec32921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10000,4096).astype(np.float64)\n",
    "b = np.random.rand(10000,4096).astype(np.float64)\n",
    "%timeit np.sum(a*b,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea377f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
