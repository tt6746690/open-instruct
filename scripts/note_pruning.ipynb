{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf2dd69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,5\n",
      "5\n",
      "Sun Nov 19 21:23:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   224W / 300W |    551MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   48C    P0   251W / 300W |    551MiB / 32510MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   48C    P0   252W / 300W |    551MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   231W / 300W |    551MiB / 32510MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   4026082      C   python                            549MiB |\n",
      "|    1   N/A  N/A   4026090      C   python                            549MiB |\n",
      "|    2   N/A  N/A   4026086      C   python                            549MiB |\n",
      "|    3   N/A  N/A   4026081      C   python                            549MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1]\n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf384bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from rosemary import jpt_in_notebook\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "# model_name = 'llama-7b'; encode_fn_type = 'sft'; md = 'llama7b'\n",
    "# model_name = 'llama-7b+lora:r=256:a=256'; encode_fn_type = 'sft'; md = 'llama7b'\n",
    "model_name = 'mistral-7b+lora:r=256:a=256'; encode_fn_type = 'sft'; md = 'mistral7b'\n",
    "# model_name = 'all-mpnet-base-v2'; encode_fn_type = 'input'; md = 'mpnet'\n",
    "# model_name = 'bge-large-en-v1.5'; encode_fn_type = 'input'; md = 'bge'\n",
    "\n",
    "# nc_list = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "nc_list = [100, 500, 1000]\n",
    "nc_list = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "sort_by_list = [\n",
    "    f'semdedup_cl=kmeansfaisscd_md={md}_dist=cd_emb={emb}_nc={nc}'\n",
    "    for nc in nc_list\n",
    "    for emb in ['text+embedding', 'grad+rp+loraB'] # 'grad+rp+loraB' \n",
    "]\n",
    "\n",
    "# sort_by_list = [\n",
    "#     'random_s=0', 'random_s=1', 'random_s=2',\n",
    "#     'log_prob', 'logit_margin', 'el2n_agg=mean',\n",
    "#     'grad_loraB_l2n', 'numtoks'\n",
    "# ]\n",
    "# sort_by_list += [\n",
    "# #     'kmeansl2_emb=grad+rp+loraB_nc=300',\n",
    "# #     'kmeansl2_emb=grad+rp+loraB_nc=1000',\n",
    "#     'kmeansl2_emb=grad+rp+loraB_nc=3000',\n",
    "# #     'kmeansl2_emb=text+embedding_nc=300',\n",
    "#     'kmeansl2_emb=text+embedding_nc=1000',\n",
    "# #     'kmeansl2_emb=text+embedding_nc=3000',\n",
    "# ]\n",
    "# sort_by_list = ['numtoks']\n",
    "\n",
    "# sort_by_list += [\n",
    "#     'dppmap_emb=grad+rp+loraB_k=Kcos', \n",
    "#     'dppmap_emb=text+embedding_k=Kcos', \n",
    "#     'dppmap_emb=grad+rp+loraB_k=Kcosp', \n",
    "#     'dppmap_emb=text+embedding_k=Kcosp',\n",
    "#     'dppmap_emb=grad+rp+loraB_k=Kcos1np', \n",
    "#     'dppmap_emb=text+embedding_k=Kcos1np',\n",
    "# ]\n",
    "# dataset_list = ['lima']\n",
    "# dataset_list = ['flan2022_1m']\n",
    "# dataset_list = ['tulu_v1_mix']\n",
    "# dataset_list = ['ultrachat']\n",
    "dataset_list = ['ultrachat15']\n",
    "# dataset_list = ['wizardlm']\n",
    "# dataset_list = ['sharegpt']\n",
    "\n",
    "\n",
    "# sort_by_list = [\n",
    "#  'log_prob',\n",
    "#  'el2n_agg=mean',\n",
    "#  'el2n_agg=l2n',\n",
    "#  'logit_margin',\n",
    "# ]\n",
    "# if 'lora' in model_name:\n",
    "#     sort_by_list += ['grad_loraB_l2n']\n",
    "# else:\n",
    "#     sort_by_list += ['grad_all_l2n', 'grad_qkv_l2n', 'grad_mlp_l2n', 'grad_last_l2n',]\n",
    "# sort_by_list = ['kmeansl2_emb=grad+rp+loraB_nc=30',\n",
    "#                 'kmeansl2_emb=text+embedding_nc=30']\n",
    "# dataset_list = ['lima']\n",
    "\n",
    "\n",
    "# model_name = 'pythia-1b-deduped'\n",
    "# model_name = 'pythia-1b-deduped+lora:r=256:a=256'\n",
    "# dataset_list = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1']\n",
    "# # sort_by_list = ['random_s=0', \n",
    "# #                 'log_prob', 'logit_margin', 'el2n_agg=mean', 'el2n_agg=l2n', \n",
    "# #                 'kmeansl2_nc=3000', 'kmeanscd_nc=3000',\n",
    "# #                 'grad_loraB_l2n',\n",
    "# #                 'grad_all_l2n', 'grad_qkv_l2n', 'grad_mlp_l2n', 'grad_last_l2n',\n",
    "# #                ]\n",
    "# sort_by_list = ['grad_loraB_l2n']\n",
    "\n",
    "from note_pruning_analysis import data_inds_dir\n",
    "\n",
    "options_list = itertools.product(dataset_list, sort_by_list)\n",
    "\n",
    "print('test_run =',test_run)\n",
    "cmds = []\n",
    "for dataset, sort_by in options_list:\n",
    "    save_dir = os.path.join(data_inds_dir, model_name, dataset)\n",
    "    cmd = f\"\"\"\n",
    "     python note_pruning.py \\\n",
    "        --dataset {dataset} \\\n",
    "        --sort_by {sort_by} \\\n",
    "        --model_name {model_name} \\\n",
    "        --encode_fn_type {encode_fn_type} \\\n",
    "        --save_dir {save_dir} \\\n",
    "    \"\"\".strip()\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'prune.{dataset}.{sort_by}', \n",
    "        nodes=1,\n",
    "        num_cpus=64, # 32\n",
    "        cpu_mem=256, # 128\n",
    "        num_gpus=1,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    cmds.append(cmd)\n",
    "    print(cmd)\n",
    "        \n",
    "print('#cmds: ', len(cmds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a7b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('note_pruning_run_cmds.sh', 'w') as f:\n",
    "    s = 'set -e\\nset -x\\n'\n",
    "    devices = os.environ['CUDA_VISIBLE_DEVICES']\n",
    "    devices = 1\n",
    "    s += '\\n\\n'.join([f\"CUDA_VISIBLE_DEVICES={devices} \"+x for x in cmds])\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363aa87",
   "metadata": {},
   "source": [
    "### Generate curriculum from pre-computed scores (via `note_pruning.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25a293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from note_curriculum import (\n",
    "    get_curriculum_scores,\n",
    "    get_curriculum,\n",
    "    generate_curriculum,\n",
    "    generate_curriculum_forall_scoring_fn,\n",
    "    scores_path_to_attrs,\n",
    "    np_random_choice_maximize_noreplacement,\n",
    "    plt_curriculum,\n",
    ")\n",
    "from note_pruning_analysis import assets_dir\n",
    "\n",
    "# model_name = 'llama-7b'; dataset = 'tulu_v1_mix'; M = 150_000\n",
    "# model_name = 'llama-7b'; dataset = 'sharegpt'; M = 150_000\n",
    "\n",
    "## mistral+ultrachat\n",
    "# model_name = 'mistral-7b'; dataset = 'ultrachat200k'; M = 50_000\n",
    "# model_name = 'mistral-7b'; dataset = 'ultrachat15'; M = 100_000\n",
    "\n",
    "## semdedup\n",
    "# model_name = 'llama-7b'; dataset = 'wizardlm'; M = 100_000\n",
    "# model_name = 'all-mpnet-base-v2'; dataset = 'wizardlm'; M = 100_000\n",
    "# model_name = 'bge-large-en-v1.5'; dataset = 'wizardlm'; M = 100_000\n",
    "# model_name = 'all-mpnet-base-v2'; dataset = 'ultrachat15'; M = 100_000\n",
    "model_name = 'mistral-7b'; dataset = 'ultrachat15'; M = 100_000\n",
    "\n",
    "\n",
    "pacing_fn_list = [\n",
    "#     f'prune_size={M}_ep=1',\n",
    "    f'prune_size={M}_ep=2',\n",
    "#     f'prune_size={M}_ep=3',\n",
    "#     f'singlestep_size={M}_startingfrac=0.1',\n",
    "#     f'singlestep_size={M}_startingfrac=0.05',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "]\n",
    "\n",
    "output_list = generate_curriculum_forall_scoring_fn(\n",
    "    model_name, dataset, pacing_fn_list, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774da90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cdbf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from note_curriculum import get_curriculum_scores, generate_curriculum, plt_curriculum\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = 'mistral-7b'; dataset = 'ultrachat'; M =  50_000\n",
    "# model_name = 'llama-7b'; dataset = 'tulu_v1_mix'; M = 150_000\n",
    "\n",
    "\n",
    "paths = glob.glob('curriculum/*/*/*/scores.pkl')\n",
    "paths = [x for x in paths if 'llama' in x and 'tulu_v1_mix' in x and 'log_prob_neg' in x]\n",
    "path = paths[0]\n",
    "\n",
    "verbose = True\n",
    "print(path)\n",
    "pacing_fn = f'prune_size={M}_ep=3'\n",
    "# pacing_fn = f'singlestep_size={M}_startingfrac=0.1'\n",
    "# pacing_fn = f'singlestep_size={M}_startingfrac=0.2'\n",
    "# pacing_fn = f'singlestep_size={M}_startingfrac=0.3'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=2'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=1.5'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.2_inc=1.5'\n",
    "# pacing_fn = f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=1.5'\n",
    "\n",
    "pacing_fn_list = [\n",
    "    f'prune_size={M}_ep=3',\n",
    "    f'singlestep_size={M}_startingfrac=0.05',\n",
    "#     f'singlestep_size={M}_startingfrac=0.1',\n",
    "#     f'singlestep_size={M}_startingfrac=0.2',\n",
    "#     f'singlestep_size={M}_startingfrac=0.3',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=1.5',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=2',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.1_inc=3',\n",
    "    f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.5',\n",
    "    f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=2',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=3',\n",
    "#     f'fep_size={M}_nsteps=5_startingfrac=0.05_inc=1.25'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "nrows = len(pacing_fn_list)\n",
    "fig, axs = plt.subplots(nrows, 3, figsize=(15,3*nrows), sharey=False, gridspec_kw={'width_ratios': [2,.5,.5]})\n",
    "\n",
    "for i, pacing_fn in enumerate(pacing_fn_list):\n",
    "\n",
    "    plt_kwargs = generate_curriculum(path, pacing_fn, verbose=True, save_output=False)\n",
    "    output = plt_kwargs.pop('output')\n",
    "    plt_kwargs.update({'fig': fig, 'axs': axs[i]})\n",
    "    plt_curriculum(**plt_kwargs)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "save_plt = 0\n",
    "if save_plt:\n",
    "    model_name, dataset, scoring_fn = output['model_name'], output['dataset'], output['scoring_fn']\n",
    "    save_path = os.path.join(\n",
    "        assets_dir, f'note_curriculum_{model_name}:{dataset}:{scoring_fn}.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight', dpi=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82473d",
   "metadata": {},
   "source": [
    "### main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1891b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from note_pruning import (\n",
    "    save_to_pickle,\n",
    "    save_sorted_inds,\n",
    "    sort_kmeans_dist_to_cluster_centers,\n",
    "    sort_dpp_map,\n",
    "    save_prune_results,\n",
    "    sort_dpp_map_memefficient,\n",
    ")\n",
    "from note_pruning_analysis import get_lm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c45b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wizardlm all-mpnet-base-v2 dppmap_emb=text+embedding_k=Kcos\n",
      "data_inds/input/all-mpnet-base-v2/wizardlm\n"
     ]
    }
   ],
   "source": [
    "test_run = False\n",
    "dataset = 'tulu_v1_human_mix'\n",
    "dataset = 'tulu_v2_human_mix'\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'lima'\n",
    "dataset = 'flan2022_1m'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat200k'\n",
    "dataset = 'ultrachat15'\n",
    "dataset = 'wizardlm'\n",
    "\n",
    "# sort_by = 'random_s=0'\n",
    "# sort_by = 'kmeansl2_nc=3000'\n",
    "# sort_by = 'kmeanscd_nc=3000'\n",
    "# sort_by = 'log_prob'\n",
    "# sort_by = 'dppmap_k=Kcos'\n",
    "# sort_by = 'dppmap_k=Kcos1np'\n",
    "# sort_by = 'el2n'\n",
    "# sort_by = 'grad_norm'\n",
    "# sort_by = 'kmeansl2_emb=grad+rp+loraB_nc=3000'\n",
    "# sort_by = 'kmeansl2_emb=text+embedding_nc=3000'\n",
    "sort_by = 'dppmap_emb=text+embedding_k=Kcos'\n",
    "# sort_by = 'logit_margin'\n",
    "# rhov1: mistral-7b base-tuned(ultrachat200k_beforesplitlongconv)\n",
    "# sort_by = 'rhov1'\n",
    "# sort_by = 'numtoks'\n",
    "\n",
    "# used for generating model output.\n",
    "# model_name = 'llama-7b'; encode_fn_type = 'sft'\n",
    "# model_name = 'llama-7b_ft=hmv1'; encode_fn_type = 'sft'\n",
    "# model_name = 'llama-7b+lora:r=256:a=256'; encode_fn_type = 'sft'\n",
    "# model_name = 'mistral-7b+lora:r=256:a=256'; encode_fn_type = 'sft'\n",
    "model_name = 'all-mpnet-base-v2'; encode_fn_type = 'input'\n",
    "# model_name = 'bge-large-en-v1.5'; encode_fn_type = 'input'\n",
    "\n",
    "# model_name = 'mistral-7b+lora:r=256:a=256__rho__mistral-7b-ultrachat200k-v1+lora:r=256:a=256'\n",
    "\n",
    "\n",
    "save_dir = f\"data_inds/\"\n",
    "save_dir = os.path.join(save_dir, '' if encode_fn_type=='sft' else encode_fn_type, model_name, dataset)\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "\n",
    "print(dataset, model_name, sort_by)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_lm_output(dataset, model_name, encode_fn_type=encode_fn_type, return_text_embedding=True)\n",
    "if test_run:\n",
    "    d = {k: v[:1000] for k, v in d.items()}\n",
    "    \n",
    "# some entries are nan, impute with mean value.\n",
    "N = d['text_embedding'].shape[0]\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b84014",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = 'dppmap_emb=text+embedding_k=Kcos'\n",
    "# 'semdedup_cl=kmeansfaisscd_md=mistral7b_dist=cd_emb=text+embedding_nc=200',\n",
    "\n",
    "sort_by = 'dppmap_cl=kmeansfaisscd_md=mpnet_emb=text+embedding_nc=200'\n",
    "\n",
    "\n",
    "match = re.search(r'k=(\\w+)', sort_by)\n",
    "kernel_type = match.group(1) if match else None\n",
    "match = re.search(r'emb=([^_]+)', sort_by)\n",
    "embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "    raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "md = re.search(r'md=([^_]+)', sort_by).group(1)\n",
    "if (md == 'mpnet' and model_name != 'all-mpnet-base-v2') or \\\n",
    "   (md == 'bge' and model_name != 'bge-large-en-v1.5') or \\\n",
    "   (md == 'llama7b' and not model_name.lower().startswith('llama-7b')) or \\\n",
    "   (md == 'mistral7b' and not model_name.lower().startswith('mistral-7b')):\n",
    "    raise ValueError(f'md={md} does not match with model_name={model_name}')\n",
    "print(kernel_type, embed_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f6c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match = re.search(r'k=(\\w+)', sort_by)\n",
    "kernel_type = match.group(1) if match else None\n",
    "match = re.search(r'emb=([^_]+)', sort_by)\n",
    "embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "    raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    md = re.search(r'md=([^_]+)', sort_by).group(1)\n",
    "    if (md == 'mpnet' and model_name != 'all-mpnet-base-v2') or \\\n",
    "       (md == 'bge' and model_name != 'bge-large-en-v1.5') or \\\n",
    "       (md == 'llama7b' and not model_name.lower().startswith('llama-7b')) or \\\n",
    "       (md == 'mistral7b' and not model_name.lower().startswith('mistral-7b')):\n",
    "        raise ValueError(f'md={md} does not match with model_name={model_name}')\n",
    "emb = d[embed_type]\n",
    "log_prob = d['log_prob']\n",
    "# inds = sort_dpp_map_memefficient(emb, log_prob, kernel_type=kernel_type, torch_compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6b3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f792932",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the copy in `note_pruning.py` is most up to date\n",
    "\n",
    "pkl_extra = {}\n",
    "inds = None\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "if any(sort_by.startswith(x) for x in [\n",
    "        'log_prob', \n",
    "        'el2n',  # el2n_agg={l2n|mean}\n",
    "        'logit_margin', \n",
    "        'grad',  # grad_{loraB|qkv|all|last}_l2n\n",
    "    ]):\n",
    "    if sort_by not in d:\n",
    "        print(f'sort_by={sort_by} not in model output: ({dataset}, {model_name})')\n",
    "    S = np.nan_to_num(d[sort_by], nan=np.nanmean(d[sort_by])).squeeze()\n",
    "elif sort_by.startswith('random'):\n",
    "    match = re.search(r's=(\\d+)', sort_by)\n",
    "    seed = int(match.group(1))\n",
    "    np.random.seed(seed)\n",
    "    S = np.random.rand(N)\n",
    "    assert(S.shape == np.unique(S).shape)\n",
    "if sort_by.startswith('kmeans'):\n",
    "    dist_fn = 'l2' if sort_by.startswith('kmeansl2') else 'cd'\n",
    "    match = re.search(r'nc=(\\d+)', sort_by)\n",
    "    n_clusters = int(match.group(1)) if match else None\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    emb = d[embed_type]\n",
    "    print(f'Running kmeans(n_clusters={n_clusters}) {{ {embed_type} }} to compute {\"euclidean\" if dist_fn == \"l2\" else \"cosine\"} distance to cluster centers.')\n",
    "    S, kms = sort_kmeans_dist_to_cluster_centers(emb, n_clusters, dist_fn=dist_fn)\n",
    "    pkl_extra['kmeans'] = kms\n",
    "elif sort_by.startswith('semdedup'):\n",
    "    import note_pruning_clustering\n",
    "    md = re.search(r'md=([^_]+)', sort_by).group(1)\n",
    "    if (md == 'mpnet' and model_name != 'all-mpnet-base-v2') or \\\n",
    "       (md == 'bge' and model_name != 'bge-large-en-v1.5') or \\\n",
    "       (md == 'llama7b' and not model_name.lower().startswith('llama-7b')) or \\\n",
    "       (md == 'mistral7b' and not model_name.lower().startswith('mistral-7b')):\n",
    "        raise ValueError(f'md={md} does not match with model_name={model_name}')\n",
    "    clustering_fn = sort_by.split('semdedup_')[-1]\n",
    "    match = re.search(r'dist=([^_]+)', sort_by)\n",
    "    dist = match.group(1)\n",
    "    assert(dist in ['cd', 'l2'])\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1))\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    save_dir_clustering = os.path.join('clustering', encode_fn_type, model_name, dataset, clustering_fn)\n",
    "    os.makedirs(save_dir_clustering, exist_ok=True)\n",
    "    # normalize embeddings to unit norm if the model that generated the embeddings does the \n",
    "    # same, e.g., mpnet, bge, or if using spherical kmeans clustering.\n",
    "    if any(x in model_name for x in ['mpnet', 'bge']) or 'kmeansfaisscd' in clustering_fn:\n",
    "        normalize_embeddings = True\n",
    "    else:\n",
    "        normalize_embeddings = False\n",
    "    kwargs = {\n",
    "        'model_name': model_name,\n",
    "        'dataset': dataset,\n",
    "        'encode_fn_type': encode_fn_type,\n",
    "        'clustering_fn': clustering_fn,\n",
    "        'embed_type': embed_type,\n",
    "        'normalize_embeddings': normalize_embeddings,\n",
    "        'first_N': None,\n",
    "        'save_dir': save_dir_clustering,\n",
    "    }\n",
    "    print(f'Calling note_pruning_clustering.main with kwargs={json.dumps(kwargs, indent=4)}')\n",
    "    X, Y, C = note_pruning_clustering.main(**kwargs)\n",
    "    print('Apply SemDeDup to discard duplicates.')\n",
    "    S = note_pruning_clustering.semdedup(X, Y, dist=dist, device='cuda')\n",
    "elif sort_by.startswith('dpp'):\n",
    "    match = re.search(r'k=(\\w+)', sort_by)\n",
    "    kernel_type = match.group(1) if match else None\n",
    "    match = re.search(r'emb=([^_]+)', sort_by)\n",
    "    embed_type = re.sub(r'[+]', '_', match.group(1)) if match else 'text_embedding'\n",
    "    if embed_type not in set(d.keys()).intersection(set(['text_embedding', 'grad_rp_loraB'])):\n",
    "        raise ValueError(f'Invalid embed_type = {embed_type}')\n",
    "    emb = d[embed_type]\n",
    "    log_prob = d['log_prob']\n",
    "    inds = sort_dpp_map_memefficient(emb, log_prob, kernel_type=kernel_type, torch_compile=False)\n",
    "elif sort_by.startswith('rho'):\n",
    "    if sort_by == 'rhov1':\n",
    "        model_names = ['mistral-7b+lora:r=256:a=256',\n",
    "                       'mistral-7b-ultrachat200k-v1+lora:r=256:a=256']\n",
    "        assert(model_name == model_names[0])\n",
    "    else:\n",
    "        raise ValueError(f'sort_by={sort_by} not implemented.')\n",
    "    assert(len(model_names) == 2)\n",
    "    ds = []\n",
    "    for x in model_names:\n",
    "        ds.append(get_lm_output(dataset, x, return_text_embedding=False, fill_nan=False))\n",
    "    ks = [set(d.keys()) for d in ds]\n",
    "    ks = ks[0] & ks[1]\n",
    "    for k in ks:\n",
    "        S0 = ds[0][k]\n",
    "        S1 = ds[1][k]\n",
    "        # handle nan entries properly.\n",
    "        nan_mask = np.logical_or(np.isnan(S0), np.isnan(S1))\n",
    "        S = np.subtract(S0, S1)\n",
    "        S[nan_mask] = np.nan\n",
    "        S = S.squeeze()\n",
    "        save_prune_results(save_dir, None, S, {}, f'{sort_by}_{k}', model_name, dataset)\n",
    "elif sort_by.startswith('numtoks'):\n",
    "    from transformers import AutoTokenizer\n",
    "    from note_pruning_analysis import get_dataset_token_lengths\n",
    "    if 'llama' in model_name or 'mistral' in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/results/baselines/huggyllama/llama-7b',\n",
    "            use_fast=False, # use_fast sometimes cause error.\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Need to supply appropriate tokenizer to count token lengths,')\n",
    "    d = get_dataset_token_lengths(dataset, tokenizer)\n",
    "\n",
    "    d['total_len'] = d['input_len'] + d['output_len']\n",
    "    for k in ['input', 'output', 'total']:\n",
    "        S = d[f'{k}_len']\n",
    "        save_prune_results(save_dir, None, S, {}, f'{sort_by}_{k}', model_name, dataset)\n",
    "\n",
    "        \n",
    "t1 = time.time()\n",
    "print(f'Rank datapoints with {sort_by} took {t1-t0:.2f} seconds.')\n",
    "\n",
    "# from note_pruning import save_prune_results\n",
    "if not any(sort_by.startswith(x) for x in ['rho', 'numtoks']):\n",
    "    save_prune_results(save_dir, inds, S, pkl_extra, sort_by, model_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5b429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f483857",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'log_prob'\n",
    "t = 100\n",
    "S0 = ds[0][k].squeeze()[::t]\n",
    "S1 = ds[1][k].squeeze()[::t]\n",
    "# log prob neg\n",
    "S0, S1 = -S0, -S1\n",
    "# S = S0-S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0ece3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 3))\n",
    "xs = np.arange(len(S0))\n",
    "inds_sorted = np.argsort(S0)\n",
    "ax.plot(xs, S0[inds_sorted], label='S_base')\n",
    "ax.plot(xs, S1[inds_sorted], label='S_train')\n",
    "ax.plot(xs, S[inds_sorted], label='S_base-S_train')\n",
    "ax.legend()\n",
    "ax.set_title(f'{model_name}  {dataset}  sort_by={sort_by}', fontsize=20)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 3))\n",
    "xs = np.arange(len(S0))\n",
    "inds_sorted = np.argsort(S1)\n",
    "ax.plot(xs, S0[inds_sorted], label='S_base')\n",
    "ax.plot(xs, S1[inds_sorted], label='S_train')\n",
    "ax.plot(xs, S[inds_sorted], label='S_base-S_train')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 3))\n",
    "xs = np.arange(len(S0))\n",
    "inds_sorted = np.argsort(S)\n",
    "ax.plot(xs, S0[inds_sorted], label='S_base')\n",
    "ax.plot(xs, S1[inds_sorted], label='S_train')\n",
    "ax.plot(xs, S[inds_sorted], label='S_base-S_train')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ebd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec32921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8895c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea377f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
