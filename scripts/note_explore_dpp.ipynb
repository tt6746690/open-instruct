{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad5c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feca4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-21 16:45:57,045] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/fast-map-dpp\")\n",
    "from dpp import dpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19df2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "\n",
    "train_file = '../data/processed/all.jsonl'\n",
    "train_file = '../data/processed/flan_v2/flan_v2_data.jsonl'\n",
    "\n",
    "\n",
    "save_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts'\n",
    "save_path = os.path.join(save_dir, 'note_explore_dpp_llama-7b_flan_v2_outputs.pkl')\n",
    "gen_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e5954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-0fa1872b4ac9a29f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_files = {'train': train_file}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "print(len(raw_datasets['train']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2e47eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58b47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87366540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-0fa1872b4ac9a29f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3a8173652244b47d_*_of_00016.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'id', 'messages', 'input_ids', 'labels', 'attention_mask'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_function = partial(\n",
    "    encode_with_messages_format,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\n",
    "lm_datasets = raw_datasets.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    num_proc=16,\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", \n",
    "                         output_all_columns=False, \n",
    "                         columns=['input_ids', 'labels', 'attention_mask'])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a7ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7434c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_embeddings:\n",
    "    \n",
    "    # collate_fn = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding='longest') \n",
    "    loader = DataLoader(train_dataset, shuffle=False, batch_size=1) \n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    text_embeddings = []\n",
    "    log_probs = []\n",
    "\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        batch = {k: v.to('cuda', non_blocking=True) for k, v in batch.items()}\n",
    "        input_ids = batch['input_ids']\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "\n",
    "        # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "        text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "\n",
    "        # sum of output token log probs\n",
    "        log_prob = -outputs['loss']\n",
    "\n",
    "        text_embeddings.append(text_embedding.detach().cpu().numpy())\n",
    "        log_probs.append(log_prob.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56e9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_embeddings:\n",
    "    d = {'text_embeddings': np.vstack(text_embeddings),\n",
    "         'log_probs': np.vstack(log_probs)}\n",
    "\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(d, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(save_path, 'rb') as f:\n",
    "        d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa0f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "M = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c137c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some entries are nan.\n",
    "d['log_probs'] = np.nan_to_num(d['log_probs'], nan=np.nanmean(d['log_probs']))\n",
    "\n",
    "text_embeddings = d['text_embeddings'][:N,:]\n",
    "log_probs = d['log_probs'][:N,:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff245855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get an idea of log-prob vs. length. see if there is bias & need to normalize by length.\n",
    "# - no obvious correlation between #tokens/seq and log P_LM(seq).\n",
    "# - true larger number of tokens, the log probability is larger\n",
    "\n",
    "num_tokens = []\n",
    "for i in range(N):\n",
    "    n = len(train_dataset[i]['input_ids'])\n",
    "    num_tokens.append(n)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.scatter(num_tokens, log_probs)\n",
    "ax.set_xlabel('#Tokens / Seq')\n",
    "ax.set_ylabel('log P_LM(Seq)')\n",
    "# ax.set_xlim((0,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fec2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## what are low/high p_LM(seq) examples?\n",
    "#  - low: input prompt has excotic knowledge, or need to come up with exotic knowledge in the answer.\n",
    "#  - high: output already exists in the input prompt (copy 1 sentence, remove punctuation, summarization)\n",
    "\n",
    "K = 10\n",
    "inds = np.argsort(log_probs)\n",
    "inds = inds[-K:] # top k\n",
    "inds = inds[:K]  # bot k\n",
    "\n",
    "for k in range(K):\n",
    "    ind = int(inds[k])\n",
    "    print('\\n'+'==='*20+f' k={k},ind={str(ind)},logprob={log_probs[ind]:.3f}')\n",
    "    print(tokenizer.decode(train_dataset[ind]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "afb5cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    T = torch.from_numpy(text_embeddings).to('cuda').to(torch.float32)\n",
    "    logP = torch.from_numpy(log_probs).to('cuda').to(torch.float32)\n",
    "    \n",
    "    T = torch.nn.functional.normalize(T, dim=-1)\n",
    "    ## out-of-memory\n",
    "    # S = T@T.T\n",
    "    ## block-wise matmul to reduce peak memory usage.\n",
    "    L = []\n",
    "    for Tn in torch.split(T, 10000):\n",
    "        L.append((Tn@T.T).to('cpu'))\n",
    "    S = torch.vstack(L)\n",
    "    P = logP.exp().to('cpu')\n",
    "\n",
    "    K_cos = S\n",
    "    K_cos_prob = P.reshape(N,1)*S*P.reshape(1,N)\n",
    "    K_cos_oneminusprob = (1-P).reshape(N,1)*S*(1-P).reshape(1,N)\n",
    "    \n",
    "    K_cos = K_cos.to('cpu').numpy()\n",
    "    K_cos_prob = K_cos_prob.to('cpu').numpy()\n",
    "    K_cos_oneminusprob = K_cos_oneminusprob.to('cpu').numpy()\n",
    "    \n",
    "else:\n",
    "    text_embeddings /= np.linalg.norm(text_embeddings, axis=-1, keepdims=True)\n",
    "    similarities = np.dot(text_embeddings, text_embeddings.T) # cosine sim\n",
    "    probs = np.exp(log_probs)\n",
    "\n",
    "    K_cos = similarities \n",
    "    K_cos_prob = probs.reshape(N, 1) * similarities * probs.reshape(1, N)\n",
    "    K_cos_oneminusprob = (1-probs).reshape(N, 1) * similarities * (1-probs).reshape(1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94fd4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add jitter ensures matrices are psd\n",
      "add jitter ensures matrices are psd done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cholesky_jitter(K, jitter=1e-5):\n",
    "    K[np.diag_indices_from(K)] += jitter\n",
    "    return K\n",
    "\n",
    "def cholesky_jitter_variable(K, jitters=[0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]):\n",
    "    for v in jitters:\n",
    "        try:\n",
    "            Kt = cholesky_jitter(K, v)\n",
    "            np.linalg.cholesky(Kt)\n",
    "            print(v)\n",
    "            return Kt\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    raise ValueError()\n",
    "\n",
    "\n",
    "print('add jitter ensures matrices are psd')\n",
    "# cholesky O(N^3) too costly\n",
    "# K_cos = cholesky_jitter_variable(K_cos)\n",
    "# K_cos_prob = cholesky_jitter_variable(K_cos_prob)\n",
    "# K_cos_oneminusprob = cholesky_jitter_variable(K_cos_oneminusprob)\n",
    "K_cos = cholesky_jitter(K_cos, 1e-3)\n",
    "K_cos_prob = cholesky_jitter(K_cos_prob, 1e-3)\n",
    "K_cos_oneminusprob = cholesky_jitter(K_cos_oneminusprob, 1e-3)\n",
    "print('add jitter ensures matrices are psd done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rosemary import plt_kernel_matrix_one\n",
    "\n",
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "plt_kernel_matrix_one(fig, ax, K_cos)\n",
    "ax = axs[1]\n",
    "plt_kernel_matrix_one(fig, ax, K_cos_prob)\n",
    "ax = axs[2]\n",
    "plt_kernel_matrix_one(fig, ax, K_cos_oneminusprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be3b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Ks = {'K_cos': K_cos, 'K_cos_prob': K_cos_prob, 'K_cos_oneminusprob': K_cos_oneminusprob}\n",
    "for kernel_matrix_name, K in Ks.items():\n",
    "    out = {}\n",
    "    s = time.time()\n",
    "    inds = dpp(K, N) # select till triggers stopping criterion\n",
    "    print(f'running: {kernel_matrix_name} has len={len(inds)} cost {time.time()-s:.2f} seconds')\n",
    "    out['K'] = inds\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'note_explore_dpp_llama-7b_flan_v2_subsets_{kernel_matrix_name}.pkl')\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(out, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5aad351",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(save_dir, 'note_explore_dpp_llama-7b_flan_v2_subsets_K_cos.pkl')\n",
    "\n",
    "with open(save_path, 'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05c3e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_cos_M=5000 4438\n",
      "K_cos_prob_M=5000 2226\n",
      "K_cos_oneminusprob_M=5000 2755\n",
      "K_cos_M=10000 4438\n",
      "K_cos_prob_M=10000 2226\n",
      "K_cos_oneminusprob_M=10000 2755\n",
      "K_cos_M=20000 4438\n",
      "K_cos_prob_M=20000 2226\n",
      "K_cos_oneminusprob_M=20000 2755\n",
      "K_cos_M=50000 4438\n",
      "K_cos_prob_M=50000 2226\n",
      "K_cos_oneminusprob_M=50000 2755\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
