{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a61370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "\n",
    "train_file = '../data/processed/all.jsonl'\n",
    "train_file = '../data/processed/flan_v2/flan_v2_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_files = {'train': train_file}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "print(len(raw_datasets['train']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87366540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode_function = partial(\n",
    "    encode_with_messages_format,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\n",
    "lm_datasets = raw_datasets.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    num_proc=16,\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", \n",
    "                         output_all_columns=False, \n",
    "                         columns=['input_ids', 'labels', 'attention_mask'])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# collate_fn = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding='longest') \n",
    "\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "\n",
    "text_embeddings = []\n",
    "log_probs = []\n",
    "\n",
    "for batch in tqdm(loader, total=len(loader)):\n",
    "    batch = {k: v.to('cuda', non_blocking=True) for k, v in batch.items()}\n",
    "    input_ids = batch['input_ids']\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "    \n",
    "    # sum of output token log probs\n",
    "    log_prob = -outputs['loss']\n",
    "    \n",
    "    text_embeddings.append(text_embedding.detach().cpu().numpy())\n",
    "    log_probs.append(log_prob.detach().cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts'\n",
    "save_path = os.path.join(save_dir, 'note_explore_dpp_llama-7b_flan_v2_outputs.pkl')\n",
    "\n",
    "import pickle\n",
    "\n",
    "d = {'text_embeddings': np.vstack(text_embeddings),\n",
    "     'log_probs': np.vstack(log_probs)}\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(d, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bda85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(save_path, 'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
