{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c00e3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Sun Dec 17 00:38:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook, jpt_parse_args, jpt_setup; jpt_setup()\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7ae114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-17 00:38:51,870] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "import pyarrow\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    CodeLlamaTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    set_seed,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2TokenizerFast, \n",
    "    OPTForCausalLM,\n",
    ")\n",
    "from transformers import Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6f05bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from open_instruct.finetune_trainer import ModelArguments, DataTrainingArguments, TrainingArguments\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1c85a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='../results/baselines/NousResearch/Llama-2-7b-hf', config_name=None, tokenizer_name='../results/baselines/NousResearch/Llama-2-7b-hf', use_flash_attn=False, cache_dir=None, use_fast_tokenizer=True, model_revision='main', token=None, use_auth_token=None, trust_remote_code=False, torch_dtype='float32', use_lora=False, lora_rank=64, lora_alpha=16, lora_dropout=0.1, load_in_8bit=False, low_cpu_mem_usage=False)\n",
      "DataTrainingArguments(dataset_name=None, dataset_config_name=None, train_file='data/processed/sharegpt/sharegptv2_data.jsonl', max_train_samples=None, streaming=False, overwrite_cache=False, preprocessing_num_workers=16, max_seq_length=2048, subsample_mixture=None, subsample_inds_file=None)\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=8,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_sampler=RandomSampler,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=5,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=results/jpt_NousResearch:Llama-2-7b-hf_humanmix/runs/Dec17_00-39-00_dcs073,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=results/jpt_NousResearch:Llama-2-7b-hf_humanmix,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=results/jpt_NousResearch:Llama-2-7b-hf_humanmix,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=5,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = 'results/huggyllama:llama-7b_human_mix-trainer_savebystep'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "model_name_or_path = '../results/baselines/codellama/CodeLlama-7b-hf'\n",
    "model_name_or_path = '../results/baselines/NousResearch/Llama-2-7b-hf'\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'\n",
    "# model_name_or_path = '../results/baselines/gpt2-medium'\n",
    "# model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "# model_name_or_path = '../results/baselines/EleutherAI/pythia-70m'\n",
    "\n",
    "# train_file = 'data/processed/all.jsonl'\n",
    "# subsample_mixture =  {'cot': 976, 'dolly': 15, 'flan_v2': 976, 'oasst1': 34}; subsample_inds_file=None\n",
    "\n",
    "train_file = 'data/processed/sharegpt/sharegptv2_data.jsonl'\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'\n",
    "# train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'\n",
    "# train_file = 'data/processed/starcoder/starcoder_commentinstr_cleaned.jsonl'\n",
    "\n",
    "\n",
    "# subsample_inds_file = os.path.join(\n",
    "#     '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "#     'note_explore_dpp_llama-7b_flan_v2_subsets_K_cos.pkl'); subsample_mixture = None\n",
    "# train_file = 'data/processed/ultrachat/ultrachat_data.jsonl'\n",
    "subsample_inds_file = None; subsample_mixture = None\n",
    "dataloader_sampler = 'SequentialSampler' # 'RandomSampler'|'SequentialSampler'\n",
    "dataloader_sampler = None\n",
    "\n",
    "#     --use_lora \\\n",
    "#     --lora_rank 8 \\\n",
    "#     --lora_alpha 8 \\\n",
    "#     --lora_dropout 0.05 \n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 5\n",
    "save_total_limit = 1\n",
    "eval_steps = save_steps\n",
    "overwrite_output_dir = True # false if want to test trainer resume from dir.\n",
    "\n",
    "#     --report_to tensorboard wandb \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --max_seq_length 2048 \\\n",
    "    --train_file {train_file} \\\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 128 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    \n",
    "    --evaluation_strategy=steps \\\n",
    "    --eval_steps={eval_steps} \\\n",
    "    --report_to none \\\n",
    "    --logging_strategy=steps \\\n",
    "    --logging_first_step \\\n",
    "    --logging_steps=1 \\\n",
    "    --save_strategy={save_strategy} \\\n",
    "    --save_steps={save_steps} \\\n",
    "    --save_total_limit={save_total_limit} \\\n",
    "    \n",
    "    --num_train_epochs 1 \\\n",
    "    --fp16 \\\n",
    "    --torch_dtype float32 \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "    --use_flash_attn False \\\n",
    "    --output_dir \"results/jpt_{':'.join(model_name_or_path.split('/')[-2:])}_humanmix\" \\\n",
    "    --overwrite_output_dir {overwrite_output_dir} \\\n",
    "    {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "        if subsample_mixture else ''} \\\n",
    "    {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "    {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "\"\"\"\n",
    "#    --subsample_mixture='{{\"flan_v2\":100000}}'\n",
    "\n",
    "#      --subsample_mixture '{{\"baize\": 8333, \"code_alpaca\": 8333, \"cot\": 8333, \"dolly\": 8333, \"flan_v2\": 8333, \"gpt4_alpaca\": 8333, \"oasst1\": 8333, \"self_instruct\": 8333, \"sharegpt\": 8333, \"stanford_alpaca\": 8333, \"super_ni\": 8333, \"unnatural_instructions\": 8333}}'\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "# wpq: convert str to dict\n",
    "if data_args.subsample_mixture is not None:\n",
    "    import json\n",
    "    import re\n",
    "    data_args.subsample_mixture = re.compile('(?<!\\\\\\\\)\\'').sub('\\\"', data_args.subsample_mixture)\n",
    "    data_args.subsample_mixture = json.loads(data_args.subsample_mixture)\n",
    "    print('subsample mixture:')\n",
    "    print(data_args.subsample_mixture)\n",
    "if data_args.subsample_mixture is not None and data_args.subsample_inds_file is not None:\n",
    "    raise ValueError('Either use mixture proportion or exact subset indices, but not both.')\n",
    "if data_args.subsample_inds_file is not None:\n",
    "    if 'flan_v2' not in data_args.train_file:\n",
    "        raise ValueError('subset indices only support flan_v2 for now.')\n",
    "\n",
    "for x in [model_args, data_args, training_args]:\n",
    "    print(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff17b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving args dict to results/jpt_NousResearch:Llama-2-7b-hf_humanmix.args.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# wpq: save args to a json file\n",
    "with training_args.main_process_first(local=False, desc=f\"Saving args to `{training_args.output_dir+'.args.json'}`\"):\n",
    "    args_dict = {\n",
    "        'model_args': asdict(model_args),\n",
    "        'data_args': asdict(data_args),\n",
    "        'training_args': asdict(training_args),\n",
    "    }\n",
    "    args_dict_path = training_args.output_dir.strip('/')+'.args.json'\n",
    "    with open(args_dict_path, 'w') as f:\n",
    "        json.dump(args_dict, f)\n",
    "    print(f'Saving args dict to {args_dict_path}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b1b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with training_args.main_process_first(local=False, desc=f\"setup wandb init\"):\n",
    "#     if 'wandb' in training_args.report_to:\n",
    "#         wandb_init_kwargs_path = os.path.join(training_args.output_dir, 'wandb_init_kwargs.json')\n",
    "#         os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "#         if not os.path.isfile(wandb_init_kwargs_path):\n",
    "#             import wandb\n",
    "#             wandb_init_kwargs = {}\n",
    "#             wandb_init_kwargs['project'] = os.environ['WANDB_PROJECT']\n",
    "#             wandb_init_kwargs['name'] = training_args.run_name\n",
    "#             wandb_init_kwargs['resume'] = 'allow' # resume if `run_id` identical to previous, otherwise start new run\n",
    "#             wandb_init_kwargs['id'] = wandb.sdk.lib.runid.generate_id()\n",
    "#             wandb_init_kwargs['mode'] = 'offline'\n",
    "#             with open(wandb_init_kwargs_path, 'w') as f:\n",
    "#                 json.dump(wandb_init_kwargs, f, indent=4)\n",
    "#         else:\n",
    "#             with open(wandb_init_kwargs_path, 'r') as f:\n",
    "#                 wandb_init_kwargs = json.load(f)\n",
    "# wandb_init_kwargs\n",
    "# import wandb\n",
    "# wandb.init(**wandb_init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a02c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        # wpq: since I do write `.json` file to output_dir, raise if there are more files.\n",
    "        if len(os.listdir(training_args.output_dir))>1:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        \n",
    "last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4870f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-66f58c7711af992c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731c0849128f4293b47a1ed904f14712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        token=model_args.token,\n",
    "        streaming=data_args.streaming,\n",
    "        download_mode=datasets.GenerateMode.FORCE_REDOWNLOAD if data_args.overwrite_cache else datasets.GenerateMode.REUSE_DATASET_IF_EXISTS,\n",
    "    )\n",
    "else: \n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "    if 'ultrachat' in data_args.train_file:\n",
    "        data_files['test'] = (\n",
    "            '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/'\n",
    "            'data/processed/ultrachat/ultrachat200k_test_data.jsonl')\n",
    "    raw_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        **dataset_args,\n",
    "    )\n",
    "    if 'ultrachat' in data_args.train_file:\n",
    "        raw_datasets['test'] = raw_datasets['test'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2b3a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"../results/baselines/NousResearch/Llama-2-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    \"trust_remote_code\": True if 'mpt' in model_args.model_name_or_path else False,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    # wpq: add support for mpt models.\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "        config.attn_config['attn_impl'] = 'triton'\n",
    "        config.init_device = 'cuda' # For fast initialization directly on GPU!\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported by this finetuning script.\"\n",
    "    )\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer if 'pythia' not in model_args.model_name_or_path else True,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this finetuning script.\"\n",
    "    )\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33366d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3bfdc6b9364ed0b36edc2f59a8c137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/transformers/src/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        token=model_args.token,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n",
    "        use_flash_attention_2=True if model_args.use_flash_attn else False,\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"No pretrained model_name_or_path is given. Training new model from scratch.\")\n",
    "    model = AutoModelForCausalLM.from_config(config, trust_remote_code=model_args.trust_remote_code)\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "\n",
    "# wpq: `use_cache=True` is incompatible with gradient checkpointing\n",
    "model.config.use_cache = True if not training_args.gradient_checkpointing else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600e8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map \n",
    "if isinstance(tokenizer, (LlamaTokenizer, LlamaTokenizerFast, CodeLlamaTokenizerFast)):\n",
    "    from transformers import AddedToken\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": AddedToken(\"<s>\", normalized=False, special=True),\n",
    "        \"eos_token\": AddedToken(\"</s>\", normalized=False, special=True),\n",
    "        \"unk_token\": AddedToken(\"<unk>\", normalized=False, special=True),\n",
    "        \"pad_token\": AddedToken(\"<pad>\", normalized=False, special=True),\n",
    "    })\n",
    "    ## wpq: for `huggyllama`/`NousResearch/Llama-2-7b-hf`, `LlamaTokenizerFast` tokenizer config not properly implemented and cannot tokenize special tokens like eos_token corretly. Need the following workaround. More details: https://github.com/huggingface/transformers/issues/23833\n",
    "    if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "        if os.path.isdir(model_args.model_name_or_path):\n",
    "            tmp_tok_path = os.path.join(\n",
    "                os.path.dirname(model_args.model_name_or_path),\n",
    "                os.path.basename(model_args.model_name_or_path)+'_fixtok')\n",
    "            if not os.path.isdir(tmp_tok_path):\n",
    "                raise ValueError(f'Not valid fixtok path: {tmp_tok_path}')\n",
    "        else:\n",
    "            from secrets import token_hex\n",
    "            tmp_tok_path = f'/tmp/wpq_tok_{token_hex(16)}'\n",
    "            tokenizer.save_pretrained(tmp_tok_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tmp_tok_path, **tokenizer_kwargs)\n",
    "    for s, s_tokenized in [\n",
    "        (\"Hi<s>Hey</s>sir<unk>what<pad><pad>\", \n",
    "        ['▁Hi', '<s>', '▁Hey', '</s>', '▁sir', '<unk>', '▁what', '<pad>', '<pad>']),\n",
    "    ]:\n",
    "        assert(tokenizer.tokenize(s, add_special_tokens=False)==s_tokenized)\n",
    "elif isinstance(tokenizer, GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, (GPT2Tokenizer, GPT2TokenizerFast)) and isinstance(model, OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({'unk_token': '<unk>'})\n",
    "elif isinstance(tokenizer, (GPT2Tokenizer, GPT2TokenizerFast)):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPT2Tokenizer should only add one special token - the pad_token.\"\n",
    "\n",
    "# resize embeddings if needed (e.g. for LlamaTokenizer)\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# wpq: use int8 training\n",
    "# wpq: put this after resize embedding!\n",
    "if model_args.load_in_8bit:\n",
    "    from peft import prepare_model_for_int8_training\n",
    "    model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd16620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wpq: add peft to finetune_trainer.py\n",
    "if model_args.use_lora:\n",
    "    logger.info(\"Initializing LORA model...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=model_args.lora_rank, \n",
    "        lora_alpha=model_args.lora_alpha, \n",
    "        lora_dropout=model_args.lora_dropout,\n",
    "        target_modules=['q_proj','k_proj','v_proj','o_proj'],\n",
    "    )\n",
    "    # wpq: the following fixes `element 0 of tensors does not require grad and does not have a grad_fn` \n",
    "    # https://github.com/huggingface/peft/issues/137\n",
    "    # https://github.com/huggingface/peft/issues/522\n",
    "    if hasattr(training_args, 'gradient_checkpointing'):\n",
    "        if training_args.gradient_checkpointing:\n",
    "            model.enable_input_require_grads()\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff116ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-66f58c7711af992c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-55b6ed986212e630_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing the datasets.\n",
    "if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_prompt_completion_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    \n",
    "\n",
    "# To speed up this part, we use multiprocessing.\n",
    "with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "    if not data_args.streaming:\n",
    "        lm_datasets = raw_datasets.map(\n",
    "            encode_function,\n",
    "            batched=False,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Tokenizing and reformatting instruction data\",\n",
    "        )\n",
    "    else:\n",
    "        lm_datasets = raw_datasets.map(\n",
    "            encode_function,\n",
    "            batched=False,\n",
    "        )\n",
    "    lm_datasets.set_format(type=\"pt\")\n",
    "\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "    ## wpq: subsample `dataset` according to `data_args.subsample_mixture`.\n",
    "    # assumes dataset in `train_file` ordered, \n",
    "    # e.g., ['cot', 'cot', 'flan_v2', 'flan_v2', ...]\n",
    "    # note `counts.items()` is ordered as well!\n",
    "    if data_args.subsample_mixture is not None:\n",
    "        counts = Counter(train_dataset['dataset'])\n",
    "        inds = []\n",
    "        cum = 0\n",
    "        for k, N in counts.items():\n",
    "            n = int(data_args.subsample_mixture.get(k, 0))\n",
    "            replace = True if n>N else False\n",
    "            replace = True # always sample with replacement, for fairer comparison.\n",
    "            inds += list(np.random.choice(N, size=n, replace=replace) + cum)\n",
    "            cum += N\n",
    "            print(k, N, n, len(inds))\n",
    "        train_dataset = train_dataset.select(inds)\n",
    "\n",
    "\n",
    "    if data_args.subsample_inds_file is not None:\n",
    "        with open(data_args.subsample_inds_file, 'rb') as f:\n",
    "            inds = pickle.load(f)['K']\n",
    "        logger.info(f'Using subsample_inds_file: {data_args.subsample_inds_file}')\n",
    "        logger.info(f'subsample_inds_file has {len(inds)} indices.')\n",
    "        train_dataset = train_dataset.select(inds)\n",
    "\n",
    "    ## \n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9814f984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fa51358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13671875 -4.7392186388606206e-05 0.26171875\n"
     ]
    }
   ],
   "source": [
    "x = model.model.embed_tokens.weight\n",
    "\n",
    "print(x.min().item(), x.mean().item(), x.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90749cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55146a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_dataset[i] for i in range(3)]\n",
    "batch = [{k: v for k, v in d.items() if k not in ['dataset', 'id', 'messages']} for d in batch]\n",
    "out = data_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77a7c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32000, 32000, 32000,  ...,  1820, 29889,     2],\n",
       "        [    1,   529, 29989,  ..., 20026, 29889,     2],\n",
       "        [32000, 32000, 32000,  ..., 31294, 30267,     2]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  1820, 29889,     2],\n",
       "        [ -100,  -100,  -100,  ..., 20026, 29889,     2],\n",
       "        [ -100,  -100,  -100,  ..., 31294, 30267,     2]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecb3db5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><s> <|user|>\n",
      "root@openvpn:/home/openvpn# ./openvpn-install.sh\n",
      "Welcome to OpenVPN-install!\n",
      "The git repository is available at: https://github.com/angristan/openvpn-install\n",
      "\n",
      "It looks like OpenVPN is already installed.\n",
      "\n",
      "What do you want to do?\n",
      " 1) Add a new user\n",
      " 2) Revoke existing user\n",
      " 3) Remove OpenVPN\n",
      " 4) Exit\n",
      "Select an option [1-4]: 1\n",
      "\n",
      "Tell me a name for the client.\n",
      "The name must consist of alphanumeric character. It may also include an underscore or a dash.\n",
      "Client name: naam\n",
      "\n",
      "Do you want to protect the configuration file with a password?\n",
      "(e.g. encrypt the private key with a password)\n",
      " 1) Add a passwordless client\n",
      " 2) Use a password for the client\n",
      "Select an option [1-2]: 1\n",
      "\n",
      "Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars\n",
      "Using SSL: openssl OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)\n",
      "-----\n",
      "Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-54848.BT2FXv/tmp.dFLd6V\n",
      "Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key:\n",
      "Check that the request matches the signature\n",
      "Signature ok\n",
      "The Subject's Distinguished Name is as follows\n",
      "commonName :ASN.1 12:'naam'\n",
      "Certificate is to be certified until Apr 21 22:48:57 2025 GMT (825 days)\n",
      "\n",
      "Write out database with 1 new entries\n",
      "Data Base Updated\n",
      "\n",
      "Client naam added.\n",
      "\n",
      "The configuration file has been written to /home/openvpn/naam.ovpn.\n",
      "Download the .ovpn file and import it in your OpenVPN client.\n",
      "root@openvpn:/home/openvpn# \n",
      "kan je op basis van deze gegevens een verslag maken hoe je iemand toevoegd op een openvpn server\n",
      "<|assistant|>\n",
      "Op basis van de gegevens in de prompt log kan worden opgemerkt dat een script genaamd \"openvpn-install.sh\" wordt uitgevoerd op een OpenVPN server. Het script lijkt te controleren of OpenVPN al is geïnstalleerd op de server en geeft vervolgens de gebruiker de optie om een nieuwe gebruiker toe te voegen, een bestaande gebruiker te intrekken, OpenVPN te verwijderen of het script te verlaten.\n",
      "\n",
      "De gebruiker kiest vervolg</s> \n",
      "<|user|>\n",
      "sorry kan je een gebruikershandleiding maken?\n",
      "<|assistant|>\n",
      "1. Open een terminal op de OpenVPN server en navigeer naar de map waar het script \"openvpn-install.sh\" is opgeslagen.\n",
      "2. Voer het script uit met behulp van de volgende opdracht: `./openvpn-install.sh`\n",
      "3. Selecteer de optie \"1\" om een nieuwe gebruiker toe te voegen.\n",
      "4. Voer een naam in voor de client, de naam mag alleen bestaan uit letters, cijfers, underscores en streepjes.\n",
      "5. Kies of je een wachtwoord wilt instellen voor de configuratiebestanden of niet.\n",
      "6. Voer eventueel een wachtwoord in voor de private key.\n",
      "7. De client zal nu worden toegevoegd en de configuratiebestanden zullen worden opgeslagen in de map \"/home/openvpn/\". Download het .ovpn bestand en importeer deze in de OpenVPN client van de gebruiker.\n",
      "8. De gebruiker kan nu verbinding maken met de OpenVPN server met behulp van de geïmporteerde configuratiebestanden en eventueel ingevoerde wachtwoord.\n",
      "9. Als de gebruiker de toegang tot de server niet meer nodig heeft, kan deze worden ingetrokken met optie 2 of verwijderd met optie 3.</s> \n",
      "<|user|>\n",
      "kan je uitleg geven waarom je als je passwordless user kiest dat je alsnog een wachtwoord moet invoeren voor wat is dat?\n",
      "\n",
      "Do you want to protect the configuration file with a password?\n",
      "(e.g. encrypt the private key with a password)\n",
      " 1) Add a passwordless client\n",
      " 2) Use a password for the client\n",
      "Select an option [1-2]: 1\n",
      "\n",
      "Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars\n",
      "Using SSL: openssl OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)\n",
      "-----\n",
      "Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-55459.c7BK5D/tmp.gWMi5Q\n",
      "Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key:\n",
      "<|assistant|>\n",
      "De optie om een \"passwordless client\" toe te voegen, betekent dat de gebruiker geen wachtwoord hoeft in te voeren om verbinding te maken met de OpenVPN server. Dit betekent echter niet dat de configuratiebestanden zelf niet beveiligd zijn.\n",
      "\n",
      "De configuratiebestanden bevatten een private key die gebruikt wordt voor encryptie tijdens de VPN verbinding. Deze private key moet worden beveiligd tegen onbevoegde toegang. Een mogelijkheid om deze key te beveiligen is door middel van een wachtwoord. Hierdoor kan alleen iemand met het juiste wachtwoord de configuratiebestanden openen en de verbinding maken.\n",
      "\n",
      "In dit geval, wanneer de gebruiker kiest voor een \"passwordless client\", wordt er nog steeds gevraagd om een wachtwoord te voeren voor de private key, Omdat deze key nog steeds beveiligd moet worden, ondanks dat de client zelf geen wachtwoord hoeft in te voeren.</s> \n",
      "<|user|>\n",
      "kan het iets korter?\n",
      "<|assistant|>\n",
      "Ja, de keuze voor een \"passwordless client\" betekent dat de gebruiker geen wachtwoord hoeft in te voeren om verbinding te maken met de OpenVPN server, maar de configuratiebestanden bevatten nog wel een private key die beveiligd moet worden tegen onbevoegde toegang, daarom wordt er nog steeds om een wachtwoord gevraagd voor de private key.</s>\n",
      "<s> <|user|>\n",
      "Summarize the main ideas of Jeff Walker's Product Launch Formula into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\n",
      "<|assistant|>\n",
      "Here are the main ideas of Jeff Walker's Product Launch Formula that can be applied by a growth marketing agency for their clients:\n",
      "\n",
      "1. Identify the target audience and their needs: Understand the ideal customer for the product or service, and create a messaging that resonates with them.\n",
      "2. Pre-launch: Build anticipation and excitement for the launch by creating buzz, gathering testimonials and case studies, and using social media to create awareness.\n",
      "3. Launch: Use a well-crafted launch sequence to maximize sales and conversions. This can include offering bonuses, creating scarcity, and using a deadline to create urgency.\n",
      "4. Post-launch: Follow up with customers, gather feedback, and continue to provide value to keep them engaged and loyal.\n",
      "5. Create a product suite: Once the initial product is successful, expand the offering by creating additional products or services that address other needs of the same audience.\n",
      "6. Continual optimization: Continually monitor and optimize the launch process and product suite to improve results.\n",
      "7. Build a community: Use the launch process to build a community of customers who are passionate about the product and can help spread the word.\n",
      "8. Use automation: Use technology and automation to streamline the launch process and improve efficiency.</s> \n",
      "<|user|>\n",
      "Summarize the main ideas of Brendon Burchard's Experts Academy into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\n",
      "<|assistant|>\n",
      "Here are the main ideas of Brendon Burchard's Experts Academy that can be applied by a growth marketing agency for their clients:\n",
      "\n",
      "1. Identify your expertise: Define your area of expertise and create a unique value proposition that differentiates you from others.\n",
      "2. Create a content strategy: Develop a content strategy that showcases your expertise and provides value to your audience. This can include blog posts, videos, podcasts, and social media content.\n",
      "3. Build your audience: Use social media, advertising, and other channels to build your audience and increase your visibility.\n",
      "4. Develop your products: Create products and services that align with your expertise and meet the needs of your audience. These can include digital products, courses, coaching, and consulting services.\n",
      "5. Create a brand: Develop a strong brand that reflects your expertise and resonates with your audience.\n",
      "6. Build authority: Establish yourself as an authority in your industry by speaking at events, publishing a book, or appearing on podcasts or TV shows.\n",
      "7. Monetize your expertise: Develop a monetization strategy that leverages your expertise and products to generate revenue.\n",
      "8. Build a team: As your business grows, build a team of experts to help you scale your business and provide excellent service to your clients.\n",
      "9. Continual improvement: Continually improve your products, services, and marketing strategies to stay ahead of the competition and provide the best possible experience for your clients.\n",
      "10. Focus on impact: Ultimately, focus on making a positive impact in the lives of your clients and using your expertise to make a difference in the world.</s> \n",
      "<|user|>\n",
      "What are the mental triggers in Jeff Walker's Product Launch Formula and \"Launch\" book?\n",
      "<|assistant|>\n",
      "Here are the mental triggers described in Jeff Walker's Product Launch Formula and \"Launch\" book:\n",
      "\n",
      "1. Scarcity: Creating a sense of urgency by limiting the availability of the product or service being launched.\n",
      "2. Authority: Establishing oneself as an expert or authority in the industry to build credibility and trust with the audience.\n",
      "3. Social proof: Using testimonials, case studies, and other forms of social proof to demonstrate the effectiveness and value of the product or service.\n",
      "4. Reciprocity: Offering something of value to the audience before asking for a sale or commitment.\n",
      "5. Likeability: Building a personal connection with the audience and creating a likable persona to increase the chances of a sale.\n",
      "6. Anticipation: Building anticipation and excitement leading up to the launch to generate buzz and increase interest in the product or service.\n",
      "7. Exclusivity: Creating a feeling of exclusivity by offering early access or special deals to a select group of people.\n",
      "8. Belonging: Creating a sense of community or belonging among the audience to increase engagement and loyalty to the brand.\n",
      "9. Storytelling: Using storytelling to connect with the audience on an emotional level and make the product or service more relatable and memorable.\n",
      "\n",
      "By leveraging these mental triggers, businesses can create an effective product launch strategy that resonates with their target audience and drives sales and growth.</s> \n",
      "<|user|>\n",
      "Write a summary of why scarcity and urgency are the strongest mental triggers and have been the driving force behind many of our best performing campaigns over the last 8 years.\n",
      "<|assistant|>\n",
      "Scarcity and urgency are two of the strongest mental triggers in marketing because they create a sense of exclusivity and importance around a product or service. By creating a sense of limited availability or a deadline for a promotion, businesses can motivate customers to take action and make a purchase before it's too late.\n",
      "\n",
      "Over the last eight years, many businesses have leveraged these triggers in their marketing campaigns to great success. Scarcity and urgency have been used to sell everything from digital products to physical goods and services, and have proven to be highly effective in driving conversions and increasing revenue.\n",
      "\n",
      "This is because scarcity and urgency tap into the natural human desire for instant gratification and fear of missing out (FOMO). When customers believe that they may miss out on a valuable opportunity if they don't act quickly, they are more likely to take action and make a purchase.\n",
      "\n",
      "Furthermore, scarcity and urgency can be combined with other mental triggers, such as social proof and authority, to create even more powerful marketing campaigns. By demonstrating the popularity and effectiveness of a product or service, businesses can increase the perceived value and urgency of a promotion, leading to even more sales and revenue.\n",
      "\n",
      "Overall, scarcity and urgency are powerful mental triggers that have been proven to drive results in marketing campaigns. By leveraging these triggers, businesses can create a sense of urgency and exclusivity around their products or services, motivating customers to take action and make a purchase before it's too late.</s> \n",
      "<|user|>\n",
      "Summarize Russell Brunson's Perfect Webinar Script...\n",
      "<|assistant|>\n",
      "Russell Brunson's Perfect Webinar Script is a framework for delivering webinars that are designed to sell a product or service. Here are the key components of the Perfect Webinar Script:\n",
      "\n",
      "1. Introduction: The webinar starts with an introduction that establishes the presenter's credibility and establishes the topic.\n",
      "2. Story: The presenter then tells a personal story that is related to the topic and establishes a connection with the audience.\n",
      "3. Content: The presenter provides valuable content related to the topic, demonstrating their expertise and building trust with the audience.\n",
      "4. Offer: The presenter then introduces the product or service they are selling, explaining how it solves the audience's problem and providing social proof to support their claims.\n",
      "5. Bonuses: The presenter offers bonuses to sweeten the deal and make the offer more enticing.\n",
      "6. Scarcity: The presenter creates a sense of urgency by limiting the availability of the offer or including a deadline for purchase.\n",
      "7. FAQs: The presenter answers common questions about the offer, addressing objections and building trust with the audience.\n",
      "8. Closing: The presenter then delivers a clear call to action, directing the audience to take a specific action to purchase the product or service.\n",
      "\n",
      "Overall, the Perfect Webinar Script is designed to build trust with the audience, establish the presenter's credibility, and provide valuable content, while also creating a sense of urgency and scarcity around the offer being presented. By following this framework, businesses can use webinars as a powerful tool to sell their products and services to a targeted audience.</s>\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><s> <|user|>\n",
      "提供一個媒體招待會的流程表，內容是執行完「來彰化跑步」這個活動結束後，透過媒體招待會來展示這段期間的活動成果 ，現場並有抽獎活動，獎勵這段期間的活動參加者\n",
      "<|assistant|>\n",
      "以下是「來彰化跑步」活動結束後媒體招待會的流程表：\n",
      "\n",
      "| 時間 | 流程 | 負責人 |\n",
      "| --- | --- | --- |\n",
      "| 14:00 | 媒體招待會開始，歡迎詞 | 主持人 |\n",
      "| 14:10 | 活動成果報告 | 活動負責人 |\n",
      "| 14:30 | 影片放映 | 影片製作人 |\n",
      "| 14:40 | 抽獎活動 | 主持人 |\n",
      "| 15:00 | 自由交流時間 | 全體參與人員 |\n",
      "| 15:30 | 活動結束 | 主持人 |\n",
      "\n",
      "備註：\n",
      "\n",
      "1. 活動成果報告內容包括活動的規模、參與人數、重點活動內容等。\n",
      "2. 影片放映包括活動當天的精彩片段及花絮等。\n",
      "3. 抽獎活動包括抽取本次活動的參與者及媒體代表參與，獎品可設計為與跑步相關的商品或紀念品。\n",
      "4. 自由交流時間讓參與人員有機會互相交流活動的經驗及意見，也可讓媒體代表進一步採訪活動相關內容。</s> \n",
      "<|user|>\n",
      "希望來參加招待會的媒體朋友，可以有個很美好的體驗，並且寫出更多精彩的報導。有這個方向再修改一下流程表\n",
      "<|assistant|>\n",
      "非常好的想法！以下是修改後的「來彰化跑步」活動結束後媒體招待會的流程表，希望可以讓媒體朋友們有更美好的體驗：\n",
      "\n",
      "| 時間 | 流程 | 負責人 |\n",
      "| --- | --- | --- |\n",
      "| 14:00 | 媒體招待會開始，歡迎詞 | 主持人 |\n",
      "| 14:10 | 活動成果報告 | 活動負責人 |\n",
      "| 14:30 | 影片放映 | 影片製作人 |\n",
      "| 14:40 | 與參與者對談 | 活動負責人 |\n",
      "| 15:00 | 優質體驗分享 | 參與者代表 |\n",
      "| 15:20 | 抽獎活動 | 主持人 |\n",
      "| 15:40 | 自由交流時間 | 全體參與人員 |\n",
      "| 16:00 | 活動結束 | 主持人 |\n",
      "\n",
      "備註：\n",
      "\n",
      "1. 活動成果報告內容包括活動的規模、參與人數、重點活動內容等。\n",
      "2. 影片放映包括活動當天的精彩片段及花絮等。\n",
      "3. 與參與者對談讓媒體朋友可以更深入的了解參與者的活動經驗及感受，並且可以更貼近實際活動情況的進行採訪。\n",
      "4. 優質體驗分享可以邀請參與者代表分享他們參與活動的體驗及心得，讓媒體朋友可以透過參與者的分享更貼近活動。\n",
      "5. 抽獎活動包括抽取本次活動的參與者及媒體代表參與，獎品可設計為與跑步相關的商品或紀念品。\n",
      "6. 自由交流時間讓參與人員有機會互相交流活動的經驗及意見，也可讓媒體代表進一步採訪活動相關內容。</s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tokenizer.decode(out['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46605be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|user|>\n",
      "Summarize the main ideas of Jeff Walker's Product Launch Formula into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\n",
      "<|assistant|>\n",
      "Here are the main ideas of Jeff Walker's Product Launch Formula that can be applied by a growth marketing agency for their clients:\n",
      "\n",
      "1. Identify the target audience and their needs: Understand the ideal customer for the product or service, and create a messaging that resonates with them.\n",
      "2. Pre-launch: Build anticipation and excitement for the launch by creating buzz, gathering testimonials and case studies, and using social media to create awareness.\n",
      "3. Launch: Use a well-crafted launch sequence to maximize sales and conversions. This can include offering bonuses, creating scarcity, and using a deadline to create urgency.\n",
      "4. Post-launch: Follow up with customers, gather feedback, and continue to provide value to keep them engaged and loyal.\n",
      "5. Create a product suite: Once the initial product is successful, expand the offering by creating additional products or services that address other needs of the same audience.\n",
      "6. Continual optimization: Continually monitor and optimize the launch process and product suite to improve results.\n",
      "7. Build a community: Use the launch process to build a community of customers who are passionate about the product and can help spread the word.\n",
      "8. Use automation: Use technology and automation to streamline the launch process and improve efficiency.</s> \n",
      "<|user|>\n",
      "Summarize the main ideas of Brendon Burchard's Experts Academy into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\n",
      "<|assistant|>\n",
      "Here are the main ideas of Brendon Burchard's Experts Academy that can be applied by a growth marketing agency for their clients:\n",
      "\n",
      "1. Identify your expertise: Define your area of expertise and create a unique value proposition that differentiates you from others.\n",
      "2. Create a content strategy: Develop a content strategy that showcases your expertise and provides value to your audience. This can include blog posts, videos, podcasts, and social media content.\n",
      "3. Build your audience: Use social media, advertising, and other channels to build your audience and increase your visibility.\n",
      "4. Develop your products: Create products and services that align with your expertise and meet the needs of your audience. These can include digital products, courses, coaching, and consulting services.\n",
      "5. Create a brand: Develop a strong brand that reflects your expertise and resonates with your audience.\n",
      "6. Build authority: Establish yourself as an authority in your industry by speaking at events, publishing a book, or appearing on podcasts or TV shows.\n",
      "7. Monetize your expertise: Develop a monetization strategy that leverages your expertise and products to generate revenue.\n",
      "8. Build a team: As your business grows, build a team of experts to help you scale your business and provide excellent service to your clients.\n",
      "9. Continual improvement: Continually improve your products, services, and marketing strategies to stay ahead of the competition and provide the best possible experience for your clients.\n",
      "10. Focus on impact: Ultimately, focus on making a positive impact in the lives of your clients and using your expertise to make a difference in the world.</s> \n",
      "<|user|>\n",
      "What are the mental triggers in Jeff Walker's Product Launch Formula and \"Launch\" book?\n",
      "<|assistant|>\n",
      "Here are the mental triggers described in Jeff Walker's Product Launch Formula and \"Launch\" book:\n",
      "\n",
      "1. Scarcity: Creating a sense of urgency by limiting the availability of the product or service being launched.\n",
      "2. Authority: Establishing oneself as an expert or authority in the industry to build credibility and trust with the audience.\n",
      "3. Social proof: Using testimonials, case studies, and other forms of social proof to demonstrate the effectiveness and value of the product or service.\n",
      "4. Reciprocity: Offering something of value to the audience before asking for a sale or commitment.\n",
      "5. Likeability: Building a personal connection with the audience and creating a likable persona to increase the chances of a sale.\n",
      "6. Anticipation: Building anticipation and excitement leading up to the launch to generate buzz and increase interest in the product or service.\n",
      "7. Exclusivity: Creating a feeling of exclusivity by offering early access or special deals to a select group of people.\n",
      "8. Belonging: Creating a sense of community or belonging among the audience to increase engagement and loyalty to the brand.\n",
      "9. Storytelling: Using storytelling to connect with the audience on an emotional level and make the product or service more relatable and memorable.\n",
      "\n",
      "By leveraging these mental triggers, businesses can create an effective product launch strategy that resonates with their target audience and drives sales and growth.</s> \n",
      "<|user|>\n",
      "Write a summary of why scarcity and urgency are the strongest mental triggers and have been the driving force behind many of our best performing campaigns over the last 8 years.\n",
      "<|assistant|>\n",
      "Scarcity and urgency are two of the strongest mental triggers in marketing because they create a sense of exclusivity and importance around a product or service. By creating a sense of limited availability or a deadline for a promotion, businesses can motivate customers to take action and make a purchase before it's too late.\n",
      "\n",
      "Over the last eight years, many businesses have leveraged these triggers in their marketing campaigns to great success. Scarcity and urgency have been used to sell everything from digital products to physical goods and services, and have proven to be highly effective in driving conversions and increasing revenue.\n",
      "\n",
      "This is because scarcity and urgency tap into the natural human desire for instant gratification and fear of missing out (FOMO). When customers believe that they may miss out on a valuable opportunity if they don't act quickly, they are more likely to take action and make a purchase.\n",
      "\n",
      "Furthermore, scarcity and urgency can be combined with other mental triggers, such as social proof and authority, to create even more powerful marketing campaigns. By demonstrating the popularity and effectiveness of a product or service, businesses can increase the perceived value and urgency of a promotion, leading to even more sales and revenue.\n",
      "\n",
      "Overall, scarcity and urgency are powerful mental triggers that have been proven to drive results in marketing campaigns. By leveraging these triggers, businesses can create a sense of urgency and exclusivity around their products or services, motivating customers to take action and make a purchase before it's too late.</s> \n",
      "<|user|>\n",
      "Summarize Russell Brunson's Perfect Webinar Script...\n",
      "<|assistant|>\n",
      "Russell Brunson's Perfect Webinar Script is a framework for delivering webinars that are designed to sell a product or service. Here are the key components of the Perfect Webinar Script:\n",
      "\n",
      "1. Introduction: The webinar starts with an introduction that establishes the presenter's credibility and establishes the topic.\n",
      "2. Story: The presenter then tells a personal story that is related to the topic and establishes a connection with the audience.\n",
      "3. Content: The presenter provides valuable content related to the topic, demonstrating their expertise and building trust with the audience.\n",
      "4. Offer: The presenter then introduces the product or service they are selling, explaining how it solves the audience's problem and providing social proof to support their claims.\n",
      "5. Bonuses: The presenter offers bonuses to sweeten the deal and make the offer more enticing.\n",
      "6. Scarcity: The presenter creates a sense of urgency by limiting the availability of the offer or including a deadline for purchase.\n",
      "7. FAQs: The presenter answers common questions about the offer, addressing objections and building trust with the audience.\n",
      "8. Closing: The presenter then delivers a clear call to action, directing the audience to take a specific action to purchase the product or service.\n",
      "\n",
      "Overall, the Perfect Webinar Script is designed to build trust with the audience, establish the presenter's credibility, and provide valuable content, while also creating a sense of urgency and scarcity around the offer being presented. By following this framework, businesses can use webinars as a powerful tool to sell their products and services to a targeted audience.</s>\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "print(tokenizer.decode(train_dataset[i]['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ed386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_instruct.finetune_trainer import MyTrainer\n",
    "\n",
    "# initalize a trainer\n",
    "# here we use a custom trainer that moves the model to CPU when saving the checkpoint in FSDP mode\n",
    "# we can switch to the default trainer after moving to deepspeed (let's don't change too much for now)\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=lm_datasets['test'] if (training_args.do_eval and 'test' in lm_datasets) else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76e6dc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8ad784c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 1.53846154e-06, 3.07692308e-06, 4.61538462e-06,\n",
       "       6.15384615e-06, 7.69230769e-06, 9.23076923e-06, 1.07692308e-05,\n",
       "       1.23076923e-05, 1.38461538e-05, 1.53846154e-05, 1.69230769e-05,\n",
       "       1.84615385e-05, 2.00000000e-05, 1.99504950e-05, 1.99009901e-05,\n",
       "       1.98514851e-05, 1.98019802e-05, 1.97524752e-05, 1.97029703e-05,\n",
       "       1.96534653e-05, 1.96039604e-05, 1.95544554e-05, 1.95049505e-05,\n",
       "       1.94554455e-05, 1.94059406e-05, 1.93564356e-05, 1.93069307e-05,\n",
       "       1.92574257e-05, 1.92079208e-05, 1.91584158e-05, 1.91089109e-05,\n",
       "       1.90594059e-05, 1.90099010e-05, 1.89603960e-05, 1.89108911e-05,\n",
       "       1.88613861e-05, 1.88118812e-05, 1.87623762e-05, 1.87128713e-05,\n",
       "       1.86633663e-05, 1.86138614e-05, 1.85643564e-05, 1.85148515e-05,\n",
       "       1.84653465e-05, 1.84158416e-05, 1.83663366e-05, 1.83168317e-05,\n",
       "       1.82673267e-05, 1.82178218e-05, 1.81683168e-05, 1.81188119e-05,\n",
       "       1.80693069e-05, 1.80198020e-05, 1.79702970e-05, 1.79207921e-05,\n",
       "       1.78712871e-05, 1.78217822e-05, 1.77722772e-05, 1.77227723e-05,\n",
       "       1.76732673e-05, 1.76237624e-05, 1.75742574e-05, 1.75247525e-05,\n",
       "       1.74752475e-05, 1.74257426e-05, 1.73762376e-05, 1.73267327e-05,\n",
       "       1.72772277e-05, 1.72277228e-05, 1.71782178e-05, 1.71287129e-05,\n",
       "       1.70792079e-05, 1.70297030e-05, 1.69801980e-05, 1.69306931e-05,\n",
       "       1.68811881e-05, 1.68316832e-05, 1.67821782e-05, 1.67326733e-05,\n",
       "       1.66831683e-05, 1.66336634e-05, 1.65841584e-05, 1.65346535e-05,\n",
       "       1.64851485e-05, 1.64356436e-05, 1.63861386e-05, 1.63366337e-05,\n",
       "       1.62871287e-05, 1.62376238e-05, 1.61881188e-05, 1.61386139e-05,\n",
       "       1.60891089e-05, 1.60396040e-05, 1.59900990e-05, 1.59405941e-05,\n",
       "       1.58910891e-05, 1.58415842e-05, 1.57920792e-05, 1.57425743e-05,\n",
       "       1.56930693e-05, 1.56435644e-05, 1.55940594e-05, 1.55445545e-05,\n",
       "       1.54950495e-05, 1.54455446e-05, 1.53960396e-05, 1.53465347e-05,\n",
       "       1.52970297e-05, 1.52475248e-05, 1.51980198e-05, 1.51485149e-05,\n",
       "       1.50990099e-05, 1.50495050e-05, 1.50000000e-05, 1.49504950e-05,\n",
       "       1.49009901e-05, 1.48514851e-05, 1.48019802e-05, 1.47524752e-05,\n",
       "       1.47029703e-05, 1.46534653e-05, 1.46039604e-05, 1.45544554e-05,\n",
       "       1.45049505e-05, 1.44554455e-05, 1.44059406e-05, 1.43564356e-05,\n",
       "       1.43069307e-05, 1.42574257e-05, 1.42079208e-05, 1.41584158e-05,\n",
       "       1.41089109e-05, 1.40594059e-05, 1.40099010e-05, 1.39603960e-05,\n",
       "       1.39108911e-05, 1.38613861e-05, 1.38118812e-05, 1.37623762e-05,\n",
       "       1.37128713e-05, 1.36633663e-05, 1.36138614e-05, 1.35643564e-05,\n",
       "       1.35148515e-05, 1.34653465e-05, 1.34158416e-05, 1.33663366e-05,\n",
       "       1.33168317e-05, 1.32673267e-05, 1.32178218e-05, 1.31683168e-05,\n",
       "       1.31188119e-05, 1.30693069e-05, 1.30198020e-05, 1.29702970e-05,\n",
       "       1.29207921e-05, 1.28712871e-05, 1.28217822e-05, 1.27722772e-05,\n",
       "       1.27227723e-05, 1.26732673e-05, 1.26237624e-05, 1.25742574e-05,\n",
       "       1.25247525e-05, 1.24752475e-05, 1.24257426e-05, 1.23762376e-05,\n",
       "       1.23267327e-05, 1.22772277e-05, 1.22277228e-05, 1.21782178e-05,\n",
       "       1.21287129e-05, 1.20792079e-05, 1.20297030e-05, 1.19801980e-05,\n",
       "       1.19306931e-05, 1.18811881e-05, 1.18316832e-05, 1.17821782e-05,\n",
       "       1.17326733e-05, 1.16831683e-05, 1.16336634e-05, 1.15841584e-05,\n",
       "       1.15346535e-05, 1.14851485e-05, 1.14356436e-05, 1.13861386e-05,\n",
       "       1.13366337e-05, 1.12871287e-05, 1.12376238e-05, 1.11881188e-05,\n",
       "       1.11386139e-05, 1.10891089e-05, 1.10396040e-05, 1.09900990e-05,\n",
       "       1.09405941e-05, 1.08910891e-05, 1.08415842e-05, 1.07920792e-05,\n",
       "       1.07425743e-05, 1.06930693e-05, 1.06435644e-05, 1.05940594e-05,\n",
       "       1.05445545e-05, 1.04950495e-05, 1.04455446e-05, 1.03960396e-05,\n",
       "       1.03465347e-05, 1.02970297e-05, 1.02475248e-05, 1.01980198e-05,\n",
       "       1.01485149e-05, 1.00990099e-05, 1.00495050e-05, 1.00000000e-05,\n",
       "       9.95049505e-06, 9.90099010e-06, 9.85148515e-06, 9.80198020e-06,\n",
       "       9.75247525e-06, 9.70297030e-06, 9.65346535e-06, 9.60396040e-06,\n",
       "       9.55445545e-06, 9.50495050e-06, 9.45544554e-06, 9.40594059e-06,\n",
       "       9.35643564e-06, 9.30693069e-06, 9.25742574e-06, 9.20792079e-06,\n",
       "       9.15841584e-06, 9.10891089e-06, 9.05940594e-06, 9.00990099e-06,\n",
       "       8.96039604e-06, 8.91089109e-06, 8.86138614e-06, 8.81188119e-06,\n",
       "       8.76237624e-06, 8.71287129e-06, 8.66336634e-06, 8.61386139e-06,\n",
       "       8.56435644e-06, 8.51485149e-06, 8.46534653e-06, 8.41584158e-06,\n",
       "       8.36633663e-06, 8.31683168e-06, 8.26732673e-06, 8.21782178e-06,\n",
       "       8.16831683e-06, 8.11881188e-06, 8.06930693e-06, 8.01980198e-06,\n",
       "       7.97029703e-06, 7.92079208e-06, 7.87128713e-06, 7.82178218e-06,\n",
       "       7.77227723e-06, 7.72277228e-06, 7.67326733e-06, 7.62376238e-06,\n",
       "       7.57425743e-06, 7.52475248e-06, 7.47524752e-06, 7.42574257e-06,\n",
       "       7.37623762e-06, 7.32673267e-06, 7.27722772e-06, 7.22772277e-06,\n",
       "       7.17821782e-06, 7.12871287e-06, 7.07920792e-06, 7.02970297e-06,\n",
       "       6.98019802e-06, 6.93069307e-06, 6.88118812e-06, 6.83168317e-06,\n",
       "       6.78217822e-06, 6.73267327e-06, 6.68316832e-06, 6.63366337e-06,\n",
       "       6.58415842e-06, 6.53465347e-06, 6.48514851e-06, 6.43564356e-06,\n",
       "       6.38613861e-06, 6.33663366e-06, 6.28712871e-06, 6.23762376e-06,\n",
       "       6.18811881e-06, 6.13861386e-06, 6.08910891e-06, 6.03960396e-06,\n",
       "       5.99009901e-06, 5.94059406e-06, 5.89108911e-06, 5.84158416e-06,\n",
       "       5.79207921e-06, 5.74257426e-06, 5.69306931e-06, 5.64356436e-06,\n",
       "       5.59405941e-06, 5.54455446e-06, 5.49504950e-06, 5.44554455e-06,\n",
       "       5.39603960e-06, 5.34653465e-06, 5.29702970e-06, 5.24752475e-06,\n",
       "       5.19801980e-06, 5.14851485e-06, 5.09900990e-06, 5.04950495e-06,\n",
       "       5.00000000e-06, 4.95049505e-06, 4.90099010e-06, 4.85148515e-06,\n",
       "       4.80198020e-06, 4.75247525e-06, 4.70297030e-06, 4.65346535e-06,\n",
       "       4.60396040e-06, 4.55445545e-06, 4.50495050e-06, 4.45544554e-06,\n",
       "       4.40594059e-06, 4.35643564e-06, 4.30693069e-06, 4.25742574e-06,\n",
       "       4.20792079e-06, 4.15841584e-06, 4.10891089e-06, 4.05940594e-06,\n",
       "       4.00990099e-06, 3.96039604e-06, 3.91089109e-06, 3.86138614e-06,\n",
       "       3.81188119e-06, 3.76237624e-06, 3.71287129e-06, 3.66336634e-06,\n",
       "       3.61386139e-06, 3.56435644e-06, 3.51485149e-06, 3.46534653e-06,\n",
       "       3.41584158e-06, 3.36633663e-06, 3.31683168e-06, 3.26732673e-06,\n",
       "       3.21782178e-06, 3.16831683e-06, 3.11881188e-06, 3.06930693e-06,\n",
       "       3.01980198e-06, 2.97029703e-06, 2.92079208e-06, 2.87128713e-06,\n",
       "       2.82178218e-06, 2.77227723e-06, 2.72277228e-06, 2.67326733e-06,\n",
       "       2.62376238e-06, 2.57425743e-06, 2.52475248e-06, 2.47524752e-06,\n",
       "       2.42574257e-06, 2.37623762e-06, 2.32673267e-06, 2.27722772e-06,\n",
       "       2.22772277e-06, 2.17821782e-06, 2.12871287e-06, 2.07920792e-06,\n",
       "       2.02970297e-06, 1.98019802e-06, 1.93069307e-06, 1.88118812e-06,\n",
       "       1.83168317e-06, 1.78217822e-06, 1.73267327e-06, 1.68316832e-06,\n",
       "       1.63366337e-06, 1.58415842e-06, 1.53465347e-06, 1.48514851e-06,\n",
       "       1.43564356e-06, 1.38613861e-06, 1.33663366e-06, 1.28712871e-06,\n",
       "       1.23762376e-06, 1.18811881e-06, 1.13861386e-06, 1.08910891e-06,\n",
       "       1.03960396e-06, 9.90099010e-07, 9.40594059e-07, 8.91089109e-07,\n",
       "       8.41584158e-07, 7.92079208e-07, 7.42574257e-07, 6.93069307e-07,\n",
       "       6.43564356e-07, 5.94059406e-07, 5.44554455e-07, 4.95049505e-07,\n",
       "       4.45544554e-07, 3.96039604e-07, 3.46534653e-07, 2.97029703e-07,\n",
       "       2.47524752e-07, 1.98019802e-07, 1.48514851e-07, 9.90099010e-08,\n",
       "       4.95049505e-08])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.array(list(range(417)))\n",
    "ys = [0.0, 0.07692307692307693, 0.15384615384615385, 0.23076923076923078, 0.3076923076923077, 0.38461538461538464, 0.46153846153846156, 0.5384615384615384, 0.6153846153846154, 0.6923076923076923, 0.7692307692307693, 0.8461538461538461, 0.9230769230769231, 1.0, 0.9975247524752475, 0.995049504950495, 0.9925742574257426, 0.9900990099009901, 0.9876237623762376, 0.9851485148514851, 0.9826732673267327, 0.9801980198019802, 0.9777227722772277, 0.9752475247524752, 0.9727722772277227, 0.9702970297029703, 0.9678217821782178, 0.9653465346534653, 0.9628712871287128, 0.9603960396039604, 0.9579207920792079, 0.9554455445544554, 0.9529702970297029, 0.9504950495049505, 0.948019801980198, 0.9455445544554455, 0.943069306930693, 0.9405940594059405, 0.9381188118811881, 0.9356435643564357, 0.9331683168316832, 0.9306930693069307, 0.9282178217821783, 0.9257425742574258, 0.9232673267326733, 0.9207920792079208, 0.9183168316831684, 0.9158415841584159, 0.9133663366336634, 0.9108910891089109, 0.9084158415841584, 0.905940594059406, 0.9034653465346535, 0.900990099009901, 0.8985148514851485, 0.8960396039603961, 0.8935643564356436, 0.8910891089108911, 0.8886138613861386, 0.8861386138613861, 0.8836633663366337, 0.8811881188118812, 0.8787128712871287, 0.8762376237623762, 0.8737623762376238, 0.8712871287128713, 0.8688118811881188, 0.8663366336633663, 0.8638613861386139, 0.8613861386138614, 0.8589108910891089, 0.8564356435643564, 0.8539603960396039, 0.8514851485148515, 0.849009900990099, 0.8465346534653465, 0.844059405940594, 0.8415841584158416, 0.8391089108910891, 0.8366336633663366, 0.8341584158415841, 0.8316831683168316, 0.8292079207920792, 0.8267326732673267, 0.8242574257425742, 0.8217821782178217, 0.8193069306930693, 0.8168316831683168, 0.8143564356435643, 0.8118811881188119, 0.8094059405940595, 0.806930693069307, 0.8044554455445545, 0.801980198019802, 0.7995049504950495, 0.7970297029702971, 0.7945544554455446, 0.7920792079207921, 0.7896039603960396, 0.7871287128712872, 0.7846534653465347, 0.7821782178217822, 0.7797029702970297, 0.7772277227722773, 0.7747524752475248, 0.7722772277227723, 0.7698019801980198, 0.7673267326732673, 0.7648514851485149, 0.7623762376237624, 0.7599009900990099, 0.7574257425742574, 0.754950495049505, 0.7524752475247525, 0.75, 0.7475247524752475, 0.745049504950495, 0.7425742574257426, 0.7400990099009901, 0.7376237623762376, 0.7351485148514851, 0.7326732673267327, 0.7301980198019802, 0.7277227722772277, 0.7252475247524752, 0.7227722772277227, 0.7202970297029703, 0.7178217821782178, 0.7153465346534653, 0.7128712871287128, 0.7103960396039604, 0.7079207920792079, 0.7054455445544554, 0.7029702970297029, 0.7004950495049505, 0.698019801980198, 0.6955445544554455, 0.693069306930693, 0.6905940594059405, 0.6881188118811881, 0.6856435643564357, 0.6831683168316832, 0.6806930693069307, 0.6782178217821783, 0.6757425742574258, 0.6732673267326733, 0.6707920792079208, 0.6683168316831684, 0.6658415841584159, 0.6633663366336634, 0.6608910891089109, 0.6584158415841584, 0.655940594059406, 0.6534653465346535, 0.650990099009901, 0.6485148514851485, 0.6460396039603961, 0.6435643564356436, 0.6410891089108911, 0.6386138613861386, 0.6361386138613861, 0.6336633663366337, 0.6311881188118812, 0.6287128712871287, 0.6262376237623762, 0.6237623762376238, 0.6212871287128713, 0.6188118811881188, 0.6163366336633663, 0.6138613861386139, 0.6113861386138614, 0.6089108910891089, 0.6064356435643564, 0.6039603960396039, 0.6014851485148515, 0.599009900990099, 0.5965346534653465, 0.594059405940594, 0.5915841584158416, 0.5891089108910891, 0.5866336633663366, 0.5841584158415841, 0.5816831683168316, 0.5792079207920792, 0.5767326732673267, 0.5742574257425742, 0.5717821782178217, 0.5693069306930693, 0.5668316831683168, 0.5643564356435643, 0.5618811881188119, 0.5594059405940595, 0.556930693069307, 0.5544554455445545, 0.551980198019802, 0.5495049504950495, 0.5470297029702971, 0.5445544554455446, 0.5420792079207921, 0.5396039603960396, 0.5371287128712872, 0.5346534653465347, 0.5321782178217822, 0.5297029702970297, 0.5272277227722773, 0.5247524752475248, 0.5222772277227723, 0.5198019801980198, 0.5173267326732673, 0.5148514851485149, 0.5123762376237624, 0.5099009900990099, 0.5074257425742574, 0.504950495049505, 0.5024752475247525, 0.5, 0.4975247524752475, 0.49504950495049505, 0.49257425742574257, 0.4900990099009901, 0.4876237623762376, 0.48514851485148514, 0.48267326732673266, 0.4801980198019802, 0.4777227722772277, 0.4752475247524752, 0.47277227722772275, 0.47029702970297027, 0.46782178217821785, 0.46534653465346537, 0.4628712871287129, 0.4603960396039604, 0.45792079207920794, 0.45544554455445546, 0.452970297029703, 0.4504950495049505, 0.44801980198019803, 0.44554455445544555, 0.4430693069306931, 0.4405940594059406, 0.4381188118811881, 0.43564356435643564, 0.43316831683168316, 0.4306930693069307, 0.4282178217821782, 0.42574257425742573, 0.42326732673267325, 0.4207920792079208, 0.4183168316831683, 0.4158415841584158, 0.41336633663366334, 0.41089108910891087, 0.4084158415841584, 0.40594059405940597, 0.4034653465346535, 0.400990099009901, 0.39851485148514854, 0.39603960396039606, 0.3935643564356436, 0.3910891089108911, 0.3886138613861386, 0.38613861386138615, 0.38366336633663367, 0.3811881188118812, 0.3787128712871287, 0.37623762376237624, 0.37376237623762376, 0.3712871287128713, 0.3688118811881188, 0.36633663366336633, 0.36386138613861385, 0.3613861386138614, 0.3589108910891089, 0.3564356435643564, 0.35396039603960394, 0.35148514851485146, 0.349009900990099, 0.3465346534653465, 0.34405940594059403, 0.3415841584158416, 0.33910891089108913, 0.33663366336633666, 0.3341584158415842, 0.3316831683168317, 0.3292079207920792, 0.32673267326732675, 0.32425742574257427, 0.3217821782178218, 0.3193069306930693, 0.31683168316831684, 0.31435643564356436, 0.3118811881188119, 0.3094059405940594, 0.3069306930693069, 0.30445544554455445, 0.30198019801980197, 0.2995049504950495, 0.297029702970297, 0.29455445544554454, 0.29207920792079206, 0.2896039603960396, 0.2871287128712871, 0.28465346534653463, 0.28217821782178215, 0.27970297029702973, 0.27722772277227725, 0.2747524752475248, 0.2722772277227723, 0.2698019801980198, 0.26732673267326734, 0.26485148514851486, 0.2623762376237624, 0.2599009900990099, 0.25742574257425743, 0.25495049504950495, 0.2524752475247525, 0.25, 0.24752475247524752, 0.24504950495049505, 0.24257425742574257, 0.2400990099009901, 0.2376237623762376, 0.23514851485148514, 0.23267326732673269, 0.2301980198019802, 0.22772277227722773, 0.22524752475247525, 0.22277227722772278, 0.2202970297029703, 0.21782178217821782, 0.21534653465346534, 0.21287128712871287, 0.2103960396039604, 0.2079207920792079, 0.20544554455445543, 0.20297029702970298, 0.2004950495049505, 0.19801980198019803, 0.19554455445544555, 0.19306930693069307, 0.1905940594059406, 0.18811881188118812, 0.18564356435643564, 0.18316831683168316, 0.1806930693069307, 0.1782178217821782, 0.17574257425742573, 0.17326732673267325, 0.1707920792079208, 0.16831683168316833, 0.16584158415841585, 0.16336633663366337, 0.1608910891089109, 0.15841584158415842, 0.15594059405940594, 0.15346534653465346, 0.15099009900990099, 0.1485148514851485, 0.14603960396039603, 0.14356435643564355, 0.14108910891089108, 0.13861386138613863, 0.13613861386138615, 0.13366336633663367, 0.1311881188118812, 0.12871287128712872, 0.12623762376237624, 0.12376237623762376, 0.12128712871287128, 0.1188118811881188, 0.11633663366336634, 0.11386138613861387, 0.11138613861386139, 0.10891089108910891, 0.10643564356435643, 0.10396039603960396, 0.10148514851485149, 0.09900990099009901, 0.09653465346534654, 0.09405940594059406, 0.09158415841584158, 0.0891089108910891, 0.08663366336633663, 0.08415841584158416, 0.08168316831683169, 0.07920792079207921, 0.07673267326732673, 0.07425742574257425, 0.07178217821782178, 0.06930693069306931, 0.06683168316831684, 0.06435643564356436, 0.06188118811881188, 0.0594059405940594, 0.05693069306930693, 0.054455445544554455, 0.05198019801980198, 0.04950495049504951, 0.04702970297029703, 0.04455445544554455, 0.04207920792079208, 0.039603960396039604, 0.03712871287128713, 0.034653465346534656, 0.03217821782178218, 0.0297029702970297, 0.027227722772277228, 0.024752475247524754, 0.022277227722772276, 0.019801980198019802, 0.017326732673267328, 0.01485148514851485, 0.012376237623762377, 0.009900990099009901, 0.007425742574257425, 0.0049504950495049506, 0.0024752475247524753]\n",
    "\n",
    "ys = np.array(ys)*2e-5\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d32a8e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama1:\n",
    "    \n",
    "create_scheduler kwargs = {'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'optimizer': AdamW (\n",
    "Parameter Group 0\n",
    "    betas: (0.9, 0.999)\n",
    "    correct_bias: True\n",
    "    eps: 1e-08\n",
    "    initial_lr: 2e-05\n",
    "    lr: 0.0\n",
    "    weight_decay: 0.0\n",
    "\n",
    "Parameter Group 1\n",
    "    betas: (0.9, 0.999)\n",
    "    correct_bias: True\n",
    "    eps: 1e-08\n",
    "    initial_lr: 2e-05\n",
    "    lr: 0.0\n",
    "    weight_decay: 0.0\n",
    "), 'num_warmup_steps': 13, 'num_training_steps': 416}\n",
    "\n",
    "    \n",
    "    \n",
    "llama2: \n",
    "\n",
    "create_scheduler kwargs = {'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'optimizer': AdamW (\n",
    "Parameter Group 0\n",
    "    betas: (0.9, 0.999)\n",
    "    correct_bias: True\n",
    "    eps: 1e-08\n",
    "    initial_lr: 2e-05\n",
    "    lr: 0.0\n",
    "    weight_decay: 0.0\n",
    "\n",
    "Parameter Group 1\n",
    "    betas: (0.9, 0.999)\n",
    "    correct_bias: True\n",
    "    eps: 1e-08\n",
    "    initial_lr: 2e-05\n",
    "    lr: 0.0\n",
    "    weight_decay: 0.0\n",
    "), 'num_warmup_steps': 13, 'num_training_steps': 416}\n",
    "\n",
    "    create_scheduler: lr_lambda=functools.partial(<function _get_linear_schedule_with_warmup_lr_lambda at 0x7fff43850940>, \n",
    "    num_warmup_steps=13, num_training_steps=416)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99cae195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AcceleratedOptimizer (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 2e-05\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 2e-05\n",
       "    lr: 0.0\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c58e57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3980815347721823"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "166/417\n",
    "\n",
    "create_scheduler: lr_lambda=functools.partial(<function _get_cosine_schedule_with_warmup_lr_lambda at 0x7fff426c8d30>, \n",
    "                                              num_warmup_steps=12, num_training_steps=378, num_cycles=0.5)create_scheduler: lr_lambda=functools.partial(<function _get_cosine_schedule_with_warmup_lr_lambda at 0x7fff54758d30>, num_warmup_steps=12, num_training_steps=378, num_cycles=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d005e6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 1.66666667e-06, 3.33333333e-06, 5.00000000e-06,\n",
       "       6.66666667e-06, 8.33333333e-06, 1.00000000e-05, 1.16666667e-05,\n",
       "       1.33333333e-05, 1.50000000e-05, 1.66666667e-05, 1.83333333e-05,\n",
       "       2.00000000e-05, 1.99996316e-05, 1.99985265e-05, 1.99966847e-05,\n",
       "       1.99941063e-05, 1.99907917e-05, 1.99867409e-05, 1.99819543e-05,\n",
       "       1.99764323e-05, 1.99701753e-05, 1.99631836e-05, 1.99554579e-05,\n",
       "       1.99469988e-05, 1.99378067e-05, 1.99278825e-05, 1.99172267e-05,\n",
       "       1.99058404e-05, 1.98937241e-05, 1.98808790e-05, 1.98673058e-05,\n",
       "       1.98530056e-05, 1.98379795e-05, 1.98222286e-05, 1.98057539e-05,\n",
       "       1.97885569e-05, 1.97706386e-05, 1.97520004e-05, 1.97326437e-05,\n",
       "       1.97125700e-05, 1.96917807e-05, 1.96702772e-05, 1.96480614e-05,\n",
       "       1.96251346e-05, 1.96014987e-05, 1.95771554e-05, 1.95521065e-05,\n",
       "       1.95263538e-05, 1.94998992e-05, 1.94727447e-05, 1.94448923e-05,\n",
       "       1.94163440e-05, 1.93871019e-05, 1.93571682e-05, 1.93265451e-05,\n",
       "       1.92952348e-05, 1.92632397e-05, 1.92305621e-05, 1.91972044e-05,\n",
       "       1.91631690e-05, 1.91284586e-05, 1.90930756e-05, 1.90570226e-05,\n",
       "       1.90203024e-05, 1.89829175e-05, 1.89448708e-05, 1.89061651e-05,\n",
       "       1.88668032e-05, 1.88267880e-05, 1.87861225e-05, 1.87448096e-05,\n",
       "       1.87028524e-05, 1.86602540e-05, 1.86170176e-05, 1.85731463e-05,\n",
       "       1.85286433e-05, 1.84835120e-05, 1.84377556e-05, 1.83913775e-05,\n",
       "       1.83443812e-05, 1.82967701e-05, 1.82485477e-05, 1.81997176e-05,\n",
       "       1.81502834e-05, 1.81002486e-05, 1.80496171e-05, 1.79983924e-05,\n",
       "       1.79465785e-05, 1.78941791e-05, 1.78411981e-05, 1.77876393e-05,\n",
       "       1.77335068e-05, 1.76788045e-05, 1.76235364e-05, 1.75677066e-05,\n",
       "       1.75113193e-05, 1.74543786e-05, 1.73968886e-05, 1.73388537e-05,\n",
       "       1.72802780e-05, 1.72211660e-05, 1.71615219e-05, 1.71013502e-05,\n",
       "       1.70406552e-05, 1.69794415e-05, 1.69177136e-05, 1.68554761e-05,\n",
       "       1.67927334e-05, 1.67294902e-05, 1.66657513e-05, 1.66015212e-05,\n",
       "       1.65368047e-05, 1.64716067e-05, 1.64059318e-05, 1.63397849e-05,\n",
       "       1.62731710e-05, 1.62060948e-05, 1.61385614e-05, 1.60705757e-05,\n",
       "       1.60021428e-05, 1.59332676e-05, 1.58639553e-05, 1.57942110e-05,\n",
       "       1.57240397e-05, 1.56534467e-05, 1.55824372e-05, 1.55110164e-05,\n",
       "       1.54391895e-05, 1.53669619e-05, 1.52943389e-05, 1.52213258e-05,\n",
       "       1.51479280e-05, 1.50741509e-05, 1.50000000e-05, 1.49254807e-05,\n",
       "       1.48505985e-05, 1.47753589e-05, 1.46997674e-05, 1.46238297e-05,\n",
       "       1.45475514e-05, 1.44709379e-05, 1.43939951e-05, 1.43167285e-05,\n",
       "       1.42391439e-05, 1.41612470e-05, 1.40830434e-05, 1.40045391e-05,\n",
       "       1.39257396e-05, 1.38466510e-05, 1.37672789e-05, 1.36876293e-05,\n",
       "       1.36077080e-05, 1.35275209e-05, 1.34470738e-05, 1.33663728e-05,\n",
       "       1.32854238e-05, 1.32042327e-05, 1.31228056e-05, 1.30411483e-05,\n",
       "       1.29592670e-05, 1.28771677e-05, 1.27948563e-05, 1.27123391e-05,\n",
       "       1.26296220e-05, 1.25467112e-05, 1.24636127e-05, 1.23803328e-05,\n",
       "       1.22968774e-05, 1.22132528e-05, 1.21294652e-05, 1.20455207e-05,\n",
       "       1.19614254e-05, 1.18771857e-05, 1.17928076e-05, 1.17082974e-05,\n",
       "       1.16236614e-05, 1.15389058e-05, 1.14540367e-05, 1.13690606e-05,\n",
       "       1.12839836e-05, 1.11988119e-05, 1.11135520e-05, 1.10282100e-05,\n",
       "       1.09427922e-05, 1.08573050e-05, 1.07717546e-05, 1.06861474e-05,\n",
       "       1.06004896e-05, 1.05147875e-05, 1.04290476e-05, 1.03432760e-05,\n",
       "       1.02574791e-05, 1.01716633e-05, 1.00858348e-05, 1.00000000e-05,\n",
       "       9.91416519e-06, 9.82833670e-06, 9.74252086e-06, 9.65672399e-06,\n",
       "       9.57095242e-06, 9.48521245e-06, 9.39951041e-06, 9.31385262e-06,\n",
       "       9.22824538e-06, 9.14269500e-06, 9.05720778e-06, 8.97179003e-06,\n",
       "       8.88644803e-06, 8.80118808e-06, 8.71601645e-06, 8.63093942e-06,\n",
       "       8.54596326e-06, 8.46109423e-06, 8.37633859e-06, 8.29170257e-06,\n",
       "       8.20719241e-06, 8.12281435e-06, 8.03857459e-06, 7.95447934e-06,\n",
       "       7.87053480e-06, 7.78674716e-06, 7.70312258e-06, 7.61966723e-06,\n",
       "       7.53638726e-06, 7.45328880e-06, 7.37037797e-06, 7.28766089e-06,\n",
       "       7.20514365e-06, 7.12283233e-06, 7.04073299e-06, 6.95885168e-06,\n",
       "       6.87719443e-06, 6.79576727e-06, 6.71457618e-06, 6.63362716e-06,\n",
       "       6.55292616e-06, 6.47247913e-06, 6.39229201e-06, 6.31237069e-06,\n",
       "       6.23272106e-06, 6.15334900e-06, 6.07426035e-06, 5.99546094e-06,\n",
       "       5.91695658e-06, 5.83875304e-06, 5.76085609e-06, 5.68327148e-06,\n",
       "       5.60600490e-06, 5.52906207e-06, 5.45244864e-06, 5.37617027e-06,\n",
       "       5.30023257e-06, 5.22464114e-06, 5.14940154e-06, 5.07451932e-06,\n",
       "       5.00000000e-06, 4.92584907e-06, 4.85207198e-06, 4.77867419e-06,\n",
       "       4.70566109e-06, 4.63303806e-06, 4.56081046e-06, 4.48898360e-06,\n",
       "       4.41756278e-06, 4.34655326e-06, 4.27596027e-06, 4.20578902e-06,\n",
       "       4.13604467e-06, 4.06673236e-06, 3.99785719e-06, 3.92942426e-06,\n",
       "       3.86143858e-06, 3.79390518e-06, 3.72682903e-06, 3.66021507e-06,\n",
       "       3.59406821e-06, 3.52839333e-06, 3.46319525e-06, 3.39847879e-06,\n",
       "       3.33424872e-06, 3.27050976e-06, 3.20726661e-06, 3.14452394e-06,\n",
       "       3.08228635e-06, 3.02055845e-06, 2.95934478e-06, 2.89864984e-06,\n",
       "       2.83847812e-06, 2.77883403e-06, 2.71972199e-06, 2.66114634e-06,\n",
       "       2.60311139e-06, 2.54562143e-06, 2.48868069e-06, 2.43229337e-06,\n",
       "       2.37646361e-06, 2.32119554e-06, 2.26649322e-06, 2.21236069e-06,\n",
       "       2.15880193e-06, 2.10582090e-06, 2.05342148e-06, 2.00160755e-06,\n",
       "       1.95038292e-06, 1.89975137e-06, 1.84971662e-06, 1.80028237e-06,\n",
       "       1.75145225e-06, 1.70322986e-06, 1.65561876e-06, 1.60862245e-06,\n",
       "       1.56224440e-06, 1.51648802e-06, 1.47135669e-06, 1.42685372e-06,\n",
       "       1.38298240e-06, 1.33974596e-06, 1.29714759e-06, 1.25519042e-06,\n",
       "       1.21387755e-06, 1.17321202e-06, 1.13319682e-06, 1.09383491e-06,\n",
       "       1.05512918e-06, 1.01708248e-06, 9.79697630e-07, 9.42977369e-07,\n",
       "       9.06924407e-07, 8.71541400e-07, 8.36830955e-07, 8.02795630e-07,\n",
       "       7.69437931e-07, 7.36760317e-07, 7.04765196e-07, 6.73454925e-07,\n",
       "       6.42831810e-07, 6.12898107e-07, 5.83656023e-07, 5.55107712e-07,\n",
       "       5.27255277e-07, 5.00100770e-07, 4.73646192e-07, 4.47893492e-07,\n",
       "       4.22844567e-07, 3.98501263e-07, 3.74865374e-07, 3.51938640e-07,\n",
       "       3.29722752e-07, 3.08219346e-07, 2.87430005e-07, 2.67356263e-07,\n",
       "       2.47999598e-07, 2.29361435e-07, 2.11443149e-07, 1.94246059e-07,\n",
       "       1.77771433e-07, 1.62020484e-07, 1.46994373e-07, 1.32694207e-07,\n",
       "       1.19121039e-07, 1.06275870e-07, 9.41596454e-08, 8.27732586e-08,\n",
       "       7.21175484e-08, 6.21932997e-08, 5.30012439e-08, 4.45420580e-08,\n",
       "       3.68163655e-08, 2.98247355e-08, 2.35676831e-08, 1.80456694e-08,\n",
       "       1.32591012e-08, 9.20833105e-09, 5.89365754e-09, 3.31532486e-09,\n",
       "       1.47352295e-09, 3.68387523e-10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.array(list(range(378)))\n",
    "ys = np.array([0.0, 0.08333333333333333, 0.16666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0, 0.999981580623855, 0.9999263238525135, 0.9998342337571565, 0.9997053171227526, 0.9995395834475577, 0.9993370449424153, 0.999097716529857, 0.9988216158430032, 0.9985087632242633, 0.9981591817238378, 0.9977728970980191, 0.9973499378072945, 0.9968903350142493, 0.99639412258127, 0.9958613370680507, 0.9952920177288985, 0.9946862065098414, 0.9940439480455385, 0.9933652896559907, 0.9926502813430544, 0.9918989757867582, 0.9911114283414204, 0.9902876970315715, 0.9894278425476789, 0.9885319282416754, 0.9876000201222912, 0.9866321868501913, 0.9856284997329157, 0.9845890327196267, 0.9835138623956602, 0.9824030679768823, 0.981256731303854, 0.9800749368358008, 0.9788577716443901, 0.9776053254073158, 0.9763176904016913, 0.9749949614972505, 0.9736372361493584, 0.9722446143918306, 0.970817198829563, 0.9693550946309721, 0.9678584095202468, 0.9663272537694112, 0.9647617401902002, 0.9631619841257475, 0.9615281034420881, 0.9598602185194733, 0.9581584522435024, 0.9564229299960678, 0.9546537796461179, 0.9528511315402357, 0.9510151184930353, 0.9491458757773765, 0.9472435411143978, 0.9453082546633702, 0.94334015901137, 0.9413393991627736, 0.9393061225285742, 0.9372404789155198, 0.9351426205150777, 0.9330127018922194, 0.930850879974034, 0.9286573140381662, 0.9264321657010799, 0.924175598906152, 0.9218877799115928, 0.9195688772781969, 0.9172190618569236, 0.9148385067763094, 0.9124273874297122, 0.9099858814623886, 0.9075141687584056, 0.9050124314273875, 0.902480853791098, 0.8999196223698598, 0.8973289258688125, 0.89470895516401, 0.8920599032883553, 0.8893819654173803, 0.8866753388548649, 0.8839402230183, 0.8811768194241951, 0.8783853316732313, 0.8755659654352599, 0.8727189284341501, 0.8698444304324835, 0.8669426832160996, 0.8640139005784924, 0.8610582983050581, 0.8580760941571967, 0.855067507856268, 0.8520327610674028, 0.8489720773831717, 0.845885682307111, 0.8427738032371077, 0.8396366694486466, 0.8364745120779165, 0.8332875641047817, 0.8300760603356159, 0.826840237386003, 0.8235803336633032, 0.8202965893490877, 0.8169892463814433, 0.813658548437147, 0.8103047409137114, 0.806928070911306, 0.8035287872145502, 0.8001071402741842, 0.796663382188616, 0.7931977666853478, 0.7897105491022818, 0.7862019863689074, 0.7826723369873714, 0.7791218610134323, 0.7755508200373, 0.7719594771643623, 0.7683480969958003, 0.7647169456090925, 0.7610662905384125, 0.7573964007549154, 0.7537075466469227, 0.75, 0.7462740339769323, 0.7425299230975981, 0.7387679432187442, 0.73498837151366, 0.7311914864517575, 0.727377567778053, 0.723546896492557, 0.7196997548295707, 0.7158364262368919, 0.7119571953549304, 0.7080623479957372, 0.7041521711219467, 0.7002269528256333, 0.696286982307086, 0.6923325498535006, 0.6883639468175926, 0.6843814655961301, 0.6803853996083917, 0.6763760432745475, 0.6723536919939669, 0.6683186421234552, 0.6642711909554174, 0.6602116366959556, 0.6561402784428974, 0.652057416163759, 0.6479633506736446, 0.6438583836130835, 0.6397428174258047, 0.635616955336455, 0.6314811013282573, 0.6273355601206143, 0.6231806371466574, 0.6190166385307427, 0.6148438710658979, 0.6106626421912162, 0.6064732599692079, 0.6022760330631005, 0.5980712707140985, 0.5938592827185993, 0.5896403794053678, 0.5854148716126721, 0.5811830706653819, 0.576945288352031, 0.572701836901845, 0.5684530289617377, 0.5641991775732755, 0.5599405961496137, 0.5556775984524044, 0.5514104985686802, 0.5471396108877122, 0.5428652500778471, 0.5385877310633232, 0.5343073690010671, 0.5300244792574742, 0.5257393773851733, 0.5214523790997773, 0.5171638002566218, 0.5128739568274944, 0.5085831648773538, 0.5042917405410435, 0.5, 0.4957082594589565, 0.4914168351226463, 0.48712604317250574, 0.48283619974337827, 0.47854762090022274, 0.4742606226148267, 0.46997552074252585, 0.46569263099893293, 0.4614122689366768, 0.4571347499221528, 0.4528603891122878, 0.44858950143131976, 0.44432240154759556, 0.4400594038503864, 0.43580082242672447, 0.43154697103826234, 0.42729816309815505, 0.42305471164796904, 0.4188169293346183, 0.414585128387328, 0.41035962059463227, 0.4061407172814008, 0.40192872928590156, 0.3977239669368997, 0.39352674003079224, 0.38933735780878376, 0.3851561289341023, 0.38098336146925726, 0.37681936285334267, 0.3726644398793857, 0.3685188986717427, 0.364383044663545, 0.3602571825741953, 0.35614161638691655, 0.35203664932635537, 0.3479425838362411, 0.3438597215571027, 0.3397883633040445, 0.33572880904458263, 0.3316813578765449, 0.3276463080060331, 0.3236239567254526, 0.3196146003916084, 0.31561853440386994, 0.3116360531824074, 0.3076674501464994, 0.30371301769291414, 0.2997730471743667, 0.2958478288780533, 0.2919376520042628, 0.2880428046450697, 0.2841635737631082, 0.28030024517042906, 0.27645310350744295, 0.27262243222194726, 0.26880851354824276, 0.2650116284863402, 0.26123205678125594, 0.257470076902402, 0.25372596602306785, 0.2500000000000001, 0.24629245335307737, 0.2426035992450848, 0.23893370946158754, 0.2352830543909074, 0.2316519030041998, 0.2280405228356377, 0.22444917996270003, 0.22087813898656772, 0.21732766301262868, 0.21379801363109258, 0.21028945089771817, 0.2068022333146522, 0.20333661781138407, 0.19989285972581594, 0.19647121278544993, 0.19307192908869397, 0.1896952590862886, 0.1863414515628531, 0.18301075361855673, 0.17970341065091244, 0.17641966633669703, 0.17315976261399696, 0.16992393966438407, 0.1667124358952184, 0.16352548792208355, 0.16036333055135343, 0.1572261967628923, 0.15411431769288908, 0.15102792261682813, 0.1479672389325971, 0.14493249214373222, 0.14192390584280346, 0.1389417016949419, 0.13598609942150763, 0.13305731678390048, 0.13015556956751667, 0.12728107156585, 0.12443403456474017, 0.12161466832676887, 0.11882318057580488, 0.1160597769817, 0.11332466114513512, 0.11061803458261976, 0.10794009671164484, 0.10529104483599022, 0.10267107413118742, 0.10008037763014033, 0.09751914620890206, 0.09498756857261242, 0.09248583124159437, 0.09001411853761149, 0.08757261257028776, 0.08516149322369054, 0.08278093814307635, 0.08043112272180308, 0.07811222008840718, 0.07582440109384808, 0.07356783429892022, 0.0713426859618338, 0.0691491200259659, 0.06698729810778065, 0.06485737948492237, 0.06275952108448019, 0.060693877471425906, 0.05866060083722624, 0.05665984098862992, 0.05469174533662979, 0.05275645888560232, 0.05085412422262364, 0.04898488150696467, 0.04714886845976429, 0.04534622035388214, 0.04357707000393224, 0.04184154775649768, 0.04013978148052677, 0.038471896557912, 0.036838015874252505, 0.03523825980979989, 0.0336727462305888, 0.032141590479753235, 0.03064490536902792, 0.02918280117043709, 0.02775538560816937, 0.02636276385064157, 0.025005038502749488, 0.02368230959830875, 0.022394674592684183, 0.02114222835560986, 0.01992506316419912, 0.018743268696145954, 0.017596932023117684, 0.016486137604339812, 0.015410967280373222, 0.014371500267084336, 0.013367813149808727, 0.012399979877708744, 0.011468071758324594, 0.010572157452321096, 0.009712302968428566, 0.008888571658579703, 0.008101024213241825, 0.007349718656945503, 0.006634710344009254, 0.005956051954461472, 0.005313793490158536, 0.0047079822711015296, 0.004138662931949255, 0.0036058774187299747, 0.0031096649857508263, 0.0026500621927054713, 0.0022271029019809707, 0.001840818276162226, 0.0014912367757366485, 0.0011783841569968367, 0.0009022834701429838, 0.0006629550575847354, 0.00046041655244233315, 0.0002946828772473764, 0.00016576624284347918, 7.36761474865455e-05, 1.841937614505129e-05])\n",
    "ys = 2e-5*ys\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7230f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 417.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# s = trainer.lr_scheduler\n",
    "# # s.num_warmup_steps, s.num_training_steps\n",
    "\n",
    "# lr_lambda = s.lr_lambdas[0]\n",
    "\n",
    "\n",
    "\n",
    "# xs = np.linspace(0, 7, 100)\n",
    "# ys = np.array(list(lr_lambda(x) for x in xs))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(xs, ys)\n",
    "ax.plot(xs+166, ys)\n",
    "ax.set_xlim((0, 417))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7150df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.217821782178218e-06"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[417-166]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c956b553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "scheduler = trainer.lr_scheduler\n",
    "optimizer = trainer.optimizer\n",
    "\n",
    "learning_rates = []\n",
    "steps = []\n",
    "# Training loop\n",
    "total_steps = 2000  # Adjust as needed\n",
    "for step in range(total_steps):\n",
    "    # Your training code here\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Store learning rate and step\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    steps.append(step)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(steps, learning_rates, label='Learning Rate')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce0352ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 31.75 GiB total capacity; 29.75 GiB already allocated; 674.88 MiB free; 30.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m last_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m last_checkpoint\n\u001b[0;32m----> 6\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py:1506\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py:1801\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1801\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1804\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1807\u001b[0m ):\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py:2650\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2650\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2653\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/trainer.py:2673\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2672\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2673\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py:581\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/accelerate/utils/operations.py:569\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:917\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module(\u001b[38;5;241m*\u001b[39minputs, past_key_value, output_attentions, padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 917\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_custom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    922\u001b[0m         hidden_states,\n\u001b[1;32m    923\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m         padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask,\n\u001b[1;32m    929\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/utils/checkpoint.py:269\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing context_fn is only supported when use_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    272\u001b[0m         function,\n\u001b[1;32m    273\u001b[0m         preserve,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    277\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/utils/checkpoint.py:111\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    108\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mLlamaModel.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# None for past_key_value\u001b[39;00m\n\u001b[0;32m--> 913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:633\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:387\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 31.75 GiB total capacity; 29.75 GiB already allocated; 674.88 MiB free; 30.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bbfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31156159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: json in the raw data folder is not a supported dataset. We will skip it.\r\n",
      "Processing starcoder data...\r\n"
     ]
    }
   ],
   "source": [
    "!cd .. && python open_instruct/reformat_datasets.py --raw_data_dir data/raw_train --output_dir data/processed --dataset starcoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "54a4c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/starcoder/json/default-81508a354854d02d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/starcoder/json/default-122cd053399382de/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927eac2053444d4a85e605e882e368bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4c83c1ad6749ec802b0cab01978d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/data/processed/starcoder/json/default-122cd053399382de/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from note_pruning_analysis import get_dataset\n",
    "\n",
    "dss = get_dataset('starcoder_simple')\n",
    "dsr = get_dataset('starcoder_role')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2edda8c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# instrs = list(zip(\n",
    "#     [x[0]['content'] for x in dss[:100]['messages']],\n",
    "#     [x[0]['content'] for x in dsr[:100]['messages']],\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4d792906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Task: Implement a convolutional neural network (CNN) for image classification using Keras. The model should use CIFAR-10 dataset, which consists of 50,000 training images and 10,000 testing images. The aim is to classify the images into 10 different classes. The layers of the model should include Conv2D, Activation, MaxPooling2D, Dropout, and Dense. The activation function should be relu in the first two Conv2D layers and softmax in the last Dense layer. The optimizer should be RMSprop with a learning rate of 0.0001 and a decay of 1e-6. The loss function should be categorical crossentropy. The model should train for 100 epochs and should be evaluated on the test set.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "from __future__ import print_function\n",
      "import keras\n",
      "from keras.datasets import cifar10\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "batch_size = 32\n",
      "num_classes = 10\n",
      "epochs = 100\n",
      "\n",
      "# The data, split between train and test sets:\n",
      "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
      "print('x_train shape:', x_train.shape)\n",
      "print(x_train.shape[0], 'train samples')\n",
      "print(x_test.shape[0], 'test samples')\n",
      "\n",
      "# Convert class vectors to binary class matrices.\n",
      "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
      "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Conv2D(32, (3, 3), padding='same',\n",
      "                 input_shape=x_train.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(Conv2D(32, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Conv2D(64, (3, 3), padding='same'))\n",
      "model.add(Activation('relu'))\n",
      "model.add(Conv2D(64, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Flatten())\n",
      "model.add(Dense(512))\n",
      "model.add(Activation('relu'))\n",
      "model.add(Dropout(0.5))\n",
      "model.add(Dense(num_classes))\n",
      "model.add(Activation('softmax'))\n",
      "\n",
      "# initiate RMSprop optimizer\n",
      "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
      "\n",
      "# Let's train the model using RMSprop\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer=opt,\n",
      "              metrics=['accuracy'])\n",
      "\n",
      "x_train = x_train.astype('float32')\n",
      "x_test = x_test.astype('float32')\n",
      "x_train /= 255\n",
      "x_test /= 255\n",
      "\n",
      "print('Not using data augmentation.')\n",
      "model.fit(x_train, y_train,\n",
      "          batch_size=batch_size,\n",
      "          epochs=epochs,\n",
      "          # validation_data=(x_test, y_test),\n",
      "          shuffle=True)\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a program using Python and Keras library to construct a Convolutional Neural Network (CNN) model for the CIFAR-10 dataset. This model should consist of two sets of convolutional layers followed by max pooling and dropout to avoid overfitting, followed by flattening the data and two fully connected layers. Use RMSprop as the optimizer, categorical cross entropy as the loss function, and accuracy as the metric to track. The model should be trained for 100 epochs with a batch size of 32. Also, convert the class vectors to binary class matrices.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Please, write a Python program using matplotlib to create a line plot that visualizes the median salaries of Python developers, JavaScript developers, and all developers grouped by age. Use the xkcd style. The x-axis should represent age, and the y-axis should represent the median salary. Use the plot function to plot each group of developers, with different styles for each group. Add a legend to the plot and use the tight_layout function to ensure everything fits nicely on the plot. Save the plot as 'plot.png' and show it. The ages should range from 18 to 55, and the corresponding salaries are given in the lists 'py_dev_y', 'js_dev_y', and 'dev_y' respectively.\n",
      "assistant:\n",
      "\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "plt.xkcd()\n",
      "\n",
      "ages_x = [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "          36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]\n",
      "\n",
      "py_dev_y = [20046, 17100, 20000, 24744, 30500, 37732, 41247, 45372, 48876, 53850, 57287, 63016, 65998, 70003, 70000, 71496, 75370, 83640, 84666,\n",
      "            84392, 78254, 85000, 87038, 91991, 100000, 94796, 97962, 93302, 99240, 102736, 112285, 100771, 104708, 108423, 101407, 112542, 122870, 120000]\n",
      "plt.plot(ages_x, py_dev_y, label='Python')\n",
      "\n",
      "js_dev_y = [16446, 16791, 18942, 21780, 25704, 29000, 34372, 37810, 43515, 46823, 49293, 53437, 56373, 62375, 66674, 68745, 68746, 74583, 79000,\n",
      "            78508, 79996, 80403, 83820, 88833, 91660, 87892, 96243, 90000, 99313, 91660, 102264, 100000, 100000, 91660, 99240, 108000, 105000, 104000]\n",
      "plt.plot(ages_x, js_dev_y, label='JavaScript')\n",
      "\n",
      "dev_y = [17784, 16500, 18012, 20628, 25206, 30252, 34368, 38496, 42000, 46752, 49320, 53200, 56000, 62316, 64928, 67317, 68748, 73752, 77232,\n",
      "         78000, 78508, 79536, 82488, 88935, 90000, 90056, 95000, 90000, 91633, 91660, 98150, 98964, 100000, 98988, 100000, 108923, 105000, 103117]\n",
      "plt.plot(ages_x, dev_y, color='#444444', linestyle='--', label='All Devs')\n",
      "\n",
      "plt.xlabel('Ages')\n",
      "plt.ylabel('Median Salary (USD)')\n",
      "plt.title('Median Salary (USD) by Age')\n",
      "\n",
      "plt.legend()\n",
      "\n",
      "plt.tight_layout()\n",
      "\n",
      "plt.savefig('plot.png')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "**Assuming the role of Data Scientist**, you will be analyzing the given data and visualizing it using Python and matplotlib. However, instead of just displaying all the data points, the visualization should focus on the difference in salaries across different age groups for Python, JavaScript, and All Devs categories. The aim is to provide insights that highlight the salary difference between these age groups for each category. The resulting plot should be a clear and concise visual display of this information, with clear and understandable labels and a title that describes what the plot is about. Use different colors or line styles to differentiate between the categories. The result should be a compelling and effective visualization that helps to compare different age groups' salaries.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Can you help with this?\n",
      "\n",
      "Implement a ROS2 action client in Python that can send a goal to a Fibonacci action server, using the rclpy library.\n",
      "\n",
      "assistant:\n",
      "\n",
      "import rclpy\n",
      "from rclpy.action import ActionClient\n",
      "from rclpy.node import Node\n",
      "\n",
      "from action_tutorials_interfaces.action import Fibonacci\n",
      "\n",
      "\n",
      "class FibonacciActionClient(Node):\n",
      "\n",
      "    def __init__(self):\n",
      "        super().__init__('fibonacci_action_client')\n",
      "        self._action_client = ActionClient(self, Fibonacci, 'fibonacci')\n",
      "\n",
      "    def send_goal(self, order):\n",
      "        goal_msg = Fibonacci.Goal()\n",
      "        goal_msg.order = order\n",
      "\n",
      "        self._action_client.wait_for_server()\n",
      "\n",
      "        return self._action_client.send_goal_async(goal_msg)\n",
      "\n",
      "\n",
      "def main(args=None):\n",
      "    rclpy.init(args=args)\n",
      "\n",
      "    action_client = FibonacciActionClient()\n",
      "\n",
      "    future = action_client.send_goal(10)\n",
      "\n",
      "    rclpy.spin_until_future_complete(action_client, future)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Plase answer the following request: Understanding the FibonacciActionClient, create a new class inheriting from 'Node' in the ROS2 environment. Implement an action client for the Fibonacci action interface. Use the send_goal_async() method to asynchronously send the goal to the server. Modify the code to make the action client wait for the action server to become available before sending the goal. Also, ensure that the action client spins until the future is complete after sending the goal.\n",
      "Answer:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Create a function that takes a batch of 3D points X and a batch of camera parameters, applies an orthographic projection using the `orthographic_projection` function, and then applies a rotation based on the rotation matrix derived from the axis-angle representation of the camera parameters using the `rodrigues` function. The camera parameters should be converted to an angle-axis representation and then to a rotation matrix using the `quat2mat` function.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "import torch\n",
      "\n",
      "def rodrigues(theta):\n",
      "    \"\"\"Convert axis-angle representation to rotation matrix.\n",
      "    Args:\n",
      "        theta: size = [B, 3]\n",
      "    Returns:\n",
      "        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n",
      "    \"\"\"\n",
      "    l1norm = torch.norm(theta + 1e-8, p = 2, dim = 1)\n",
      "    angle = torch.unsqueeze(l1norm, -1)\n",
      "    normalized = torch.div(theta, angle)\n",
      "    angle = angle * 0.5\n",
      "    v_cos = torch.cos(angle)\n",
      "    v_sin = torch.sin(angle)\n",
      "    quat = torch.cat([v_cos, v_sin * normalized], dim = 1)\n",
      "    return quat2mat(quat)\n",
      "\n",
      "def quat2mat(quat):\n",
      "    \"\"\"Convert quaternion coefficients to rotation matrix.\n",
      "    Args:\n",
      "        quat: size = [B, 4] 4 <===>(w, x, y, z)\n",
      "    Returns:\n",
      "        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n",
      "    \"\"\" \n",
      "    norm_quat = quat\n",
      "    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)\n",
      "    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]\n",
      "\n",
      "    B = quat.size(0)\n",
      "\n",
      "    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n",
      "    wx, wy, wz = w*x, w*y, w*z\n",
      "    xy, xz, yz = x*y, x*z, y*z\n",
      "\n",
      "    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\n",
      "                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\n",
      "                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\n",
      "    return rotMat    \n",
      "    \n",
      "def orthographic_projection(X, camera):\n",
      "    \"\"\"Perform orthographic projection of 3D points X using the camera parameters\n",
      "    Args:\n",
      "        X: size = [B, N, 3]\n",
      "        camera: size = [B, 3]\n",
      "    Returns:\n",
      "        Projected 2D points -- size = [B, N, 2]\n",
      "    \"\"\" \n",
      "    camera = camera.view(-1, 1, 3)\n",
      "    X_trans = X[:, :, :2] + camera[:, :, 1:]\n",
      "    shape = X_trans.shape\n",
      "    X_2d = (camera[:, :, 0] * X_trans.view(shape[0], -1)).view(shape)\n",
      "    return X_2d\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are given two functions rodrigues and quat2mat that are responsible for converting the representation of 3D rotation angles between different forms (Rodrigues' rotation formula and quaternion representation). Your task is to interpret these functions and explain them in a simplified, intuitive manner. Additionally, you need to analyze the function orthographic_projection and propose a way to increase its efficiency by reducing computational resources, without altering its functionality.\n",
      "\n",
      "### Response:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a Python function that will convert a dictionary with op_id and op_type keys into a JSON string using the 'from_alipay_dict' and 'to_alipay_dict' methods of the 'MemberCardOperator' class.\n",
      "assistant:\n",
      "\n",
      "\n",
      "import json\n",
      "\n",
      "from alipay.aop.api.constant.ParamConstants import *\n",
      "\n",
      "\n",
      "class MemberCardOperator(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self._op_id = None\n",
      "        self._op_type = None\n",
      "\n",
      "    @property\n",
      "    def op_id(self):\n",
      "        return self._op_id\n",
      "\n",
      "    @op_id.setter\n",
      "    def op_id(self, value):\n",
      "        self._op_id = value\n",
      "    @property\n",
      "    def op_type(self):\n",
      "        return self._op_type\n",
      "\n",
      "    @op_type.setter\n",
      "    def op_type(self, value):\n",
      "        self._op_type = value\n",
      "\n",
      "\n",
      "    def to_alipay_dict(self):\n",
      "        params = dict()\n",
      "        if self.op_id:\n",
      "            if hasattr(self.op_id, 'to_alipay_dict'):\n",
      "                params['op_id'] = self.op_id.to_alipay_dict()\n",
      "            else:\n",
      "                params['op_id'] = self.op_id\n",
      "        if self.op_type:\n",
      "            if hasattr(self.op_type, 'to_alipay_dict'):\n",
      "                params['op_type'] = self.op_type.to_alipay_dict()\n",
      "            else:\n",
      "                params['op_type'] = self.op_type\n",
      "        return params\n",
      "\n",
      "    @staticmethod\n",
      "    def from_alipay_dict(d):\n",
      "        if not d:\n",
      "            return None\n",
      "        o = MemberCardOperator()\n",
      "        if 'op_id' in d:\n",
      "            o.op_id = d['op_id']\n",
      "        if 'op_type' in d:\n",
      "            o.op_type = d['op_type']\n",
      "        return o\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Can you write a Python class named `MemberCardActivator` that extends `MemberCardOperator`, adds a new property `activation_date` and modifies the `to_alipay_dict` and `from_alipay_dict` methods to include `activation_date`?\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "In the provided Python code, create a unit test for the 'KoubeiMarketingCampaignOpenDeliveryDeleteModel' class. Write the test function such that it tests the 'to_alipay_dict' and 'from_alipay_dict' methods. The test function should ensure that the output of 'to_alipay_dict' is correctly converted into the expected format by 'from_alipay_dict'.\n",
      "Response:\n",
      "assistant:\n",
      "\n",
      "import json\n",
      "\n",
      "from alipay.aop.api.constant.ParamConstants import *\n",
      "\n",
      "\n",
      "class KoubeiMarketingCampaignOpenDeliveryDeleteModel(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self._delivery_type = None\n",
      "        self._partner_id = None\n",
      "        self._shop_id = None\n",
      "\n",
      "    @property\n",
      "    def delivery_type(self):\n",
      "        return self._delivery_type\n",
      "\n",
      "    @delivery_type.setter\n",
      "    def delivery_type(self, value):\n",
      "        self._delivery_type = value\n",
      "    @property\n",
      "    def partner_id(self):\n",
      "        return self._partner_id\n",
      "\n",
      "    @partner_id.setter\n",
      "    def partner_id(self, value):\n",
      "        self._partner_id = value\n",
      "    @property\n",
      "    def shop_id(self):\n",
      "        return self._shop_id\n",
      "\n",
      "    @shop_id.setter\n",
      "    def shop_id(self, value):\n",
      "        self._shop_id = value\n",
      "\n",
      "\n",
      "    def to_alipay_dict(self):\n",
      "        params = dict()\n",
      "        if self.delivery_type:\n",
      "            if hasattr(self.delivery_type, 'to_alipay_dict'):\n",
      "                params['delivery_type'] = self.delivery_type.to_alipay_dict()\n",
      "            else:\n",
      "                params['delivery_type'] = self.delivery_type\n",
      "        if self.partner_id:\n",
      "            if hasattr(self.partner_id, 'to_alipay_dict'):\n",
      "                params['partner_id'] = self.partner_id.to_alipay_dict()\n",
      "            else:\n",
      "                params['partner_id'] = self.partner_id\n",
      "        if self.shop_id:\n",
      "            if hasattr(self.shop_id, 'to_alipay_dict'):\n",
      "                params['shop_id'] = self.shop_id.to_alipay_dict()\n",
      "            else:\n",
      "                params['shop_id'] = self.shop_id\n",
      "        return params\n",
      "\n",
      "    @staticmethod\n",
      "    def from_alipay_dict(d):\n",
      "        if not d:\n",
      "            return None\n",
      "        o = KoubeiMarketingCampaignOpenDeliveryDeleteModel()\n",
      "        if 'delivery_type' in d:\n",
      "            o.delivery_type = d['delivery_type']\n",
      "        if 'partner_id' in d:\n",
      "            o.partner_id = d['partner_id']\n",
      "        if 'shop_id' in d:\n",
      "            o.shop_id = d['shop_id']\n",
      "        return o\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "The provided code defines a Python class called KoubeiMarketingCampaignOpenDeliveryDeleteModel. This class has three properties: delivery_type, partner_id, and shop_id. These properties have getter and setter methods. Additionally, it has a method to_alipay_dict that converts the properties into a dictionary format and a class method from_alipay_dict that creates an instance of the class from a dictionary.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Modify the 'outputCommands' field of the 'OutALCARECOTkAlZMuMu_noDrop' variable so that the line 'drop *' is inserted at the beginning of the list of commands.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "import FWCore.ParameterSet.Config as cms\n",
      "\n",
      "# AlCaReco for track based alignment using ZMuMu events\n",
      "OutALCARECOTkAlZMuMu_noDrop = cms.PSet(\n",
      "    SelectEvents = cms.untracked.PSet(\n",
      "        SelectEvents = cms.vstring('pathALCARECOTkAlZMuMu')\n",
      "    ),\n",
      "    outputCommands = cms.untracked.vstring(\n",
      "        'keep *_ALCARECOTkAlZMuMu_*_*', \n",
      "        'keep L1AcceptBunchCrossings_*_*_*',\n",
      "        'keep L1GlobalTriggerReadoutRecord_gtDigis_*_*',\n",
      "        'keep *_TriggerResults_*_*',\n",
      "        'keep DcsStatuss_scalersRawToDigi_*_*',\n",
      "\t'keep *_offlinePrimaryVertices_*_*')\n",
      ")\n",
      "\n",
      "import copy\n",
      "OutALCARECOTkAlZMuMu = copy.deepcopy(OutALCARECOTkAlZMuMu_noDrop)\n",
      "OutALCARECOTkAlZMuMu.outputCommands.insert(0, \"drop *\")\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Tell me how would you respond to the following request.\n",
      "Review and understand the purpose of the provided Python script. It appears to be part of the ALCARECO (Alignment and Calibration using Collisions) package in the CMS software. Your task is to determine its functionality, correct any syntax errors, and propose possible improvements in a software engineering perspective.\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Task: Plot two sets of points and draw a connection between them. The points on the first graph should be at (0.2, 0.2) and (0.8, 0.8). The points on the second graph should be at (0.3, 0.2). Create a connection from the first graph to the second one.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "from matplotlib.patches import ConnectionPatch\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
      "\n",
      "xyA = (0.2, 0.2)\n",
      "xyB = (0.8, 0.8)\n",
      "coordsA = \"data\"\n",
      "coordsB = \"data\"\n",
      "con = ConnectionPatch(xyA, xyB, coordsA, coordsB,\n",
      "                      arrowstyle=\"-|>\", shrinkA=5, shrinkB=5,\n",
      "                      mutation_scale=20, fc=\"w\")\n",
      "ax1.plot([xyA[0], xyB[0]], [xyA[1], xyB[1]], \"o\")\n",
      "ax1.add_artist(con)\n",
      "\n",
      "xy = (0.3, 0.2)\n",
      "coordsA = \"data\"\n",
      "coordsB = \"data\"\n",
      "con = ConnectionPatch(xyA=xy, xyB=xy, coordsA=coordsA, coordsB=coordsB,\n",
      "                      axesA=ax2, axesB=ax1,\n",
      "                      arrowstyle=\"->\", shrinkB=5)\n",
      "ax2.add_artist(con)\n",
      "\n",
      "ax1.set_xlim(0, 1)\n",
      "ax1.set_ylim(0, 1)\n",
      "ax2.set_xlim(0, .5)\n",
      "ax2.set_ylim(0, .5)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Analyze the provided code snippet and design an aesthetically pleasing visualization using matplotlib that conveys data or information in a clear and understandable way. You do not need to write the code, but you should consider the shape, color, and alignment of the elements as well as the overall layout of the plot when designing.\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a Python function named 'get_data_provider_by_name' that takes two arguments, 'name' and 'train_params'. This function should return the required data provider class based on the 'name' argument. The data provider classes are imported from .cifar and .svhn. If the 'name' argument is 'C10', it should return an instance of Cifar10DataProvider with the 'train_params' arguments. If the 'name' argument is 'C10+', it should return an instance of Cifar10AugmentedDataProvider with the 'train_params' arguments. Similarly, it should handle other conditions for 'C100', 'C100+', and 'SVHN' names. If the 'name' argument does not match any of these, it should print a message saying \"Sorry, data provider for `%s` dataset was not implemented yet\" and exit the program.\n",
      "\n",
      "assistant:\n",
      "\n",
      "from .cifar import Cifar10DataProvider, Cifar100DataProvider, \\\n",
      "    Cifar10AugmentedDataProvider, Cifar100AugmentedDataProvider\n",
      "from .svhn import SVHNDataProvider\n",
      "\n",
      "\n",
      "def get_data_provider_by_name(name, train_params):\n",
      "    \"\"\"Return required data provider class\"\"\"\n",
      "    if name == 'C10':\n",
      "        return Cifar10DataProvider(**train_params)\n",
      "    if name == 'C10+':\n",
      "        return Cifar10AugmentedDataProvider(**train_params)\n",
      "    if name == 'C100':\n",
      "        return Cifar100DataProvider(**train_params)\n",
      "    if name == 'C100+':\n",
      "        return Cifar100AugmentedDataProvider(**train_params)\n",
      "    if name == 'SVHN':\n",
      "        return SVHNDataProvider(**train_params)\n",
      "    else:\n",
      "        print(\"Sorry, data provider for `%s` dataset \"\n",
      "              \"was not implemented yet\" % name)\n",
      "        exit()\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Instruction: Given a dataset name, write a Python function to return the corresponding data provider. The function should accept two parameters: the dataset name and a dictionary of training parameters. The function should use if-else statements to match the dataset name and return the corresponding data provider. If no matching data provider is found, the function should print an error message and terminate the program.\n",
      "Output:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a function in Python that takes a list of strings as input. This function should return a new list that has the same content as the input list but with all strings that start with a vowel (a, e, i, o, u) removed.\n",
      "\n",
      "### Response:\n",
      "assistant:\n",
      "\n",
      "import FWCore.ParameterSet.Config as cms\n",
      "\n",
      "# AlCaReco for track based alignment using MinBias events\n",
      "OutALCARECOTkAlMinBias_noDrop = cms.PSet(\n",
      "    SelectEvents = cms.untracked.PSet(\n",
      "        SelectEvents = cms.vstring('pathALCARECOTkAlMinBias')\n",
      "    ),\n",
      "    outputCommands = cms.untracked.vstring(\n",
      "        'keep *_ALCARECOTkAlMinBias_*_*', \n",
      "        'keep L1AcceptBunchCrossings_*_*_*',\n",
      "        'keep L1GlobalTriggerReadoutRecord_gtDigis_*_*',\n",
      "        'keep *_TriggerResults_*_*',\n",
      "        'keep DcsStatuss_scalersRawToDigi_*_*',\n",
      "        'keep *_offlinePrimaryVertices_*_*',\n",
      "        'keep *_offlineBeamSpot_*_*')\n",
      ")\n",
      "\n",
      "import copy\n",
      "OutALCARECOTkAlMinBias = copy.deepcopy(OutALCARECOTkAlMinBias_noDrop)\n",
      "OutALCARECOTkAlMinBias.outputCommands.insert(0, \"drop *\")\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Task: Using your expertise in coding and understanding the CMS software framework, revise the configuration for the event selection module. Ensure that all necessary inputs are correctly specified, and that the output is handled optimally. Incorporate the new configuration into the existing framework.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Tell me how would you respond to the following request.\n",
      "Understand and generate the code to remove a currency from the 'currency_list' attribute of the AlipayOverseasTravelRateCurrencyBatchqueryResponse class while ensuring that it does not contain any duplicates.\n",
      "\n",
      "assistant:\n",
      "\n",
      "import json\n",
      "\n",
      "from alipay.aop.api.response.AlipayResponse import AlipayResponse\n",
      "from alipay.aop.api.domain.RateCurrency import RateCurrency\n",
      "\n",
      "\n",
      "class AlipayOverseasTravelRateCurrencyBatchqueryResponse(AlipayResponse):\n",
      "\n",
      "    def __init__(self):\n",
      "        super(AlipayOverseasTravelRateCurrencyBatchqueryResponse, self).__init__()\n",
      "        self._currency_list = None\n",
      "\n",
      "    @property\n",
      "    def currency_list(self):\n",
      "        return self._currency_list\n",
      "\n",
      "    @currency_list.setter\n",
      "    def currency_list(self, value):\n",
      "        if isinstance(value, list):\n",
      "            self._currency_list = list()\n",
      "            for i in value:\n",
      "                if isinstance(i, RateCurrency):\n",
      "                    self._currency_list.append(i)\n",
      "                else:\n",
      "                    self._currency_list.append(RateCurrency.from_alipay_dict(i))\n",
      "\n",
      "    def parse_response_content(self, response_content):\n",
      "        response = super(AlipayOverseasTravelRateCurrencyBatchqueryResponse, self).parse_response_content(response_content)\n",
      "        if 'currency_list' in response:\n",
      "            self.currency_list = response['currency_list']\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a method in Python that handles the response from the AlipayOverseasTravelRateCurrencyBatchqueryResponse API. This method should be able to parse the response content, convert it to a JSON object, and store the currency_list into an object. It should also check if each item in the list is a RateCurrency object or convert it from a dictionary if not. Please make sure to handle any exceptions that might occur during this process.\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Use TensorFlow to implement an autoencoder model on the MNIST dataset. The model should have an encoder part with 2 layers of 256 and 64 neurons respectively, and a decoder part with the same number of neurons. Train the model for 20 epochs and validate on a separate test set. Print the encoding of the first image in the test set.\n",
      "\n",
      "### Response:\n",
      "assistant:\n",
      "\n",
      "from __future__ import division, print_function, absolute_import\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import tflearn\n",
      "\n",
      "# Data loading and preprocessing\n",
      "import tflearn.datasets.mnist as mnist\n",
      "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
      "\n",
      "# Building the encoder\n",
      "encoder = tflearn.input_data(shape=[None, 784])\n",
      "encoder = tflearn.fully_connected(encoder, 256)\n",
      "encoder = tflearn.fully_connected(encoder, 64)\n",
      "\n",
      "# Building the decoder\n",
      "decoder = tflearn.fully_connected(encoder, 256)\n",
      "decoder = tflearn.fully_connected(decoder, 784, activation='sigmoid')\n",
      "\n",
      "# Regression, with mean square error\n",
      "net = tflearn.regression(decoder, optimizer='adam', learning_rate=0.001,\n",
      "                         loss='mean_square', metric=None)\n",
      "\n",
      "# Training the auto encoder\n",
      "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
      "model.fit(X, X, n_epoch=20, validation_set=(testX, testX),\n",
      "          run_id=\"auto_encoder\", batch_size=256)\n",
      "\n",
      "# Encoding X[0] for test\n",
      "print(\"\\nTest encoding of X[0]:\")\n",
      "# New model, re-using the same session, for weights sharing\n",
      "encoding_model = tflearn.DNN(encoder, session=model.session)\n",
      "print(encoding_model.predict([X[0]]))\n",
      "\n",
      "# Testing the image reconstruction on new data (test set)\n",
      "print(\"\\nVisualizing results after being encoded and decoded:\")\n",
      "testX = tflearn.data_utils.shuffle(testX)[0]\n",
      "# Applying encode and decode over test set\n",
      "encode_decode = model.predict(testX)\n",
      "# Compare original images with their reconstructions\n",
      "f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
      "for i in range(10):\n",
      "    temp = [[ii, ii, ii] for ii in list(testX[i])]\n",
      "    a[0][i].imshow(np.reshape(temp, (28, 28, 3)))\n",
      "    temp = [[ii, ii, ii] for ii in list(encode_decode[i])]\n",
      "    a[1][i].imshow(np.reshape(temp, (28, 28, 3)))\n",
      "f.show()\n",
      "plt.draw()\n",
      "plt.waitforbuttonpress()\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "You are working with a dataset from the MNIST database (Modified National Institute of Standards and Technology database), a dataset of handwritten digits. The aim of your task is to develop a type of model called an autoencoder. This is a type of artificial neural network that is used to learn efficient data codings in an unsupervised manner. The model will learn the structure of the dataset and be capable of reconstructing the original data from its encodings\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a function named 'customize_inception_resnet_v2' that takes in an existing InceptionResnetV2 model (an instance of the inception_resnet_v2 class) and two additional arguments: 'new_classes' and 'dropout_rate'. This function should modify the number of output classes in the model to the value of 'new_classes', and modify the dropout rate in the model to the value of 'dropout_rate', without changing the architecture of the model itself. Return the modified model instance. Ensure that your function handles possible errors and invalid input types and values correctly.\n",
      "assistant:\n",
      "\n",
      "\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "slim = tf.contrib.slim\n",
      "\n",
      "\n",
      "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
      "  \"\"\"Builds the 35x35 resnet block.\"\"\"\n",
      "  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n",
      "    with tf.variable_scope('Branch_0'):\n",
      "      tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n",
      "    with tf.variable_scope('Branch_1'):\n",
      "      tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
      "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n",
      "    with tf.variable_scope('Branch_2'):\n",
      "      tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
      "      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n",
      "      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n",
      "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n",
      "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
      "                     activation_fn=None, scope='Conv2d_1x1')\n",
      "    net += scale * up\n",
      "    if activation_fn:\n",
      "      net = activation_fn(net)\n",
      "  return net\n",
      "\n",
      "\n",
      "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
      "  \"\"\"Builds the 17x17 resnet block.\"\"\"\n",
      "  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n",
      "    with tf.variable_scope('Branch_0'):\n",
      "      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
      "    with tf.variable_scope('Branch_1'):\n",
      "      tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n",
      "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n",
      "                                  scope='Conv2d_0b_1x7')\n",
      "      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n",
      "                                  scope='Conv2d_0c_7x1')\n",
      "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n",
      "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
      "                     activation_fn=None, scope='Conv2d_1x1')\n",
      "    net += scale * up\n",
      "    if activation_fn:\n",
      "      net = activation_fn(net)\n",
      "  return net\n",
      "\n",
      "\n",
      "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
      "  \"\"\"Builds the 8x8 resnet block.\"\"\"\n",
      "  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n",
      "    with tf.variable_scope('Branch_0'):\n",
      "      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
      "    with tf.variable_scope('Branch_1'):\n",
      "      tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n",
      "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n",
      "                                  scope='Conv2d_0b_1x3')\n",
      "      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n",
      "                                  scope='Conv2d_0c_3x1')\n",
      "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n",
      "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
      "                     activation_fn=None, scope='Conv2d_1x1')\n",
      "    net += scale * up\n",
      "    if activation_fn:\n",
      "      net = activation_fn(net)\n",
      "  return net\n",
      "\n",
      "\n",
      "def inception_resnet_v2_base(inputs,\n",
      "                             final_endpoint='Conv2d_7b_1x1',\n",
      "                             output_stride=16,\n",
      "                             align_feature_maps=False,\n",
      "                             scope=None):\n",
      "  \"\"\"Inception model from  http://arxiv.org/abs/1602.07261.\n",
      "\n",
      "  Constructs an Inception Resnet v2 network from inputs to the given final\n",
      "  endpoint. This method can construct the network up to the final inception\n",
      "  block Conv2d_7b_1x1.\n",
      "\n",
      "  Args:\n",
      "    inputs: a tensor of size [batch_size, height, width, channels].\n",
      "    final_endpoint: specifies the endpoint to construct the network up to. It\n",
      "      can be one of ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n",
      "      'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3',\n",
      "      'Mixed_5b', 'Mixed_6a', 'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']\n",
      "    output_stride: A scalar that specifies the requested ratio of input to\n",
      "      output spatial resolution. Only supports 8 and 16.\n",
      "    align_feature_maps: When true, changes all the VALID paddings in the network\n",
      "      to SAME padding so that the feature maps are aligned.\n",
      "    scope: Optional variable_scope.\n",
      "\n",
      "  Returns:\n",
      "    tensor_out: output tensor corresponding to the final_endpoint.\n",
      "    end_points: a set of activations for external use, for example summaries or\n",
      "                losses.\n",
      "\n",
      "  Raises:\n",
      "    ValueError: if final_endpoint is not set to one of the predefined values,\n",
      "      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n",
      "      we request an end point after 'PreAuxLogits'.\n",
      "  \"\"\"\n",
      "  if output_stride != 8 and output_stride != 16:\n",
      "    raise ValueError('output_stride must be 8 or 16.')\n",
      "\n",
      "  padding = 'SAME' if align_feature_maps else 'VALID'\n",
      "\n",
      "  end_points = {}\n",
      "\n",
      "  def add_and_check_final(name, net):\n",
      "    end_points[name] = net\n",
      "    return name == final_endpoint\n",
      "\n",
      "  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):\n",
      "    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
      "                        stride=1, padding='SAME'):\n",
      "      # 149 x 149 x 32\n",
      "      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n",
      "                        scope='Conv2d_1a_3x3')\n",
      "      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n",
      "\n",
      "      # 147 x 147 x 32\n",
      "      net = slim.conv2d(net, 32, 3, padding=padding,\n",
      "                        scope='Conv2d_2a_3x3')\n",
      "      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n",
      "      # 147 x 147 x 64\n",
      "      net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n",
      "      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n",
      "      # 73 x 73 x 64\n",
      "      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n",
      "                            scope='MaxPool_3a_3x3')\n",
      "      if add_and_check_final('MaxPool_3a_3x3', net): return net, end_points\n",
      "      # 73 x 73 x 80\n",
      "      net = slim.conv2d(net, 80, 1, padding=padding,\n",
      "                        scope='Conv2d_3b_1x1')\n",
      "      if add_and_check_final('Conv2d_3b_1x1', net): return net, end_points\n",
      "      # 71 x 71 x 192\n",
      "      net = slim.conv2d(net, 192, 3, padding=padding,\n",
      "                        scope='Conv2d_4a_3x3')\n",
      "      if add_and_check_final('Conv2d_4a_3x3', net): return net, end_points\n",
      "      # 35 x 35 x 192\n",
      "      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n",
      "                            scope='MaxPool_5a_3x3')\n",
      "      if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points\n",
      "\n",
      "      # 35 x 35 x 320\n",
      "      with tf.variable_scope('Mixed_5b'):\n",
      "        with tf.variable_scope('Branch_0'):\n",
      "          tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n",
      "        with tf.variable_scope('Branch_1'):\n",
      "          tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n",
      "          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n",
      "                                      scope='Conv2d_0b_5x5')\n",
      "        with tf.variable_scope('Branch_2'):\n",
      "          tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n",
      "          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n",
      "                                      scope='Conv2d_0b_3x3')\n",
      "          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n",
      "                                      scope='Conv2d_0c_3x3')\n",
      "        with tf.variable_scope('Branch_3'):\n",
      "          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n",
      "                                       scope='AvgPool_0a_3x3')\n",
      "          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n",
      "                                     scope='Conv2d_0b_1x1')\n",
      "        net = tf.concat(\n",
      "            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n",
      "\n",
      "      if add_and_check_final('Mixed_5b', net): return net, end_points\n",
      "      # TODO(alemi): Register intermediate endpoints\n",
      "      net = slim.repeat(net, 10, block35, scale=0.17)\n",
      "\n",
      "      # 17 x 17 x 1088 if output_stride == 8,\n",
      "      # 33 x 33 x 1088 if output_stride == 16\n",
      "      use_atrous = output_stride == 8\n",
      "\n",
      "      with tf.variable_scope('Mixed_6a'):\n",
      "        with tf.variable_scope('Branch_0'):\n",
      "          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n",
      "                                   padding=padding,\n",
      "                                   scope='Conv2d_1a_3x3')\n",
      "        with tf.variable_scope('Branch_1'):\n",
      "          tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
      "          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n",
      "                                      scope='Conv2d_0b_3x3')\n",
      "          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n",
      "                                      stride=1 if use_atrous else 2,\n",
      "                                      padding=padding,\n",
      "                                      scope='Conv2d_1a_3x3')\n",
      "        with tf.variable_scope('Branch_2'):\n",
      "          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n",
      "                                       padding=padding,\n",
      "                                       scope='MaxPool_1a_3x3')\n",
      "        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n",
      "\n",
      "      if add_and_check_final('Mixed_6a', net): return net, end_points\n",
      "\n",
      "      # TODO(alemi): register intermediate endpoints\n",
      "      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n",
      "        net = slim.repeat(net, 20, block17, scale=0.10)\n",
      "      if add_and_check_final('PreAuxLogits', net): return net, end_points\n",
      "\n",
      "      if output_stride == 8:\n",
      "        # TODO(gpapan): Properly support output_stride for the rest of the net.\n",
      "        raise ValueError('output_stride==8 is only supported up to the '\n",
      "                         'PreAuxlogits end_point for now.')\n",
      "\n",
      "      # 8 x 8 x 2080\n",
      "      with tf.variable_scope('Mixed_7a'):\n",
      "        with tf.variable_scope('Branch_0'):\n",
      "          tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
      "          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n",
      "                                     padding=padding,\n",
      "                                     scope='Conv2d_1a_3x3')\n",
      "        with tf.variable_scope('Branch_1'):\n",
      "          tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
      "          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n",
      "                                      padding=padding,\n",
      "                                      scope='Conv2d_1a_3x3')\n",
      "        with tf.variable_scope('Branch_2'):\n",
      "          tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
      "          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n",
      "                                      scope='Conv2d_0b_3x3')\n",
      "          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n",
      "                                      padding=padding,\n",
      "                                      scope='Conv2d_1a_3x3')\n",
      "        with tf.variable_scope('Branch_3'):\n",
      "          tower_pool = slim.max_pool2d(net, 3, stride=2,\n",
      "                                       padding=padding,\n",
      "                                       scope='MaxPool_1a_3x3')\n",
      "        net = tf.concat(\n",
      "            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n",
      "\n",
      "      if add_and_check_final('Mixed_7a', net): return net, end_points\n",
      "\n",
      "      # TODO(alemi): register intermediate endpoints\n",
      "      net = slim.repeat(net, 9, block8, scale=0.20)\n",
      "      net = block8(net, activation_fn=None)\n",
      "\n",
      "      # 8 x 8 x 1536\n",
      "      net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n",
      "      if add_and_check_final('Conv2d_7b_1x1', net): return net, end_points\n",
      "\n",
      "    raise ValueError('final_endpoint (%s) not recognized', final_endpoint)\n",
      "\n",
      "\n",
      "def inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n",
      "                        dropout_keep_prob=0.8,\n",
      "                        reuse=None,\n",
      "                        scope='InceptionResnetV2',\n",
      "                        create_aux_logits=True):\n",
      "  \"\"\"Creates the Inception Resnet V2 model.\n",
      "\n",
      "  Args:\n",
      "    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
      "    num_classes: number of predicted classes.\n",
      "    is_training: whether is training or not.\n",
      "    dropout_keep_prob: float, the fraction to keep before final layer.\n",
      "    reuse: whether or not the network and its variables should be reused. To be\n",
      "      able to reuse 'scope' must be given.\n",
      "    scope: Optional variable_scope.\n",
      "    create_aux_logits: Whether to include the auxilliary logits.\n",
      "\n",
      "  Returns:\n",
      "    logits: the logits outputs of the model.\n",
      "    end_points: the set of end_points from the inception model.\n",
      "  \"\"\"\n",
      "  end_points = {}\n",
      "\n",
      "  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs, num_classes],\n",
      "                         reuse=reuse) as scope:\n",
      "    with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
      "                        is_training=is_training):\n",
      "\n",
      "      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n",
      "\n",
      "      if create_aux_logits:\n",
      "        with tf.variable_scope('AuxLogits'):\n",
      "          aux = end_points['PreAuxLogits']\n",
      "          aux = slim.avg_pool2d(aux, 5, stride=3, padding='VALID',\n",
      "                                scope='Conv2d_1a_3x3')\n",
      "          aux = slim.conv2d(aux, 128, 1, scope='Conv2d_1b_1x1')\n",
      "          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n",
      "                            padding='VALID', scope='Conv2d_2a_5x5')\n",
      "          aux = slim.flatten(aux)\n",
      "          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n",
      "                                     scope='Logits')\n",
      "          end_points['AuxLogits'] = aux\n",
      "\n",
      "      with tf.variable_scope('Logits'):\n",
      "        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n",
      "                              scope='AvgPool_1a_8x8')\n",
      "        net = slim.flatten(net)\n",
      "\n",
      "        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
      "                           scope='Dropout')\n",
      "\n",
      "        end_points['PreLogitsFlatten'] = net\n",
      "        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n",
      "                                      scope='Logits')\n",
      "        end_points['Logits'] = logits\n",
      "        end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n",
      "\n",
      "    return logits, end_points\n",
      "inception_resnet_v2.default_image_size = 299\n",
      "\n",
      "\n",
      "def inception_resnet_v2_arg_scope(weight_decay=0.00004,\n",
      "                                  batch_norm_decay=0.9997,\n",
      "                                  batch_norm_epsilon=0.001,\n",
      "                                  trainable=True):\n",
      "  \"\"\"Returns the scope with the default parameters for inception_resnet_v2.\n",
      "\n",
      "  Args:\n",
      "    weight_decay: the weight decay for weights variables.\n",
      "    batch_norm_decay: decay for the moving average of batch_norm momentums.\n",
      "    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n",
      "\n",
      "  Returns:\n",
      "    a arg_scope with the parameters needed for inception_resnet_v2.\n",
      "  \"\"\"\n",
      "  # Set weight_decay for weights in conv2d and fully_connected layers.\n",
      "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
      "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
      "                      biases_regularizer=slim.l2_regularizer(weight_decay),\n",
      "                      trainable=trainable):\n",
      "\n",
      "    batch_norm_params = {\n",
      "        'decay': batch_norm_decay,\n",
      "        'epsilon': batch_norm_epsilon,\n",
      "        'trainable': trainable\n",
      "    }\n",
      "    # Set activation_fn and parameters for batch_norm.\n",
      "    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n",
      "                        normalizer_fn=slim.batch_norm,\n",
      "                        normalizer_params=batch_norm_params) as scope:\n",
      "      return scope\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Given the 'inception_resnet_v2_base' function in the provided code, which is part of a TensorFlow Inception ResNet v2 model, create a new function called 'inception_resnet_v2_new'. The new function should incorporate the inception_resnet_v2_base and add a new layer on top, preferably a fully connected layer. The new layer should have 256 units and use the ReLU activation function. The function should be trainable and use the 'inception_resnet_v2_arg_scope' for setting the parameters. Ensure the function adheres to the established TensorFlow and Slim practices. Also, it should be capable of predicting 'num_classes' classes and return the logits and endpoints.\"\n",
      "\n",
      "Answer:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Instruction: Write a unit test in Python to verify that the AltmarketsUserStreamTracker is correctly retrieving user stream data from the Altmarkets API. The test should simulate the retrieval of user stream data for a period of time and confirm that data is being returned.\n",
      "\n",
      "assistant:\n",
      "\n",
      "import sys\n",
      "import asyncio\n",
      "import logging\n",
      "import unittest\n",
      "import conf\n",
      "\n",
      "from os.path import join, realpath\n",
      "from hummingbot.connector.exchange.altmarkets.altmarkets_user_stream_tracker import AltmarketsUserStreamTracker\n",
      "from hummingbot.connector.exchange.altmarkets.altmarkets_auth import AltmarketsAuth\n",
      "from hummingbot.core.utils.async_utils import safe_ensure_future\n",
      "from hummingbot.logger.struct_logger import METRICS_LOG_LEVEL\n",
      "\n",
      "\n",
      "sys.path.insert(0, realpath(join(__file__, \"../../../../../\")))\n",
      "logging.basicConfig(level=METRICS_LOG_LEVEL)\n",
      "\n",
      "\n",
      "class AltmarketsUserStreamTrackerUnitTest(unittest.TestCase):\n",
      "    api_key = conf.altmarkets_api_key\n",
      "    api_secret = conf.altmarkets_secret_key\n",
      "\n",
      "    @classmethod\n",
      "    def setUpClass(cls):\n",
      "        cls.ev_loop: asyncio.BaseEventLoop = asyncio.get_event_loop()\n",
      "        cls.trading_pairs = [\"BTC-USD\"]\n",
      "        cls.user_stream_tracker: AltmarketsUserStreamTracker = AltmarketsUserStreamTracker(\n",
      "            altmarkets_auth=AltmarketsAuth(cls.api_key, cls.api_secret),\n",
      "            trading_pairs=cls.trading_pairs)\n",
      "        cls.user_stream_tracker_task: asyncio.Task = safe_ensure_future(cls.user_stream_tracker.start())\n",
      "\n",
      "    def test_user_stream(self):\n",
      "        # Wait process some msgs.\n",
      "        print(\"\\nSleeping for 30s to gather some user stream messages.\")\n",
      "        self.ev_loop.run_until_complete(asyncio.sleep(30.0))\n",
      "        print(self.user_stream_tracker.user_stream)\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Can you help with this?\n",
      "\n",
      "Create a test case for the `AltmarketsUserStreamTracker` class. This test case should wait for a few seconds to gather user stream messages from the Altmarkets API. If the `user_stream` property contains the expected updates, mark the test as passed.\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Tell me how would you respond to the following request.\n",
      "Write a function that combines and normalizes two sets of MNIST data, given their filenames. The images should be normalized to have values between 0 and 1. The combined dataset should contain the normalized images as well as their labels. The output should be a dictionary where the keys are \"images\" and \"labels\".\n",
      "\n",
      "assistant:\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def load_MNIST_images(filename):\n",
      "    \"\"\"\n",
      "    returns a 28x28x[number of MNIST images] matrix containing\n",
      "    the raw MNIST images\n",
      "    :param filename: input data file\n",
      "    \"\"\"\n",
      "    with open(filename, \"r\") as f:\n",
      "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
      "\n",
      "        num_images = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
      "        num_rows = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
      "        num_cols = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
      "\n",
      "        images = np.fromfile(f, dtype=np.ubyte)\n",
      "        images = images.reshape((num_images, num_rows * num_cols)).transpose()\n",
      "        images = images.astype(np.float64) / 255\n",
      "\n",
      "        f.close()\n",
      "\n",
      "        return images\n",
      "\n",
      "\n",
      "def load_MNIST_labels(filename):\n",
      "    \"\"\"\n",
      "    returns a [number of MNIST images]x1 matrix containing\n",
      "    the labels for the MNIST images\n",
      "\n",
      "    :param filename: input file with labels\n",
      "    \"\"\"\n",
      "    with open(filename, 'r') as f:\n",
      "        magic = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
      "\n",
      "        num_labels = np.fromfile(f, dtype=np.dtype('>i4'), count=1)\n",
      "\n",
      "        labels = np.fromfile(f, dtype=np.ubyte)\n",
      "\n",
      "        f.close()\n",
      "\n",
      "        return labels\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Assuming the role of Software Engineer with expertise in Coding, your task is to review and optimize the existing code for loading images from a binary file. Specifically, the code loads the MNIST dataset, a large database of handwritten digits, but it is inefficient due to the use of low-level file operations. The challenge is to refactor the code to improve its efficiency, focusing on enhancing the readability, maintainability, and performance of the code.\n",
      "\n",
      "### Response:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Modify the code to display a Scatter plot with multiple dimensions, where each data point is a list containing three values (x, y, and size). The Scatter plot should be able to update its data with different data points by pressing a button, and have a title and tooltip that display the name and size of the data point.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "from pyecharts import options as opts\n",
      "from pyecharts.charts import Scatter\n",
      "from pyecharts.commons.utils import JsCode\n",
      "from pyecharts.faker import Faker\n",
      "\n",
      "c = (\n",
      "    Scatter()\n",
      "    .add_xaxis(Faker.choose())\n",
      "    .add_yaxis(\n",
      "        \"商家A\",\n",
      "        [list(z) for z in zip(Faker.values(), Faker.choose())],\n",
      "        label_opts=opts.LabelOpts(\n",
      "            formatter=JsCode(\n",
      "                \"function(params){return params.value[1] +' : '+ params.value[2];}\"\n",
      "            )\n",
      "        ),\n",
      "    )\n",
      "    .set_global_opts(\n",
      "        title_opts=opts.TitleOpts(title=\"Scatter-多维度数据\"),\n",
      "        tooltip_opts=opts.TooltipOpts(\n",
      "            formatter=JsCode(\n",
      "                \"function (params) {return params.name + ' : ' + params.value[2];}\"\n",
      "            )\n",
      "        ),\n",
      "        visualmap_opts=opts.VisualMapOpts(\n",
      "            type_=\"color\", max_=150, min_=20, dimension=1\n",
      "        ),\n",
      "    )\n",
      "    .render(\"scatter_multi_dimension.html\")\n",
      ")\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Create a Scatter plot using the 'pyecharts' library, using the provided Faker data. The X-axis should be generated from Faker.choose(), and the Y-axis should be generated from a combination of Faker.values() and Faker.choose(). The plot should be labelled in a way that includes the value and the randomly chosen item. It should also have a title 'Scatter-多维度数据', tooltips should also be labelled in a similar way. Finally, the color dimension should be assigned to the Y-axis data. Render the chart to 'scatter_multi_dimension.html'.\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Can you help with this?\n",
      "\n",
      "Implement a new feature that allows users to register for an account through a secure Django-based RESTful API. This feature should include steps for user validation and input sanitization to prevent SQL injection and other potential security threats. The new class should have methods for both registration and login, each having their own respective routes. All data should be validated by using JSON web tokens (JWT) or any other token method you are comfortable with. All responses from the API should be returned in the JSON format.\n",
      "\n",
      "assistant:\n",
      "\n",
      "import json\n",
      "\n",
      "from django.contrib.auth import authenticate, login, logout\n",
      "from django.http import JsonResponse\n",
      "from django.middleware.csrf import get_token\n",
      "from django.views.decorators.http import require_POST\n",
      "from rest_framework.authentication import SessionAuthentication, BasicAuthentication\n",
      "from rest_framework.permissions import IsAuthenticated\n",
      "from rest_framework.views import APIView\n",
      "\n",
      "\n",
      "def get_csrf(request):\n",
      "    response = JsonResponse({'detail': 'CSRF cookie set'})\n",
      "    response['X-CSRFToken'] = get_token(request)\n",
      "    return response\n",
      "\n",
      "\n",
      "@require_POST\n",
      "def login_view(request):\n",
      "    data = json.loads(request.body)\n",
      "    username = data.get('username')\n",
      "    password = data.get('password')\n",
      "\n",
      "    if username is None or password is None:\n",
      "        return JsonResponse({'detail': 'Please provide username and password.'}, status=400)\n",
      "\n",
      "    user = authenticate(username=username, password=password)\n",
      "\n",
      "    if user is None:\n",
      "        return JsonResponse({'detail': 'Invalid credentials.'}, status=400)\n",
      "\n",
      "    login(request, user)\n",
      "    return JsonResponse({'detail': 'Successfully logged in.'})\n",
      "\n",
      "\n",
      "def logout_view(request):\n",
      "    if not request.user.is_authenticated:\n",
      "        return JsonResponse({'detail': 'You\\'re not logged in.'}, status=400)\n",
      "\n",
      "    logout(request)\n",
      "    return JsonResponse({'detail': 'Successfully logged out.'})\n",
      "\n",
      "\n",
      "class SessionView(APIView):\n",
      "    authentication_classes = [SessionAuthentication, BasicAuthentication]\n",
      "    permission_classes = [IsAuthenticated]\n",
      "\n",
      "    @staticmethod\n",
      "    def get(request, format=None):\n",
      "        return JsonResponse({'isAuthenticated': True})\n",
      "\n",
      "\n",
      "class WhoAmIView(APIView):\n",
      "    authentication_classes = [SessionAuthentication, BasicAuthentication]\n",
      "    permission_classes = [IsAuthenticated]\n",
      "\n",
      "    @staticmethod\n",
      "    def get(request, format=None):\n",
      "        return JsonResponse({'username': request.user.username})\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please analyze the provided Django code snippet. The code seems to manage user authentication and session management using Django's built-in authentication system.\n",
      "\n",
      "### Response:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Please create a new instance of the SyncFullSchemaTableColumn class without initializing the instance variables (i.e., set their values to None).\n",
      "\n",
      "assistant:\n",
      "\n",
      "from msrest.serialization import Model\n",
      "\n",
      "\n",
      "class SyncFullSchemaTableColumn(Model):\n",
      "    \"\"\"Properties of the column in the table of database full schema.\n",
      "\n",
      "    Variables are only populated by the server, and will be ignored when\n",
      "    sending a request.\n",
      "\n",
      "    :ivar data_size: Data size of the column.\n",
      "    :vartype data_size: str\n",
      "    :ivar data_type: Data type of the column.\n",
      "    :vartype data_type: str\n",
      "    :ivar error_id: Error id of the column.\n",
      "    :vartype error_id: str\n",
      "    :ivar has_error: If there is error in the table.\n",
      "    :vartype has_error: bool\n",
      "    :ivar is_primary_key: If it is the primary key of the table.\n",
      "    :vartype is_primary_key: bool\n",
      "    :ivar name: Name of the column.\n",
      "    :vartype name: str\n",
      "    :ivar quoted_name: Quoted name of the column.\n",
      "    :vartype quoted_name: str\n",
      "    \"\"\"\n",
      "\n",
      "    _validation = {\n",
      "        'data_size': {'readonly': True},\n",
      "        'data_type': {'readonly': True},\n",
      "        'error_id': {'readonly': True},\n",
      "        'has_error': {'readonly': True},\n",
      "        'is_primary_key': {'readonly': True},\n",
      "        'name': {'readonly': True},\n",
      "        'quoted_name': {'readonly': True},\n",
      "    }\n",
      "\n",
      "    _attribute_map = {\n",
      "        'data_size': {'key': 'dataSize', 'type': 'str'},\n",
      "        'data_type': {'key': 'dataType', 'type': 'str'},\n",
      "        'error_id': {'key': 'errorId', 'type': 'str'},\n",
      "        'has_error': {'key': 'hasError', 'type': 'bool'},\n",
      "        'is_primary_key': {'key': 'isPrimaryKey', 'type': 'bool'},\n",
      "        'name': {'key': 'name', 'type': 'str'},\n",
      "        'quoted_name': {'key': 'quotedName', 'type': 'str'},\n",
      "    }\n",
      "\n",
      "    def __init__(self, **kwargs) -> None:\n",
      "        super(SyncFullSchemaTableColumn, self).__init__(**kwargs)\n",
      "        self.data_size = None\n",
      "        self.data_type = None\n",
      "        self.error_id = None\n",
      "        self.has_error = None\n",
      "        self.is_primary_key = None\n",
      "        self.name = None\n",
      "        self.quoted_name = None\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Review this Python class, which manages the syncing of database schema. Identify if there are any methods or attributes that could be enhanced to enhance system performance or efficiency.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a new test function for the given Python code that ensures all elements of a vector (of size 4) are incremented by 1, but only when the loop index `i` is less than the value of variable `n`, using vectorization, and handle the situation where the 'If' condition has 'le' or 'ge' conditions.\n",
      "Response:\n",
      "assistant:\n",
      "\n",
      "import tvm\n",
      "\n",
      "def test_vectorize_loop():\n",
      "    dtype = 'int64'\n",
      "    n = tvm.var('n')\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32\", name=\"A\")\n",
      "    with ib.for_range(0, n) as i:\n",
      "        with ib.for_range(0, 4, for_type=\"vectorize\") as j:\n",
      "            A[j] = tvm.const(1, A.dtype)\n",
      "    stmt = ib.get()\n",
      "\n",
      "    assert isinstance(stmt.body, tvm.stmt.For)\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert isinstance(stmt, tvm.stmt.For)\n",
      "    assert not isinstance(stmt.body, tvm.stmt.For)\n",
      "    assert isinstance(stmt.body.index, tvm.expr.Ramp)\n",
      "    assert isinstance(stmt.body.value, tvm.expr.Broadcast)\n",
      "\n",
      "def test_vectorize_vector():\n",
      "    dtype = 'int64'\n",
      "    n = tvm.var('n')\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32x4\", name=\"A\")\n",
      "    with ib.for_range(0, n) as i:\n",
      "        with ib.for_range(0, 4, for_type=\"vectorize\") as j:\n",
      "            A[j] = tvm.const(1, A.dtype)\n",
      "    stmt = ib.get()\n",
      "    assert isinstance(stmt.body, tvm.stmt.For)\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert isinstance(stmt, tvm.stmt.For)\n",
      "    assert not isinstance(stmt.body, tvm.stmt.For)\n",
      "    assert isinstance(stmt.body.index, tvm.expr.Ramp)\n",
      "    assert isinstance(stmt.body.value, tvm.expr.Broadcast)\n",
      "\n",
      "\n",
      "def test_vectorize_with_if():\n",
      "    n = tvm.var('n')\n",
      "    x = tvm.var('x')\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32\", name=\"A\")\n",
      "    with ib.for_range(0, 4, for_type=\"vectorize\") as i:\n",
      "        with ib.if_scope(x < n):\n",
      "            A[i] = A[i] + 1\n",
      "        with ib.else_scope():\n",
      "            with ib.if_scope(i < n):\n",
      "                A[i] = 2.0\n",
      "    stmt = ib.get()\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert isinstance(stmt, tvm.stmt.IfThenElse)\n",
      "    assert isinstance(stmt.then_case.index, tvm.expr.Ramp)\n",
      "    assert isinstance(stmt.then_case.value, tvm.expr.Add)\n",
      "    assert stmt.then_case.value.dtype == \"float32x4\"\n",
      "    assert isinstance(stmt.else_case, tvm.stmt.For)\n",
      "\n",
      "def test_vectorize_with_le_cond():\n",
      "    n = tvm.var('n')\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32\", name=\"A\")\n",
      "    with ib.for_range(0, 4, for_type=\"vectorize\") as i:\n",
      "        with ib.if_scope(i <= n):\n",
      "            A[i] = A[i] + 1\n",
      "    stmt = ib.get()\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert isinstance(stmt, tvm.stmt.For)\n",
      "\n",
      "def test_vectorize_with_ge_cond():\n",
      "    n = tvm.var('n')\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32\", name=\"A\")\n",
      "    with ib.for_range(0, 4, for_type=\"vectorize\") as i:\n",
      "        with ib.if_scope(i >= n):\n",
      "            A[i] = A[i] + 1\n",
      "    stmt = ib.get()\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert isinstance(stmt, tvm.stmt.For)\n",
      "\n",
      "def test_vectorize_if_then_else():\n",
      "    n = tvm.var('n')\n",
      "    x = tvm.var('x')\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32\", name=\"A\")\n",
      "    with ib.for_range(0, 4, for_type=\"vectorize\") as i:\n",
      "        A[i] = tvm.call_intrin(\"float32\", \"tvm_if_then_else\",\n",
      "                               i > 0,\n",
      "                               A[i] + 1, A[i])\n",
      "    stmt = ib.get()\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert isinstance(stmt, tvm.stmt.For)\n",
      "\n",
      "\n",
      "    ib = tvm.ir_builder.create()\n",
      "    A = ib.pointer(\"float32\", name=\"A\")\n",
      "    with ib.for_range(0, n) as k:\n",
      "        with ib.for_range(0, 4, for_type=\"vectorize\") as i:\n",
      "            A[k * 4 + i] = tvm.call_intrin(\"float32\", \"tvm_if_then_else\",\n",
      "                                           k > 0,\n",
      "                                           A[k * 4 + i], 0)\n",
      "    stmt = ib.get()\n",
      "    assert isinstance(stmt.body, tvm.stmt.For)\n",
      "    stmt = tvm.ir_pass.VectorizeLoop(stmt)\n",
      "    assert not isinstance(stmt.body, tvm.stmt.For)\n",
      "    assert isinstance(stmt.body.value.args[2], tvm.expr.Broadcast)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test_vectorize_vector()\n",
      "    test_vectorize_with_if()\n",
      "    test_vectorize_loop()\n",
      "    test_vectorize_if_then_else()\n",
      "    test_vectorize_with_le_cond()\n",
      "    test_vectorize_with_ge_cond()\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Write a set of unit tests using Python for the VectorizedLoop function in the tvm library. The function needs to pass the following tests:\n",
      "\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "simple:\n",
      "\n",
      "user:\n",
      "\n",
      "The code provides a collection of utilities for a deep learning model. The main classes and functions are: AverageMeter for calculating average values, accuracy for evaluating model performance, dist_collect and reduce_tensor for distributed data processing, MyHelpFormatter for customizing argparse command line interface, and DistributedShuffle for shuffling data in a distributed setting. The shuffling in DistributedShuffle can be used to make use of BatchNorm. This is a challenging task as it requires deep understanding of PyTorch's distributed computing capabilities and the use of command line arguments for model configuration.\n",
      "\n",
      "\n",
      "assistant:\n",
      "\n",
      "import argparse\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "\n",
      "class AverageMeter(object):\n",
      "    \"\"\"Computes and stores the average and current value\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.val = 0\n",
      "        self.avg = 0\n",
      "        self.sum = 0\n",
      "        self.count = 0\n",
      "        self.reset()\n",
      "\n",
      "    def reset(self):\n",
      "        self.val = 0\n",
      "        self.avg = 0\n",
      "        self.sum = 0\n",
      "        self.count = 0\n",
      "\n",
      "    def update(self, val, n=1):\n",
      "        self.val = val\n",
      "        self.sum += val * n\n",
      "        self.count += n\n",
      "        self.avg = self.sum / self.count\n",
      "\n",
      "\n",
      "def accuracy(output, target, topk=(1,)):\n",
      "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
      "    with torch.no_grad():\n",
      "        maxk = max(topk)\n",
      "        batch_size = target.size(0)\n",
      "\n",
      "        _, pred = output.topk(maxk, 1, True, True)\n",
      "        pred = pred.t()\n",
      "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
      "\n",
      "        res = []\n",
      "        for k in topk:\n",
      "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
      "            res.append(correct_k.mul_(100.0 / batch_size))\n",
      "        return res\n",
      "\n",
      "\n",
      "def dist_collect(x):\n",
      "    \"\"\" collect all tensor from all GPUs\n",
      "    args:\n",
      "        x: shape (mini_batch, ...)\n",
      "    returns:\n",
      "        shape (mini_batch * num_gpu, ...)\n",
      "    \"\"\"\n",
      "    x = x.contiguous()\n",
      "    out_list = [torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
      "                for _ in range(dist.get_world_size())]\n",
      "    dist.all_gather(out_list, x)\n",
      "    return torch.cat(out_list, dim=0)\n",
      "\n",
      "\n",
      "def reduce_tensor(tensor):\n",
      "    rt = tensor.clone()\n",
      "    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
      "    rt /= dist.get_world_size()\n",
      "    return rt\n",
      "\n",
      "\n",
      "class MyHelpFormatter(argparse.MetavarTypeHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):\n",
      "    pass\n",
      "\n",
      "class DistributedShuffle:\n",
      "\n",
      "    @staticmethod\n",
      "    def forward_shuffle(x):\n",
      "        \"\"\"\n",
      "        Batch shuffle, for making use of BatchNorm.\n",
      "        *** Only support DistributedDataParallel (DDP) model. ***\n",
      "        \"\"\"\n",
      "        # gather from all gpus\n",
      "        batch_size_this = x.shape[0]\n",
      "        x_gather = dist_collect(x)\n",
      "        batch_size_all = x_gather.shape[0]\n",
      "\n",
      "        num_gpus = batch_size_all // batch_size_this\n",
      "\n",
      "        # random shuffle index\n",
      "        idx_shuffle = torch.randperm(batch_size_all).cuda()\n",
      "\n",
      "        # broadcast to all gpus\n",
      "        dist.broadcast(idx_shuffle, src=0)\n",
      "\n",
      "        # index for restoring\n",
      "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
      "\n",
      "        # shuffled index for this gpu\n",
      "        gpu_idx = dist.get_rank()\n",
      "        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n",
      "\n",
      "        return x_gather[idx_this], idx_unshuffle\n",
      "\n",
      "    @staticmethod\n",
      "    def backward_shuffle(x, idx_unshuffle, return_local=True):\n",
      "        \"\"\"\n",
      "        Undo batch shuffle.\n",
      "        *** Only support DistributedDataParallel (DDP) model. ***\n",
      "        \"\"\"\n",
      "        # gather from all gpus\n",
      "        batch_size_this = x.shape[0]\n",
      "        x_gather = dist_collect(x)\n",
      "        batch_size_all = x_gather.shape[0]\n",
      "\n",
      "        num_gpus = batch_size_all // batch_size_this\n",
      "\n",
      "        if return_local:\n",
      "            # restored index for this gpu\n",
      "            gpu_idx = dist.get_rank()\n",
      "            idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n",
      "            return x_gather[idx_unshuffle], x_gather[idx_this]\n",
      "        else:\n",
      "            return x_gather[idx_unshuffle]\n",
      "\n",
      "    @staticmethod\n",
      "    def get_local_id(ids):\n",
      "        return ids.chunk(dist.get_world_size())[dist.get_rank()]\n",
      "\n",
      "    @staticmethod\n",
      "    def get_shuffle_ids(bsz, epoch):\n",
      "        \"\"\"generate shuffle ids for ShuffleBN\"\"\"\n",
      "        torch.manual_seed(epoch)\n",
      "        # global forward shuffle id  for all process\n",
      "        forward_inds = torch.randperm(bsz).long().cuda()\n",
      "\n",
      "        # global backward shuffle id\n",
      "        backward_inds = torch.zeros(forward_inds.shape[0]).long().cuda()\n",
      "        value = torch.arange(bsz).long().cuda()\n",
      "        backward_inds.index_copy_(0, forward_inds, value)\n",
      "\n",
      "        return forward_inds, backward_inds\n",
      "\n",
      "role:\n",
      "\n",
      "user:\n",
      "\n",
      "Plase answer the following request: Using Python's argparse module, develop a parser for command line arguments. This parser should also use a custom formatter to display both metavar help and argument default values.\n",
      "Answer:\n",
      "assistant:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('\\nsimple:\\n')\n",
    "    print('user:\\n')\n",
    "    messages = dss[i]['messages']\n",
    "    print(messages[0]['content'])\n",
    "    print('assistant:\\n')\n",
    "    print(messages[1]['content'])\n",
    "    print('\\nrole:\\n')\n",
    "    print('user:\\n')\n",
    "    messages = dsr[i]['messages']\n",
    "    print(messages[0]['content'])\n",
    "    print('assistant:\\n')\n",
    "#     print(messages[1]['content'])\n",
    "    \n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = (\n",
    "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4115173",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
