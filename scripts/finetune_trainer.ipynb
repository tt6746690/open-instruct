{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c00e3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Sat Nov 11 16:04:50 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   41C    P0   221W / 300W |    491MiB / 32510MiB |     84%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   45C    P0   199W / 300W |    491MiB / 32510MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   42C    P0   199W / 300W |    491MiB / 32510MiB |     94%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   44C    P0   213W / 300W |    491MiB / 32510MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    475795      C   python                            489MiB |\n",
      "|    1   N/A  N/A    470101      C   python                            489MiB |\n",
      "|    3   N/A  N/A    470099      C   python                            489MiB |\n",
      "|    4   N/A  N/A    471236      C   python                            489MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook, jpt_parse_args, jpt_setup; jpt_setup()\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7ae114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-11 16:04:52,372] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "import pyarrow\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    set_seed,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2TokenizerFast, \n",
    "    OPTForCausalLM,\n",
    ")\n",
    "from transformers import Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "## jobs submitted in notebook inherits env variables.\n",
    "cache_dir = '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/cache'\n",
    "os.environ['WANDB_DIR'] = cache_dir\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_PROJECT'] = 'mitibm'\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6f05bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from open_instruct.finetune_trainer import ModelArguments, DataTrainingArguments, TrainingArguments\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1c85a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='../results/baselines/huggyllama/llama-7b', config_name=None, tokenizer_name='../results/baselines/huggyllama/llama-7b', cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype='float32', use_lora=False, lora_rank=64, lora_alpha=16, lora_dropout=0.1, load_in_8bit=False),\n",
       " DataTrainingArguments(dataset_name=None, dataset_config_name=None, train_file='data/processed/ultrachat/ultrachat200k_train_data.jsonl', max_train_samples=None, streaming=False, overwrite_cache=False, preprocessing_num_workers=16, max_seq_length=2048, subsample_mixture=None, subsample_inds_file=None),\n",
       " TrainingArguments(output_dir='results/jpt_huggyllama:llama-7b_humanmix', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=128, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='results/jpt_huggyllama:llama-7b_humanmix/runs/Nov11_16-05-04_dcs163', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=1.0, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=5, save_total_limit=1, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=5, dataloader_num_workers=8, past_index=-1, run_name='results/jpt_huggyllama:llama-7b_humanmix', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, dataloader_sampler='RandomSampler'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = 'results/huggyllama:llama-7b_human_mix-trainer_savebystep'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name_or_path = 'mosaicml/mpt-7b'\n",
    "# model_name_or_path = '../results/baselines/gpt2-medium'\n",
    "# model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "# model_name_or_path = '../results/baselines/EleutherAI/pythia-70m'\n",
    "\n",
    "# train_file = 'data/processed/all.jsonl'\n",
    "# subsample_mixture =  {'cot': 976, 'dolly': 15, 'flan_v2': 976, 'oasst1': 34}; subsample_inds_file=None\n",
    "\n",
    "# train_file = 'data/processed/flan_v2/flan_v2_data.jsonl'\n",
    "train_file = 'data/processed/ultrachat/ultrachat200k_train_data.jsonl'\n",
    "\n",
    "\n",
    "# subsample_inds_file = os.path.join(\n",
    "#     '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "#     'note_explore_dpp_llama-7b_flan_v2_subsets_K_cos.pkl'); subsample_mixture = None\n",
    "# train_file = 'data/processed/ultrachat/ultrachat_data.jsonl'\n",
    "subsample_inds_file = None; subsample_mixture = None\n",
    "dataloader_sampler = 'SequentialSampler' # 'RandomSampler'|'SequentialSampler'\n",
    "dataloader_sampler = None\n",
    "\n",
    "#     --use_lora \\\n",
    "#     --lora_rank 8 \\\n",
    "#     --lora_alpha 8 \\\n",
    "#     --lora_dropout 0.05 \n",
    "\n",
    "save_strategy = 'steps'\n",
    "save_steps = 5\n",
    "save_total_limit = 1\n",
    "eval_steps = save_steps\n",
    "overwrite_output_dir = True # false if want to test trainer resume from dir.\n",
    "\n",
    "#     --report_to tensorboard wandb \\\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name {model_name_or_path} \\\n",
    "    --use_fast_tokenizer True \\\n",
    "    --train_file data/processed/dolly/dolly_data.jsonl \\\n",
    "    --max_seq_length 2048 \\\n",
    "    --train_file {train_file} \\\n",
    "    --do_train \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 128 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0. \\\n",
    "    \n",
    "    --evaluation_strategy=steps \\\n",
    "    --eval_steps={eval_steps} \\\n",
    "    --report_to none \\\n",
    "    --logging_strategy=steps \\\n",
    "    --logging_first_step \\\n",
    "    --logging_steps=1 \\\n",
    "    --save_strategy={save_strategy} \\\n",
    "    --save_steps={save_steps} \\\n",
    "    --save_total_limit={save_total_limit} \\\n",
    "    \n",
    "    --num_train_epochs 2 \\\n",
    "    --fp16 \\\n",
    "    --torch_dtype float32 \\\n",
    "    --dataloader_num_workers 8 \\\n",
    "    --output_dir \"results/jpt_{':'.join(model_name_or_path.split('/')[-2:])}_humanmix\" \\\n",
    "    --overwrite_output_dir {overwrite_output_dir} \\\n",
    "    {'--subsample_mixture=\"'+str(subsample_mixture).replace(': ', ':').replace(', ', ',')+'\"'\n",
    "        if subsample_mixture else ''} \\\n",
    "    {'--subsample_inds_file='+subsample_inds_file if subsample_inds_file else ''} \\\n",
    "    {'--dataloader_sampler '+str(dataloader_sampler) if dataloader_sampler else ''} \\\n",
    "\"\"\"\n",
    "#    --subsample_mixture='{{\"flan_v2\":100000}}'\n",
    "\n",
    "#      --subsample_mixture '{{\"baize\": 8333, \"code_alpaca\": 8333, \"cot\": 8333, \"dolly\": 8333, \"flan_v2\": 8333, \"gpt4_alpaca\": 8333, \"oasst1\": 8333, \"self_instruct\": 8333, \"sharegpt\": 8333, \"stanford_alpaca\": 8333, \"super_ni\": 8333, \"unnatural_instructions\": 8333}}'\n",
    "\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "# wpq: convert str to dict\n",
    "if data_args.subsample_mixture is not None:\n",
    "    import json\n",
    "    import re\n",
    "    data_args.subsample_mixture = re.compile('(?<!\\\\\\\\)\\'').sub('\\\"', data_args.subsample_mixture)\n",
    "    data_args.subsample_mixture = json.loads(data_args.subsample_mixture)\n",
    "    print('subsample mixture:')\n",
    "    print(data_args.subsample_mixture)\n",
    "if data_args.subsample_mixture is not None and data_args.subsample_inds_file is not None:\n",
    "    raise ValueError('Either use mixture proportion or exact subset indices, but not both.')\n",
    "if data_args.subsample_inds_file is not None:\n",
    "    if 'flan_v2' not in data_args.train_file:\n",
    "        raise ValueError('subset indices only support flan_v2 for now.')\n",
    "\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff17b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving args dict to results/jpt_huggyllama:llama-7b_humanmix.args.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# wpq: save args to a json file\n",
    "with training_args.main_process_first(local=False, desc=f\"Saving args to `{training_args.output_dir+'.args.json'}`\"):\n",
    "    args_dict = {\n",
    "        'model_args': asdict(model_args),\n",
    "        'data_args': asdict(data_args),\n",
    "        'training_args': asdict(training_args),\n",
    "    }\n",
    "    args_dict_path = training_args.output_dir.strip('/')+'.args.json'\n",
    "    with open(args_dict_path, 'w') as f:\n",
    "        json.dump(args_dict, f)\n",
    "    print(f'Saving args dict to {args_dict_path}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with training_args.main_process_first(local=False, desc=f\"setup wandb init\"):\n",
    "#     if 'wandb' in training_args.report_to:\n",
    "#         wandb_init_kwargs_path = os.path.join(training_args.output_dir, 'wandb_init_kwargs.json')\n",
    "#         os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "#         if not os.path.isfile(wandb_init_kwargs_path):\n",
    "#             import wandb\n",
    "#             wandb_init_kwargs = {}\n",
    "#             wandb_init_kwargs['project'] = os.environ['WANDB_PROJECT']\n",
    "#             wandb_init_kwargs['name'] = training_args.run_name\n",
    "#             wandb_init_kwargs['resume'] = 'allow' # resume if `run_id` identical to previous, otherwise start new run\n",
    "#             wandb_init_kwargs['id'] = wandb.sdk.lib.runid.generate_id()\n",
    "#             wandb_init_kwargs['mode'] = 'offline'\n",
    "#             with open(wandb_init_kwargs_path, 'w') as f:\n",
    "#                 json.dump(wandb_init_kwargs, f, indent=4)\n",
    "#         else:\n",
    "#             with open(wandb_init_kwargs_path, 'r') as f:\n",
    "#                 wandb_init_kwargs = json.load(f)\n",
    "# wandb_init_kwargs\n",
    "# import wandb\n",
    "# wandb.init(**wandb_init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a02c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        # wpq: since I do write `.json` file to output_dir, raise if there are more files.\n",
    "        if len(os.listdir(training_args.output_dir))>1:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "        \n",
    "last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4870f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-797659fa87698b47/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac71af0a72a8429082b2c44e9366ce0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        streaming=data_args.streaming,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "    if 'ultrachat' in data_args.train_file:\n",
    "        data_files['test'] = (\n",
    "            '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/'\n",
    "            'data/processed/ultrachat/ultrachat200k_test_data.jsonl')\n",
    "    raw_datasets = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        **dataset_args,\n",
    "    )\n",
    "    if 'ultrachat' in data_args.train_file:\n",
    "        raw_datasets['test'] = raw_datasets['test'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2b3a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"../results/baselines/huggyllama/llama-7b\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"max_sequence_length\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.35.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "    \"trust_remote_code\": True if 'mpt' in model_args.model_name_or_path else False,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    # wpq: add support for mpt models.\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "        config.attn_config['attn_impl'] = 'triton'\n",
    "        config.init_device = 'cuda' # For fast initialization directly on GPU!\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported by this finetuning script.\"\n",
    "    )\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer if 'pythia' not in model_args.model_name_or_path else True,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this finetuning script.\"\n",
    "    )\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33366d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path:\n\u001b[1;32m      2\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m         model_args\u001b[38;5;241m.\u001b[39mtorch_dtype\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mtorch_dtype \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(torch, model_args\u001b[38;5;241m.\u001b[39mtorch_dtype)\n\u001b[1;32m      6\u001b[0m     )\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# wpq: 8bit training\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model_name_or_path is given. Training new model from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/modeling_utils.py:3105\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[1;32m   3104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3105\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:961\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:787\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/modules/sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        torch_dtype=torch_dtype,\n",
    "        # wpq: 8bit training\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "        trust_remote_code=bool('mpt' in model_args.model_name_or_path),\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"No pretrained model_name_or_path is given. Training new model from scratch.\")\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "    logger.info(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")\n",
    "    \n",
    "\n",
    "# wpq: `use_cache=True` is incompatible with gradient checkpointing\n",
    "model.config.use_cache = True if not training_args.gradient_checkpointing else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600e8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# no default pad token for llama!\n",
    "# here we add all special tokens again, because the default ones are not in the special_tokens_map \n",
    "if isinstance(tokenizer, (LlamaTokenizer, LlamaTokenizerFast)):\n",
    "    from transformers import AddedToken\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": AddedToken(\"<s>\", normalized=False, special=True),\n",
    "        \"eos_token\": AddedToken(\"</s>\", normalized=False, special=True),\n",
    "        \"unk_token\": AddedToken(\"<unk>\", normalized=False, special=True),\n",
    "        \"pad_token\": AddedToken(\"<pad>\", normalized=False, special=True),\n",
    "    })\n",
    "    ## wpq: for `huggyllama`/`NousResearch/Llama-2-7b-hf`, `LlamaTokenizerFast` tokenizer config not properly implemented and cannot tokenize special tokens like eos_token corretly. Need the followign workaround. More details: https://github.com/huggingface/transformers/issues/23833\n",
    "    if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "        from secrets import token_hex\n",
    "        tmp_tok_path = f'/tmp/wpq_tok_{token_hex(16)}'\n",
    "        tokenizer.save_pretrained(tmp_tok_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tmp_tok_path, **tokenizer_kwargs)\n",
    "\n",
    "    for s, s_tokenized in [\n",
    "        (\"Hi<s>Hey</s>sir<unk>what<pad><pad>\", \n",
    "        ['▁Hi', '<s>', '▁Hey', '</s>', '▁sir', '<unk>', '▁what', '<pad>', '<pad>']),\n",
    "    ]:\n",
    "        assert(tokenizer.tokenize(s, add_special_tokens=False)==s_tokenized)\n",
    "elif isinstance(tokenizer, GPTNeoXTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPTNeoXTokenizer should only add one special token - the pad_token.\"\n",
    "elif isinstance(tokenizer, (GPT2Tokenizer, GPT2TokenizerFast)) and isinstance(model, OPTForCausalLM):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({'unk_token': '<unk>'})\n",
    "## wpq: add support for gpt2 tokenizer.\n",
    "elif isinstance(tokenizer, (GPT2Tokenizer, GPT2TokenizerFast)):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens == 1, \"GPT2Tokenizer should only add one special token - the pad_token.\"\n",
    "\n",
    "# resize embeddings if needed (e.g. for LlamaTokenizer)\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    \n",
    "# wpq: use int8 training\n",
    "# wpq: put this after resize embedding!\n",
    "if model_args.load_in_8bit:\n",
    "    from peft import prepare_model_for_int8_training\n",
    "    model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd16620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wpq: add peft to finetune_trainer.py\n",
    "if model_args.use_lora:\n",
    "    logger.info(\"Initializing LORA model...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=model_args.lora_rank, \n",
    "        lora_alpha=model_args.lora_alpha, \n",
    "        lora_dropout=model_args.lora_dropout,\n",
    "        target_modules=['q_proj','k_proj','v_proj','o_proj'],\n",
    "    )\n",
    "    # wpq: the following fixes `element 0 of tensors does not require grad and does not have a grad_fn` \n",
    "    # https://github.com/huggingface/peft/issues/137\n",
    "    # https://github.com/huggingface/peft/issues/522\n",
    "    if hasattr(training_args, 'gradient_checkpointing'):\n",
    "        if training_args.gradient_checkpointing:\n",
    "            model.enable_input_require_grads()\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff116ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Preprocessing the datasets.\n",
    "if \"prompt\" in raw_datasets[\"train\"].column_names and \"completion\" in raw_datasets[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_prompt_completion_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "elif \"messages\" in raw_datasets[\"train\"].column_names:\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"You need to have either 'prompt'&'completion' or 'messages' in your column names.\")\n",
    "\n",
    "raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    \n",
    "\n",
    "# To speed up this part, we use multiprocessing.\n",
    "with training_args.main_process_first(local=False, desc=\"Processing instruction data\"):\n",
    "    if not data_args.streaming:\n",
    "        lm_datasets = raw_datasets.map(\n",
    "            encode_function,\n",
    "            batched=False,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Tokenizing and reformatting instruction data\",\n",
    "        )\n",
    "    else:\n",
    "        lm_datasets = raw_datasets.map(\n",
    "            encode_function,\n",
    "            batched=False,\n",
    "        )\n",
    "    lm_datasets.set_format(type=\"pt\")\n",
    "\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "    ## wpq: subsample `dataset` according to `data_args.subsample_mixture`.\n",
    "    # assumes dataset in `train_file` ordered, \n",
    "    # e.g., ['cot', 'cot', 'flan_v2', 'flan_v2', ...]\n",
    "    # note `counts.items()` is ordered as well!\n",
    "    if data_args.subsample_mixture is not None:\n",
    "        counts = Counter(train_dataset['dataset'])\n",
    "        inds = []\n",
    "        cum = 0\n",
    "        for k, N in counts.items():\n",
    "            n = int(data_args.subsample_mixture.get(k, 0))\n",
    "            replace = True if n>N else False\n",
    "            replace = True # always sample with replacement, for fairer comparison.\n",
    "            inds += list(np.random.choice(N, size=n, replace=replace) + cum)\n",
    "            cum += N\n",
    "            print(k, N, n, len(inds))\n",
    "        train_dataset = train_dataset.select(inds)\n",
    "\n",
    "\n",
    "    if data_args.subsample_inds_file is not None:\n",
    "        with open(data_args.subsample_inds_file, 'rb') as f:\n",
    "            inds = pickle.load(f)['K']\n",
    "        logger.info(f'Using subsample_inds_file: {data_args.subsample_inds_file}')\n",
    "        logger.info(f'subsample_inds_file has {len(inds)} indices.')\n",
    "        train_dataset = train_dataset.select(inds)\n",
    "\n",
    "    ## \n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5ed386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_instruct.finetune_trainer import MyTrainer\n",
    "\n",
    "# initalize a trainer\n",
    "# here we use a custom trainer that moves the model to CPU when saving the checkpoint in FSDP mode\n",
    "# we can switch to the default trainer after moving to deepspeed (let's don't change too much for now)\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=lm_datasets['test'] if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce0352ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:38, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48696698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31156159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'id', 'messages', 'input_ids', 'labels', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = (\n",
    "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4115173",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
