{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576e57e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Fri Jan 12 14:11:37 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    36W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    53W / 300W |    359MiB / 32510MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    36W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    37W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   47C    P0   301W / 300W |   1341MiB / 32510MiB |     97%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A    657489      C   ...8.8/build/bin/./mdrun_mpi      357MiB |\n",
      "|    5   N/A  N/A    631244      C   python                           1339MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[0]\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce9b562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-12 14:11:47,403] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import GPT2LMHeadModel\n",
    "from eval.utils import (\n",
    "    generate_completions,\n",
    "    load_hf_lm_and_tokenizer,\n",
    "    query_openai_chat_model,\n",
    "    dynamic_import_function,\n",
    ")\n",
    "from eval.gsm.examplars import EXAMPLARS as GSM_EXAMPLARS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb5a825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_dir='../data/eval/gsm/', max_num_examples=None, save_dir='../results/dpo1/llama-7b+sharegptv2ep2_ultrafeedback_ep=2/eval/gsm/', model_name_or_path='../results/dpo1/llama-7b+sharegptv2ep2_ultrafeedback_ep=2', tokenizer_name_or_path=None, use_slow_tokenizer=False, openai_engine=None, n_shot=8, no_cot=False, eval_batch_size=3, load_in_8bit=False, gptq=False, use_chat_format=True, chat_formatting_function='eval.templates.create_prompt_with_tulu_chat_format', max_new_tokens=256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/mgsm\")\n",
    "parser.add_argument(\"--max_num_examples\", type=int, default=None, help=\"maximum number of examples to evaluate.\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/mgsm\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--use_slow_tokenizer\", action=\"store_true\", help=\"If given, we will use the slow tokenizer.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--n_shot\", type=int, default=8, help=\"max number of examples to use for demonstration.\")\n",
    "parser.add_argument(\"--no_cot\", action=\"store_true\", help=\"If given, we're evaluating a model without chain-of-thought.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--max_new_tokens\", type=int, default=256)\n",
    "\n",
    "\n",
    "# model_name_or_path = 'gpt2-Large'\n",
    "# model_name_or_path = '../results/gpt2-Large_human_mix'\n",
    "# model_name_or_path = 't5-Large'\n",
    "# model_name_or_path = 'google/flan-t5-large'\n",
    "# model_name_or_path = '../results/google/flan-t5-small'\n",
    "# model_name_or_path = 'huggyllama/llama-7b'\n",
    "# model_name_or_path = '../results/baselines/mosaicml/mpt-7b'\n",
    "# model_name_or_path = '../results/baselines/mosaicml/mpt-7b'\n",
    "# model_name_or_path = '../results/baselines/t5-11b'\n",
    "# model_name_or_path = '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-200'\n",
    "# model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name_or_path = 'results/ft1_ep=2/llama-7b_tuluv1m'\n",
    "# model_name_or_path = 'results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'\n",
    "# model_name_or_path = 'results/oi5_tulu_v1_mix:llama-7b/llama-7b_tuluv1m:50k_log_prob_decr'\n",
    "# model_name_or_path = '../results/llama-7b_cot'\n",
    "# model_name_or_path = '../results/baselines/gpt2-medium'\n",
    "# model_name_or_path = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/doremi/results/drm2/pythia-1.4b_humanmix_uniform:200k_baseline'\n",
    "# model_name_or_path = '../results/checkpoints-1600'\n",
    "\n",
    "model_name_or_path = '../results/dpo1/llama-7b+sharegptv2ep2_ultrafeedback_ep=2'\n",
    "\n",
    "#     \n",
    "#     --max_num_examples 50 \\\n",
    "cmd = f\"\"\"\n",
    "    --data_dir ../data/eval/gsm/ \\\n",
    "    --save_dir {model_name_or_path}/eval/gsm/ \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 3 \\\n",
    "    --n_shot 8 \\\n",
    "    --max_new_tokens 256 \\\n",
    "    --use_chat_format \\\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (args.openai_engine is None), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5934d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "test_data = []\n",
    "with open(os.path.join(args.data_dir, f\"test.jsonl\")) as fin:\n",
    "    for line in fin:\n",
    "        example = json.loads(line)\n",
    "        test_data.append({\n",
    "            \"question\": example[\"question\"],\n",
    "            \"answer\": example[\"answer\"].split(\"####\")[1].strip()\n",
    "        })\n",
    "\n",
    "# some numbers are in the `x,xxx` format, and we want to remove the comma\n",
    "for example in test_data:\n",
    "    example[\"answer\"] = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", example[\"answer\"])\n",
    "    assert float(example[\"answer\"]), f\"answer is not a valid number: {example['answer']}\"\n",
    "    \n",
    "\n",
    "if args.max_num_examples and len(test_data) > args.max_num_examples:\n",
    "    test_data = random.sample(test_data, args.max_num_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f552ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4baf9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = '../'+args.model_name_or_path\n",
    "# tokenizer_name_or_path = None\n",
    "# load_in_8bit = args.load_in_8bit\n",
    "# dtype = torch.bfloat16\n",
    "# gptq_model = args.gptq\n",
    "# use_fast_tokenizer = True\n",
    "# padding_side = 'left'\n",
    "# device_map = 'auto'\n",
    "\n",
    "\n",
    "# checkpoint_name = os.path.join(model_name_or_path, 'adapter_model.bin')\n",
    "# if os.path.exists(checkpoint_name):\n",
    "#     from peft import PeftModel, PeftConfig\n",
    "#     peft_model_name_or_path = model_name_or_path\n",
    "#     config = PeftConfig.from_pretrained(peft_model_name_or_path)\n",
    "#     model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "#         model_name_or_path=config.base_model_name_or_path, \n",
    "#         tokenizer_name_or_path=config.base_model_name_or_path,\n",
    "#         device_map=device_map,\n",
    "#         gptq_model=gptq_model,\n",
    "#         load_in_8bit=load_in_8bit,\n",
    "#         dtype=dtype,\n",
    "#         use_fast_tokenizer=use_fast_tokenizer,\n",
    "#         padding_side=padding_side,\n",
    "#     )\n",
    "#     peft_model = PeftModel.from_pretrained(model, peft_model_name_or_path)\n",
    "#     model = peft_model.base_model.merge_and_unload() # merge LoRA weights to base model weights.\n",
    "#     embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "#     if len(tokenizer) > embedding_size:\n",
    "#         print(f\"The vocabulary size of the tokenizer in the LoRA model folder\"\n",
    "#               f\"contains {len(tokenizer)-embedding_size} more tokens than the base model.\\n\"\n",
    "#                \"Resizing the token embeddings of the merged model...\")\n",
    "#         model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92eb77a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55cc3d2c6a54e979016b8a9e1292db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval.utils import load_hf_lm_and_tokenizer\n",
    "\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        model_name_or_path=args.model_name_or_path, \n",
    "        tokenizer_name_or_path=args.tokenizer_name_or_path, \n",
    "        load_in_8bit=args.load_in_8bit, \n",
    "        device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "        gptq_model=args.gptq,\n",
    "        use_fast_tokenizer=not args.use_slow_tokenizer,\n",
    "    )\n",
    "\n",
    "\n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e360a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_prefix(n_shot):\n",
    "    global GSM_EXAMPLARS\n",
    "    if n_shot:\n",
    "        if len(GSM_EXAMPLARS) > n_shot:\n",
    "            examples = random.sample(GSM_EXAMPLARS, n_shot)\n",
    "        else:\n",
    "            examples = GSM_EXAMPLARS\n",
    "        demonstrations = []\n",
    "        for example in examples:\n",
    "            if args.no_cot:\n",
    "                demonstrations.append(\n",
    "                    \"Quesion: \" + example[\"question\"] + \"\\n\" + \"Answer: \" + example[\"short_answer\"]\n",
    "                )\n",
    "            else:\n",
    "                demonstrations.append(\n",
    "                    \"Question: \" + example[\"question\"] + \"\\n\" + \"Answer: \" + example[\"cot_answer\"]\n",
    "                )\n",
    "        prompt_prefix = \"Answer the following questions.\\n\\n\" + \"\\n\\n\".join(demonstrations) + \"\\n\\n\"\n",
    "    else:\n",
    "        prompt_prefix = \"Answer the following question.\\n\\n\"\n",
    "    return prompt_prefix\n",
    "\n",
    "# wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    max_input_seq_len = model.config.max_position_embeddings - args.max_new_tokens\n",
    "else:\n",
    "    max_input_seq_len = 2048 - args.max_new_tokens\n",
    "\n",
    "prompts = []\n",
    "for example in test_data:\n",
    "    ## wpq: Use <n_shot prompt if exceeds `max_input_seq_len`.\n",
    "    for n_shot in list(range(args.n_shot+1)[::-1]):\n",
    "        prompt_prefix = get_prompt_prefix(n_shot)\n",
    "        if args.use_chat_format:\n",
    "            prompt = \"<|user|>\\n\" + prompt_prefix + \"Question: \" + example[\"question\"].strip() + \"\\n<|assistant|>\\n\" + \"Answer:\"\n",
    "        else:\n",
    "            prompt = prompt_prefix + \"Question: \" + example[\"question\"].strip() + \"\\nAnswer:\"\n",
    "        tokenized_prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "        if tokenized_prompt_len < max_input_seq_len:\n",
    "            break\n",
    "    if n_shot != args.n_shot:\n",
    "        print(f'n_shot: {args.n_shot} -> {n_shot}')\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompts2 = []\n",
    "# chat_formatting_function = dynamic_import_function(args.chat_formatting_function) if args.use_chat_format else None\n",
    "# for example in test_data:\n",
    "#     ## wpq: Use <n_shot prompt if exceeds `max_input_seq_len`.\n",
    "#     for n_shot in list(range(args.n_shot+1)[::-1]):\n",
    "#         prompt_prefix = get_prompt_prefix(n_shot)\n",
    "#         prompt = prompt_prefix + \"Question: \" + example[\"question\"].strip()\n",
    "#         if args.use_chat_format:\n",
    "#             messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#             prompt = chat_formatting_function(messages, add_bos=False)\n",
    "#             if prompt[-1] in [\"\\n\", \" \"]:\n",
    "#                 prompt += \"Answer:\"\n",
    "#             else:\n",
    "#                 prompt += \" Answer:\"\n",
    "#         else:\n",
    "#             prompt += \"\\nAnswer:\"\n",
    "#         tokenized_prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "#         if tokenized_prompt_len < max_input_seq_len:\n",
    "#             break\n",
    "#     if n_shot != args.n_shot:\n",
    "#         print(f'n_shot: {args.n_shot} -> {n_shot}')\n",
    "#     prompts2.append(prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60cda247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Completions: 100%|██████████| 1/1 [00:04<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   529, 29989,  1792, 29989, 29958,    13, 22550,   278,  1494,\n",
      "          5155, 29889,    13,    13, 16492, 29901,  1670,   526, 29871, 29896,\n",
      "         29945, 10697,   297,   278,  4071,   345, 29889,  6070,   345, 17162,\n",
      "           674,  8024, 10697,   297,   278,  4071,   345,  9826, 29889,  2860,\n",
      "           896,   526,  2309, 29892,   727,   674,   367, 29871, 29906, 29896,\n",
      "         10697, 29889,  1128,  1784, 10697,  1258,   278,  4071,   345, 17162,\n",
      "          8024,  9826, 29973,    13, 22550, 29901,  1670,   526, 29871, 29896,\n",
      "         29945, 10697, 10437, 29889,  1987,   727,   892, 29871, 29906, 29896,\n",
      "         10697,  1156,   777,   901,   892,  8024,   287, 29889,  1105,   727,\n",
      "          1818,   505,  1063, 29871, 29906, 29896,   448, 29871, 29896, 29945,\n",
      "           353, 29871, 29953, 29889,  1105,   278,  1234,   338, 29871, 29953,\n",
      "         29889,    13,    13, 16492, 29901,   960,   727,   526, 29871, 29941,\n",
      "         18647,   297,   278,   610,  9292,  3287,   322, 29871, 29906,   901,\n",
      "         18647, 18331, 29892,   920,  1784, 18647,   526,   297,   278,   610,\n",
      "          9292,  3287, 29973,    13, 22550, 29901,  1670,   526, 10437, 29871,\n",
      "         29941, 18647, 29889, 29871, 29906,   901, 18647, 18331, 29889, 29871,\n",
      "         29941,   718, 29871, 29906,   353, 29871, 29945, 29889,  1105,   278,\n",
      "          1234,   338, 29871, 29945, 29889,    13,    13, 16492, 29901,   951,\n",
      "           801,   750, 29871, 29941, 29906,   521,   542,   324,  1078,   322,\n",
      "           902,  9883,   750, 29871, 29946, 29906, 29889,   960,   896,   263,\n",
      "           371, 29871, 29941, 29945, 29892,   920,  1784, 12785,   437,   896,\n",
      "           505,  2175,   297,  3001, 29973,    13, 22550, 29901, 22118,   635,\n",
      "         29892,   951,   801,   750, 29871, 29941, 29906,   521,   542,   324,\n",
      "          1078, 29889,  2439,  9883,   750, 29871, 29946, 29906, 29889,  1105,\n",
      "           297,  3001,   896,   750, 29871, 29941, 29906,   718, 29871, 29946,\n",
      "         29906,   353, 29871, 29955, 29946, 29889,  2860,   321,  1218, 29871,\n",
      "         29941, 29945, 29892,   896,   750, 29871, 29955, 29946,   448, 29871,\n",
      "         29941, 29945,   353, 29871, 29941, 29929, 29889,  1105,   278,  1234,\n",
      "           338, 29871, 29941, 29929, 29889,    13,    13, 16492, 29901, 21776,\n",
      "           750, 29871, 29906, 29900,   301,   324,  3466,  3554, 29889,   940,\n",
      "          4846,  3384,  1460,   777,   301,   324,  3466,  3554, 29889,  2567,\n",
      "         21776,   756, 29871, 29896, 29906,   301,   324,  3466,  3554, 29889,\n",
      "          1128,  1784,   301,   324,  3466,  3554,  1258, 21776,  2367,   304,\n",
      "          3384,  1460, 29973,    13, 22550, 29901, 21776,  4687,   411, 29871,\n",
      "         29906, 29900,   301,   324,  3466,  3554, 29889,  1987,   540,   750,\n",
      "         29871, 29896, 29906,  1156,  6820,   777,   304,  3384,  1460, 29889,\n",
      "          1105,   540,  4846,  3384,  1460, 29871, 29906, 29900,   448, 29871,\n",
      "         29896, 29906,   353, 29871, 29947, 29889,  1105,   278,  1234,   338,\n",
      "         29871, 29947, 29889,    13,    13, 16492, 29901,  1383, 18101,   756,\n",
      "          5320,   304,   952, 29889,  1152, 17661, 29892,   540,  2355,  1023,\n",
      "           304,   952,  1269,   515,   670, 16823,   322,   270,   328, 29889,\n",
      "          1128,  1784,   304,   952,   947,   540,   505,  1286, 29973,    13,\n",
      "         22550, 29901,  1383, 18101,  4687,   411, 29871, 29945,   304,   952,\n",
      "         29889,   960,   540,  2355, 29871, 29906,   304,   952,  1269,   515,\n",
      "           670, 16823,   322,   270,   328, 29892,   769,   393,   338, 29871,\n",
      "         29946,   901,   304,   952, 29889, 29871, 29945,   718, 29871, 29946,\n",
      "           353, 29871, 29929, 29889,  1105,   278,  1234,   338, 29871, 29929,\n",
      "         29889,    13,    13, 16492, 29901,  1670,   892, 14183, 23226,   297,\n",
      "           278,  1923,  5716, 29889, 22853,   901, 23226,   892,  5130,  1269,\n",
      "          2462, 29892,   515,  7398,   388,   304,   266,  1295,  3250, 29889,\n",
      "          1128,  1784, 23226,   526,  1286,   297,   278,  1923,  5716, 29973,\n",
      "            13, 22550, 29901,  1670,   892, 10437, 29871, 29929, 23226, 29889,\n",
      "          1152,  1269,   310, 29871, 29946,  3841, 29892, 29871, 29945,   901,\n",
      "         23226,   892,  2715, 29889,  1105, 29871, 29945,   334, 29871, 29946,\n",
      "           353, 29871, 29906, 29900, 23226,   892,  2715, 29889, 29871, 29929,\n",
      "           718, 29871, 29906, 29900,   338, 29871, 29906, 29929, 29889,  1105,\n",
      "           278,  1234,   338, 29871, 29906, 29929, 29889,    13,    13, 16492,\n",
      "         29901,  5765,   750, 29871, 29945, 29947, 29416, 26563, 29889,  1551,\n",
      "           260,  1041,  3250, 29892,   540,  5714, 29871, 29906, 29941, 29416,\n",
      "         26563, 29889,  1551, 14837,  4515,  3250, 29892,   540,  5714, 29871,\n",
      "         29906,   901, 29889,  1128,  1784, 29416, 26563,  1258,   540,   505,\n",
      "           472,   278,  1095,   310, 14837,  4515,  3250, 29973,    13, 22550,\n",
      "         29901,  5765,  4687,   411, 29871, 29945, 29947, 29416, 26563, 29889,\n",
      "          2860, 19035, 29871, 29906, 29941,   373,   260,  1041,  3250, 29892,\n",
      "           540,   750, 29871, 29945, 29947,   448, 29871, 29906, 29941,   353,\n",
      "         29871, 29941, 29945, 29889,  2860, 19035, 29871, 29906,   901, 29892,\n",
      "           540,   750, 29871, 29941, 29945,   448, 29871, 29906,   353, 29871,\n",
      "         29941, 29941, 29416, 26563, 29889,  1105,   278,  1234,   338, 29871,\n",
      "         29941, 29941, 29889,    13,    13, 16492, 29901, 19802,   423,   756,\n",
      "           395, 29906, 29941, 29889,  2296, 18093,  5320, 19548,  1379,   363,\n",
      "           395, 29941,  1269, 29889,  1128,  1568,  6909,   947,  1183,   505,\n",
      "          2175, 29973,    13, 22550, 29901, 19802,   423,   750, 29871, 29906,\n",
      "         29941, 17208, 29889, 29871, 29945, 19548,  1379,   363, 29871, 29941,\n",
      "         17208,  1269,   674,   367, 29871, 29945,   921, 29871, 29941,   353,\n",
      "         29871, 29896, 29945, 17208, 29889,  1105,  1183,   756, 29871, 29906,\n",
      "         29941,   448, 29871, 29896, 29945, 17208,  2175, 29889, 29871, 29906,\n",
      "         29941,   448, 29871, 29896, 29945,   338, 29871, 29947, 29889,  1105,\n",
      "           278,  1234,   338, 29871, 29947, 29889,    13,    13, 16492, 29901,\n",
      "          2627,   300, 30010, 29879,   868,  4684,  6568, 29871, 29896, 29953,\n",
      "         29808,   639,  2462, 29889,  2296,   321,  1446,  2211,   363, 26044,\n",
      "          1432,  7250,   322,   289,  6926,   286,  3096,  1144,   363,   902,\n",
      "          7875,  1432,  2462,   411,  3023, 29889,  2296,   269, 10071,   278,\n",
      "         21162,   472,   278,  2215, 13269, 29915,  9999, 14218,   363,   395,\n",
      "         29906,   639, 10849,   868,   384, 19710, 29889,  1128,  1568,   297,\n",
      "         17208,   947,  1183,  1207,  1432,  2462,   472,   278,  2215, 13269,\n",
      "         29915,  9999, 29973,    13, 29966, 29989,   465, 22137, 29989, 29958,\n",
      "            13, 22550, 29901,  2627,   300,   756, 29871, 29896, 29953, 29808,\n",
      "           639,  2462, 29889,  2296,   321,  1446, 29871, 29941, 29808,   639,\n",
      "          2462, 29889,  2296,   289,  6926, 29871, 29946,   286,  3096,  1144,\n",
      "           639,  2462, 29889,  2296,   269, 10071, 29871, 29896, 29953,   448,\n",
      "         29871, 29941,   448, 29871, 29946,   353, 29871, 29947, 29808,   639,\n",
      "          2462, 29889,  2296,   269, 10071, 29871, 29947, 29808,   639,  2462,\n",
      "           363,   395, 29906,   639, 19710, 29889,  1105,  1183,  3732, 29871,\n",
      "         29947,   921, 29871, 29906,   353, 29871, 29896, 29953, 17208,   639,\n",
      "          2462, 29889,  1105,   278,  1234,   338, 29871, 29896, 29953, 21106,\n",
      "         29879, 29958, 32000]], device='cuda:0')\n",
      "Time = 4.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Janet has 16 eggs per day. She eats 3 eggs per day. She bakes 4 muffins per day. She sells 16 - 3 - 4 = 8 eggs per day. She sells 8 eggs per day for $2 per egg. So she makes 8 x 2 = 16 dollars per day. So the answer is 16.</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval.utils import generate_completions\n",
    "\n",
    "\n",
    "# get the last token because the tokenizer may add space tokens at the start.\n",
    "# wpq: t5 tokenizer strips `\\n`. don't use `\\n` as stop sequence. just generate to max length or encounters <\\s>. \n",
    "new_line_token = tokenizer.encode(\"\\n\", add_special_tokens=False)\n",
    "stop_id_sequences = [[new_line_token[-1]]] if new_line_token else None\n",
    "\n",
    "# wpq: modify `max_new_tokens=512` to `256` for faster generation.\n",
    "generation_kwargs = {'max_new_tokens': args.max_new_tokens}\n",
    "# generation_kwargs.update({'eos_token_id': tokenizer.eos_token_id})\n",
    "\n",
    "t0 = time.time()\n",
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts[:1],\n",
    "    batch_size=1, # args.eval_batch_size\n",
    "    stop_id_sequences=stop_id_sequences,\n",
    "    **generation_kwargs,\n",
    ")\n",
    "\n",
    "t = time.time()-t0\n",
    "print(f'Time = {t:.2f}')\n",
    "outputs[0]\n",
    "\n",
    "\n",
    "# batch_siz = 20\n",
    "# 4*60+44 / 20 = 14.2 / data\n",
    "# 276.43 / 20 = 13.8 / data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7074a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= 16 dollars per day. So the answer is 16.</s><pad>\n"
     ]
    }
   ],
   "source": [
    "# after update + left padding fix\n",
    "# with chatfmt\n",
    "batch_outputs = torch.tensor([[353, 29871, 29896, 29953, 17208,   639,\n",
    "          2462, 29889,  1105,   278,  1234,   338, 29871, 29896, 29953, 21106,\n",
    "         29879, 29958, 32000]])\n",
    "\n",
    "\n",
    "# # after transformes update.\n",
    "# # with chatfmt\n",
    "# batch_outputs = torch.tensor([[    17208,   639,\n",
    "#           2462, 29889,  1105,   278,  1234,   338, 29871, 29896, 29953, 21106,\n",
    "#          29879, 29958, 32000]])\n",
    "# # without chatfmt\n",
    "# batch_outputs = torch.tensor([[10849,   868,   384, 19710, 29889,  1105,\n",
    "#           1183,  3732, 29871, 29929,   921, 29871, 29906,   353, 29871, 29896,\n",
    "#          29947, 17208, 29889,  1105,   278,  1234,   338, 29871, 29896, 29947,\n",
    "#          29889, 32000]])\n",
    "\n",
    "# # before transformes update\n",
    "# # without chatfmt\n",
    "# batch_outputs = torch.tensor([[29896, 29953, 29889,\n",
    "#           1105,   278,  1234,   338, 29871, 29896, 29953, 29889,     2]])\n",
    "# # chatfmt\n",
    "# batch_outputs = torch.tensor([[   278,  1234,   338, 29871, 29941, 29906, 29889,     2]])\n",
    "\n",
    "\n",
    "print(tokenizer.batch_decode(batch_outputs, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b0bc6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='results/ft1_ep=2/llama-7b_tuluv1m', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "188f3350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "model.get_input_embeddings().weight.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e129aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"Loading model and tokenizer...\")\n",
    "# model2, tokenizer2 = load_hf_lm_and_tokenizer(\n",
    "#     model_name_or_path='results/oi5_tulu_v1_mix:llama-7b/llama-7b_tuluv1m_score=log:prob:neg_pace=prune:size=150000:ep=3', \n",
    "#     tokenizer_name_or_path=None, \n",
    "#     load_in_8bit=args.load_in_8bit, \n",
    "#     device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "#     gptq_model=args.gptq,\n",
    "#     use_fast_tokenizer=not args.use_slow_tokenizer,\n",
    "# )\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name_or_path = 'results/oi4_tulu_v1_mix_ep=3/llama-7b_tuluv1m:50k_log_prob_decr'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False)\n",
    "tokenizer_name_or_path = 'results/ft1_ep=2/llama-7b_tuluv1m'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False)\n",
    "\n",
    "print(tokenizer.added_tokens_decoder == tokenizer2.added_tokens_decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f09a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6d22cdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_side\n",
      "right\n",
      "right\n",
      "vocab_size\n",
      "32000\n",
      "32000\n",
      "model_max_length\n",
      "2048\n",
      "2048\n",
      "truncation_side\n",
      "right\n",
      "right\n",
      "clean_up_tokenization_spaces\n",
      "False\n",
      "False\n",
      "added_tokens_decoder\n",
      "{0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}\n",
      "{0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}\n"
     ]
    }
   ],
   "source": [
    "for k in [\n",
    "    'padding_side',\n",
    "    'vocab_size',\n",
    "    'model_max_length',\n",
    "    'truncation_side',\n",
    "    'clean_up_tokenization_spaces',\n",
    "    'added_tokens_decoder'\n",
    "]:\n",
    "    print(k)\n",
    "    print(getattr(tokenizer1, k))\n",
    "    print(getattr(tokenizer2, k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6f260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e68a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde6259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_prompts = [\n",
    "#     ' '.join(['summer']*20), \n",
    "#     ' '.join(['winter']*10),\n",
    "# ]\n",
    "\n",
    "# tokenized_prompts = tokenizer(batch_prompts, padding=\"longest\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "# batch_input_ids = tokenized_prompts.input_ids.cuda()\n",
    "# attention_mask = tokenized_prompts.attention_mask.cuda()\n",
    "\n",
    "\n",
    "# from transformers import StoppingCriteriaList\n",
    "\n",
    "# # wpq: Use `StoppingCriteriaList` instead of `List`\n",
    "# stopping_criteria = StoppingCriteriaList([KeyWordsCriteria(stop_id_sequences)]) if stop_id_sequences else None\n",
    "    \n",
    "# batch_outputs = model.generate(\n",
    "#     input_ids=batch_input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "#     **generation_kwargs\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f08753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = []\n",
    "# for p in prompts:\n",
    "#     tokenized_prompts = tokenizer(p, return_tensors='pt', add_special_tokens=False)\n",
    "#     L.append(tokenized_prompts.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils.generate_completions*\n",
    "\n",
    "\n",
    "# if 'gpt2' in model_name_or_path:\n",
    "#     generation_kwargs = {'max_length': model.config.max_position_embeddings} # 1024\n",
    "# else:\n",
    "#     generation_kwargs = {'max_new_tokens': 512}\n",
    "# print(generation_kwargs)\n",
    "    \n",
    "# batch_size = 5\n",
    "# i = 0\n",
    "# batch_prompts = prompts[i:i+batch_size]\n",
    "# # batch_prompts = [\n",
    "# #     'Is the following sentence positive or negative: I hate the food',\n",
    "# #     'translate from english to german: the weather is great!']\n",
    "# tokenized_prompts = tokenizer(batch_prompts, padding=\"longest\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "# batch_input_ids = tokenized_prompts.input_ids\n",
    "# attention_mask = tokenized_prompts.attention_mask\n",
    "\n",
    "# print(batch_input_ids.shape, batch_input_ids.device)\n",
    "# print(attention_mask.shape, attention_mask.device)\n",
    "# batch_input_ids = batch_input_ids.cuda()\n",
    "# attention_mask = attention_mask.cuda()\n",
    "# print(model.device)\n",
    "\n",
    "\n",
    "# from transformers import StoppingCriteriaList\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# stopping_criteria = StoppingCriteriaList([KeyWordsCriteria(stop_id_sequences)]) if stop_id_sequences else None\n",
    "\n",
    "# batch_outputs = model.generate(\n",
    "#     input_ids=batch_input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "#     **generation_kwargs,\n",
    "# )\n",
    "# end = time.time()\n",
    "# end-start\n",
    "# print(end-start)\n",
    "# print(batch_outputs.shape)\n",
    "\n",
    "# if stop_id_sequences:\n",
    "#     for output_idx in range(batch_outputs.shape[0]):\n",
    "#         for token_idx in range(batch_input_ids.shape[1], batch_outputs.shape[1]):\n",
    "#             if any(batch_outputs[output_idx, token_idx: token_idx+len(stop_sequence)].tolist() == stop_sequence for stop_sequence in stop_id_sequences):\n",
    "#                 batch_outputs[output_idx, token_idx:] = tokenizer.pad_token_id\n",
    "#                 break\n",
    "\n",
    "\n",
    "# # \n",
    "# # 15s\n",
    "# # 18.369792938232422\n",
    "\n",
    "# num_return_sequences = 1\n",
    "# # remove the prompt from the output\n",
    "# # we need to re-encode the prompt because we need to make sure the special tokens are treated the same way as in the outputs.\n",
    "# # we changed our previous way of truncating the output token ids dicrectly because some tokenizer (e.g., llama) won't add space token before the first token.\n",
    "# # space is important for some tasks (e.g., code completion).\n",
    "# batch_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "# batch_prompts = tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True)\n",
    "# # duplicate the prompts to match the number of return sequences\n",
    "# batch_prompts = [prompt for prompt in batch_prompts for _ in range(num_return_sequences)]\n",
    "# batch_generations = [\n",
    "#     output[len(prompt):] for prompt, output in zip(batch_prompts, batch_outputs)\n",
    "# ]\n",
    "\n",
    "# print([len(x) for x in batch_outputs])\n",
    "# print([len(x) for x in batch_prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f257c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_return_sequences = 1\n",
    "# batch_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "# batch_prompts = tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True)\n",
    "# # duplicate the prompts to match the number of return sequences\n",
    "# batch_prompts = [prompt for prompt in batch_prompts for _ in range(num_return_sequences)]\n",
    "# batch_generations = [\n",
    "#     output[len(prompt):] for prompt, output in zip(batch_prompts, batch_outputs)\n",
    "# ]\n",
    "# batch_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc493fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_kwargs = {'max_new_tokens': 512}\n",
    "# disable_tqdm = False\n",
    "# stop_id_sequences = [[new_line_token]]\n",
    "# batch_size=args.eval_batch_size\n",
    "\n",
    "\n",
    "\n",
    "# generations = []\n",
    "# if not disable_tqdm:\n",
    "#     progress = tqdm.tqdm(total=len(prompts), desc=\"Generating Completions\")\n",
    "\n",
    "# num_return_sequences = generation_kwargs.get(\"num_return_sequences\", 1)\n",
    "# for i in range(0, len(prompts), batch_size):\n",
    "#     batch_prompts = prompts[i:i+batch_size]\n",
    "#     tokenized_prompts = tokenizer(batch_prompts, padding=\"longest\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "#     batch_input_ids = tokenized_prompts.input_ids\n",
    "#     attention_mask = tokenized_prompts.attention_mask\n",
    "\n",
    "#     if model.device.type == \"cuda\":\n",
    "#         print(torch.cuda.is_available())\n",
    "#         print(batch_input_ids.device, batch_input_ids.shape)\n",
    "#         batch_input_ids = batch_input_ids.cuda()\n",
    "#         attention_mask = attention_mask.cuda()\n",
    "\n",
    "#     try:\n",
    "#         batch_outputs = model.generate(\n",
    "#             input_ids=batch_input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             stopping_criteria=[KeyWordsCriteria(stop_id_sequences)] if stop_id_sequences else None,\n",
    "#             **generation_kwargs\n",
    "#         )\n",
    "\n",
    "#         # the stopping criteria is applied at batch level, so if other examples are not stopped, the entire batch will continue to generate.\n",
    "#         # so some outputs still have the stop sequence, which we need to remove.\n",
    "#         if stop_id_sequences:\n",
    "#             for output_idx in range(batch_outputs.shape[0]):\n",
    "#                 for token_idx in range(batch_input_ids.shape[1], batch_outputs.shape[1]):\n",
    "#                     if any(batch_outputs[output_idx, token_idx: token_idx+len(stop_sequence)].tolist() == stop_sequence for stop_sequence in stop_id_sequences):\n",
    "#                         batch_outputs[output_idx, token_idx:] = tokenizer.pad_token_id\n",
    "#                         break\n",
    "\n",
    "#         # remove the prompt from the output\n",
    "#         # we need to re-encode the prompt because we need to make sure the special tokens are treated the same way as in the outputs.\n",
    "#         # we changed our previous way of truncating the output token ids dicrectly because some tokenizer (e.g., llama) won't add space token before the first token.\n",
    "#         # space is important for some tasks (e.g., code completion).\n",
    "#         batch_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "#         batch_prompts = tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True)\n",
    "#         # duplicate the prompts to match the number of return sequences\n",
    "#         batch_prompts = [prompt for prompt in batch_prompts for _ in range(num_return_sequences)]\n",
    "#         batch_generations = [\n",
    "#             output[len(prompt):] for prompt, output in zip(batch_prompts, batch_outputs)\n",
    "#         ]\n",
    "#     except Exception as e:\n",
    "#         print(\"Error when generating completions for batch:\")\n",
    "#         print(batch_prompts)\n",
    "#         print(\"Error message:\")\n",
    "#         print(e)\n",
    "#         print(\"Use empty string as the completion.\")\n",
    "#         batch_generations = [\"\"] * len(batch_prompts) * num_return_sequences\n",
    "\n",
    "#     generations += batch_generations\n",
    "\n",
    "#     # for prompt, generation in zip(batch_prompts, batch_generations):\n",
    "#     #     print(\"========\")\n",
    "#     #     print(prompt)\n",
    "#     #     print(\"--------\")\n",
    "#     #     print(generation)\n",
    "\n",
    "#     if not disable_tqdm:\n",
    "#         progress.update(len(batch_prompts)//num_return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "for output in outputs:\n",
    "    # replace numbers like `x,xxx` with `xxxx`\n",
    "    output = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", output)\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", output)\n",
    "    if numbers:\n",
    "        predictions.append(numbers[-1])\n",
    "    else:\n",
    "        predictions.append(output)\n",
    "\n",
    "print(\"Calculating accuracy...\")\n",
    "targets = [example[\"answer\"] for example in test_data]\n",
    "\n",
    "em_score = exact_match.compute(predictions=predictions, references=targets, ignore_case=True, ignore_punctuation=True)[\"exact_match\"]\n",
    "print(f\"Exact match : {em_score}\")\n",
    "\n",
    "predictions = [{\n",
    "    \"question\": example[\"question\"],\n",
    "    \"answer\": example[\"answer\"],\n",
    "    \"model_output\": output,\n",
    "    \"prediction\": pred\n",
    "} for example, output, pred in zip(test_data, outputs, predictions)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5-Large, 0.02\n",
    "# flan-t5-large, 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86578229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(args.save_dir, f\"predictions.jsonl\"), \"w\") as fout:\n",
    "    for prediction in predictions:\n",
    "        fout.write(json.dumps(prediction) + \"\\n\") \n",
    "\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as fout:\n",
    "    json.dump({\n",
    "        \"exact_match\": em_score\n",
    "    }, fout, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a15a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e601fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
