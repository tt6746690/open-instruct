{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576e57e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/'\n",
    "                if platform.uname().processor == 'x86_64' \n",
    "                else '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce9b562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26 16:43:01,190] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import evaluate\n",
    "from transformers import GPT2LMHeadModel\n",
    "from eval.utils import generate_completions, load_hf_lm_and_tokenizer, query_openai_chat_model, KeyWordsCriteria\n",
    "from eval.gsm.examplars import EXAMPLARS as GSM_EXAMPLARS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb5a825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_dir='../data/eval/gsm/', max_num_examples=50, save_dir='../results/llama-7b_cot/eval/gsm/', model_name_or_path='../results/llama-7b_cot', tokenizer_name_or_path=None, openai_engine=None, n_shot=8, no_cot=False, eval_batch_size=3, load_in_8bit=False, gptq=False, use_chat_format=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/mgsm\")\n",
    "parser.add_argument(\"--max_num_examples\", type=int, default=None, help=\"maximum number of examples to evaluate.\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/mgsm\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--n_shot\", type=int, default=8, help=\"max number of examples to use for demonstration.\")\n",
    "parser.add_argument(\"--no_cot\", action=\"store_true\", help=\"If given, we're evaluating a model without chain-of-thought.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "\n",
    "# model_name_or_path = 'gpt2-Large'\n",
    "# model_name_or_path = '../results/gpt2-Large_human_mix'\n",
    "# model_name_or_path = 't5-Large'\n",
    "# model_name_or_path = 'google/flan-t5-large'\n",
    "# model_name_or_path = '../results/google/flan-t5-small'\n",
    "# model_name_or_path = 'huggyllama/llama-7b'\n",
    "# model_name_or_path = '../results/baselines/mosaicml/mpt-7b'\n",
    "# model_name_or_path = '../results/baselines/mosaicml/mpt-7b'\n",
    "# model_name_or_path = '../results/baselines/t5-11b'\n",
    "model_name_or_path = '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-200'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "model_name_or_path = '../results/llama-7b_cot'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --data_dir ../data/eval/gsm/ \\\n",
    "    --save_dir {model_name_or_path}/eval/gsm/ \\\n",
    "    --max_num_examples 50 \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 3 \\\n",
    "    --n_shot 8 \\\n",
    "    --use_chat_format\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (args.openai_engine is None), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5934d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "test_data = []\n",
    "with open(os.path.join(args.data_dir, f\"test.jsonl\")) as fin:\n",
    "    for line in fin:\n",
    "        example = json.loads(line)\n",
    "        test_data.append({\n",
    "            \"question\": example[\"question\"],\n",
    "            \"answer\": example[\"answer\"].split(\"####\")[1].strip()\n",
    "        })\n",
    "\n",
    "# some numbers are in the `x,xxx` format, and we want to remove the comma\n",
    "for example in test_data:\n",
    "    example[\"answer\"] = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", example[\"answer\"])\n",
    "    assert float(example[\"answer\"]), f\"answer is not a valid number: {example['answer']}\"\n",
    "    \n",
    "\n",
    "if args.max_num_examples and len(test_data) > args.max_num_examples:\n",
    "    test_data = random.sample(test_data, args.max_num_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a64e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Answer the following questions.\n",
      "\n",
      "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
      "Answer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\n",
      "\n",
      "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "Answer: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\n",
      "\n",
      "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
      "Answer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\n",
      "\n",
      "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
      "Answer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\n",
      "\n",
      "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
      "Answer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\n",
      "\n",
      "Question: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
      "Answer: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\n",
      "\n",
      "Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
      "Answer: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\n",
      "\n",
      "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "Answer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\n",
      "\n",
      "Question: The girls are trying to raise money for a carnival. Kim raises $320 more than Alexandra, who raises $430, and Maryam raises $400 more than Sarah, who raises $300. How much money, in dollars, did they all raise in total?\n",
      "<|assistant|>\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    \n",
    "global GSM_EXAMPLARS\n",
    "if args.n_shot:\n",
    "    if len(GSM_EXAMPLARS) > args.n_shot:\n",
    "        GSM_EXAMPLARS = random.sample(GSM_EXAMPLARS, args.n_shot)\n",
    "    demonstrations = []\n",
    "    for example in GSM_EXAMPLARS:\n",
    "        if args.no_cot:\n",
    "            demonstrations.append(\n",
    "                \"Quesion: \" + example[\"question\"] + \"\\n\" + \"Answer: \" + example[\"short_answer\"]\n",
    "            )\n",
    "        else:\n",
    "            demonstrations.append(\n",
    "                \"Question: \" + example[\"question\"] + \"\\n\" + \"Answer: \" + example[\"cot_answer\"]\n",
    "            )\n",
    "    prompt_prefix = \"Answer the following questions.\\n\\n\" + \"\\n\\n\".join(demonstrations) + \"\\n\\n\"\n",
    "else:\n",
    "    prompt_prefix = \"Answer the following question.\\n\\n\"\n",
    "\n",
    "prompts = []\n",
    "for example in test_data:\n",
    "    if args.use_chat_format:\n",
    "        prompt = \"<|user|>\\n\" + prompt_prefix + \"Question: \" + example[\"question\"].strip() + \"\\n<|assistant|>\\n\" + \"Answer:\"\n",
    "    else:\n",
    "        prompt = prompt_prefix + \"Question: \" + example[\"question\"].strip() + \"\\nAnswer:\"\n",
    "    prompts.append(prompt)\n",
    "    \n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4baf9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = '../'+args.model_name_or_path\n",
    "# tokenizer_name_or_path = None\n",
    "# load_in_8bit = args.load_in_8bit\n",
    "# dtype = torch.bfloat16\n",
    "# gptq_model = args.gptq\n",
    "# use_fast_tokenizer = True\n",
    "# padding_side = 'left'\n",
    "# device_map = 'auto'\n",
    "\n",
    "\n",
    "# checkpoint_name = os.path.join(model_name_or_path, 'adapter_model.bin')\n",
    "# if os.path.exists(checkpoint_name):\n",
    "#     from peft import PeftModel, PeftConfig\n",
    "#     peft_model_name_or_path = model_name_or_path\n",
    "#     config = PeftConfig.from_pretrained(peft_model_name_or_path)\n",
    "#     model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "#         model_name_or_path=config.base_model_name_or_path, \n",
    "#         tokenizer_name_or_path=config.base_model_name_or_path,\n",
    "#         device_map=device_map,\n",
    "#         gptq_model=gptq_model,\n",
    "#         load_in_8bit=load_in_8bit,\n",
    "#         dtype=dtype,\n",
    "#         use_fast_tokenizer=use_fast_tokenizer,\n",
    "#         padding_side=padding_side,\n",
    "#     )\n",
    "#     peft_model = PeftModel.from_pretrained(model, peft_model_name_or_path)\n",
    "#     model = peft_model.base_model.merge_and_unload() # merge LoRA weights to base model weights.\n",
    "#     embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "#     if len(tokenizer) > embedding_size:\n",
    "#         print(f\"The vocabulary size of the tokenizer in the LoRA model folder\"\n",
    "#               f\"contains {len(tokenizer)-embedding_size} more tokens than the base model.\\n\"\n",
    "#                \"Resizing the token embeddings of the merged model...\")\n",
    "#         model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b92eb77a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ca140b019647c8a30dce1cc5b2e062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.float16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval.utils import load_hf_lm_and_tokenizer\n",
    "\n",
    "model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=args.model_name_or_path, \n",
    "    tokenizer_name_or_path=args.tokenizer_name_or_path, \n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    dtype='auto',\n",
    "    gptq_model=args.gptq,\n",
    "    use_fast_tokenizer=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cda247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Completions:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Generating Completions:   6%|â–Œ         | 3/50 [06:20<1:39:22, 126.87s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "from eval.utils import generate_completions\n",
    "\n",
    "\n",
    "# get the last token because the tokenizer may add space tokens at the start.\n",
    "# wpq: t5 tokenizer strips `\\n`. don't use `\\n` as stop sequence. just generate to max length or encounters <\\s>. \n",
    "new_line_token = tokenizer.encode(\"\\n\", add_special_tokens=False)\n",
    "stop_id_sequences = [[new_line_token[-1]]] if new_line_token else None\n",
    "\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    # wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "    generation_kwargs = {'max_length': model.config.max_position_embeddings} # 1024\n",
    "else:\n",
    "    # wpq: modify `max_new_tokens=512` to `256` by default\n",
    "    generation_kwargs = {'max_new_tokens': 256}\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts,\n",
    "    batch_size=args.eval_batch_size,\n",
    "    stop_id_sequences=stop_id_sequences,\n",
    "    **generation_kwargs,\n",
    ")\n",
    "\n",
    "t = time.time()-t0\n",
    "print(f'Time = {t:.2f}')\n",
    "outputs[0]\n",
    "\n",
    "\n",
    "# batch_siz = 20\n",
    "# 4*60+44 / 20 = 14.2 / data\n",
    "# 276.43 / 20 = 13.8 / data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde6259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils.generate_completions*\n",
    "\n",
    "\n",
    "# if 'gpt2' in model_name_or_path:\n",
    "#     generation_kwargs = {'max_length': model.config.max_position_embeddings} # 1024\n",
    "# else:\n",
    "#     generation_kwargs = {'max_new_tokens': 512}\n",
    "# print(generation_kwargs)\n",
    "    \n",
    "# batch_size = 5\n",
    "# i = 0\n",
    "# batch_prompts = prompts[i:i+batch_size]\n",
    "# # batch_prompts = [\n",
    "# #     'Is the following sentence positive or negative: I hate the food',\n",
    "# #     'translate from english to german: the weather is great!']\n",
    "# tokenized_prompts = tokenizer(batch_prompts, padding=\"longest\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "# batch_input_ids = tokenized_prompts.input_ids\n",
    "# attention_mask = tokenized_prompts.attention_mask\n",
    "\n",
    "# print(batch_input_ids.shape, batch_input_ids.device)\n",
    "# print(attention_mask.shape, attention_mask.device)\n",
    "# batch_input_ids = batch_input_ids.cuda()\n",
    "# attention_mask = attention_mask.cuda()\n",
    "# print(model.device)\n",
    "\n",
    "\n",
    "# from transformers import StoppingCriteriaList\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# stopping_criteria = StoppingCriteriaList([KeyWordsCriteria(stop_id_sequences)]) if stop_id_sequences else None\n",
    "\n",
    "# batch_outputs = model.generate(\n",
    "#     input_ids=batch_input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "#     **generation_kwargs,\n",
    "# )\n",
    "# end = time.time()\n",
    "# end-start\n",
    "# print(end-start)\n",
    "# print(batch_outputs.shape)\n",
    "\n",
    "# if stop_id_sequences:\n",
    "#     for output_idx in range(batch_outputs.shape[0]):\n",
    "#         for token_idx in range(batch_input_ids.shape[1], batch_outputs.shape[1]):\n",
    "#             if any(batch_outputs[output_idx, token_idx: token_idx+len(stop_sequence)].tolist() == stop_sequence for stop_sequence in stop_id_sequences):\n",
    "#                 batch_outputs[output_idx, token_idx:] = tokenizer.pad_token_id\n",
    "#                 break\n",
    "\n",
    "\n",
    "# # \n",
    "# # 15s\n",
    "# # 18.369792938232422\n",
    "\n",
    "# num_return_sequences = 1\n",
    "# # remove the prompt from the output\n",
    "# # we need to re-encode the prompt because we need to make sure the special tokens are treated the same way as in the outputs.\n",
    "# # we changed our previous way of truncating the output token ids dicrectly because some tokenizer (e.g., llama) won't add space token before the first token.\n",
    "# # space is important for some tasks (e.g., code completion).\n",
    "# batch_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "# batch_prompts = tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True)\n",
    "# # duplicate the prompts to match the number of return sequences\n",
    "# batch_prompts = [prompt for prompt in batch_prompts for _ in range(num_return_sequences)]\n",
    "# batch_generations = [\n",
    "#     output[len(prompt):] for prompt, output in zip(batch_prompts, batch_outputs)\n",
    "# ]\n",
    "\n",
    "# print([len(x) for x in batch_outputs])\n",
    "# print([len(x) for x in batch_prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f257c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_return_sequences = 1\n",
    "# batch_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "# batch_prompts = tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True)\n",
    "# # duplicate the prompts to match the number of return sequences\n",
    "# batch_prompts = [prompt for prompt in batch_prompts for _ in range(num_return_sequences)]\n",
    "# batch_generations = [\n",
    "#     output[len(prompt):] for prompt, output in zip(batch_prompts, batch_outputs)\n",
    "# ]\n",
    "# batch_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc493fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_kwargs = {'max_new_tokens': 512}\n",
    "# disable_tqdm = False\n",
    "# stop_id_sequences = [[new_line_token]]\n",
    "# batch_size=args.eval_batch_size\n",
    "\n",
    "\n",
    "\n",
    "# generations = []\n",
    "# if not disable_tqdm:\n",
    "#     progress = tqdm.tqdm(total=len(prompts), desc=\"Generating Completions\")\n",
    "\n",
    "# num_return_sequences = generation_kwargs.get(\"num_return_sequences\", 1)\n",
    "# for i in range(0, len(prompts), batch_size):\n",
    "#     batch_prompts = prompts[i:i+batch_size]\n",
    "#     tokenized_prompts = tokenizer(batch_prompts, padding=\"longest\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "#     batch_input_ids = tokenized_prompts.input_ids\n",
    "#     attention_mask = tokenized_prompts.attention_mask\n",
    "\n",
    "#     if model.device.type == \"cuda\":\n",
    "#         print(torch.cuda.is_available())\n",
    "#         print(batch_input_ids.device, batch_input_ids.shape)\n",
    "#         batch_input_ids = batch_input_ids.cuda()\n",
    "#         attention_mask = attention_mask.cuda()\n",
    "\n",
    "#     try:\n",
    "#         batch_outputs = model.generate(\n",
    "#             input_ids=batch_input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             stopping_criteria=[KeyWordsCriteria(stop_id_sequences)] if stop_id_sequences else None,\n",
    "#             **generation_kwargs\n",
    "#         )\n",
    "\n",
    "#         # the stopping criteria is applied at batch level, so if other examples are not stopped, the entire batch will continue to generate.\n",
    "#         # so some outputs still have the stop sequence, which we need to remove.\n",
    "#         if stop_id_sequences:\n",
    "#             for output_idx in range(batch_outputs.shape[0]):\n",
    "#                 for token_idx in range(batch_input_ids.shape[1], batch_outputs.shape[1]):\n",
    "#                     if any(batch_outputs[output_idx, token_idx: token_idx+len(stop_sequence)].tolist() == stop_sequence for stop_sequence in stop_id_sequences):\n",
    "#                         batch_outputs[output_idx, token_idx:] = tokenizer.pad_token_id\n",
    "#                         break\n",
    "\n",
    "#         # remove the prompt from the output\n",
    "#         # we need to re-encode the prompt because we need to make sure the special tokens are treated the same way as in the outputs.\n",
    "#         # we changed our previous way of truncating the output token ids dicrectly because some tokenizer (e.g., llama) won't add space token before the first token.\n",
    "#         # space is important for some tasks (e.g., code completion).\n",
    "#         batch_outputs = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "#         batch_prompts = tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True)\n",
    "#         # duplicate the prompts to match the number of return sequences\n",
    "#         batch_prompts = [prompt for prompt in batch_prompts for _ in range(num_return_sequences)]\n",
    "#         batch_generations = [\n",
    "#             output[len(prompt):] for prompt, output in zip(batch_prompts, batch_outputs)\n",
    "#         ]\n",
    "#     except Exception as e:\n",
    "#         print(\"Error when generating completions for batch:\")\n",
    "#         print(batch_prompts)\n",
    "#         print(\"Error message:\")\n",
    "#         print(e)\n",
    "#         print(\"Use empty string as the completion.\")\n",
    "#         batch_generations = [\"\"] * len(batch_prompts) * num_return_sequences\n",
    "\n",
    "#     generations += batch_generations\n",
    "\n",
    "#     # for prompt, generation in zip(batch_prompts, batch_generations):\n",
    "#     #     print(\"========\")\n",
    "#     #     print(prompt)\n",
    "#     #     print(\"--------\")\n",
    "#     #     print(generation)\n",
    "\n",
    "#     if not disable_tqdm:\n",
    "#         progress.update(len(batch_prompts)//num_return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "for output in outputs:\n",
    "    # replace numbers like `x,xxx` with `xxxx`\n",
    "    output = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", output)\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", output)\n",
    "    if numbers:\n",
    "        predictions.append(numbers[-1])\n",
    "    else:\n",
    "        predictions.append(output)\n",
    "\n",
    "print(\"Calculating accuracy...\")\n",
    "targets = [example[\"answer\"] for example in test_data]\n",
    "\n",
    "em_score = exact_match.compute(predictions=predictions, references=targets, ignore_case=True, ignore_punctuation=True)[\"exact_match\"]\n",
    "print(f\"Exact match : {em_score}\")\n",
    "\n",
    "predictions = [{\n",
    "    \"question\": example[\"question\"],\n",
    "    \"answer\": example[\"answer\"],\n",
    "    \"model_output\": output,\n",
    "    \"prediction\": pred\n",
    "} for example, output, pred in zip(test_data, outputs, predictions)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5-Large, 0.02\n",
    "# flan-t5-large, 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86578229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(args.save_dir, f\"predictions.jsonl\"), \"w\") as fout:\n",
    "    for prediction in predictions:\n",
    "        fout.write(json.dumps(prediction) + \"\\n\") \n",
    "\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as fout:\n",
    "    json.dump({\n",
    "        \"exact_match\": em_score\n",
    "    }, fout, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a15a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e601fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
