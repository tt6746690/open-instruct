{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"LM_outputs.flan_v2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=1 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset flan_v2 --model_name_or_path ../results/baselines/huggyllama/llama-7b --save_dir /gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b --use_dist --shuffle\n",
      "[{'args': 'sbatch --job-name=LM_outputs.flan_v2 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp1z28kjtb', 'job_id': 994464}]\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook\n",
    "from llm.submit import submit_job, multiline_to_singleline\n",
    "\n",
    "shell_scripts_template = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "nodes = 1; gpus=6\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'\n",
    "\n",
    "save_dir = f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/{model_name}\"\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v2_human_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 2; gpus=6; cpu_mem = 512\n",
    "\n",
    "## for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=2; cpu_mem = 512\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    cmd = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset {dataset} \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --save_dir {save_dir} \\\n",
    "        --use_dist \\\n",
    "        --shuffle\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd, log_dir=log_dir, save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'LM_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dab267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b/lima.pkl'\n",
    "with open(p, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e976e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(np.exp(x['log_probs']), label='probs')\n",
    "ax.plot(x['el2ns'], label='el2n')\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "ax.scatter(np.exp(x['log_probs']), x['el2ns'])\n",
    "ax.set_xlabel('prob')\n",
    "ax.set_ylabel('el2n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "datasets = []\n",
    "for dataset in os.listdir(processed_dir) + ['tulu_v1_human_mix', 'tulu_v2_human_mix']:\n",
    "    dataset_path = os.path.join(processed_dir, dataset)\n",
    "    save_path = os.path.join(save_dir, f'{dataset}.pkl')\n",
    "    if 'tulu'==dataset:\n",
    "        continue\n",
    "    if 'tulu' not in dataset and not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "    datasets.append(dataset)\n",
    "    \n",
    "datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554b1e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-05 22:52:37,807] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import combine_lm_outputs_for_mixes, datasets_shard_chunk_size, compute_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "shuffle = False\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "model_name = 'pythia-1.4b'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1.4b'\n",
    "\n",
    "save_dir = f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/{model_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd9434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='cuda:0',\n",
    "    torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'lima'\n",
    "# dataset = 'flan_v2'\n",
    "\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "\n",
    "\n",
    "if dataset in ['tulu_v1_human_mix', 'tulu_v2_human_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05b73003b2646ac9cd7e0d01bd17d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lima dataset length = 1030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 0/1030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "\n",
    "encode_function = partial(\n",
    "    encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "\n",
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    # if test_run:\n",
    "    #     raw_datasets['train'] = raw_datasets['train'].select(range(100))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    # if test_run:\n",
    "    #     raw_datasets['train'] = raw_datasets['train'].select(range(100))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e2f99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1030 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = defaultdict(list)\n",
    "for batch in tqdm(loader, disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "#     with torch.inference_mode():\n",
    "    outputs = model(**batch, output_hidden_states=True)\n",
    "\n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "    # average of output token log probs\n",
    "    log_prob = -outputs['loss']\n",
    "    # compute EL2N score\n",
    "    losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "\n",
    "    output['text_embedding'].append(text_embedding.detach().cpu().to(torch.float32))\n",
    "    output['log_prob'].append(log_prob.detach().cpu())\n",
    "    for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "        output[k].append(losses[k].detach().cpu())\n",
    "\n",
    "    break\n",
    "    \n",
    "for k, v in output.items():\n",
    "    output[k] = torch.vstack(v).to(torch.float32).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape) for k, v in output.items()]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e561999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# outputs['loss'].backward()\n",
    "# logits[0, class_idx].backward(retain_graph=True)\n",
    "\n",
    "# bsz, seq, dim\n",
    "outputs['logits'][0,0,1000].backward(retain_graph=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d94ced3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for gpt_neox.embed_in.weight: torch.Size([50304, 2048])\n",
      "Gradient for gpt_neox.layers.0.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.0.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.0.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.0.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.0.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.0.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.0.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.0.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.0.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.0.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.0.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.0.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.1.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.1.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.1.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.1.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.1.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.1.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.1.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.1.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.1.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.1.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.1.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.1.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.2.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.2.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.2.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.2.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.2.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.2.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.2.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.2.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.2.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.2.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.2.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.2.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.3.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.3.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.3.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.3.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.3.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.3.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.3.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.3.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.3.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.3.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.3.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.3.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.4.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.4.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.4.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.4.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.4.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.4.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.4.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.4.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.4.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.4.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.4.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.4.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.5.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.5.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.5.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.5.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.5.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.5.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.5.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.5.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.5.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.5.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.5.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.5.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.6.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.6.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.6.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.6.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.6.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.6.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.6.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.6.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.6.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.6.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.6.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.6.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.7.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.7.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.7.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.7.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.7.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.7.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.7.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.7.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.7.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.7.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.7.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.7.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.8.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.8.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.8.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.8.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.8.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.8.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.8.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.8.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.8.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.8.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.8.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.8.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.9.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.9.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.9.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.9.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.9.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.9.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.9.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.9.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.9.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.9.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.9.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.9.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.10.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.10.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.10.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.10.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.10.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.10.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.10.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.10.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.10.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.10.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.10.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.10.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.11.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.11.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.11.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.11.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.11.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.11.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.11.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.11.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.11.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.11.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.11.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.11.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.12.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.12.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.12.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.12.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.12.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.12.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.12.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.12.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.12.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.12.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.12.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.12.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.13.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.13.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.13.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.13.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.13.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.13.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.13.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.13.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.13.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.13.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.13.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.13.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.14.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.14.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.14.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.14.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.14.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.14.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.14.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.14.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.14.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.14.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.14.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.14.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.15.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.15.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.15.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.15.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.15.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.15.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.15.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.15.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.15.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.15.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.15.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.15.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.16.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.16.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.16.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.16.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.16.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.16.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.16.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.16.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.16.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.16.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.16.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.16.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.17.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.17.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.17.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.17.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.17.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.17.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.17.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.17.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.17.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.17.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.17.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.17.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.18.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.18.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.18.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.18.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.18.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.18.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.18.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.18.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.18.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.18.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.18.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.18.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.19.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.19.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.19.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.19.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.19.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.19.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.19.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.19.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.19.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.19.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.19.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.19.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.20.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.20.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.20.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.20.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.20.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.20.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.20.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.20.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.20.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.20.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.20.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.20.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.21.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.21.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.21.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.21.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.21.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.21.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.21.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.21.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.21.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.21.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.21.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.21.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.22.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.22.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.22.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.22.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.22.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.22.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.22.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.22.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.22.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.22.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.22.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.22.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.23.input_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.23.input_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.23.post_attention_layernorm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.23.post_attention_layernorm.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.23.attention.query_key_value.weight: torch.Size([6144, 2048])\n",
      "Gradient for gpt_neox.layers.23.attention.query_key_value.bias: torch.Size([6144])\n",
      "Gradient for gpt_neox.layers.23.attention.dense.weight: torch.Size([2048, 2048])\n",
      "Gradient for gpt_neox.layers.23.attention.dense.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.layers.23.mlp.dense_h_to_4h.weight: torch.Size([8192, 2048])\n",
      "Gradient for gpt_neox.layers.23.mlp.dense_h_to_4h.bias: torch.Size([8192])\n",
      "Gradient for gpt_neox.layers.23.mlp.dense_4h_to_h.weight: torch.Size([2048, 8192])\n",
      "Gradient for gpt_neox.layers.23.mlp.dense_4h_to_h.bias: torch.Size([2048])\n",
      "Gradient for gpt_neox.final_layer_norm.weight: torch.Size([2048])\n",
      "Gradient for gpt_neox.final_layer_norm.bias: torch.Size([2048])\n",
      "Gradient for embed_out.weight: torch.Size([50304, 2048])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "grads = []\n",
    "\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        print(f\"Gradient for {param_name}: {param.grad.shape}\")\n",
    "        grads.append(param.grad.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bb222c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1414647808, 1]),\n",
       " tensor(inf, device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = [x.reshape(-1,1) for x in grads]\n",
    "grads = torch.vstack(grads)\n",
    "grads.shape, torch.sum(grads*grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29d436a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.27 GiB (GPU 0; 31.75 GiB total capacity; 22.86 GiB already allocated; 4.03 GiB free; 26.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    635\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    566\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    570\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor_str.py:309\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_neg()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcomplex32:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfloat()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.27 GiB (GPU 0; 31.75 GiB total capacity; 22.86 GiB already allocated; 4.03 GiB free; 26.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f3658c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compute gradients for all logits at once\n",
    "class_indices = torch.arange(logits_all_classes.size(1))  # Assuming logits are in the second dimension\n",
    "logit_gradients = torch.autograd.grad(outputs=logits_all_classes[:, class_indices], inputs=model.parameters(), retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb3770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from note_llama_embeddings import compute_losses\n",
    "\n",
    "labels = batch['labels']\n",
    "logits = outputs['logits']\n",
    "\n",
    "\n",
    "compute_losses(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca042ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if logits.shape[0]!=1:\n",
    "    raise ValueError('compute_el2n supports bsz=1 only.')\n",
    "vocab_size = logits.shape[-1]\n",
    "device = logits.device\n",
    "# Shift so that tokens < n predict n\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "# (Bsz*|Seq|, |Vocab|)\n",
    "shift_logits = shift_logits.view(-1, vocab_size)\n",
    "shift_probs = torch.nn.functional.softmax(shift_logits, dim=-1)\n",
    "shift_labels = shift_labels.view(-1)\n",
    "# only compute loss on the output tokens\n",
    "output_tok_indices = (shift_labels != -100).nonzero().reshape(-1)\n",
    "shift_labels = shift_labels[output_tok_indices]\n",
    "shift_probs = shift_probs[output_tok_indices]\n",
    "shift_logits = shift_logits[output_tok_indices]\n",
    "# Enable model parallelism\n",
    "shift_labels = shift_labels.to(device)\n",
    "\n",
    "losses = {}\n",
    "# Compute EL2N = || prob - one-hot-label ||_2\n",
    "shift_probs_minus_onehot_target = shift_probs.clone()\n",
    "shift_probs_minus_onehot_target[torch.arange(shift_probs.size(0)), shift_labels] -= 1\n",
    "loss_tokenwise = torch.linalg.norm(shift_probs_minus_onehot_target, dim=-1, ord=2)\n",
    "losses['el2n_agg=mean'] = loss_tokenwise.mean()\n",
    "losses['el2n_agg=l2n'] =  torch.linalg.norm(loss_tokenwise, ord=2)\n",
    "# Classification logit margin\n",
    "shift_logits_true = torch.gather(shift_logits, 1, shift_labels.view(-1, 1)).squeeze()\n",
    "shift_logits_other = shift_logits.clone()\n",
    "shift_logits_other[torch.arange(shift_logits.size(0)), shift_labels] = float('-inf')\n",
    "shift_logits_other_max, _ = torch.max(shift_logits_other, 1)\n",
    "losses['logit_margin'] = (shift_logits_true-shift_logits_other_max).mean()\n",
    "\n",
    "\n",
    "# Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5cdc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "output = {k: [] for k in output_keys}\n",
    "for batch in tqdm(loader, disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**batch, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    labels = batch['labels']\n",
    "    logits = outputs['logits']\n",
    "\n",
    "\n",
    "\n",
    "    if logits.shape[0]!=1:\n",
    "        raise ValueError('compute_el2n supports bsz=1 only.')\n",
    "    vocab_size = logits.shape[-1]\n",
    "    device = logits.device\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    # Flatten the tokens\n",
    "    # (Bsz*|Seq|, |Vocab|)\n",
    "    shift_logits = shift_logits.view(-1, vocab_size)\n",
    "    shift_probs = torch.nn.functional.softmax(shift_logits, dim=-1)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # only compute loss on the output tokens\n",
    "    output_tok_indices = (shift_labels != -100).nonzero().reshape(-1)\n",
    "    shift_labels = shift_labels[output_tok_indices]\n",
    "    shift_probs = shift_probs[output_tok_indices]\n",
    "    shift_logits = shift_logits[output_tok_indices]\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(device)\n",
    "\n",
    "\n",
    "    losses = {}\n",
    "    # Compute EL2N = || prob - one-hot-label ||_2\n",
    "    shift_probs_minus_onehot_target = shift_probs.clone()\n",
    "    shift_probs_minus_onehot_target[torch.arange(shift_probs.size(0)), shift_labels] -= 1\n",
    "    loss_tokenwise = torch.linalg.norm(shift_probs_minus_onehot_target, dim=-1, ord=2)\n",
    "    losses['el2n_agg=mean'] = loss_tokenwise.mean()\n",
    "    losses['el2n_agg=l2n'] =  torch.linalg.norm(loss_tokenwise, ord=2)\n",
    "\n",
    "    shift_logits_true = torch.gather(shift_logits, 1, shift_labels.view(-1, 1)).squeeze()\n",
    "    shift_logits_other = shift_logits.clone()\n",
    "    shift_logits_other[torch.arange(shift_logits.size(0)), shift_labels] = float('-inf')\n",
    "    shift_logits_other_max, _ = torch.max(shift_logits_other, 1)\n",
    "    losses['logit_margin'] = (shift_logits_true-shift_logits_other_max).mean()\n",
    "    \n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_logits_other_max.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f235f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try KL(prob_true, prob_nottrue) over tokens in sequence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
