{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.flan2022v2_1m\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": true,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=flan2022v2_1m --model_name_or_path=../results/baselines/huggyllama/llama-7b --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --use_dist --shuffle --compute_grad --use_lora --lora_rank=256 --lora_alpha=256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "lora_rank = 256; lora_alpha = lora_rank\n",
    "nodes = 1; gpus=6\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'\n",
    "\n",
    "# compute_grad = False; use_lora = False\n",
    "compute_grad = True; use_lora = True; lora_rank = 256; lora_alpha = lora_rank\n",
    "# compute_grad = True; use_lora = False\n",
    "\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v2_human_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=2; cpu_mem = 512\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    cmd = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --save_dir={save_dir} \\\n",
    "        --use_dist \\\n",
    "        --shuffle \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "datasets = []\n",
    "for dataset in os.listdir(processed_dir) + ['tulu_v1_human_mix', 'tulu_v2_human_mix']:\n",
    "    dataset_path = os.path.join(processed_dir, dataset)\n",
    "    save_path = os.path.join(save_dir, f'{dataset}.pkl')\n",
    "    if 'tulu'==dataset:\n",
    "        continue\n",
    "    if 'tulu' not in dataset and not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "    datasets.append(dataset)\n",
    "    \n",
    "datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554b1e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 17 21:59:50 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   40C    P0   244W / 300W |   8281MiB / 32510MiB |     57%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   44C    P0   264W / 300W |   8281MiB / 32510MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   45C    P0   291W / 300W |   8281MiB / 32510MiB |     76%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   41C    P0   248W / 300W |   8281MiB / 32510MiB |     58%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     35554      C   ...h/envs/nasref/bin/python3     8279MiB |\n",
      "|    1   N/A  N/A     35555      C   ...h/envs/nasref/bin/python3     8279MiB |\n",
      "|    2   N/A  N/A     35556      C   ...h/envs/nasref/bin/python3     8279MiB |\n",
      "|    3   N/A  N/A     35557      C   ...h/envs/nasref/bin/python3     8279MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "dataset = 'lima'\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "compute_grad = True\n",
    "use_lora = True\n",
    "lora_rank = 2\n",
    "lora_alpha = lora_rank\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1.4b'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1.4b'\n",
    "# model_name = 'pythia-160m-deduped'; model_name_or_path = 'EleutherAI/pythia-160m-deduped'\n",
    "\n",
    "\n",
    "save_dir = f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', 'tulu_v2_human_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd9434f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650e45d97fb740eab056c3e2749fe40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing lora(r=2,a=2)\n",
      "trainable params: 1048576 || all params: 6740512768 || trainable%: 0.02\n"
     ]
    }
   ],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if 'llama' in model_name_or_path:\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if compute_grad:\n",
    "    if 'llama' in model_name_or_path and not use_lora:\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-cc0d3b28308e6ae5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aceb99b077e420889b6b9b1beaff65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oasst1 dataset length = 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "\n",
    "encode_function = partial(\n",
    "    encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "\n",
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loraB': 'lora_B\\\\.[a-zA-Z_]+\\\\.weight'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2f99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local_rank/global=0/0] output=[('text_embedding', (1, 4096), dtype('float32')), ('log_prob', (1, 1), dtype('float32')), ('el2n_agg=mean', (1, 1), dtype('float32')), ('el2n_agg=l2n', (1, 1), dtype('float32')), ('logit_margin', (1, 1), dtype('float32')), ('grad_loraB_l2n', (1, 1), dtype('float32'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from note_llama_embeddings import compute_grad_statistic\n",
    "i = 0\n",
    "\n",
    "output = defaultdict(list)\n",
    "for batch in tqdm(loader, disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "    \n",
    "     # average of output token log probs\n",
    "    output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "    \n",
    "    # el2n scores\n",
    "    losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "    for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "        output[k].append(losses[k].detach().cpu())\n",
    "\n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "    \n",
    "    i += 1\n",
    "    if i == 300:\n",
    "        break\n",
    "    break\n",
    "        \n",
    "    \n",
    "for k, v in output.items():\n",
    "    output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    \n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da34cf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['text_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c002a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_embedding': array([[-1.0107422 ,  0.3317871 , -0.32666016, ...,  0.02334595,\n",
       "          0.48999023,  0.3395996 ],\n",
       "        [-1.1972656 ,  1.6523438 , -0.7988281 , ...,  0.22070312,\n",
       "          1.1552734 ,  0.8984375 ],\n",
       "        [-0.76123047, -0.5239258 ,  0.2758789 , ...,  1.1494141 ,\n",
       "          0.50878906,  0.15466309],\n",
       "        ...,\n",
       "        [-0.77246094,  0.8745117 , -0.36572266, ...,  0.27905273,\n",
       "          1.1367188 , -0.0027523 ],\n",
       "        [-1.1875    ,  0.27075195, -1.0205078 , ..., -0.29956055,\n",
       "          0.625     ,  0.04116821],\n",
       "        [-0.96435547,  0.26367188, -0.9785156 , ...,  0.25585938,\n",
       "          0.85595703,  0.39916992]], dtype=float32),\n",
       " 'log_prob': array([[-1.857992 ],\n",
       "        [-2.2157798],\n",
       "        [-1.5617336],\n",
       "        ...,\n",
       "        [-2.0540106],\n",
       "        [-1.8325413],\n",
       "        [-1.8836874]], dtype=float32),\n",
       " 'el2n_agg=mean': array([[0.60279465],\n",
       "        [0.66627216],\n",
       "        [0.5199953 ],\n",
       "        ...,\n",
       "        [0.6193907 ],\n",
       "        [0.62892884],\n",
       "        [0.59504163]], dtype=float32),\n",
       " 'el2n_agg=l2n': array([[14.990085],\n",
       "        [21.382166],\n",
       "        [14.278932],\n",
       "        ...,\n",
       "        [16.200666],\n",
       "        [14.664999],\n",
       "        [18.864405]], dtype=float32),\n",
       " 'logit_margin': array([[0.74559724],\n",
       "        [0.37695178],\n",
       "        [2.4927025 ],\n",
       "        ...,\n",
       "        [0.5752499 ],\n",
       "        [0.61999184],\n",
       "        [0.77086574]], dtype=float32),\n",
       " 'grad_all_l2n': array([[6.6466317],\n",
       "        [5.330539 ],\n",
       "        [7.0292325],\n",
       "        ...,\n",
       "        [6.262384 ],\n",
       "        [7.1088624],\n",
       "        [5.392297 ]], dtype=float32),\n",
       " 'grad_qkv_l2n': array([[3.4231753],\n",
       "        [3.1698806],\n",
       "        [4.256021 ],\n",
       "        ...,\n",
       "        [3.521213 ],\n",
       "        [3.891328 ],\n",
       "        [3.1992273]], dtype=float32),\n",
       " 'grad_mlp_l2n': array([[4.468009 ],\n",
       "        [3.2091637],\n",
       "        [4.648811 ],\n",
       "        ...,\n",
       "        [3.8274431],\n",
       "        [4.463747 ],\n",
       "        [3.211554 ]], dtype=float32),\n",
       " 'grad_last_l2n': array([[3.7513244],\n",
       "        [3.087605 ],\n",
       "        [3.4862404],\n",
       "        ...,\n",
       "        [3.726857 ],\n",
       "        [4.228278 ],\n",
       "        [3.1267066]], dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b/lima.pkl'\n",
    "with open(p, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02c0fba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885143061073219"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "scipy.stats.spearmanr(x['grad_all_l2n'], x['grad_qkv_l2n']).statistic\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "param_names = []\n",
    "grads = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        param_names.append(param_name)\n",
    "        grads.append(param.grad.to(torch.float32))\n",
    "\n",
    "data = []\n",
    "for param_name, grad in zip(param_names, grads):\n",
    "    data.append({\n",
    "        'param_name': param_name,\n",
    "        'shape': grad.shape,\n",
    "        'numel': grad.numel(),\n",
    "        'mean': grad.mean().detach().cpu().item(),\n",
    "        'norm': grad.norm().detach().cpu().item(),\n",
    "    })\n",
    "        \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# pattern = r'\\bquery_key_value\\.weight\\b'\n",
    "# # pattern = r'\\bmlp\\..*?\\.weight\\b'\n",
    "pattern = r'\\blora_B\\b'\n",
    "\n",
    "df = df[df['param_name'].apply(lambda x: True if re.search(pattern, x) else False)]\n",
    "df = df.sort_values(['norm'], ascending=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fullgrad = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_fullgrad.update({f'grad_loraB(r={lora_rank})_l2n': output['grad_loraB_l2n']})\n",
    "# del output_fullgrad['grad_loraB_l2n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import plt_kernel_matrix_one\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "metrics_name = ['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']\n",
    "metrics_name += [x for x in output_fullgrad if 'grad' in x and 'sum' not in x]\n",
    "metrics = [output_fullgrad[k] for k in metrics_name]\n",
    "any_isnan_mask = np.hstack([np.isnan(x) for x in metrics]).any(-1)\n",
    "metrics = [x[~any_isnan_mask] for x in metrics]\n",
    "N = len(metrics)\n",
    "K = np.zeros((N, N))\n",
    "for i, mi in enumerate(metrics):\n",
    "    for j, mj in enumerate(metrics):\n",
    "        s = stats.spearmanr(mi, mj)\n",
    "        K[i,j] = s.statistic\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt_kernel_matrix_one(\n",
    "    fig, ax, K, title=f'Spearmanr ({model_name}:{dataset})', \n",
    "    vmin=-1, vmax=1, cmap='bwr', n_ticks=N, annotate=True)\n",
    "ax.set_xticklabels(metrics_name, rotation=-45)\n",
    "ax.set_yticklabels(metrics_name)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ffeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
