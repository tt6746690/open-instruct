{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7ddd718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.sharegpt\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.sharegpt --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpljdnzfpz', 'job_id': 962317}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.open_orca\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.open_orca --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpi3rf1kc5', 'job_id': 962318}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.dolly\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.dolly --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpm4g4he3s', 'job_id': 962319}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.gpt4_alpaca\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.gpt4_alpaca --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp96tf7gdw', 'job_id': 962320}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.flan_v2\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.flan_v2 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp8yh7_uvd', 'job_id': 962321}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.wizardlm\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.wizardlm --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpsusi8yi4', 'job_id': 962322}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.super_ni\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.super_ni --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpgippsgty', 'job_id': 962323}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.stanford_alpaca\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.stanford_alpaca --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpq9h190l6', 'job_id': 962324}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.baize\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.baize --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpoz37838f', 'job_id': 962325}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.code_alpaca\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.code_alpaca --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmprla4nb_o', 'job_id': 962326}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.self_instruct\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.self_instruct --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmppp8t8tp6', 'job_id': 962327}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.unnatural_instructions\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.unnatural_instructions --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp3oautcrt', 'job_id': 962328}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs.oasst1\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs.oasst1 --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpfq8ngsqj', 'job_id': 962329}]\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_in_notebook\n",
    "from llm.submit import submit_job\n",
    "\n",
    "shell_scripts_template = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "master_port=10002\n",
    "RDZV_ENDPOINT=$master_addr:$master_port\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\"\n",
    "\n",
    "test_run = False\n",
    "model_name_or_path = \"../results/baselines/huggyllama/llama-7b\"\n",
    "save_dir = \"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/llama-7b_outputs/\"\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "\n",
    "for dataset in datasets:\n",
    "    cmd = f\"\"\"\n",
    "    torchrun --nnodes=1 --nproc_per_node=4 \\\n",
    "        --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=localhost:29400 \\\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset {dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --save_dir={save_dir} \\\n",
    "    \"\"\".strip()\n",
    "\n",
    "\n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        cmd=cmd, log_dir=log_dir, save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'compute_LM_outputs.{dataset}', \n",
    "        nodes=1,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=64,\n",
    "        num_gpus=6,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57b7802b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sharegpt',\n",
       " 'open_orca',\n",
       " 'dolly',\n",
       " 'gpt4_alpaca',\n",
       " 'flan_v2',\n",
       " 'wizardlm',\n",
       " 'super_ni',\n",
       " 'stanford_alpaca',\n",
       " 'baize',\n",
       " 'code_alpaca',\n",
       " 'self_instruct',\n",
       " 'unnatural_instructions',\n",
       " 'oasst1']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = []\n",
    "for dataset in os.listdir(processed_dir):\n",
    "    dataset_path = os.path.join(processed_dir, dataset)\n",
    "    save_path = os.path.join(save_dir, f'{dataset}.pkl')\n",
    "    if dataset in ['tulu'] or not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "    datasets.append(dataset)\n",
    "datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554b1e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-27 17:43:11,401] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "device = 'cuda'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "\n",
    "save_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/llama-7b_outputs'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='cuda:0',\n",
    "    torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de663e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datasets_shard_chunk_size(N, num_shards, index):\n",
    "    div = N // num_shards\n",
    "    mod = N % num_shards\n",
    "    start = div * index + min(index, mod)\n",
    "    end = start + div + (1 if index < mod else 0)\n",
    "    return end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'flan_v2'\n",
    "use_dist = False\n",
    "\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    world_size = torch.cuda.device_count()\n",
    "else:\n",
    "    rank = 0\n",
    "    world_size = 2\n",
    "\n",
    "device = f'cuda:{str(rank)}'\n",
    "\n",
    "train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "data_files = {'train': train_file}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "if test_run:\n",
    "    raw_datasets['train'] = raw_datasets['train'].select(range(10))\n",
    "print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "\n",
    "encode_function = partial(\n",
    "    encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "\n",
    "if rank == 0:\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) \n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "log_probs = []\n",
    "for batch in tqdm(loader, disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**batch, output_hidden_states=True)\n",
    "\n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "    # sum of output token log probs\n",
    "    log_prob = -outputs['loss']\n",
    "\n",
    "    text_embeddings.append(text_embedding.detach().cpu())\n",
    "    log_probs.append(log_prob.detach().cpu())\n",
    "\n",
    "text_embeddings = torch.vstack(text_embeddings).to(torch.float32)\n",
    "log_probs = torch.vstack(log_probs)\n",
    "\n",
    "\n",
    "chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "               for i in range(world_size)]\n",
    "\n",
    "def gather_2d_tensors(tensor):\n",
    "    D = tensor.shape[1]\n",
    "    tensor_list = [torch.zeros((B, D), dtype=torch.float32) \n",
    "                   for B in chunk_sizes]\n",
    "    if rank == 0:\n",
    "        dist.gather(tensor, gather_list=tensor_list, dst=0)\n",
    "    else:\n",
    "        dist.gather(tensor, gather_list=[], dst=0)\n",
    "    return tensor_list\n",
    "\n",
    "if use_dist:\n",
    "    text_embeddings = gather_2d_tensors(text_embeddings)\n",
    "    log_probs = gather_2d_tensors(log_probs)\n",
    "    \n",
    "\n",
    "if rank == 0:\n",
    "    output = {'text_embeddings': text_embeddings,\n",
    "              'log_probs': log_probs}\n",
    "    print([(k, v.shape) for k, v in output.items()])\n",
    "    if not test_run:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(output, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f99ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e561999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset.shard(num_shards=3, index=0, contiguous=True)['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddf1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shard(num_shards=3, index=1, contiguous=True)['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shard(num_shards=3, index=2, contiguous=True)['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
