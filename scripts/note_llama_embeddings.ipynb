{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.lima\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=1 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=lima --model_name_or_path=EleutherAI/pythia-410M-deduped --use_dist --shuffle --compute_loss --compute_grad --compute_grad_embeddings --save_grad_embeddings --grad_randproj_components 8192 --max_seq_len=2048 --encode_fn_type=sft --text_pooling_type=meanpool --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/sft/pythia-410M-deduped\n",
      "[{'args': 'sbatch --job-name=lm_outputs.lima --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpjp4ibss9', 'job_id': 1266178}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "nodes = 1; gpus=6\n",
    "use_lora = False\n",
    "text_pooling_type = 'meanpool'\n",
    "grad_randproj_components = 2048\n",
    "save_grad_embeddings = False\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "\n",
    "#### auto-regressive model, compute loss/embedding/grad\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'; max_seq_len = 2048\n",
    "# model_name = 'llama2-7b'; model_name_or_path = '../results/baselines/NousResearch/Llama-2-7b-hf'; max_seq_len = 2048\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'; max_seq_len = 2048\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'; max_seq_len = 2048\n",
    "model_name = 'pythia-410M-deduped'; model_name_or_path = 'EleutherAI/pythia-410M-deduped'; max_seq_len = 2048\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'; max_seq_len = 2048\n",
    "# model_name = 'mistral-7b+ultrachat200kv1'; model_name_or_path = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'; max_seq_len = 2048\n",
    "# model_name = 'llama-7b+lima'; model_name_or_path = '../results/oi2/llama-7b_lima_ep=5'; max_seq_len = 2048\n",
    "# model_name = 'codellama-7b'; model_name_or_path = '../results/baselines/codellama/CodeLlama-7b-hf'; max_seq_len = 2048\n",
    "\n",
    "compute_loss = True; compute_grad = True; encode_fn_type = 'sft' # 'sft'\n",
    "# use_lora = True;  lora_rank = 256; lora_alpha = lora_rank; grad_randproj_components = 2048; add_rsum = True\n",
    "use_lora = False; lora_rank = 256; lora_alpha = lora_rank; grad_randproj_components = 8192; add_rsum = False\n",
    "####\n",
    "\n",
    "# # encoder-based model, for computing embedding only\n",
    "# model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'; max_seq_len = 512; text_pooling_type = 'meanpool'\n",
    "# # model_name = 'bge-large-en-v1.5'; model_name_or_path = '../results/baselines/BAAI/bge-large-en-v1.5'; max_seq_len = 128; text_pooling_type = 'cls'\n",
    "\n",
    "# compute_loss = False; compute_grad = False; encode_fn_type = 'input'\n",
    "######\n",
    "\n",
    "##### \n",
    "# datasets = ['open_orca_slim']; nodes=5; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegptv2']; nodes=5; gpus=6; cpu_mem=512\n",
    "# datasets = ['tulu_v2']; nodes=10; gpu=6; cpu_mem=512\n",
    "# datasets = ['stanford_alpaca']; nodes=5; gpu=6; cpu_mem=512\n",
    "# datasets = ['flan_v2', 'oasst1']; nodes=1; gpu=6; cpu_mem=512\n",
    "# datasets = ['dolly']; nodes=1; gpu=6; cpu_mem=512\n",
    "\n",
    "#####\n",
    "# tulu_v1_mix:\n",
    "# datasets = ['flan_v2', 'cot', 'dolly', 'oasst1', 'gpt4_alpaca', 'code_alpaca', 'sharegpt']; nodes = 5; gpu=6; cpu_mem = 512\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v1_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# tulu_v2_mix:\n",
    "# datasets = ['tulu_v2']; nodes=1; gpus=1; cpu_mem=512\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1', 'gpt4_alpaca', 'code_alpaca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = ['wizardlm']; nodes = 3; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegpt', 'ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['ultrachat15']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = [f'ultrachat15_{i}' for i in [0, 2]]; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['starcoder_commentinstr_cleaned',\n",
    "#             'starcoder_commentinstr',]; nodes=1; gpus=6; cpu_mem=512\n",
    "# datasets = [\n",
    "#     'starcoder_commentinstrv2', # cleaned version\n",
    "# #     'starcoder_commentinstrv3',\n",
    "# ]; nodes = 3; gpus = 6; cpu_mem=512\n",
    "\n",
    "# datasets = ['starcoder_commentinstrv5']; nodes = 3; gpus = 6; cpu_mem=512\n",
    "\n",
    "# datasets = ['tulu_v2_human_mix', 'tulu_v2_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512 # if not compute grad embeddings\n",
    "# datasets = ['flan2022_1m']; nodes = 15; gpus=6; cpu_mem = 512 # if do compute grad embeddings\n",
    "# datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # for testing\n",
    "datasets = ['lima']; nodes = 1; gpus=6; cpu_mem = 512; save_grad_embeddings = True\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{encode_fn_type}/{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "\n",
    "use_dist = True if nodes*gpus>1 else False\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if use_dist:\n",
    "        prefix = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prefix = 'python'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "        {prefix}\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        {'--use_dist' if use_dist else ''} \\\n",
    "        --shuffle \\\n",
    "        {'--compute_loss' if compute_loss else ''} \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        --compute_grad_embeddings \\\n",
    "        {'--save_grad_embeddings' if save_grad_embeddings else ''} \\\n",
    "        --grad_randproj_components {grad_randproj_components} \\\n",
    "        --max_seq_len={max_seq_len} \\\n",
    "        --encode_fn_type={encode_fn_type} \\\n",
    "        --text_pooling_type={text_pooling_type} \\\n",
    "        {'--add_rsum' if add_rsum else ''} \\\n",
    "        --save_dir={save_dir} \\\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d836f4ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 23:25:33,239] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "rank/local_rank/world_size: 0/0/1\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Initializing lora(r=256,a=256)\n",
      "trainable params: 134217728 || all params: 7006851072 || trainable%: 1.92\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading and preparing dataset json/default to /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-75cf0d06f4677faa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4809.98it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Dataset json downloaded and prepared to /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-75cf0d06f4677faa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "open_orca_slim dataset length = 517982\n",
      "  0%|                                                | 0/517982 [00:00<?, ?it/s]Fitting random projection for loraB (134217728 -> 2048)\n",
      "Fitting random projection in 219.714s with random matrix size 189.759 MB\n",
      "Log statistics of projection matrix to ensure same initialization cross procs:\n",
      "8.629235890111886e-05, 2.3784141540527344, 2.8352906156214885e-06\n",
      "  0%|                                  | 163/517982 [06:13<130:42:02,  1.10it/s]^C\n",
      "  0%|                                  | 163/517982 [06:14<330:08:58,  2.30s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/note_llama_embeddings.py\", line 711, in <module>\n",
      "    compute_lm_outputs(**vars(args))\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/note_llama_embeddings.py\", line 566, in compute_lm_outputs\n",
      "    outputs['loss'].backward()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 204, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4 python note_llama_embeddings.py --dataset=open_orca_slim --model_name_or_path=../results/baselines/NousResearch/Llama-2-7b-hf --shuffle --compute_loss --compute_grad --use_lora --lora_rank=256 --lora_alpha=256 --compute_grad_embeddings --grad_randproj_components 2048 --max_seq_len=2048 --encode_fn_type=output --text_pooling_type=meanpool --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/output/llama2-7b+lora:r=256:a=256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554b1e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n",
      "1\n",
      "Wed Dec 27 12:25:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   38C    P0   131W / 300W |  15993MiB / 32510MiB |     60%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   35C    P0   132W / 300W |  26775MiB / 32510MiB |     45%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   38C    P0   124W / 300W |  15777MiB / 32510MiB |     56%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   38C    P0   127W / 300W |  15927MiB / 32510MiB |     58%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A    277874      C   python                          15991MiB |\n",
      "|    3   N/A  N/A    279932      C   .../open-instruct/bin/python    26773MiB |\n",
      "|    4   N/A  N/A    277831      C   python                          15775MiB |\n",
      "|    5   N/A  N/A    277832      C   python                          15925MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-27 12:25:48,141] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    encode_just_one_role,\n",
    "    sklearn_rp_mat_size,\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    gather_grad_embeddings,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_outputs/output/pythia-410M-deduped\n"
     ]
    }
   ],
   "source": [
    "test_run = True\n",
    "\n",
    "save_grad_embeddings = False\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat15_0'\n",
    "dataset = 'wizardlm'\n",
    "dataset = 'stanford_alpaca'\n",
    "dataset = 'lima'; save_grad_embeddings = True\n",
    "\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "lora_rank = 256\n",
    "lora_alpha = lora_rank\n",
    "grad_randproj_components = 2048\n",
    "encode_fn_type = 'input'  # input, output, sft\n",
    "encode_fn_type = 'output'\n",
    "max_seq_len = 2048\n",
    "text_pooling_type = 'meanpool'\n",
    "add_rsum = False\n",
    "\n",
    "#####\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = 'EleutherAI/pythia-1b-deduped'\n",
    "# model_name = 'pythia-70m-deduped'; model_name_or_path = 'EleutherAI/pythia-70m-deduped'\n",
    "model_name = 'pythia-410M-deduped'; model_name_or_path = 'EleutherAI/pythia-410M-deduped'\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "compute_grad_embeddings = True; use_lora = True; compute_grad=True; compute_loss = True\n",
    "compute_grad_embeddings = True; use_lora = False; compute_grad=True; compute_loss = True\n",
    "#####\n",
    "\n",
    "# #####\n",
    "# model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'\n",
    "# compute_grad_embeddings = False; use_lora = False; compute_grad = False; compute_loss = False\n",
    "# #####\n",
    "\n",
    "\n",
    "save_dir = f\"model_outputs/{encode_fn_type}/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', \n",
    "               'tulu_v1_mix',\n",
    "               'tulu_v2_human_mix',\n",
    "               'tulu_v2_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "33324799",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sentence-transformers' in model_name_or_path:\n",
    "    from transformers import AutoModel\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch.float16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405334016 || all params: 405334016 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # # the following also applies lora to MLP layers.\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if compute_grad:\n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2bd9434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "elif 'ultrachat' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'ultrachat', f'{dataset}_data.jsonl')\n",
    "elif 'open_orca' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'open_orca', f'{dataset}_data.jsonl')\n",
    "elif 'sharegpt' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'sharegpt', f'{dataset}_data.jsonl')\n",
    "elif 'starcoder' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'starcoder', f'{dataset}.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "if encode_fn_type in ['input', 'output']:\n",
    "    encode_function = partial(\n",
    "        encode_just_one_role,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        encode_fn_type=encode_fn_type,\n",
    "        # add eos token to causal models, e.g., llama, since its not added by default.\n",
    "        add_eos_token=False if any(y in model_name_or_path for y in ['mpnet', 'bge']) else True,\n",
    "    )\n",
    "elif encode_fn_type == 'sft':    \n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        add_eos_token=False,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f'encode_fn_type={encode_fn_type} not implemented.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2e051e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d88794e0f754204a49429931d103f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4dcb5011d833d828_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    # print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    # print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': '.*',\n",
       " 'qkv': '\\\\bquery_key_value\\\\.weight\\\\b',\n",
       " 'mlp': '\\\\bmlp\\\\..*?\\\\.weight\\\\b',\n",
       " 'last': '\\\\bembed_out\\\\.weight\\\\b'}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0e2f99ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random projection for qkv (75497472 -> 2048)\n",
      "Fitting random projection in 145.680s with random matrix size 142.364 MB\n",
      "Log statistics of projection matrix for qkv to ensure same initialization cross procs:\n",
      "0.00011509272735564052, 2.059767246246338, -3.1920640140015166e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [02:26<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from note_llama_embeddings import mean_pooling\n",
    "\n",
    "if compute_grad_embeddings:\n",
    "    rps = {}\n",
    "    for k in grad_statistic_patterns.keys():\n",
    "        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "\n",
    "output = defaultdict(list)\n",
    "for i, batch in tqdm(enumerate(loader), disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    last_hidden_state = outputs['hidden_states'][-1]\n",
    "    if text_pooling_type == 'meanpool':\n",
    "        text_embedding = mean_pooling(last_hidden_state, batch['attention_mask'])\n",
    "    elif text_pooling_type == 'cls':\n",
    "        text_embedding = last_hidden_state[:, 0]\n",
    "    else:\n",
    "        raise ValueError(f'text_pooling_type={text_pooling_type} not supported.')\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "\n",
    "    if compute_loss:\n",
    "        # average of output token log probs\n",
    "        if 'loss' in outputs:\n",
    "            output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "\n",
    "        # el2n scores\n",
    "        losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "        for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "            output[k].append(losses[k].detach().cpu())\n",
    "            \n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "            \n",
    "        if compute_grad_embeddings:\n",
    "            grad_embeddings = gather_grad_embeddings(\n",
    "                model,\n",
    "                {k: v for k, v in grad_statistic_patterns.items() if k in ['qkv', 'loraB']},\n",
    "                stacked=True,\n",
    "                add_rsum=add_rsum,\n",
    "            )\n",
    "            if save_grad_embeddings:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    output[f'grad_{k}'].append(v)\n",
    "            if i==0:\n",
    "                rps = {}\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    t0 = time.time()\n",
    "                    if 'rsum' not in k:\n",
    "                        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "                    else:\n",
    "                        rps[k] = GaussianRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "                    print(f\"Fitting random projection for {k} ({v.size} -> {grad_randproj_components})\")\n",
    "                    rps[k] = rps[k].fit(v[np.newaxis,...])\n",
    "                    print(f\"Fitting random projection in {time.time() - t0:0.3f}s \"\n",
    "                        f\"with random matrix size {sklearn_rp_mat_size(rps[k]) / 1e6:0.3f} MB\")\n",
    "                    print(f'Log statistics of projection matrix for {k} to ensure same initialization cross procs:\\n'\n",
    "                        f\"{np.mean(rps[k].components_ != 0)}, {np.max(rps[k].components_)}, {np.mean(rps[k].components_[0])}\")\n",
    "            for k in grad_embeddings.keys():\n",
    "                rp = rps[k]\n",
    "                g = grad_embeddings[k]\n",
    "                output[f'grad_rp_{k}'].append(rp.transform(g[np.newaxis,...]).squeeze())\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4c8fdb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local_rank/global=0/0] output=[('text_embedding', (1, 1024), dtype('float32')), ('log_prob', (1, 1), dtype('float16')), ('el2n_agg=mean', (1, 1), dtype('float16')), ('el2n_agg=l2n', (1, 1), dtype('float16')), ('logit_margin', (1, 1), dtype('float16')), ('grad_all_l2n', (1, 1), dtype('float32')), ('grad_qkv_l2n', (1, 1), dtype('float32')), ('grad_mlp_l2n', (1, 1), dtype('float32')), ('grad_last_l2n', (1, 1), dtype('float32')), ('grad_qkv', (1, 75497472), dtype('float32')), ('grad_rp_qkv', (1, 2048), dtype('float32'))]\n"
     ]
    }
   ],
   "source": [
    "for k, v in output.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    else:\n",
    "        output[k] = np.vstack(v)\n",
    "\n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'model_outputs/llama-7b+lora:r=256:a=256/lima.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for k in grad_embeddings.keys():\n",
    "    rp = rps[k]\n",
    "    g = grad_embeddings[k]\n",
    "    rp.transform(g[np.newaxis,...]).squeeze()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d608989",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'note_llama_embeddings_{dataset}:{model_name}'+\\\n",
    "    (f'+lora(r={lora_rank},a={lora_alpha})' if use_lora else '')+'.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(output, f)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o1 = pickle.load(f)\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=1,a=1).pkl'\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=256,a=256).pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o2 = pickle.load(f)\n",
    "    \n",
    "output = {}\n",
    "for k in ['grad_qkv', 'grad_last', 'grad_rp_qkv', 'grad_rp_last']:\n",
    "    output[k] = o1[k]\n",
    "for k in ['grad_loraB', 'grad_rp_loraB']:\n",
    "    output[k] = o2[k]\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd8a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-27 15:45:52,609] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Computing distance for grad_loraB of size (1030, 18874368)\n",
      "set chunk_size = 85 for (N, D) = (1030, 18874368)\n",
      "Computing distance for grad_loraBrsum of size (1030, 6144)\n",
      "set chunk_size = 1030 for (N, D) = (1030, 6144)\n",
      "Computing distance for grad_loraBrsumcyc of size (1030, 6144)\n",
      "set chunk_size = 1030 for (N, D) = (1030, 6144)\n",
      "Computing distance for grad_rp_loraB of size (1030, 2048)\n",
      "set chunk_size = 1030 for (N, D) = (1030, 2048)\n",
      "Computing distance for grad_rp_loraBrsum of size (1030, 2048)\n",
      "set chunk_size = 1030 for (N, D) = (1030, 2048)\n",
      "Computing distance for grad_rp_loraBrsumcyc of size (1030, 2048)\n",
      "set chunk_size = 1030 for (N, D) = (1030, 2048)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from note_llama_embeddings import torch_cdist_chunked, plt_pair_of_dists\n",
    "\n",
    "\n",
    "model_name = 'pythia-410M-deduped+lora:r=256:a=256'\n",
    "dataset = 'lima'\n",
    "\n",
    "Ds = {}\n",
    "save_path = os.path.join('model_outputs', 'sft', model_name, f'{dataset}.pkl')\n",
    "with open(save_path, 'rb') as f:\n",
    "    o = pickle.load(f)\n",
    "    for k in [x for x in o.keys() if x.startswith('grad') and 'l2n' not in x]:\n",
    "        print(f'Computing distance for {k} of size {o[k].shape}')\n",
    "        Ds[k] = torch_cdist_chunked(o[k], device='cuda', chunk_size='auto', mem_total=12)\n",
    "    del o\n",
    "    \n",
    "save_path = os.path.join('model_outputs', 'sft', model_name, f'{dataset}_dist.pkl')\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(Ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance for grad_qkv of size (1030, 75497472)\n",
      "set chunk_size = 21 for (N, D) = (1030, 75497472)\n"
     ]
    }
   ],
   "source": [
    "Ds = {}\n",
    "for k in [x for x in o.keys() if x.startswith('grad') and 'l2n' not in x]:\n",
    "    print(f'Computing distance for {k} of size {o[k].shape}')\n",
    "    Ds[k] = torch_cdist_chunked(o[k], device='cuda', chunk_size='auto', mem_total=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'pythia-410M'\n",
    "dataset = 'lima'\n",
    "save_path = os.path.join('model_outputs', 'sft', model_name, f'{dataset}_dist.pkl')\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(Ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c89fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = os.path.join('model_outputs', 'sft', 'pythia-410M-deduped', 'lima.pkl')\n",
    "with open(save_path, 'rb') as f:\n",
    "    o = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f907a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_qkv (1030, 75497472)\n",
      "grad_rp_qkv (1030, 8192)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    for k in [x for x in o.keys() if x.startswith('grad') and 'l2n' not in x]:\n",
    "        print(f'Computing distance for {k} of size {o[k].shape}')\n",
    "        Ds[k] = torch_cdist_chunked(o[k], device='cuda', chunk_size='auto', mem_total=12)D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325e0c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grad_loraB': array([[  0.48663786, 196.28995   , 116.134636  , ..., 114.07352   ,\n",
       "         118.03757   , 200.18613   ],\n",
       "        [196.28995   ,   1.5334805 , 202.33885   , ..., 213.8344    ,\n",
       "         213.30598   , 257.0312    ],\n",
       "        [116.134636  , 202.33885   ,   0.8244316 , ..., 136.88992   ,\n",
       "         137.33588   , 204.2138    ],\n",
       "        ...,\n",
       "        [114.073494  , 213.83441   , 136.88991   , ...,   0.        ,\n",
       "         145.2757    , 213.38147   ],\n",
       "        [118.03756   , 213.30603   , 137.33586   , ..., 145.2757    ,\n",
       "           0.        , 216.55844   ],\n",
       "        [200.18614   , 257.03116   , 204.2138    , ..., 213.38147   ,\n",
       "         216.55844   ,   0.        ]], dtype=float32),\n",
       " 'grad_loraBrsum': array([[0.00000000e+00, 2.29321045e+02, 1.15307007e+02, ...,\n",
       "         1.07796211e+02, 9.17635651e+01, 1.73650650e+02],\n",
       "        [2.29321045e+02, 0.00000000e+00, 2.59919891e+02, ...,\n",
       "         2.53984772e+02, 2.36575714e+02, 2.79136566e+02],\n",
       "        [1.15307007e+02, 2.59919891e+02, 4.41941731e-02, ...,\n",
       "         1.27685356e+02, 1.20814430e+02, 1.89931046e+02],\n",
       "        ...,\n",
       "        [1.07796211e+02, 2.53984772e+02, 1.27685356e+02, ...,\n",
       "         8.83883461e-02, 1.11073730e+02, 1.97719482e+02],\n",
       "        [9.17635651e+01, 2.36575699e+02, 1.20814430e+02, ...,\n",
       "         1.11073730e+02, 9.88211781e-02, 1.84588242e+02],\n",
       "        [1.73650650e+02, 2.79136597e+02, 1.89931046e+02, ...,\n",
       "         1.97719482e+02, 1.84588242e+02, 1.87500000e-01]], dtype=float32),\n",
       " 'grad_loraBrsumcyc': array([[4.41941731e-02, 1.91827545e+02, 1.17420235e+02, ...,\n",
       "         1.12797295e+02, 1.19326523e+02, 2.01848892e+02],\n",
       "        [1.91827545e+02, 0.00000000e+00, 2.02475845e+02, ...,\n",
       "         2.12486313e+02, 2.10014862e+02, 2.53295929e+02],\n",
       "        [1.17420235e+02, 2.02475845e+02, 1.25000000e-01, ...,\n",
       "         1.38722916e+02, 1.35295380e+02, 2.02187912e+02],\n",
       "        ...,\n",
       "        [1.12797295e+02, 2.12486313e+02, 1.38722916e+02, ...,\n",
       "         2.07289055e-01, 1.44609543e+02, 2.11745377e+02],\n",
       "        [1.19326523e+02, 2.10014862e+02, 1.35295380e+02, ...,\n",
       "         1.44609543e+02, 0.00000000e+00, 2.12480316e+02],\n",
       "        [2.01848892e+02, 2.53295929e+02, 2.02187912e+02, ...,\n",
       "         2.11745377e+02, 2.12480316e+02, 0.00000000e+00]], dtype=float32),\n",
       " 'grad_rp_loraB': array([[7.9672180e-02, 1.9911594e+02, 1.1621076e+02, ..., 1.1486511e+02,\n",
       "         1.1602427e+02, 1.9978410e+02],\n",
       "        [1.9911592e+02, 2.5000000e-01, 2.0148135e+02, ..., 2.1338860e+02,\n",
       "         2.1505441e+02, 2.5695358e+02],\n",
       "        [1.1621076e+02, 2.0148135e+02, 8.8388346e-02, ..., 1.3193080e+02,\n",
       "         1.3789348e+02, 2.0437373e+02],\n",
       "        ...,\n",
       "        [1.1486512e+02, 2.1338860e+02, 1.3193080e+02, ..., 0.0000000e+00,\n",
       "         1.4492079e+02, 2.0980795e+02],\n",
       "        [1.1602427e+02, 2.1505441e+02, 1.3789348e+02, ..., 1.4492079e+02,\n",
       "         0.0000000e+00, 2.1657193e+02],\n",
       "        [1.9978410e+02, 2.5695358e+02, 2.0437373e+02, ..., 2.0980795e+02,\n",
       "         2.1657193e+02, 1.7677669e-01]], dtype=float32),\n",
       " 'grad_rp_loraBrsum': array([[4.9410589e-02, 2.3039697e+02, 1.1870129e+02, ..., 1.0937551e+02,\n",
       "         9.1823822e+01, 1.7752182e+02],\n",
       "        [2.3039697e+02, 1.2500000e-01, 2.6085901e+02, ..., 2.5902560e+02,\n",
       "         2.3704323e+02, 2.8366586e+02],\n",
       "        [1.1870129e+02, 2.6085901e+02, 9.8821178e-02, ..., 1.2933179e+02,\n",
       "         1.2165880e+02, 1.9697905e+02],\n",
       "        ...,\n",
       "        [1.0937551e+02, 2.5902563e+02, 1.2933179e+02, ..., 0.0000000e+00,\n",
       "         1.1289554e+02, 2.0215907e+02],\n",
       "        [9.1823822e+01, 2.3704323e+02, 1.2165880e+02, ..., 1.1289554e+02,\n",
       "         0.0000000e+00, 1.8781277e+02],\n",
       "        [1.7752182e+02, 2.8366586e+02, 1.9697905e+02, ..., 2.0215907e+02,\n",
       "         1.8781277e+02, 6.2500000e-02]], dtype=float32),\n",
       " 'grad_rp_loraBrsumcyc': array([[0.00000000e+00, 1.96466476e+02, 1.16387794e+02, ...,\n",
       "         1.11060379e+02, 1.19987152e+02, 1.99645172e+02],\n",
       "        [1.96466476e+02, 2.79508501e-01, 2.06286957e+02, ...,\n",
       "         2.13614578e+02, 2.12605682e+02, 2.56882507e+02],\n",
       "        [1.16387794e+02, 2.06286957e+02, 8.83883461e-02, ...,\n",
       "         1.36432846e+02, 1.35769806e+02, 2.02316452e+02],\n",
       "        ...,\n",
       "        [1.11060379e+02, 2.13614563e+02, 1.36432846e+02, ...,\n",
       "         0.00000000e+00, 1.43964615e+02, 2.08287094e+02],\n",
       "        [1.19987152e+02, 2.12605682e+02, 1.35769806e+02, ...,\n",
       "         1.43964615e+02, 7.65465572e-02, 2.12471848e+02],\n",
       "        [1.99645187e+02, 2.56882507e+02, 2.02316452e+02, ...,\n",
       "         2.08287094e+02, 2.12471848e+02, 0.00000000e+00]], dtype=float32)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = 'lima'; model_name = 'pythia-410M-deduped+lora:r=256:a=256'\n",
    "\n",
    "for k in D.keys():\n",
    "    if k == 'grad_qkv':\n",
    "        continue\n",
    "    fig, axs = plt_pair_of_dists(\n",
    "        D['grad_qkv'], \n",
    "        dists[k]/(dists[k].mean()/dists['grad_qkv'].mean()), \n",
    "        n_components,\n",
    "        use_hexbin=False)\n",
    "    fig.suptitle(f'{dataset}:{model_name} grad_qkv vs. {k}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from note_llama_embeddings import torch_cdist\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_components = 2048\n",
    "k = 'qkv'\n",
    "# ['qkv', 'last', 'rp_qkv', 'rp_last', 'loraB', 'rp_loraB']\n",
    "\n",
    "device = 'cuda'\n",
    "data = output[f'grad_qkv']\n",
    "projected_data = output[f'grad_rp_qkv']\n",
    "\n",
    "\n",
    "dists = {}\n",
    "ks = output.keys()\n",
    "ks = ['grad_loraB', 'grad_rp_loraB']\n",
    "\n",
    "for k in ks:\n",
    "    if 'last' in k:\n",
    "        continue\n",
    "    data = output[k]\n",
    "#     data = torch.from_numpy(data).to(torch.float32).to('cuda')\n",
    "#     D = torch_cdist(data, device).ravel()\n",
    "    D = scipy.spatial.distance.pdist(data)\n",
    "#     valid_dist = np.logical_and(~np.isnan(dists), dists!=0)\n",
    "#     dists = dists[valid_dist]\n",
    "#     print(f'percent valid: {100*np.sum(valid_dist)/dists.size:.2f}')\n",
    "    dists[k] = D\n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "# rp = SparseRandomProjection(n_components=n_components)  \n",
    "# projected_data = rp.fit_transform(output[f'grad_{k}'])\n",
    "# print(\n",
    "#     f\"Projected in\"\n",
    "#     f\"{time.time() - t0:0.3f}s\"\n",
    "# )\n",
    "# if hasattr(rp, \"components_\"):\n",
    "#     n_bytes = rp.components_.data.nbytes\n",
    "#     n_bytes += rp.components_.indices.nbytes\n",
    "#     print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n",
    "\n",
    "# projected_dists = torch_cdist(projected_data, device).ravel()\n",
    "# del projected_data\n",
    "# projected_dists = projected_dists[valid_dist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e5211013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function note_llama_embeddings.plt_pair_of_dists(D1, D2, n_components, use_hexbin=True)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from note_llama_embeddings import torch_cdist, plt_pair_of_dists\n",
    "\n",
    "plt_pair_of_dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "from note_llama_embeddings import torch_cdist, plt_pair_of_dists\n",
    "\n",
    "dataset = 'lima'; model_name = 'pythia-160m-deduped+lora(rank=256,alpha=256)'\n",
    "\n",
    "for k in dists.keys():\n",
    "    if k == 'grad_qkv':\n",
    "        continue\n",
    "    fig, axs = plt_pair_of_dists(\n",
    "        dists['grad_qkv'], \n",
    "        dists[k]/(dists[k].mean()/dists['grad_qkv'].mean()), \n",
    "        n_components,\n",
    "        use_hexbin=False)\n",
    "    fig.suptitle(f'{dataset}:{model_name} grad_qkv vs. {k}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists['grad_loraB'].mean()/dists['grad_qkv'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "param_names = []\n",
    "grads = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        param_names.append(param_name)\n",
    "        grads.append(param.grad.to(torch.float32))\n",
    "\n",
    "data = []\n",
    "for param_name, grad in zip(param_names, grads):\n",
    "    data.append({\n",
    "        'param_name': param_name,\n",
    "        'shape': grad.shape,\n",
    "        'numel': grad.numel(),\n",
    "        'mean': grad.mean().detach().cpu().item(),\n",
    "        'norm': grad.norm().detach().cpu().item(),\n",
    "    })\n",
    "        \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# pattern = r'\\bquery_key_value\\.weight\\b'\n",
    "# # pattern = r'\\bmlp\\..*?\\.weight\\b'\n",
    "pattern = r'\\blora_B\\b'\n",
    "\n",
    "df = df[df['param_name'].apply(lambda x: True if re.search(pattern, x) else False)]\n",
    "df = df.sort_values(['norm'], ascending=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fullgrad = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_fullgrad.update({f'grad_loraB(r={lora_rank})_l2n': output['grad_loraB_l2n']})\n",
    "# del output_fullgrad['grad_loraB_l2n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import plt_kernel_matrix_one\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "metrics_name = ['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']\n",
    "metrics_name += [x for x in output_fullgrad if 'grad' in x and 'sum' not in x]\n",
    "metrics = [output_fullgrad[k] for k in metrics_name]\n",
    "any_isnan_mask = np.hstack([np.isnan(x) for x in metrics]).any(-1)\n",
    "metrics = [x[~any_isnan_mask] for x in metrics]\n",
    "N = len(metrics)\n",
    "K = np.zeros((N, N))\n",
    "for i, mi in enumerate(metrics):\n",
    "    for j, mj in enumerate(metrics):\n",
    "        s = stats.spearmanr(mi, mj)\n",
    "        K[i,j] = s.statistic\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt_kernel_matrix_one(\n",
    "    fig, ax, K, title=f'Spearmanr ({model_name}:{dataset})', \n",
    "    vmin=-1, vmax=1, cmap='bwr', n_ticks=N, annotate=True)\n",
    "ax.set_xticklabels(metrics_name, rotation=-45)\n",
    "ax.set_yticklabels(metrics_name)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ffeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
