{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf5a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"compute_LM_outputs\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "[{'args': 'sbatch --job-name=compute_LM_outputs --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=64GB --gres=gpu:1 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpu4cqh430', 'job_id': 960366}]\n"
     ]
    }
   ],
   "source": [
    "from llm.submit import submit_job\n",
    "\n",
    "shell_scripts = \"\"\"\n",
    "echo \"Running on $SLURM_JOB_NODELIST\"\n",
    "echo \"======\"\n",
    "\n",
    "source ~/.profile\n",
    "conda activate open-instruct\n",
    "cd /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts\n",
    "\n",
    "set -e\n",
    "set -x\n",
    "echo \"======\"\n",
    "srun {cmd}\n",
    "\n",
    "[ ! -f \"{log_dir}/$SLURM_JOB_ID*.out\" ] && mv {log_dir}/$SLURM_JOB_ID*.out {save_dir}\n",
    "\"\"\".format(\n",
    "    cmd='python note_llama_embeddings.py',\n",
    "    log_dir='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/',\n",
    "    save_dir='/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/llama-7b_outputs/',\n",
    ")\n",
    "\n",
    "test_run = False\n",
    "\n",
    "out = submit_job(\n",
    "    shell_scripts, \n",
    "    job_name='compute_LM_outputs', \n",
    "    nodes=1,\n",
    "    num_cpus=32,\n",
    "    cpu_mem=64,\n",
    "    num_gpus=1,\n",
    "    gpu_type='v100',\n",
    "    test_run=test_run,\n",
    "    job_duration=6,\n",
    ")\n",
    "if not test_run:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c062e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25fd4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7ed47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = False\n",
    "device = 'cuda'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "\n",
    "save_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/llama-7b_outputs'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e918fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038aed034e574491983031ad54fe9f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b01a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset in os.listdir(processed_dir):\n",
    "    dataset_path = os.path.join(processed_dir, dataset)\n",
    "    if dataset in ['tulu'] or not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "    assert(os.path.isfile(train_file))\n",
    "    \n",
    "    s = time.time()\n",
    "    \n",
    "    data_files = {'train': train_file}\n",
    "    raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(100))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "\n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "    train_dataset = lm_datasets['train']\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", output_all_columns=False, columns=['input_ids', 'labels', 'attention_mask'])\n",
    "    loader = DataLoader(train_dataset, shuffle=False, batch_size=1) \n",
    "\n",
    "\n",
    "    text_embeddings = []\n",
    "    log_probs = []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        batch = {k: v.to('cuda', non_blocking=True) for k, v in batch.items()}\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "\n",
    "        # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "        text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "        # sum of output token log probs\n",
    "        log_prob = -outputs['loss']\n",
    "\n",
    "        text_embeddings.append(text_embedding.detach().cpu().numpy().astype(np.float32))\n",
    "        log_probs.append(log_prob.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    output = {'text_embeddings': np.vstack(text_embeddings),\n",
    "              'log_probs': np.vstack(log_probs)}\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'{dataset}.pkl')\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(output, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    e = time.time()\n",
    "    print(f\"Finished computing embedding/logprob for {dataset} in {e-s:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44ce25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbd344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffca10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
