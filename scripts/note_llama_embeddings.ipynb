{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.ultrachat15_0\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=ultrachat15_0 --model_name_or_path=../results/baselines/mistralai/Mistral-7B-v0.1 --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/mistral-7b+lora:r=256:a=256 --use_dist --shuffle --compute_grad --use_lora --lora_rank=256 --lora_alpha=256 --compute_grad_embeddings --grad_randproj_components 2048\n",
      "[{'args': 'sbatch --job-name=lm_outputs.ultrachat15_0 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpuxzy203x', 'job_id': 1164288}]\n",
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.ultrachat15_2\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=ultrachat15_2 --model_name_or_path=../results/baselines/mistralai/Mistral-7B-v0.1 --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/mistral-7b+lora:r=256:a=256 --use_dist --shuffle --compute_grad --use_lora --lora_rank=256 --lora_alpha=256 --compute_grad_embeddings --grad_randproj_components 2048\n",
      "[{'args': 'sbatch --job-name=lm_outputs.ultrachat15_2 --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpperuwn0d', 'job_id': 1164289}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "lora_rank = 256; lora_alpha = lora_rank\n",
    "nodes = 1; gpus=6\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'\n",
    "model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "# model_name = 'mistral-7b+ultrachat200kv1'; model_name_or_path = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'\n",
    "\n",
    "\n",
    "compute_grad = True; use_lora = True; lora_rank = 256; lora_alpha = lora_rank\n",
    "# compute_grad = True; use_lora = False\n",
    "\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1', 'gpt4_alpaca', 'code_alpaca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = ['wizardlm', 'open_orca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = ['sharegpt', 'ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "datasets = [f'ultrachat15_{i}' for i in [0, 2]]; nodes = 5; gpus=6; cpu_mem = 512\n",
    "#. range(10)\n",
    "\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v1_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['tulu_v2_human_mix', 'tulu_v2_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512 # if not compute grad embeddings\n",
    "# datasets = ['flan2022_1m']; nodes = 15; gpus=6; cpu_mem = 512 # if do compute grad embeddings\n",
    "# datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    cmd = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --save_dir={save_dir} \\\n",
    "        --use_dist \\\n",
    "        --shuffle \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        --compute_grad_embeddings \\\n",
    "        --grad_randproj_components 2048\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390e381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_rank, lora_alpha = 256,256\n",
    "target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    bias='none',\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha, \n",
    "    lora_dropout=0.,\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "# https://github.com/huggingface/peft/issues/137\n",
    "model.enable_input_require_grads()\n",
    "model = get_peft_model(model, peft_config)\n",
    "from note_llama_embeddings import print_trainable_parameters\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    sklearn_rp_mat_size,\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    gather_grad_embeddings,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat15_0'\n",
    "\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "compute_grad = True\n",
    "use_lora = True\n",
    "lora_rank = 256\n",
    "lora_alpha = lora_rank\n",
    "compute_grad_embeddings = True\n",
    "grad_randproj_components = 2048\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = 'EleutherAI/pythia-1b-deduped'\n",
    "# model_name = 'pythia-160m-deduped'; model_name_or_path = 'EleutherAI/pythia-160m-deduped'\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "\n",
    "save_dir = f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', \n",
    "               'tulu_v1_mix',\n",
    "               'tulu_v2_human_mix',\n",
    "               'tulu_v2_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # # the following also applies lora to MLP layers.\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if compute_grad:\n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "\n",
    "encode_function = partial(\n",
    "    encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "\n",
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f99ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if compute_grad_embeddings:\n",
    "    rps = {}\n",
    "    for k in grad_statistic_patterns.keys():\n",
    "        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "\n",
    "output = defaultdict(list)\n",
    "for i, batch in tqdm(enumerate(loader), disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "    \n",
    "     # average of output token log probs\n",
    "    output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "    \n",
    "    # el2n scores\n",
    "    losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "    for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "        output[k].append(losses[k].detach().cpu())\n",
    "\n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "            \n",
    "        if compute_grad_embeddings:\n",
    "            grad_embeddings = gather_grad_embeddings(\n",
    "                model,\n",
    "                {k: v for k, v in grad_statistic_patterns.items() if k in ['qkv', 'loraB']},\n",
    "                stacked=True,\n",
    "            )\n",
    "            if test_run:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    output[f'grad_{k}'].append(v)\n",
    "            if i==0:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    t0 = time.time()\n",
    "                    print(f\"Fitting random projection for {k} ({v.size} -> {grad_randproj_components})\")\n",
    "                    rps[k] = rps[k].fit(v[np.newaxis,...])\n",
    "                    print(f\"Fitting random projection in {time.time() - t0:0.3f}s \"\n",
    "                          f\"with random matrix size {sklearn_rp_mat_size(rps[k]) / 1e6:0.3f} MB\")\n",
    "            for k in grad_embeddings.keys():\n",
    "                rp = rps[k]\n",
    "                g = grad_embeddings[k]\n",
    "                output[f'grad_rp_{k}'].append(rp.transform(g[np.newaxis,...]).squeeze())\n",
    "\n",
    "    i += 1\n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "    \n",
    "for k, v in output.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    else:\n",
    "        output[k] = np.vstack(v)\n",
    "\n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'model_outputs/llama-7b+lora:r=256:a=256/lima.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for k in grad_embeddings.keys():\n",
    "    rp = rps[k]\n",
    "    g = grad_embeddings[k]\n",
    "    rp.transform(g[np.newaxis,...]).squeeze()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d608989",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'note_llama_embeddings_{dataset}:{model_name}'+\\\n",
    "    (f'+lora(r={lora_rank},a={lora_alpha})' if use_lora else '')+'.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(output, f)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o1 = pickle.load(f)\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=1,a=1).pkl'\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=256,a=256).pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o2 = pickle.load(f)\n",
    "    \n",
    "output = {}\n",
    "for k in ['grad_qkv', 'grad_last', 'grad_rp_qkv', 'grad_rp_last']:\n",
    "    output[k] = o1[k]\n",
    "for k in ['grad_loraB', 'grad_rp_loraB']:\n",
    "    output[k] = o2[k]\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4759b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np rp\n",
    "# 10%|▉         | 99/1000 [00:22<03:24,  4.40it/s]\n",
    "\n",
    "# +rp for qkv/last\n",
    "#  10%|▉         | 99/1000 [00:50<07:35,  1.98it/s]\n",
    "\n",
    "# +rp for qkv 21,233,664 -> 2048\n",
    "# 10%|▉         | 99/1000 [03:50<34:56,  2.33s/it]  \n",
    "\n",
    "# +rp for lora(rank=1) qkv. 27,648 -> 2048\n",
    "# 10%|▉         | 99/1000 [00:10<01:34,  9.58it/s]\n",
    "\n",
    "# +rp for lora(rank=256) qkv (7,077,888 -> 2048)\n",
    "# 10%|▉         | 99/1000 [00:57<08:41,  1.73it/s] \n",
    "\n",
    "# (134,217,728 -> 2048) 180MB random matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a688cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,np.stack(v).shape, np.stack(v).size) for k,v in grad_embeddings.items()]\n",
    "# Pythia-160m: [('qkv', (12, 2304, 768)), ('last', (1, 50304, 768))]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75687add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n =1\n",
    "\n",
    "projected_data = {}\n",
    "rp_transform = torch.compile(rp.transform)\n",
    "for n in [1, 10, 100]:\n",
    "    t0 = time()\n",
    "    projected_data[n] = rp_transform(data[:n])\n",
    "\n",
    "    print(f\"Projected {n} samples from {n_features} to {n_components} in \"\n",
    "          f\"{time() - t0:0.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(projected_data)) / projected_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from note_llama_embeddings import torch_cdist\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_components = 2048\n",
    "k = 'qkv'\n",
    "# ['qkv', 'last', 'rp_qkv', 'rp_last', 'loraB', 'rp_loraB']\n",
    "\n",
    "device = 'cuda'\n",
    "data = output[f'grad_qkv']\n",
    "projected_data = output[f'grad_rp_qkv']\n",
    "\n",
    "\n",
    "dists = {}\n",
    "ks = output.keys()\n",
    "ks = ['grad_loraB', 'grad_rp_loraB']\n",
    "\n",
    "for k in ks:\n",
    "    if 'last' in k:\n",
    "        continue\n",
    "    data = output[k]\n",
    "#     data = torch.from_numpy(data).to(torch.float32).to('cuda')\n",
    "#     D = torch_cdist(data, device).ravel()\n",
    "    D = scipy.spatial.distance.pdist(data)\n",
    "#     valid_dist = np.logical_and(~np.isnan(dists), dists!=0)\n",
    "#     dists = dists[valid_dist]\n",
    "#     print(f'percent valid: {100*np.sum(valid_dist)/dists.size:.2f}')\n",
    "    dists[k] = D\n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "# rp = SparseRandomProjection(n_components=n_components)  \n",
    "# projected_data = rp.fit_transform(output[f'grad_{k}'])\n",
    "# print(\n",
    "#     f\"Projected in\"\n",
    "#     f\"{time.time() - t0:0.3f}s\"\n",
    "# )\n",
    "# if hasattr(rp, \"components_\"):\n",
    "#     n_bytes = rp.components_.data.nbytes\n",
    "#     n_bytes += rp.components_.indices.nbytes\n",
    "#     print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n",
    "\n",
    "# projected_dists = torch_cdist(projected_data, device).ravel()\n",
    "# del projected_data\n",
    "# projected_dists = projected_dists[valid_dist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5211013",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "from note_llama_embeddings import torch_cdist, plt_pair_of_dists\n",
    "\n",
    "dataset = 'lima'; model_name = 'pythia-160m-deduped+lora(rank=256,alpha=256)'\n",
    "\n",
    "for k in dists.keys():\n",
    "    if k == 'grad_qkv':\n",
    "        continue\n",
    "    fig, axs = plt_pair_of_dists(\n",
    "        dists['grad_qkv'], \n",
    "        dists[k]/(dists[k].mean()/dists['grad_qkv'].mean()), \n",
    "        n_components,\n",
    "        use_hexbin=False)\n",
    "    fig.suptitle(f'{dataset}:{model_name} grad_qkv vs. {k}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists['grad_loraB'].mean()/dists['grad_qkv'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "param_names = []\n",
    "grads = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        param_names.append(param_name)\n",
    "        grads.append(param.grad.to(torch.float32))\n",
    "\n",
    "data = []\n",
    "for param_name, grad in zip(param_names, grads):\n",
    "    data.append({\n",
    "        'param_name': param_name,\n",
    "        'shape': grad.shape,\n",
    "        'numel': grad.numel(),\n",
    "        'mean': grad.mean().detach().cpu().item(),\n",
    "        'norm': grad.norm().detach().cpu().item(),\n",
    "    })\n",
    "        \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# pattern = r'\\bquery_key_value\\.weight\\b'\n",
    "# # pattern = r'\\bmlp\\..*?\\.weight\\b'\n",
    "pattern = r'\\blora_B\\b'\n",
    "\n",
    "df = df[df['param_name'].apply(lambda x: True if re.search(pattern, x) else False)]\n",
    "df = df.sort_values(['norm'], ascending=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fullgrad = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_fullgrad.update({f'grad_loraB(r={lora_rank})_l2n': output['grad_loraB_l2n']})\n",
    "# del output_fullgrad['grad_loraB_l2n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import plt_kernel_matrix_one\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "metrics_name = ['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']\n",
    "metrics_name += [x for x in output_fullgrad if 'grad' in x and 'sum' not in x]\n",
    "metrics = [output_fullgrad[k] for k in metrics_name]\n",
    "any_isnan_mask = np.hstack([np.isnan(x) for x in metrics]).any(-1)\n",
    "metrics = [x[~any_isnan_mask] for x in metrics]\n",
    "N = len(metrics)\n",
    "K = np.zeros((N, N))\n",
    "for i, mi in enumerate(metrics):\n",
    "    for j, mj in enumerate(metrics):\n",
    "        s = stats.spearmanr(mi, mj)\n",
    "        K[i,j] = s.statistic\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt_kernel_matrix_one(\n",
    "    fig, ax, K, title=f'Spearmanr ({model_name}:{dataset})', \n",
    "    vmin=-1, vmax=1, cmap='bwr', n_ticks=N, annotate=True)\n",
    "ax.set_xticklabels(metrics_name, rotation=-45)\n",
    "ax.set_yticklabels(metrics_name)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ffeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
