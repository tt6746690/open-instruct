{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.wizardlm\",\n",
      "    \"nodes\": 1,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=1 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=wizardlm --model_name_or_path=../results/baselines/sentence-transformers/all-mpnet-base-v2 --use_dist --shuffle --compute_grad_embeddings --grad_randproj_components 2048 --max_seq_len=512 --encode_fn_type=input --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/input/all-mpnet-base-v2\n",
      "[{'args': 'sbatch --job-name=lm_outputs.wizardlm --partition=el8 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpn7qsr1y8', 'job_id': 1174521}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "nodes = 1; gpus=6\n",
    "use_lora = False\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "#### auto-regressive model, compute loss/embedding/grad\n",
    "#\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'; max_seq_len = 2048\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'; max_seq_len = 2048\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'; max_seq_len = 2048\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'; max_seq_len = 2048\n",
    "# model_name = 'mistral-7b+ultrachat200kv1'; model_name_or_path = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'; max_seq_len = 2048\n",
    "\n",
    "# compute_loss = True; compute_grad = True; encode_fn_type = 'sft'\n",
    "# use_lora = True; lora_rank = 256; lora_alpha = lora_rank\n",
    "####\n",
    "\n",
    "#### encoder-based model, for computing embedding only\n",
    "model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'; max_seq_len = 512\n",
    "# model_name = 'bge-large-en-v1.5'; model_name_or_path = '../results/baselines/BAAI/bge-large-en-v1.5'; max_seq_len = 128\n",
    "\n",
    "compute_loss = False; compute_grad = False; encode_fn_type = 'input'\n",
    "####\n",
    "\n",
    "\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1', 'gpt4_alpaca', 'code_alpaca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = ['wizardlm', 'open_orca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "datasets = ['wizardlm']; nodes = 1; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegpt', 'ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = [f'ultrachat15_{i}' for i in [0, 2]]; nodes = 5; gpus=6; cpu_mem = 512\n",
    "#. range(10)\n",
    "\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v1_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['tulu_v2_human_mix', 'tulu_v2_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512 # if not compute grad embeddings\n",
    "# datasets = ['flan2022_1m']; nodes = 15; gpus=6; cpu_mem = 512 # if do compute grad embeddings\n",
    "# datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{encode_fn_type+'/' if encode_fn_type!='sft' else ''}{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "\n",
    "use_dist = True if nodes*gpus>1 else False\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if use_dist:\n",
    "        prefix = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prefix = 'python'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "        {prefix}\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        {'--use_dist' if use_dist else ''} \\\n",
    "        --shuffle \\\n",
    "        {'--compute_loss' if compute_loss else ''} \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        --compute_grad_embeddings \\\n",
    "        --grad_randproj_components 2048 \\\n",
    "        --max_seq_len={max_seq_len} \\\n",
    "        --encode_fn_type={encode_fn_type} \\\n",
    "        --save_dir={save_dir} \\\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390e381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fd5e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_rank, lora_alpha = 256,256\n",
    "target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    bias='none',\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha, \n",
    "    lora_dropout=0.,\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "# https://github.com/huggingface/peft/issues/137\n",
    "model.enable_input_require_grads()\n",
    "model = get_peft_model(model, peft_config)\n",
    "from note_llama_embeddings import print_trainable_parameters\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554b1e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Sun Nov 12 00:05:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    79W / 300W |  19265MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    88W / 300W |  21199MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    80W / 300W |  24415MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   33C    P0   111W / 300W |  27969MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    54W / 300W |    811MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    278536      C   ...da3/envs/llava/bin/python    30177MiB |\n",
      "|    1   N/A  N/A    278537      C   ...da3/envs/llava/bin/python    30509MiB |\n",
      "|    2   N/A  N/A    278538      C   ...da3/envs/llava/bin/python    31397MiB |\n",
      "|    3   N/A  N/A    278539      C   ...da3/envs/llava/bin/python    31847MiB |\n",
      "|    4   N/A  N/A    288182      C   .../open-instruct/bin/python      809MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-12 00:05:06,216] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    sklearn_rp_mat_size,\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    gather_grad_embeddings,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n",
    "from note_pruning_analysis import encode_just_one_role\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat15_0'\n",
    "dataset = 'wizardlm'\n",
    "\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "compute_loss = False # True\n",
    "compute_grad = False # True\n",
    "use_lora = False # True\n",
    "lora_rank = 256\n",
    "lora_alpha = lora_rank\n",
    "compute_grad_embeddings = False # True\n",
    "grad_randproj_components = 2048\n",
    "encode_fn_type = 'input'  # input, output, sft\n",
    "max_seq_len = 2048\n",
    "\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = 'EleutherAI/pythia-1b-deduped'\n",
    "# model_name = 'pythia-160m-deduped'; model_name_or_path = 'EleutherAI/pythia-160m-deduped'\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "\n",
    "save_dir = f\"model_outputs/{encode_fn_type}/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', \n",
    "               'tulu_v1_mix',\n",
    "               'tulu_v2_human_mix',\n",
    "               'tulu_v2_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33324799",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sentence-transformers' in model_name_or_path:\n",
    "    from transformers import AutoModel\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch.float16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49dce015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0, 2027, 2007, 2023, 2746, 6255,    2],\n",
      "        [   0, 2173, 6255, 2007, 4995,    2,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]])}\n",
      "Sentence embeddings:\n",
      "tensor([[ 0.0225, -0.0783, -0.0230,  ..., -0.0083,  0.0265, -0.0020],\n",
      "        [ 0.0417,  0.0011, -0.0155,  ..., -0.0218, -0.0636, -0.0088]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ffc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('results/baselines/huggyllama/llama-7b_fixtok', use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2', use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba7e4ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toks</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>role_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there</td>\n",
       "      <td>2049</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>are</td>\n",
       "      <td>2028</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fruits</td>\n",
       "      <td>10966</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apple</td>\n",
       "      <td>6211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>1014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>orange</td>\n",
       "      <td>4593</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>1014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>and</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pine</td>\n",
       "      <td>7226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>##apple</td>\n",
       "      <td>23808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "      <td>1016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       toks  input_ids  role_type_ids  attention_mask\n",
       "0       <s>          0              0               1\n",
       "1     there       2049              0               1\n",
       "2       are       2028              0               1\n",
       "3         3       1021              0               1\n",
       "4    fruits      10966              0               1\n",
       "5         :       1028              0               1\n",
       "6     apple       6211              1               1\n",
       "7         ,       1014              1               1\n",
       "8    orange       4593              1               1\n",
       "9         ,       1014              1               1\n",
       "10      and       2002              1               1\n",
       "11     pine       7226              1               1\n",
       "12  ##apple      23808              1               1\n",
       "13        .       1016              1               1\n",
       "14     </s>          2              1               1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from note_pruning_analysis import viz_tokenizer_outputs\n",
    "\n",
    "max_seq_len = 2048\n",
    "example = sentences[0]\n",
    "\n",
    "\n",
    "example = {'messages': [{'content': 'there are 3 fruits: ', 'role': 'user'},\n",
    "                        {'content': 'apple, orange, and pineapple.', 'role': 'assistant'}]}\n",
    "outputs = encode_with_sentencetransformer_prompt_completion_format(\n",
    "    example, tokenizer, max_seq_len)\n",
    "\n",
    "# outputs = encode_with_messages_format(example, tokenizer, max_seq_len)\n",
    "\n",
    "# s = '<s>there are 3 fruits: apple, orange, and pineapple.</s>'\n",
    "# outputs.update({'toks': tokenizer.tokenize(s)})\n",
    "\n",
    "df = viz_tokenizer_outputs(outputs, tokenizer=tokenizer)\n",
    "df\n",
    "\n",
    "\n",
    "# print(json.dumps(L, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25ab75fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toks</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁&lt;</td>\n",
       "      <td>529</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user</td>\n",
       "      <td>1792</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>29958</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>G</td>\n",
       "      <td>29954</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ive</td>\n",
       "      <td>573</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>▁me</td>\n",
       "      <td>592</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>29941</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>▁f</td>\n",
       "      <td>285</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ruits</td>\n",
       "      <td>21211</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>:</td>\n",
       "      <td>29901</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>29966</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ass</td>\n",
       "      <td>465</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>istant</td>\n",
       "      <td>22137</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>29958</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>apple</td>\n",
       "      <td>11548</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>,</td>\n",
       "      <td>29892</td>\n",
       "      <td>29892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>▁orange</td>\n",
       "      <td>24841</td>\n",
       "      <td>24841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "      <td>29892</td>\n",
       "      <td>29892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>▁and</td>\n",
       "      <td>322</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>▁p</td>\n",
       "      <td>282</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ine</td>\n",
       "      <td>457</td>\n",
       "      <td>457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>apple</td>\n",
       "      <td>11548</td>\n",
       "      <td>11548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>.</td>\n",
       "      <td>29889</td>\n",
       "      <td>29889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>▁</td>\n",
       "      <td>29871</td>\n",
       "      <td>29871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>29966</td>\n",
       "      <td>29966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>user</td>\n",
       "      <td>1792</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>29958</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>what</td>\n",
       "      <td>5816</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>▁is</td>\n",
       "      <td>338</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>▁your</td>\n",
       "      <td>596</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>▁favorite</td>\n",
       "      <td>25448</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>?</td>\n",
       "      <td>29973</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>29966</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ass</td>\n",
       "      <td>465</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>istant</td>\n",
       "      <td>22137</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>|</td>\n",
       "      <td>29989</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>29958</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>&lt;0x0A&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>apple</td>\n",
       "      <td>11548</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>.</td>\n",
       "      <td>29889</td>\n",
       "      <td>29889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         toks  input_ids  labels  attention_mask\n",
       "0         <s>          1    -100               1\n",
       "1          ▁<        529    -100               1\n",
       "2           |      29989    -100               1\n",
       "3        user       1792    -100               1\n",
       "4           |      29989    -100               1\n",
       "5           >      29958    -100               1\n",
       "6      <0x0A>         13    -100               1\n",
       "7           G      29954    -100               1\n",
       "8         ive        573    -100               1\n",
       "9         ▁me        592    -100               1\n",
       "10          ▁      29871    -100               1\n",
       "11          3      29941    -100               1\n",
       "12         ▁f        285    -100               1\n",
       "13      ruits      21211    -100               1\n",
       "14          :      29901    -100               1\n",
       "15     <0x0A>         13    -100               1\n",
       "16          <      29966    -100               1\n",
       "17          |      29989    -100               1\n",
       "18        ass        465    -100               1\n",
       "19     istant      22137    -100               1\n",
       "20          |      29989    -100               1\n",
       "21          >      29958    -100               1\n",
       "22     <0x0A>         13    -100               1\n",
       "23      apple      11548    -100               1\n",
       "24          ,      29892   29892               1\n",
       "25    ▁orange      24841   24841               1\n",
       "26          ,      29892   29892               1\n",
       "27       ▁and        322     322               1\n",
       "28         ▁p        282     282               1\n",
       "29        ine        457     457               1\n",
       "30      apple      11548   11548               1\n",
       "31          .      29889   29889               1\n",
       "32       </s>          2       2               1\n",
       "33          ▁      29871   29871               1\n",
       "34     <0x0A>         13      13               1\n",
       "35          <      29966   29966               1\n",
       "36          |      29989    -100               1\n",
       "37       user       1792    -100               1\n",
       "38          |      29989    -100               1\n",
       "39          >      29958    -100               1\n",
       "40     <0x0A>         13    -100               1\n",
       "41       what       5816    -100               1\n",
       "42        ▁is        338    -100               1\n",
       "43      ▁your        596    -100               1\n",
       "44  ▁favorite      25448    -100               1\n",
       "45          ?      29973    -100               1\n",
       "46     <0x0A>         13    -100               1\n",
       "47          <      29966    -100               1\n",
       "48          |      29989    -100               1\n",
       "49        ass        465    -100               1\n",
       "50     istant      22137    -100               1\n",
       "51          |      29989    -100               1\n",
       "52          >      29958    -100               1\n",
       "53     <0x0A>         13    -100               1\n",
       "54      apple      11548    -100               1\n",
       "55          .      29889   29889               1\n",
       "56       </s>          2       2               1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def encode_with_sentencetransformer_prompt_completion_format(\n",
    "        example, tokenizer, max_seq_length, add_eos_token=False, role_type='user'):\n",
    "    '''\n",
    "    Here we assume each example has 'prompt' and 'completion' fields.\n",
    "    We concatenate prompt and completion and tokenize them together because otherwise prompt will be padded/trancated \n",
    "    and it doesn't make sense to follow directly with the completion.\n",
    "\n",
    "    `add_eos_token` if set to False, then assumes tokenizer adds eos token by default.\n",
    "    `segments` \n",
    "    '''\n",
    "    assert(len(example['messages'])==2 and [x['role'] for x in example['messages']]==['user', 'assistant'])\n",
    "    prompt = example['messages'][0]['content']\n",
    "    completion = example['messages'][1]['content']\n",
    "    \n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not prompt.endswith((' ', '\\n', '\\t')) and not completion.startswith((' ', '\\n', '\\t')):\n",
    "        example_text = prompt + ' ' + completion\n",
    "    else:\n",
    "        example_text = prompt + completion\n",
    "    if add_eos_token:\n",
    "        example_text = example_text + tokenizer.eos_token\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    \n",
    "    \n",
    "    role_type_ids = torch.zeros_like(input_ids)\n",
    "    tokenized_prompt = tokenizer(prompt.rstrip(' '), return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    start = tokenized_prompt.input_ids.shape[1]\n",
    "    if tokenized_prompt.input_ids[0,-1].item() == tokenizer.eos_token_id:\n",
    "        start -= 1\n",
    "    role_type_ids[:, start:] = 1\n",
    "    \n",
    "#     labels = input_ids.clone()\n",
    "#     ## wpq: `rstrip()` due to the space between prompt & completion, is a stand-alone token when tokenizing prompt, but not prompt+completion.\n",
    "#     ## wpq: make sure to discount trailing eos token added by default.\n",
    "#     l = tokenized_prompt.input_ids.shape[1]\n",
    "#     if tokenized_prompt.input_ids[0,-1].item() == tokenizer.eos_token_id:\n",
    "#         l -= 1\n",
    "#     # mask the prompt part for avoiding loss\n",
    "#     labels[:, :l] = -100\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'role_type_ids': role_type_ids.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_messages_with_tulu_chat_format(messages, bos=\"<s>\", eos=\"</s>\", add_bos=True):\n",
    "    formatted_text = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            formatted_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + eos + \"\\n\"\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Tulu chat template only supports 'system', 'user' and 'assistant' roles. Invalid role: {}.\".format(message[\"role\"])\n",
    "                )\n",
    "    formatted_text = bos + formatted_text if add_bos else formatted_text\n",
    "    return formatted_text\n",
    "\n",
    "\n",
    "def encode_messages_with_completion_format(messages, bos=\"<s>\", eos=\"</s>\", add_bos=True):\n",
    "    assert(len(example['messages'])==2 and [x['role'] for x in example['messages']]==['user', 'assistant'])\n",
    "    prompt = example['messages'][0]['content']\n",
    "    completion = example['messages'][1]['content']\n",
    "    # if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "    if not prompt.endswith((' ', '\\n', '\\t')) and not completion.startswith((' ', '\\n', '\\t')):\n",
    "        example_text = prompt + ' ' + completion\n",
    "    else:\n",
    "        example_text = prompt + completion\n",
    "    example_text = example_text + eos\n",
    "    if add_bos:\n",
    "        example_text = bos + example_text\n",
    "    return example_text\n",
    "\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length, chatfmt_fn='tulu'):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "        \n",
    "    if chatfmt_fn == 'tulu':\n",
    "        chatfmt_fn = encode_messages_with_tulu_chat_format\n",
    "    elif chatfmt_fn == 'completion':\n",
    "        chatfmt_fn = encode_messages_with_completion_format\n",
    "    else:\n",
    "        raise ValueError(f'{chatfmt_fn} not supported.')\n",
    "        \n",
    "    example_text = chatfmt_fn(\n",
    "        messages, bos=tokenizer.bos_token, eos=tokenizer.eos_token, add_bos=False).strip()\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the non-assistant part for avoiding loss\n",
    "    for message_idx, message in enumerate(messages):\n",
    "        if message[\"role\"] != \"assistant\":\n",
    "            if message_idx == 0:\n",
    "                message_start_idx = 0\n",
    "            else:\n",
    "                message_start_idx = tokenizer(\n",
    "                    chatfmt_fn(messages[:message_idx]), return_tensors='pt', max_length=max_seq_length, truncation=True\n",
    "                ).input_ids.shape[1]\n",
    "            if message_idx < len(messages) - 1 and messages[message_idx+1][\"role\"] == \"assistant\":\n",
    "                # here we also ignore the role of the assistant\n",
    "                messages_so_far = chatfmt_fn(messages[:message_idx+1]) + \"<|assistant|>\\n\"\n",
    "            else:\n",
    "                messages_so_far = chatfmt_fn(messages[:message_idx+1])\n",
    "            message_end_idx = tokenizer(\n",
    "                messages_so_far,\n",
    "                return_tensors='pt',\n",
    "                max_length=max_seq_length,\n",
    "                truncation=True\n",
    "            ).input_ids.shape[1]\n",
    "            labels[:, message_start_idx:message_end_idx] = -100\n",
    "            \n",
    "            if message_end_idx >= max_seq_length:\n",
    "                break\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }\n",
    "\n",
    "example = {'messages': [{'content': 'Give me 3 fruits: ', 'role': 'user'},\n",
    "                        {'content': 'apple, orange, and pineapple.', 'role': 'assistant'},\n",
    "                        {'content': 'what is your favorite? ', 'role': 'user'},\n",
    "                        {'content': 'apple.', 'role': 'assistant'}\n",
    "                       ]}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'results/baselines/huggyllama/llama-7b_fixtok', use_fast=False)\n",
    "\n",
    "outputs = encode_with_messages_format(\n",
    "    example, tokenizer, max_seq_len)\n",
    "\n",
    "df = viz_tokenizer_outputs(outputs, tokenizer=tokenizer)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "76aeaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max_seq_len\n",
    "add_eos_token = False\n",
    "\n",
    "# if prompt doesn't end with space and completion doesn't start with space, add space\n",
    "if not example['prompt'].endswith((' ', '\\n', '\\t')) and not example['completion'].startswith((' ', '\\n', '\\t')):\n",
    "    example_text = example['prompt'] + ' ' + example['completion']\n",
    "else:\n",
    "    example_text = example['prompt'] + example['completion']\n",
    "if add_eos_token:\n",
    "    example_text = example_text + tokenizer.eos_token\n",
    "tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "input_ids = tokenized_example.input_ids\n",
    "labels = input_ids.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9da661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d844da6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toks</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there</td>\n",
       "      <td>2049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>are</td>\n",
       "      <td>2028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fruits</td>\n",
       "      <td>10966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     toks  input_ids  attention_mask\n",
       "0     <s>          0               1\n",
       "1   there       2049               1\n",
       "2     are       2028               1\n",
       "3       3       1021               1\n",
       "4  fruits      10966               1\n",
       "5       :       1028               1\n",
       "6    </s>          2               1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = {'input_ids': torch.tensor([[    1,   727,   526, 29871, 29941,   285, 21211, 29901, 29871]]), \n",
    "           'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "outputs = {'input_ids': torch.tensor([[    0,  2049,  2028,  1021, 10966,  1028,     2]]),\n",
    "           'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
    "\n",
    "df = viz_tokenizer_outputs(outputs, tokenizer=tokenizer)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3bd0914a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = \"This is a tokenization example\"\n",
    "encoded = tokenizer(s)\n",
    "\n",
    "# Check the full mapping token -> word:\n",
    "encoded.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd9434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 109486464 || all params: 109486464 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # # the following also applies lora to MLP layers.\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if compute_grad:\n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "if encode_fn_type in ['input', 'output']:\n",
    "    encode_function = partial(\n",
    "        encode_just_one_role,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        encode_fn_type=encode_fn_type)\n",
    "elif encode_fn_type == 'sft':    \n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len)\n",
    "else:\n",
    "    raise ValueError(f'encode_fn_type={encode_fn_type} not implemented.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e051e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-a386ff309e3a3fc4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cfd1bd1d8643f8a740f165d9b1557d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-a386ff309e3a3fc4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-54bb2c79fdb7ab29_*_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wizardlm dataset length = 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': '.*'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e2f99ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local_rank/global=0/0] output=[('text_embedding', (1, 768), dtype('float32'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if compute_grad_embeddings:\n",
    "    rps = {}\n",
    "    for k in grad_statistic_patterns.keys():\n",
    "        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "\n",
    "output = defaultdict(list)\n",
    "for i, batch in tqdm(enumerate(loader), disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    last_hidden_state = outputs['hidden_states'][-1]\n",
    "    text_embedding = mean_pooling(last_hidden_state, batch['attention_mask'])\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "\n",
    "    if compute_loss:\n",
    "        # average of output token log probs\n",
    "        if 'loss' in outputs:\n",
    "            output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "\n",
    "        # el2n scores\n",
    "        losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "        for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "            output[k].append(losses[k].detach().cpu())\n",
    "\n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "            \n",
    "        if compute_grad_embeddings:\n",
    "            grad_embeddings = gather_grad_embeddings(\n",
    "                model,\n",
    "                {k: v for k, v in grad_statistic_patterns.items() if k in ['qkv', 'loraB']},\n",
    "                stacked=True,\n",
    "            )\n",
    "            if test_run:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    output[f'grad_{k}'].append(v)\n",
    "            if i==0:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    t0 = time.time()\n",
    "                    print(f\"Fitting random projection for {k} ({v.size} -> {grad_randproj_components})\")\n",
    "                    rps[k] = rps[k].fit(v[np.newaxis,...])\n",
    "                    print(f\"Fitting random projection in {time.time() - t0:0.3f}s \"\n",
    "                          f\"with random matrix size {sklearn_rp_mat_size(rps[k]) / 1e6:0.3f} MB\")\n",
    "            for k in grad_embeddings.keys():\n",
    "                rp = rps[k]\n",
    "                g = grad_embeddings[k]\n",
    "                output[f'grad_rp_{k}'].append(rp.transform(g[np.newaxis,...]).squeeze())\n",
    "\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "    \n",
    "for k, v in output.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    else:\n",
    "        output[k] = np.vstack(v)\n",
    "\n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'model_outputs/llama-7b+lora:r=256:a=256/lima.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for k in grad_embeddings.keys():\n",
    "    rp = rps[k]\n",
    "    g = grad_embeddings[k]\n",
    "    rp.transform(g[np.newaxis,...]).squeeze()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d608989",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'note_llama_embeddings_{dataset}:{model_name}'+\\\n",
    "    (f'+lora(r={lora_rank},a={lora_alpha})' if use_lora else '')+'.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(output, f)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o1 = pickle.load(f)\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=1,a=1).pkl'\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=256,a=256).pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o2 = pickle.load(f)\n",
    "    \n",
    "output = {}\n",
    "for k in ['grad_qkv', 'grad_last', 'grad_rp_qkv', 'grad_rp_last']:\n",
    "    output[k] = o1[k]\n",
    "for k in ['grad_loraB', 'grad_rp_loraB']:\n",
    "    output[k] = o2[k]\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4759b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np rp\n",
    "# 10%|▉         | 99/1000 [00:22<03:24,  4.40it/s]\n",
    "\n",
    "# +rp for qkv/last\n",
    "#  10%|▉         | 99/1000 [00:50<07:35,  1.98it/s]\n",
    "\n",
    "# +rp for qkv 21,233,664 -> 2048\n",
    "# 10%|▉         | 99/1000 [03:50<34:56,  2.33s/it]  \n",
    "\n",
    "# +rp for lora(rank=1) qkv. 27,648 -> 2048\n",
    "# 10%|▉         | 99/1000 [00:10<01:34,  9.58it/s]\n",
    "\n",
    "# +rp for lora(rank=256) qkv (7,077,888 -> 2048)\n",
    "# 10%|▉         | 99/1000 [00:57<08:41,  1.73it/s] \n",
    "\n",
    "# (134,217,728 -> 2048) 180MB random matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a688cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,np.stack(v).shape, np.stack(v).size) for k,v in grad_embeddings.items()]\n",
    "# Pythia-160m: [('qkv', (12, 2304, 768)), ('last', (1, 50304, 768))]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75687add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n =1\n",
    "\n",
    "projected_data = {}\n",
    "rp_transform = torch.compile(rp.transform)\n",
    "for n in [1, 10, 100]:\n",
    "    t0 = time()\n",
    "    projected_data[n] = rp_transform(data[:n])\n",
    "\n",
    "    print(f\"Projected {n} samples from {n_features} to {n_components} in \"\n",
    "          f\"{time() - t0:0.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(projected_data)) / projected_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from note_llama_embeddings import torch_cdist\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_components = 2048\n",
    "k = 'qkv'\n",
    "# ['qkv', 'last', 'rp_qkv', 'rp_last', 'loraB', 'rp_loraB']\n",
    "\n",
    "device = 'cuda'\n",
    "data = output[f'grad_qkv']\n",
    "projected_data = output[f'grad_rp_qkv']\n",
    "\n",
    "\n",
    "dists = {}\n",
    "ks = output.keys()\n",
    "ks = ['grad_loraB', 'grad_rp_loraB']\n",
    "\n",
    "for k in ks:\n",
    "    if 'last' in k:\n",
    "        continue\n",
    "    data = output[k]\n",
    "#     data = torch.from_numpy(data).to(torch.float32).to('cuda')\n",
    "#     D = torch_cdist(data, device).ravel()\n",
    "    D = scipy.spatial.distance.pdist(data)\n",
    "#     valid_dist = np.logical_and(~np.isnan(dists), dists!=0)\n",
    "#     dists = dists[valid_dist]\n",
    "#     print(f'percent valid: {100*np.sum(valid_dist)/dists.size:.2f}')\n",
    "    dists[k] = D\n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "# rp = SparseRandomProjection(n_components=n_components)  \n",
    "# projected_data = rp.fit_transform(output[f'grad_{k}'])\n",
    "# print(\n",
    "#     f\"Projected in\"\n",
    "#     f\"{time.time() - t0:0.3f}s\"\n",
    "# )\n",
    "# if hasattr(rp, \"components_\"):\n",
    "#     n_bytes = rp.components_.data.nbytes\n",
    "#     n_bytes += rp.components_.indices.nbytes\n",
    "#     print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n",
    "\n",
    "# projected_dists = torch_cdist(projected_data, device).ravel()\n",
    "# del projected_data\n",
    "# projected_dists = projected_dists[valid_dist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5211013",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "from note_llama_embeddings import torch_cdist, plt_pair_of_dists\n",
    "\n",
    "dataset = 'lima'; model_name = 'pythia-160m-deduped+lora(rank=256,alpha=256)'\n",
    "\n",
    "for k in dists.keys():\n",
    "    if k == 'grad_qkv':\n",
    "        continue\n",
    "    fig, axs = plt_pair_of_dists(\n",
    "        dists['grad_qkv'], \n",
    "        dists[k]/(dists[k].mean()/dists['grad_qkv'].mean()), \n",
    "        n_components,\n",
    "        use_hexbin=False)\n",
    "    fig.suptitle(f'{dataset}:{model_name} grad_qkv vs. {k}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists['grad_loraB'].mean()/dists['grad_qkv'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "param_names = []\n",
    "grads = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        param_names.append(param_name)\n",
    "        grads.append(param.grad.to(torch.float32))\n",
    "\n",
    "data = []\n",
    "for param_name, grad in zip(param_names, grads):\n",
    "    data.append({\n",
    "        'param_name': param_name,\n",
    "        'shape': grad.shape,\n",
    "        'numel': grad.numel(),\n",
    "        'mean': grad.mean().detach().cpu().item(),\n",
    "        'norm': grad.norm().detach().cpu().item(),\n",
    "    })\n",
    "        \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# pattern = r'\\bquery_key_value\\.weight\\b'\n",
    "# # pattern = r'\\bmlp\\..*?\\.weight\\b'\n",
    "pattern = r'\\blora_B\\b'\n",
    "\n",
    "df = df[df['param_name'].apply(lambda x: True if re.search(pattern, x) else False)]\n",
    "df = df.sort_values(['norm'], ascending=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fullgrad = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_fullgrad.update({f'grad_loraB(r={lora_rank})_l2n': output['grad_loraB_l2n']})\n",
    "# del output_fullgrad['grad_loraB_l2n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import plt_kernel_matrix_one\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "metrics_name = ['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']\n",
    "metrics_name += [x for x in output_fullgrad if 'grad' in x and 'sum' not in x]\n",
    "metrics = [output_fullgrad[k] for k in metrics_name]\n",
    "any_isnan_mask = np.hstack([np.isnan(x) for x in metrics]).any(-1)\n",
    "metrics = [x[~any_isnan_mask] for x in metrics]\n",
    "N = len(metrics)\n",
    "K = np.zeros((N, N))\n",
    "for i, mi in enumerate(metrics):\n",
    "    for j, mj in enumerate(metrics):\n",
    "        s = stats.spearmanr(mi, mj)\n",
    "        K[i,j] = s.statistic\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt_kernel_matrix_one(\n",
    "    fig, ax, K, title=f'Spearmanr ({model_name}:{dataset})', \n",
    "    vmin=-1, vmax=1, cmap='bwr', n_ticks=N, annotate=True)\n",
    "ax.set_xticklabels(metrics_name, rotation=-45)\n",
    "ax.set_yticklabels(metrics_name)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ffeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
