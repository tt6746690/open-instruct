{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.flan2022_1m\",\n",
      "    \"nodes\": 15,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=15 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=flan2022_1m --model_name_or_path=../results/baselines/huggyllama/llama-7b --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/llama-7b+lora:r=256:a=256 --use_dist --shuffle --compute_grad --use_lora --lora_rank=256 --lora_alpha=256 --compute_grad_embeddings --grad_randproj_components 2048\n",
      "[{'args': 'sbatch --job-name=lm_outputs.flan2022_1m --partition=el8 --nodes=15 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmp65_d7_5d', 'job_id': 1072370}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "lora_rank = 256; lora_alpha = lora_rank\n",
    "nodes = 1; gpus=6\n",
    "\n",
    "test_run = 0\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'\n",
    "\n",
    "compute_grad = True; use_lora = True; lora_rank = 256; lora_alpha = lora_rank\n",
    "# compute_grad = True; use_lora = False\n",
    "\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v2_human_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512 # if not compute grad embeddings\n",
    "datasets = ['flan2022_1m']; nodes = 15; gpus=6; cpu_mem = 512 # if do compute grad embeddings\n",
    "# datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    cmd = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        --save_dir={save_dir} \\\n",
    "        --use_dist \\\n",
    "        --shuffle \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        --compute_grad_embeddings \\\n",
    "        --grad_randproj_components 2048\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "datasets = []\n",
    "for dataset in os.listdir(processed_dir) + ['tulu_v1_human_mix', 'tulu_v2_human_mix']:\n",
    "    dataset_path = os.path.join(processed_dir, dataset)\n",
    "    save_path = os.path.join(save_dir, f'{dataset}.pkl')\n",
    "    if 'tulu'==dataset:\n",
    "        continue\n",
    "    if 'tulu' not in dataset and not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "    datasets.append(dataset)\n",
    "    \n",
    "datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554b1e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Wed Oct 18 19:09:53 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    56W / 300W |    489MiB / 32510MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    39W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 300W |    509MiB / 32510MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A   3729836      C   ...8.8/build/bin/./mdrun_mpi      487MiB |\n",
      "|    3   N/A  N/A   3729959      C   ...8.8/build/bin/./mdrun_mpi      507MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-18 19:09:55,130] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    sklearn_rp_mat_size,\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    gather_grad_embeddings,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "dataset = 'lima'\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "compute_grad = True\n",
    "use_lora = True\n",
    "lora_rank = 256\n",
    "lora_alpha = lora_rank\n",
    "compute_grad_embeddings = True\n",
    "grad_randproj_components = 2048\n",
    "\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = 'EleutherAI/pythia-1b-deduped'\n",
    "# model_name = 'pythia-160m-deduped'; model_name_or_path = 'EleutherAI/pythia-160m-deduped'\n",
    "\n",
    "\n",
    "save_dir = f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', 'tulu_v2_human_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd9434f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7e03f4abb04b74a39cfc1300530426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing lora(r=256,a=256)\n",
      "trainable params: 134217728 || all params: 7006851072 || trainable%: 1.92\n"
     ]
    }
   ],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if 'llama' in model_name_or_path:\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if compute_grad:\n",
    "    if 'llama' in model_name_or_path:\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f0c3506fca446cb8fe0619070db36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d69f0d29a4f4ec64_*_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lima dataset length = 1000\n"
     ]
    }
   ],
   "source": [
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "\n",
    "encode_function = partial(\n",
    "    encode_with_messages_format, tokenizer=tokenizer, max_seq_length=2048)\n",
    "\n",
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loraB': 'lora_B\\\\.[a-zA-Z_]+\\\\.weight'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2f99ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random projection for loraB (134217728 -> 2048)\n",
      "Fitting random projection in 191.113s with random matrix size 189.759 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/1000 [05:27<49:41,  3.31s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local_rank/global=0/0] output=[('text_embedding', (100, 4096), dtype('float32')), ('log_prob', (100, 1), dtype('float32')), ('el2n_agg=mean', (100, 1), dtype('float32')), ('el2n_agg=l2n', (100, 1), dtype('float32')), ('logit_margin', (100, 1), dtype('float32')), ('grad_loraB_l2n', (100, 1), dtype('float32')), ('grad_loraB', (100, 134217728), dtype('float32')), ('grad_rp_loraB', (100, 2048), dtype('float32'))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if compute_grad_embeddings:\n",
    "    rps = {}\n",
    "    for k in grad_statistic_patterns.keys():\n",
    "        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "\n",
    "output = defaultdict(list)\n",
    "for i, batch in tqdm(enumerate(loader), disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    text_embedding = outputs['hidden_states'][-1].mean(1)\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "    \n",
    "     # average of output token log probs\n",
    "    output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "    \n",
    "    # el2n scores\n",
    "    losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "    for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "        output[k].append(losses[k].detach().cpu())\n",
    "\n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "            \n",
    "        if compute_grad_embeddings:\n",
    "            grad_embeddings = gather_grad_embeddings(\n",
    "                model,\n",
    "                {k: v for k, v in grad_statistic_patterns.items() if k in ['qkv', 'loraB']},\n",
    "                stacked=True,\n",
    "            )\n",
    "            if test_run:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    output[f'grad_{k}'].append(v)\n",
    "            if i==0:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    t0 = time.time()\n",
    "                    print(f\"Fitting random projection for {k} ({v.size} -> {grad_randproj_components})\")\n",
    "                    rps[k] = rps[k].fit(v[np.newaxis,...])\n",
    "                    print(f\"Fitting random projection in {time.time() - t0:0.3f}s \"\n",
    "                          f\"with random matrix size {sklearn_rp_mat_size(rps[k]) / 1e6:0.3f} MB\")\n",
    "            for k in grad_embeddings.keys():\n",
    "                rp = rps[k]\n",
    "                g = grad_embeddings[k]\n",
    "                output[f'grad_rp_{k}'].append(rp.transform(g[np.newaxis,...]).squeeze())\n",
    "\n",
    "    i += 1\n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "    \n",
    "for k, v in output.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    else:\n",
    "        output[k] = np.vstack(v)\n",
    "\n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4c27fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.933667188315075"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per_x_time = 8*60/172\n",
    "# per_x_time\n",
    "\n",
    "lima_c = 7*60+28\n",
    "lima_p = 2*60+22\n",
    "r = lima_c/lima_p\n",
    "\n",
    "flan2022_1m_p = 4*60*60+41*60+29\n",
    "\n",
    "flan2022_1m_p*r/60/60/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620296d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_embedding': array([[-1.0107422 ,  0.3317871 , -0.32666016, ...,  0.02334595,\n",
       "          0.48999023,  0.3395996 ],\n",
       "        [-1.1972656 ,  1.6523438 , -0.7988281 , ...,  0.22070312,\n",
       "          1.1552734 ,  0.8984375 ],\n",
       "        [-0.76123047, -0.5239258 ,  0.2758789 , ...,  1.1494141 ,\n",
       "          0.50878906,  0.15466309],\n",
       "        ...,\n",
       "        [-0.77246094,  0.8745117 , -0.36572266, ...,  0.27905273,\n",
       "          1.1367188 , -0.0027523 ],\n",
       "        [-1.1875    ,  0.27075195, -1.0205078 , ..., -0.29956055,\n",
       "          0.625     ,  0.04116821],\n",
       "        [-0.96435547,  0.26367188, -0.9785156 , ...,  0.25585938,\n",
       "          0.85595703,  0.39916992]], dtype=float32),\n",
       " 'log_prob': array([[-1.857992 ],\n",
       "        [-2.2157798],\n",
       "        [-1.5617336],\n",
       "        ...,\n",
       "        [-2.0540106],\n",
       "        [-1.8325413],\n",
       "        [-1.8836874]], dtype=float32),\n",
       " 'el2n_agg=mean': array([[0.60279465],\n",
       "        [0.66627216],\n",
       "        [0.5199953 ],\n",
       "        ...,\n",
       "        [0.6193907 ],\n",
       "        [0.62892884],\n",
       "        [0.59504163]], dtype=float32),\n",
       " 'el2n_agg=l2n': array([[14.990085],\n",
       "        [21.382166],\n",
       "        [14.278932],\n",
       "        ...,\n",
       "        [16.200666],\n",
       "        [14.664999],\n",
       "        [18.864405]], dtype=float32),\n",
       " 'logit_margin': array([[0.74559724],\n",
       "        [0.37695178],\n",
       "        [2.4927025 ],\n",
       "        ...,\n",
       "        [0.5752499 ],\n",
       "        [0.61999184],\n",
       "        [0.77086574]], dtype=float32),\n",
       " 'grad_loraB_l2n': array([[0.53822106],\n",
       "        [0.49319783],\n",
       "        [0.6681134 ],\n",
       "        ...,\n",
       "        [0.55275214],\n",
       "        [0.614306  ],\n",
       "        [0.50361997]], dtype=float32),\n",
       " 'grad_rp_loraB': array([[-1.56355463e-03,  2.01973133e-02, -1.24925170e-02, ...,\n",
       "          1.20776612e-02, -2.32812371e-02, -1.56732276e-03],\n",
       "        [-4.14139104e-05,  6.38463255e-03,  4.33865422e-03, ...,\n",
       "          3.08119139e-04,  3.69903069e-06,  1.08421817e-02],\n",
       "        [ 9.50028305e-04,  1.65276229e-02,  1.83894951e-02, ...,\n",
       "         -1.61782056e-02, -1.60093289e-02, -1.46665825e-02],\n",
       "        ...,\n",
       "        [-1.14069795e-02, -9.23063420e-03,  2.06535254e-02, ...,\n",
       "         -4.41775890e-04, -6.39451575e-03, -7.40911998e-03],\n",
       "        [ 4.33518086e-03, -3.06241866e-02, -1.61395818e-02, ...,\n",
       "         -2.39338204e-02,  8.48167762e-03, -2.53996290e-02],\n",
       "        [-1.38019081e-02, -9.17179696e-03,  1.83038414e-02, ...,\n",
       "         -7.32855266e-03, -1.49899945e-02,  8.39467905e-03]], dtype=float32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "save_path = 'model_outputs/llama-7b+lora:r=256:a=256/lima.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ca277",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for k in grad_embeddings.keys():\n",
    "    rp = rps[k]\n",
    "    g = grad_embeddings[k]\n",
    "    rp.transform(g[np.newaxis,...]).squeeze()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d434f17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'note_llama_embeddings_lima:llama-7b+lora(r=256,a=256).pkl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = f'note_llama_embeddings_{dataset}:{model_name}'+\\\n",
    "    (f'+lora(r={lora_rank},a={lora_alpha})' if use_lora else '')+'.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(output, f)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o1 = pickle.load(f)\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=1,a=1).pkl'\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=256,a=256).pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o2 = pickle.load(f)\n",
    "    \n",
    "output = {}\n",
    "for k in ['grad_qkv', 'grad_last', 'grad_rp_qkv', 'grad_rp_last']:\n",
    "    output[k] = o1[k]\n",
    "for k in ['grad_loraB', 'grad_rp_loraB']:\n",
    "    output[k] = o2[k]\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edea344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np rp\n",
    "# 10%|▉         | 99/1000 [00:22<03:24,  4.40it/s]\n",
    "\n",
    "# +rp for qkv/last\n",
    "#  10%|▉         | 99/1000 [00:50<07:35,  1.98it/s]\n",
    "\n",
    "# +rp for qkv 21,233,664 -> 2048\n",
    "# 10%|▉         | 99/1000 [03:50<34:56,  2.33s/it]  \n",
    "\n",
    "# +rp for lora(rank=1) qkv. 27,648 -> 2048\n",
    "# 10%|▉         | 99/1000 [00:10<01:34,  9.58it/s]\n",
    "\n",
    "# +rp for lora(rank=256) qkv (7,077,888 -> 2048)\n",
    "# 10%|▉         | 99/1000 [00:57<08:41,  1.73it/s] \n",
    "\n",
    "# (134,217,728 -> 2048) 180MB random matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca40a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,np.stack(v).shape, np.stack(v).size) for k,v in grad_embeddings.items()]\n",
    "# Pythia-160m: [('qkv', (12, 2304, 768)), ('last', (1, 50304, 768))]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254377b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n =1\n",
    "\n",
    "projected_data = {}\n",
    "rp_transform = torch.compile(rp.transform)\n",
    "for n in [1, 10, 100]:\n",
    "    t0 = time()\n",
    "    projected_data[n] = rp_transform(data[:n])\n",
    "\n",
    "    print(f\"Projected {n} samples from {n_features} to {n_components} in \"\n",
    "          f\"{time() - t0:0.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b48d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(projected_data)) / projected_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c91356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from note_llama_embeddings import torch_cdist\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_components = 2048\n",
    "k = 'qkv'\n",
    "# ['qkv', 'last', 'rp_qkv', 'rp_last', 'loraB', 'rp_loraB']\n",
    "\n",
    "device = 'cuda'\n",
    "data = output[f'grad_qkv']\n",
    "projected_data = output[f'grad_rp_qkv']\n",
    "\n",
    "\n",
    "dists = {}\n",
    "ks = output.keys()\n",
    "ks = ['grad_loraB', 'grad_rp_loraB']\n",
    "\n",
    "for k in ks:\n",
    "    if 'last' in k:\n",
    "        continue\n",
    "    data = output[k]\n",
    "#     data = torch.from_numpy(data).to(torch.float32).to('cuda')\n",
    "#     D = torch_cdist(data, device).ravel()\n",
    "    D = scipy.spatial.distance.pdist(data)\n",
    "#     valid_dist = np.logical_and(~np.isnan(dists), dists!=0)\n",
    "#     dists = dists[valid_dist]\n",
    "#     print(f'percent valid: {100*np.sum(valid_dist)/dists.size:.2f}')\n",
    "    dists[k] = D\n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "# rp = SparseRandomProjection(n_components=n_components)  \n",
    "# projected_data = rp.fit_transform(output[f'grad_{k}'])\n",
    "# print(\n",
    "#     f\"Projected in\"\n",
    "#     f\"{time.time() - t0:0.3f}s\"\n",
    "# )\n",
    "# if hasattr(rp, \"components_\"):\n",
    "#     n_bytes = rp.components_.data.nbytes\n",
    "#     n_bytes += rp.components_.indices.nbytes\n",
    "#     print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n",
    "\n",
    "# projected_dists = torch_cdist(projected_data, device).ravel()\n",
    "# del projected_data\n",
    "# projected_dists = projected_dists[valid_dist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "from note_llama_embeddings import torch_cdist, plt_pair_of_dists\n",
    "\n",
    "dataset = 'lima'; model_name = 'pythia-160m-deduped+lora(rank=256,alpha=256)'\n",
    "\n",
    "for k in dists.keys():\n",
    "    if k == 'grad_qkv':\n",
    "        continue\n",
    "    fig, axs = plt_pair_of_dists(\n",
    "        dists['grad_qkv'], \n",
    "        dists[k]/(dists[k].mean()/dists['grad_qkv'].mean()), \n",
    "        n_components,\n",
    "        use_hexbin=False)\n",
    "    fig.suptitle(f'{dataset}:{model_name} grad_qkv vs. {k}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists['grad_loraB'].mean()/dists['grad_qkv'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "param_names = []\n",
    "grads = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        param_names.append(param_name)\n",
    "        grads.append(param.grad.to(torch.float32))\n",
    "\n",
    "data = []\n",
    "for param_name, grad in zip(param_names, grads):\n",
    "    data.append({\n",
    "        'param_name': param_name,\n",
    "        'shape': grad.shape,\n",
    "        'numel': grad.numel(),\n",
    "        'mean': grad.mean().detach().cpu().item(),\n",
    "        'norm': grad.norm().detach().cpu().item(),\n",
    "    })\n",
    "        \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# pattern = r'\\bquery_key_value\\.weight\\b'\n",
    "# # pattern = r'\\bmlp\\..*?\\.weight\\b'\n",
    "pattern = r'\\blora_B\\b'\n",
    "\n",
    "df = df[df['param_name'].apply(lambda x: True if re.search(pattern, x) else False)]\n",
    "df = df.sort_values(['norm'], ascending=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fullgrad = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_fullgrad.update({f'grad_loraB(r={lora_rank})_l2n': output['grad_loraB_l2n']})\n",
    "# del output_fullgrad['grad_loraB_l2n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import plt_kernel_matrix_one\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "metrics_name = ['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']\n",
    "metrics_name += [x for x in output_fullgrad if 'grad' in x and 'sum' not in x]\n",
    "metrics = [output_fullgrad[k] for k in metrics_name]\n",
    "any_isnan_mask = np.hstack([np.isnan(x) for x in metrics]).any(-1)\n",
    "metrics = [x[~any_isnan_mask] for x in metrics]\n",
    "N = len(metrics)\n",
    "K = np.zeros((N, N))\n",
    "for i, mi in enumerate(metrics):\n",
    "    for j, mj in enumerate(metrics):\n",
    "        s = stats.spearmanr(mi, mj)\n",
    "        K[i,j] = s.statistic\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt_kernel_matrix_one(\n",
    "    fig, ax, K, title=f'Spearmanr ({model_name}:{dataset})', \n",
    "    vmin=-1, vmax=1, cmap='bwr', n_ticks=N, annotate=True)\n",
    "ax.set_xticklabels(metrics_name, rotation=-45)\n",
    "ax.set_yticklabels(metrics_name)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ffeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
