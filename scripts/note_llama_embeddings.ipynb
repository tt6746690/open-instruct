{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.open_orca_slim\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=open_orca_slim --model_name_or_path=../results/baselines/BAAI/bge-large-en-v1.5 --use_dist --shuffle --compute_grad_embeddings --grad_randproj_components 2048 --max_seq_len=128 --encode_fn_type=input --text_pooling_type=cls --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/input/bge-large-en-v1.5\n",
      "[{'args': 'sbatch --job-name=lm_outputs.open_orca_slim --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpetncx0jw', 'job_id': 1248518}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "nodes = 1; gpus=6\n",
    "use_lora = False\n",
    "text_pooling_type = 'meanpool'\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "# # #### auto-regressive model, compute loss/embedding/grad\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'; max_seq_len = 2048\n",
    "# # model_name = 'llama2-7b'; model_name_or_path = '../results/baselines/NousResearch/Llama-2-7b-hf'; max_seq_len = 2048\n",
    "# # model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'; max_seq_len = 2048\n",
    "# # model_name = 'pythia-1b-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'; max_seq_len = 2048\n",
    "# # model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'; max_seq_len = 2048\n",
    "# # model_name = 'mistral-7b+ultrachat200kv1'; model_name_or_path = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'; max_seq_len = 2048\n",
    "# # model_name = 'llama-7b+lima'; model_name_or_path = '../results/oi2/llama-7b_lima_ep=5'; max_seq_len = 2048\n",
    "# # model_name = 'codellama-7b'; model_name_or_path = '../results/baselines/codellama/CodeLlama-7b-hf'; max_seq_len = 2048\n",
    "\n",
    "# compute_loss = True; compute_grad = True; encode_fn_type = 'sft' # 'sft'\n",
    "# use_lora = True; lora_rank = 256; lora_alpha = lora_rank\n",
    "# # ####\n",
    "\n",
    "## encoder-based model, for computing embedding only\n",
    "# model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'; max_seq_len = 512; text_pooling_type = 'meanpool'\n",
    "model_name = 'bge-large-en-v1.5'; model_name_or_path = '../results/baselines/BAAI/bge-large-en-v1.5'; max_seq_len = 128; text_pooling_type = 'cls'\n",
    "\n",
    "compute_loss = False; compute_grad = False; encode_fn_type = 'input'\n",
    "#\n",
    "\n",
    "##### \n",
    "datasets = ['open_orca_slim']; nodes=5; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegptv2']; nodes=5; gpus=6; cpu_mem=512\n",
    "# datasets = ['tulu_v2']; nodes=10; gpu=6; cpu_mem=512\n",
    "\n",
    "#####\n",
    "# tulu_v1_mix:\n",
    "# datasets = ['flan_v2', 'cot', 'dolly', 'oasst1', 'gpt4_alpaca', 'code_alpaca', 'sharegpt']; nodes = 5; gpu=6; cpu_mem = 512\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v1_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# tulu_v2_mix:\n",
    "# datasets = ['tulu_v2']; nodes=1; gpus=1; cpu_mem=512\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1', 'gpt4_alpaca', 'code_alpaca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = ['wizardlm']; nodes = 3; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegpt', 'ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['ultrachat15']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = [f'ultrachat15_{i}' for i in [0, 2]]; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['starcoder_commentinstr_cleaned',\n",
    "#             'starcoder_commentinstr',]; nodes=1; gpus=6; cpu_mem=512\n",
    "# datasets = [\n",
    "#     'starcoder_commentinstrv2', # cleaned version\n",
    "# #     'starcoder_commentinstrv3',\n",
    "# ]; nodes = 3; gpus = 6; cpu_mem=512\n",
    "\n",
    "# datasets = ['tulu_v2_human_mix', 'tulu_v2_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512 # if not compute grad embeddings\n",
    "# datasets = ['flan2022_1m']; nodes = 15; gpus=6; cpu_mem = 512 # if do compute grad embeddings\n",
    "# datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{encode_fn_type}/{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "\n",
    "use_dist = True if nodes*gpus>1 else False\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if use_dist:\n",
    "        prefix = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prefix = 'python'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "        {prefix}\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        {'--use_dist' if use_dist else ''} \\\n",
    "        --shuffle \\\n",
    "        {'--compute_loss' if compute_loss else ''} \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        --compute_grad_embeddings \\\n",
    "        --grad_randproj_components 2048 \\\n",
    "        --max_seq_len={max_seq_len} \\\n",
    "        --encode_fn_type={encode_fn_type} \\\n",
    "        --text_pooling_type={text_pooling_type} \\\n",
    "        --save_dir={save_dir} \\\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d836f4ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 23:25:33,239] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "rank/local_rank/world_size: 0/0/1\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Initializing lora(r=256,a=256)\n",
      "trainable params: 134217728 || all params: 7006851072 || trainable%: 1.92\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading and preparing dataset json/default to /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-75cf0d06f4677faa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4809.98it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Dataset json downloaded and prepared to /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-75cf0d06f4677faa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "open_orca_slim dataset length = 517982\n",
      "  0%|                                                | 0/517982 [00:00<?, ?it/s]Fitting random projection for loraB (134217728 -> 2048)\n",
      "Fitting random projection in 219.714s with random matrix size 189.759 MB\n",
      "Log statistics of projection matrix to ensure same initialization cross procs:\n",
      "8.629235890111886e-05, 2.3784141540527344, 2.8352906156214885e-06\n",
      "  0%|                                  | 163/517982 [06:13<130:42:02,  1.10it/s]^C\n",
      "  0%|                                  | 163/517982 [06:14<330:08:58,  2.30s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/note_llama_embeddings.py\", line 711, in <module>\n",
      "    compute_lm_outputs(**vars(args))\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/note_llama_embeddings.py\", line 566, in compute_lm_outputs\n",
      "    outputs['loss'].backward()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 204, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4 python note_llama_embeddings.py --dataset=open_orca_slim --model_name_or_path=../results/baselines/NousResearch/Llama-2-7b-hf --shuffle --compute_loss --compute_grad --use_lora --lora_rank=256 --lora_alpha=256 --compute_grad_embeddings --grad_randproj_components 2048 --max_seq_len=2048 --encode_fn_type=output --text_pooling_type=meanpool --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/output/llama2-7b+lora:r=256:a=256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554b1e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,5\n",
      "5\n",
      "Sat Dec  2 18:00:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   34C    P0   106W / 300W |   1564MiB / 32510MiB |     67%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0   114W / 300W |    493MiB / 32510MiB |     66%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   35C    P0   113W / 300W |    493MiB / 32510MiB |     80%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   35C    P0   106W / 300W |    493MiB / 32510MiB |     66%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A   3152466      C   ...pmemd.cuda_SPFP.MPI.nonmv      491MiB |\n",
      "|    1   N/A  N/A   3152467      C   ...pmemd.cuda_SPFP.MPI.nonmv      357MiB |\n",
      "|    1   N/A  N/A   3152468      C   ...pmemd.cuda_SPFP.MPI.nonmv      357MiB |\n",
      "|    1   N/A  N/A   3152469      C   ...pmemd.cuda_SPFP.MPI.nonmv      357MiB |\n",
      "|    2   N/A  N/A   3152467      C   ...pmemd.cuda_SPFP.MPI.nonmv      491MiB |\n",
      "|    3   N/A  N/A   3152468      C   ...pmemd.cuda_SPFP.MPI.nonmv      491MiB |\n",
      "|    4   N/A  N/A   3152469      C   ...pmemd.cuda_SPFP.MPI.nonmv      491MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-29 19:13:19,598] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    encode_just_one_role,\n",
    "    sklearn_rp_mat_size,\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    gather_grad_embeddings,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc202479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9504"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import json\n",
    "# instruction_data_path = 'data/raw_train/starcoder/role.json'\n",
    "\n",
    "# with open(instruction_data_path, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "    \n",
    "# data = [{'messages': [\n",
    "#     {'role': 'user', 'content': x['instruction']},\n",
    "#     {'role': 'assistant', 'content': x['output']},\n",
    "# ]} for x in data]\n",
    "# len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab6e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# prompt_dir = 'data/raw_train/starcoder/'\n",
    "# prompt_name = 'prompt_detailed_v2'\n",
    "# prompt_name = 'prompt_simple_v2'\n",
    "\n",
    "# path = os.path.join(prompt_dir, prompt_name+'.txt')\n",
    "\n",
    "# with open(path, 'r') as f:\n",
    "#     template = f.read()\n",
    "    \n",
    "    \n",
    "# i = 3\n",
    "# content = data[i]['messages'][1]['content']\n",
    "    \n",
    "# print(template.format(content=content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_outputs/output/llama-7b\n"
     ]
    }
   ],
   "source": [
    "test_run = True\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat15_0'\n",
    "dataset = 'wizardlm'\n",
    "\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "lora_rank = 256\n",
    "lora_alpha = lora_rank\n",
    "grad_randproj_components = 2048\n",
    "encode_fn_type = 'input'  # input, output, sft\n",
    "encode_fn_type = 'output'\n",
    "max_seq_len = 2048\n",
    "text_pooling_type = 'meanpool'\n",
    "\n",
    "#####\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = 'EleutherAI/pythia-1b-deduped'\n",
    "# model_name = 'pythia-160m-deduped'; model_name_or_path = 'EleutherAI/pythia-160m-deduped'\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "compute_grad_embeddings = True; use_lora = True; compute_grad=True; compute_loss = True\n",
    "#####\n",
    "\n",
    "# #####\n",
    "# model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'\n",
    "# compute_grad_embeddings = False; use_lora = False; compute_grad = False; compute_loss = False\n",
    "# #####\n",
    "\n",
    "\n",
    "save_dir = f\"model_outputs/{encode_fn_type}/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', \n",
    "               'tulu_v1_mix',\n",
    "               'tulu_v2_human_mix',\n",
    "               'tulu_v2_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33324799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a528f9b725b4492815ded9ba7a07b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'sentence-transformers' in model_name_or_path:\n",
    "    from transformers import AutoModel\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch.float16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing lora(r=256,a=256)\n",
      "trainable params: 134217728 || all params: 7006851072 || trainable%: 1.92\n"
     ]
    }
   ],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # # the following also applies lora to MLP layers.\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd9434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if compute_grad:\n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "elif 'ultrachat' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'ultrachat', f'{dataset}_data.jsonl')\n",
    "elif 'starcoder' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'starcoder', f'{dataset}.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "if encode_fn_type in ['input', 'output']:\n",
    "    encode_function = partial(\n",
    "        encode_just_one_role,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        encode_fn_type=encode_fn_type,\n",
    "        # add eos token to causal models, e.g., llama, since its not added by default.\n",
    "        add_eos_token=False if any(y in model_name_or_path for y in ['mpnet', 'bge']) else True,\n",
    "    )\n",
    "elif encode_fn_type == 'sft':    \n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        add_eos_token=False,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f'encode_fn_type={encode_fn_type} not implemented.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e051e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-a386ff309e3a3fc4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d24cf19b4014c16a057284c76f813b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wizardlm dataset length = 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loraB': 'lora_B\\\\.[a-zA-Z_]+\\\\.weight'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e2f99ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local_rank/global=0/0] output=[('text_embedding', (1, 4096), dtype('float32')), ('log_prob', (1, 1), dtype('float32')), ('el2n_agg=mean', (1, 1), dtype('float32')), ('el2n_agg=l2n', (1, 1), dtype('float32')), ('logit_margin', (1, 1), dtype('float32'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from note_llama_embeddings import mean_pooling\n",
    "\n",
    "if compute_grad_embeddings:\n",
    "    rps = {}\n",
    "    for k in grad_statistic_patterns.keys():\n",
    "        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "\n",
    "output = defaultdict(list)\n",
    "for i, batch in tqdm(enumerate(loader), disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    last_hidden_state = outputs['hidden_states'][-1]\n",
    "    if text_pooling_type == 'meanpool':\n",
    "        text_embedding = mean_pooling(last_hidden_state, batch['attention_mask'])\n",
    "    elif text_pooling_type == 'cls':\n",
    "        text_embedding = last_hidden_state[:, 0]\n",
    "    else:\n",
    "        raise ValueError(f'text_pooling_type={text_pooling_type} not supported.')\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "\n",
    "    if compute_loss:\n",
    "        # average of output token log probs\n",
    "        if 'loss' in outputs:\n",
    "            output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "\n",
    "        # el2n scores\n",
    "        losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "        for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "            output[k].append(losses[k].detach().cpu())\n",
    "            \n",
    "    break\n",
    "\n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "            \n",
    "        if compute_grad_embeddings:\n",
    "            grad_embeddings = gather_grad_embeddings(\n",
    "                model,\n",
    "                {k: v for k, v in grad_statistic_patterns.items() if k in ['qkv', 'loraB']},\n",
    "                stacked=True,\n",
    "            )\n",
    "            if test_run:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    output[f'grad_{k}'].append(v)\n",
    "            if i==0:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    t0 = time.time()\n",
    "                    print(f\"Fitting random projection for {k} ({v.size} -> {grad_randproj_components})\")\n",
    "                    rps[k] = rps[k].fit(v[np.newaxis,...])\n",
    "                    print(f\"Fitting random projection in {time.time() - t0:0.3f}s \"\n",
    "                          f\"with random matrix size {sklearn_rp_mat_size(rps[k]) / 1e6:0.3f} MB\")\n",
    "            for k in grad_embeddings.keys():\n",
    "                rp = rps[k]\n",
    "                g = grad_embeddings[k]\n",
    "                output[f'grad_rp_{k}'].append(rp.transform(g[np.newaxis,...]).squeeze())\n",
    "\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "    \n",
    "for k, v in output.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    else:\n",
    "        output[k] = np.vstack(v)\n",
    "\n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'model_outputs/llama-7b+lora:r=256:a=256/lima.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for k in grad_embeddings.keys():\n",
    "    rp = rps[k]\n",
    "    g = grad_embeddings[k]\n",
    "    rp.transform(g[np.newaxis,...]).squeeze()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d608989",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'note_llama_embeddings_{dataset}:{model_name}'+\\\n",
    "    (f'+lora(r={lora_rank},a={lora_alpha})' if use_lora else '')+'.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(output, f)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o1 = pickle.load(f)\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=1,a=1).pkl'\n",
    "save_path = 'note_llama_embeddings_lima:pythia-160m-deduped+lora(r=256,a=256).pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "    o2 = pickle.load(f)\n",
    "    \n",
    "output = {}\n",
    "for k in ['grad_qkv', 'grad_last', 'grad_rp_qkv', 'grad_rp_last']:\n",
    "    output[k] = o1[k]\n",
    "for k in ['grad_loraB', 'grad_rp_loraB']:\n",
    "    output[k] = o2[k]\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4759b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np rp\n",
    "# 10%|▉         | 99/1000 [00:22<03:24,  4.40it/s]\n",
    "\n",
    "# +rp for qkv/last\n",
    "#  10%|▉         | 99/1000 [00:50<07:35,  1.98it/s]\n",
    "\n",
    "# +rp for qkv 21,233,664 -> 2048\n",
    "# 10%|▉         | 99/1000 [03:50<34:56,  2.33s/it]  \n",
    "\n",
    "# +rp for lora(rank=1) qkv. 27,648 -> 2048\n",
    "# 10%|▉         | 99/1000 [00:10<01:34,  9.58it/s]\n",
    "\n",
    "# +rp for lora(rank=256) qkv (7,077,888 -> 2048)\n",
    "# 10%|▉         | 99/1000 [00:57<08:41,  1.73it/s] \n",
    "\n",
    "# (134,217,728 -> 2048) 180MB random matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a688cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,np.stack(v).shape, np.stack(v).size) for k,v in grad_embeddings.items()]\n",
    "# Pythia-160m: [('qkv', (12, 2304, 768)), ('last', (1, 50304, 768))]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75687add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n =1\n",
    "\n",
    "projected_data = {}\n",
    "rp_transform = torch.compile(rp.transform)\n",
    "for n in [1, 10, 100]:\n",
    "    t0 = time()\n",
    "    projected_data[n] = rp_transform(data[:n])\n",
    "\n",
    "    print(f\"Projected {n} samples from {n_features} to {n_components} in \"\n",
    "          f\"{time() - t0:0.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(projected_data)) / projected_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from note_llama_embeddings import torch_cdist\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_components = 2048\n",
    "k = 'qkv'\n",
    "# ['qkv', 'last', 'rp_qkv', 'rp_last', 'loraB', 'rp_loraB']\n",
    "\n",
    "device = 'cuda'\n",
    "data = output[f'grad_qkv']\n",
    "projected_data = output[f'grad_rp_qkv']\n",
    "\n",
    "\n",
    "dists = {}\n",
    "ks = output.keys()\n",
    "ks = ['grad_loraB', 'grad_rp_loraB']\n",
    "\n",
    "for k in ks:\n",
    "    if 'last' in k:\n",
    "        continue\n",
    "    data = output[k]\n",
    "#     data = torch.from_numpy(data).to(torch.float32).to('cuda')\n",
    "#     D = torch_cdist(data, device).ravel()\n",
    "    D = scipy.spatial.distance.pdist(data)\n",
    "#     valid_dist = np.logical_and(~np.isnan(dists), dists!=0)\n",
    "#     dists = dists[valid_dist]\n",
    "#     print(f'percent valid: {100*np.sum(valid_dist)/dists.size:.2f}')\n",
    "    dists[k] = D\n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "# rp = SparseRandomProjection(n_components=n_components)  \n",
    "# projected_data = rp.fit_transform(output[f'grad_{k}'])\n",
    "# print(\n",
    "#     f\"Projected in\"\n",
    "#     f\"{time.time() - t0:0.3f}s\"\n",
    "# )\n",
    "# if hasattr(rp, \"components_\"):\n",
    "#     n_bytes = rp.components_.data.nbytes\n",
    "#     n_bytes += rp.components_.indices.nbytes\n",
    "#     print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n",
    "\n",
    "# projected_dists = torch_cdist(projected_data, device).ravel()\n",
    "# del projected_data\n",
    "# projected_dists = projected_dists[valid_dist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5211013",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b7d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import matplotlib.pyplot as plt\n",
    "from note_llama_embeddings import torch_cdist, plt_pair_of_dists\n",
    "\n",
    "dataset = 'lima'; model_name = 'pythia-160m-deduped+lora(rank=256,alpha=256)'\n",
    "\n",
    "for k in dists.keys():\n",
    "    if k == 'grad_qkv':\n",
    "        continue\n",
    "    fig, axs = plt_pair_of_dists(\n",
    "        dists['grad_qkv'], \n",
    "        dists[k]/(dists[k].mean()/dists['grad_qkv'].mean()), \n",
    "        n_components,\n",
    "        use_hexbin=False)\n",
    "    fig.suptitle(f'{dataset}:{model_name} grad_qkv vs. {k}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists['grad_loraB'].mean()/dists['grad_qkv'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "param_names = []\n",
    "grads = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is not None:\n",
    "        param_names.append(param_name)\n",
    "        grads.append(param.grad.to(torch.float32))\n",
    "\n",
    "data = []\n",
    "for param_name, grad in zip(param_names, grads):\n",
    "    data.append({\n",
    "        'param_name': param_name,\n",
    "        'shape': grad.shape,\n",
    "        'numel': grad.numel(),\n",
    "        'mean': grad.mean().detach().cpu().item(),\n",
    "        'norm': grad.norm().detach().cpu().item(),\n",
    "    })\n",
    "        \n",
    "        \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# pattern = r'\\bquery_key_value\\.weight\\b'\n",
    "# # pattern = r'\\bmlp\\..*?\\.weight\\b'\n",
    "pattern = r'\\blora_B\\b'\n",
    "\n",
    "df = df[df['param_name'].apply(lambda x: True if re.search(pattern, x) else False)]\n",
    "df = df.sort_values(['norm'], ascending=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['norm'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_fullgrad = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62166c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_fullgrad.update({f'grad_loraB(r={lora_rank})_l2n': output['grad_loraB_l2n']})\n",
    "# del output_fullgrad['grad_loraB_l2n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import plt_kernel_matrix_one\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "metrics_name = ['log_prob', 'el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']\n",
    "metrics_name += [x for x in output_fullgrad if 'grad' in x and 'sum' not in x]\n",
    "metrics = [output_fullgrad[k] for k in metrics_name]\n",
    "any_isnan_mask = np.hstack([np.isnan(x) for x in metrics]).any(-1)\n",
    "metrics = [x[~any_isnan_mask] for x in metrics]\n",
    "N = len(metrics)\n",
    "K = np.zeros((N, N))\n",
    "for i, mi in enumerate(metrics):\n",
    "    for j, mj in enumerate(metrics):\n",
    "        s = stats.spearmanr(mi, mj)\n",
    "        K[i,j] = s.statistic\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "plt_kernel_matrix_one(\n",
    "    fig, ax, K, title=f'Spearmanr ({model_name}:{dataset})', \n",
    "    vmin=-1, vmax=1, cmap='bwr', n_ticks=N, annotate=True)\n",
    "ax.set_xticklabels(metrics_name, rotation=-45)\n",
    "ax.set_yticklabels(metrics_name)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ffeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b4775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
