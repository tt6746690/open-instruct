{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b7ddd718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submiting job with:\n",
      "{\n",
      "    \"job_name\": \"lm_outputs.stanford_alpaca\",\n",
      "    \"nodes\": 5,\n",
      "    \"num_cpus\": 32,\n",
      "    \"cpu_mem\": 512,\n",
      "    \"num_gpus\": 6,\n",
      "    \"gpu_type\": \"v100\",\n",
      "    \"test_run\": false,\n",
      "    \"queue\": \"el8\",\n",
      "    \"num_jobs\": 1\n",
      "}\n",
      "torchrun --nnodes=5 --nproc_per_node=6 --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT note_llama_embeddings.py --dataset=stanford_alpaca --model_name_or_path=../results/baselines/huggyllama/llama-7b --use_dist --shuffle --compute_loss --compute_grad --use_lora --lora_rank=256 --lora_alpha=4096 --compute_grad_embeddings --grad_randproj_components 4096 --max_seq_len=2048 --encode_fn_type=sft --text_pooling_type=meanpool --torch_dtype=float16 --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/sft/llama-7b+lora:r=256:a=4096+proj=4096\n",
      "[{'args': 'sbatch --job-name=lm_outputs.stanford_alpaca --partition=el8 --nodes=5 --ntasks-per-node=1 --cpus-per-task=32 --mem=512GB --gres=gpu:6 --output=/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/%J.out --time=6:00:00 /gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/tmpx8pndhtt', 'job_id': 1269689}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rosemary import jpt_in_notebook, jpt_setup; jpt_setup()\n",
    "from llm.submit import submit_job, multiline_to_singleline, shell_scripts_template_slurm\n",
    "import numpy as np\n",
    "\n",
    "log_dir = '/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/'\n",
    "nodes = 1; gpus=6\n",
    "use_lora = False\n",
    "text_pooling_type = 'meanpool'\n",
    "grad_randproj_components = 2048\n",
    "save_grad_embeddings = False\n",
    "torch_dtype = 'float16'\n",
    "add_rsum = False\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "#### auto-regressive model, compute loss/embedding/grad\n",
    "model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'; max_seq_len = 2048\n",
    "# model_name = 'llama2-7b'; model_name_or_path = '../results/baselines/NousResearch/Llama-2-7b-hf'; max_seq_len = 2048\n",
    "# model_name = 'llama-7b_ft=hmv1'; model_name_or_path = '../results/ft1/llama-7b_humanmix'; max_seq_len = 2048\n",
    "# model_name = 'pythia-1b'; model_name_or_path = '../results/baselines/EleutherAI/pythia-1b'; max_seq_len = 2048\n",
    "# model_name = 'pythia-160m'; model_name_or_path = '../results/baselines/EleutherAI/pythia-160m-deduped'; max_seq_len = 2048; torch_dtype = 'float32'\n",
    "# model_name = 'pythia-410m'; model_name_or_path = '../results/baselines/EleutherAI/pythia-410m-deduped'; max_seq_len = 2048\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'; max_seq_len = 2048\n",
    "# model_name = 'mistral-7b+ultrachat200kv1'; model_name_or_path = '../results/ft1_ep=2/mistral-7b_ultrachat200k_beforesplitlongconv'; max_seq_len = 2048\n",
    "# model_name = 'llama-7b+lima'; model_name_or_path = '../results/oi2/llama-7b_lima_ep=5'; max_seq_len = 2048\n",
    "# model_name = 'codellama-7b'; model_name_or_path = '../results/baselines/codellama/CodeLlama-7b-hf'; max_seq_len = 2048\n",
    "\n",
    "compute_loss = True; compute_grad = True; encode_fn_type = 'sft' # 'sft'\n",
    "# use_lora = True;  lora_rank = 512; lora_alpha = int(np.sqrt(lora_rank)**3); grad_randproj_components = 4096\n",
    "use_lora = True;  lora_rank = 256; lora_alpha = int(np.sqrt(lora_rank)**3); grad_randproj_components = 4096\n",
    "\n",
    "# use_lora = True;  lora_rank = 256; lora_alpha = int(np.sqrt(lora_rank)); grad_randproj_components = 2048; add_rsum = True\n",
    "# use_lora = True;  lora_rank = 256; lora_alpha = int(np.sqrt(lora_rank)**3); grad_randproj_components = 2048; add_rsum = True\n",
    "# use_lora = True;  lora_rank = 512; lora_alpha = int(np.sqrt(lora_rank)**3); grad_randproj_components = 2048; add_rsum = True\n",
    "# use_lora = True;  lora_rank = 512; lora_alpha = int(np.sqrt(lora_rank)**3); grad_randproj_components = 4096; add_rsum = True\n",
    "# use_lora = True;  lora_rank = 512; lora_alpha = int(np.sqrt(lora_rank)**3); grad_randproj_components = 1024; add_rsum = True\n",
    "# use_lora = False; lora_rank = 256; lora_alpha = lora_rank; grad_randproj_components = 2048; add_rsum = False\n",
    "####\n",
    "\n",
    "# # encoder-based model, for computing embedding only\n",
    "# model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'; max_seq_len = 512; text_pooling_type = 'meanpool'\n",
    "# # model_name = 'bge-large-en-v1.5'; model_name_or_path = '../results/baselines/BAAI/bge-large-en-v1.5'; max_seq_len = 128; text_pooling_type = 'cls'\n",
    "\n",
    "# compute_loss = False; compute_grad = False; encode_fn_type = 'input'\n",
    "######\n",
    "\n",
    "##### \n",
    "# datasets = ['open_orca_slim']; nodes=5; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegptv2']; nodes=5; gpus=6; cpu_mem=512\n",
    "# datasets = ['tulu_v2']; nodes=10; gpu=6; cpu_mem=512\n",
    "datasets = ['stanford_alpaca']; nodes=5; gpu=6; cpu_mem=512\n",
    "# datasets = ['flan_v2', 'oasst1']; nodes=1; gpu=6; cpu_mem=512\n",
    "# datasets = ['dolly']; nodes=1; gpu=6; cpu_mem=512\n",
    "\n",
    "#####\n",
    "# tulu_v1_mix:\n",
    "# datasets = ['flan_v2', 'cot', 'dolly', 'oasst1', 'gpt4_alpaca', 'code_alpaca', 'sharegpt']; nodes = 5; gpu=6; cpu_mem = 512\n",
    "# datasets = ['tulu_v1_human_mix', 'tulu_v1_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# tulu_v2_mix:\n",
    "# datasets = ['tulu_v2']; nodes=1; gpus=1; cpu_mem=512\n",
    "# datasets = ['cot', 'dolly', 'flan_v2', 'lima', 'oasst1', 'gpt4_alpaca', 'code_alpaca']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = ['wizardlm']; nodes = 3; gpus=6; cpu_mem=512\n",
    "# datasets = ['sharegpt', 'ultrachat']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['ultrachat15']; nodes = 3; gpus=6; cpu_mem = 512\n",
    "# datasets = [f'ultrachat15_{i}' for i in [0, 2]]; nodes = 5; gpus=6; cpu_mem = 512\n",
    "# datasets = ['starcoder_commentinstr_cleaned',\n",
    "#             'starcoder_commentinstr',]; nodes=1; gpus=6; cpu_mem=512\n",
    "# datasets = [\n",
    "#     'starcoder_commentinstrv2', # cleaned version\n",
    "# #     'starcoder_commentinstrv3',\n",
    "# ]; nodes = 3; gpus = 6; cpu_mem=512\n",
    "\n",
    "# datasets = ['starcoder_commentinstrv5']; nodes = 3; gpus = 6; cpu_mem=512\n",
    "\n",
    "# datasets = ['tulu_v2_human_mix', 'tulu_v2_mix']; nodes = 1; gpus=1; cpu_mem = 64\n",
    "# datasets = ['flan_v2']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "# datasets = ['flan2022_1m']; nodes = 5; gpus=6; cpu_mem = 512 # if not compute grad embeddings\n",
    "# datasets = ['flan2022_1m']; nodes = 15; gpus=6; cpu_mem = 512 # if do compute grad embeddings\n",
    "# datasets = ['flan2022v2_1m']; nodes = 5; gpus=6; cpu_mem = 512\n",
    "\n",
    "# # # for testing\n",
    "# datasets = ['lima']; nodes = 1; gpus=6; cpu_mem = 512\n",
    "\n",
    "save_dir = (f\"/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/\"\n",
    "            f\"model_outputs/{encode_fn_type}/{model_name}\")\n",
    "if use_lora:\n",
    "    save_dir += f'+lora:r={lora_rank}:a={lora_alpha}'\n",
    "save_dir += f'+proj={grad_randproj_components}'\n",
    "\n",
    "use_dist = True if nodes*gpus>1 else False\n",
    "for dataset in datasets:\n",
    "    \n",
    "    if use_dist:\n",
    "        prefix = f\"\"\"\n",
    "    torchrun --nnodes={nodes} --nproc_per_node={gpus} \\\n",
    "        --rdzv-id=$SLURM_JOB_ID --rdzv-backend=c10d --rdzv-endpoint=$RDZV_ENDPOINT \\\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prefix = 'python'\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "        {prefix}\n",
    "        note_llama_embeddings.py \\\n",
    "        --dataset={dataset} \\\n",
    "        --model_name_or_path={model_name_or_path} \\\n",
    "        {'--use_dist' if use_dist else ''} \\\n",
    "        --shuffle \\\n",
    "        {'--compute_loss' if compute_loss else ''} \\\n",
    "        {'--compute_grad' if compute_grad else ''} \\\n",
    "        {'--use_lora' if use_lora else ''} \\\n",
    "        {'--lora_rank='+str(lora_rank) if use_lora else ''} \\\n",
    "        {'--lora_alpha='+str(lora_alpha) if use_lora else ''} \\\n",
    "        --compute_grad_embeddings \\\n",
    "        {'--save_grad_embeddings' if save_grad_embeddings else ''} \\\n",
    "        --grad_randproj_components {grad_randproj_components} \\\n",
    "        --max_seq_len={max_seq_len} \\\n",
    "        --encode_fn_type={encode_fn_type} \\\n",
    "        --text_pooling_type={text_pooling_type} \\\n",
    "        {'--add_rsum' if add_rsum else ''} \\\n",
    "        --torch_dtype={torch_dtype} \\\n",
    "        --save_dir={save_dir} \\\n",
    "    \"\"\"\n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "\n",
    "    shell_scripts = shell_scripts_template_slurm.format(\n",
    "        conda_env='open-instruct',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=save_dir)\n",
    "    out = submit_job(\n",
    "        shell_scripts, \n",
    "        job_name=f'lm_outputs.{dataset}', \n",
    "        nodes=nodes,\n",
    "        num_cpus=32,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=gpus,\n",
    "        gpu_type='v100',\n",
    "        test_run=test_run,\n",
    "        job_duration=6,\n",
    "    )\n",
    "    print(cmd)\n",
    "    if not test_run:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d836f4ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 23:25:33,239] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "rank/local_rank/world_size: 0/0/1\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Initializing lora(r=256,a=256)\n",
      "trainable params: 134217728 || all params: 7006851072 || trainable%: 1.92\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading and preparing dataset json/default to /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-75cf0d06f4677faa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4809.98it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00,  4.18it/s]\n",
      "Dataset json downloaded and prepared to /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-75cf0d06f4677faa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "open_orca_slim dataset length = 517982\n",
      "  0%|                                                | 0/517982 [00:00<?, ?it/s]Fitting random projection for loraB (134217728 -> 2048)\n",
      "Fitting random projection in 219.714s with random matrix size 189.759 MB\n",
      "Log statistics of projection matrix to ensure same initialization cross procs:\n",
      "8.629235890111886e-05, 2.3784141540527344, 2.8352906156214885e-06\n",
      "  0%|                                  | 163/517982 [06:13<130:42:02,  1.10it/s]^C\n",
      "  0%|                                  | 163/517982 [06:14<330:08:58,  2.30s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/note_llama_embeddings.py\", line 711, in <module>\n",
      "    compute_lm_outputs(**vars(args))\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/scripts/note_llama_embeddings.py\", line 566, in compute_lm_outputs\n",
      "    outputs['loss'].backward()\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/gpfs/u/scratch/PTFM/PTFMqngp/miniconda3/envs/open-instruct/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 204, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4 python note_llama_embeddings.py --dataset=open_orca_slim --model_name_or_path=../results/baselines/NousResearch/Llama-2-7b-hf --shuffle --compute_loss --compute_grad --use_lora --lora_rank=256 --lora_alpha=256 --compute_grad_embeddings --grad_randproj_components 2048 --max_seq_len=2048 --encode_fn_type=output --text_pooling_type=meanpool --save_dir=/gpfs/u/home/PTFM/PTFMqngp/scratch/github/mitibm2023/external/open-instruct/scripts/model_outputs/output/llama2-7b+lora:r=256:a=256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554b1e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,5\n",
      "5\n",
      "Thu Dec 28 18:34:30 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    68W / 300W |   5507MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    67W / 300W |   5515MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000004:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    67W / 300W |   5515MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    66W / 300W |   5495MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    37W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000035:05:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    371839      C   ...nvs/metatable/bin/python3     5505MiB |\n",
      "|    1   N/A  N/A    371840      C   ...nvs/metatable/bin/python3     5513MiB |\n",
      "|    2   N/A  N/A    371841      C   ...nvs/metatable/bin/python3     5513MiB |\n",
      "|    3   N/A  N/A    371842      C   ...nvs/metatable/bin/python3     5493MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup, jpt_in_notebook; jpt_setup()\n",
    "\n",
    "\n",
    "if jpt_in_notebook():\n",
    "    import os\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \\\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'].split(',')[1] \n",
    "    # '0,1,2,3,4,5'\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f3b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-28 18:34:37,578] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "\n",
    "import pyarrow # import before `torch`, `transformers`, `datasets`\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from open_instruct.finetune_trainer import encode_with_prompt_completion_format, encode_with_messages_format\n",
    "from note_llama_embeddings import (\n",
    "    encode_just_one_role,\n",
    "    sklearn_rp_mat_size,\n",
    "    combine_lm_outputs_for_mixes, \n",
    "    datasets_shard_chunk_size, \n",
    "    compute_losses, \n",
    "    compute_grad_statistic, \n",
    "    compute_grad_norm,\n",
    "    gather_grad_embeddings,\n",
    "    print_trainable_parameters,\n",
    "    get_grad_statistic_pattern,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6c67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_outputs/output/pythia-160m-deduped\n"
     ]
    }
   ],
   "source": [
    "test_run = True\n",
    "\n",
    "save_grad_embeddings = False\n",
    "dataset = 'flan_v2'\n",
    "dataset = 'oasst1'\n",
    "dataset = 'tulu_v1_mix'\n",
    "dataset = 'lima'\n",
    "dataset = 'ultrachat15_0'\n",
    "dataset = 'wizardlm'\n",
    "dataset = 'stanford_alpaca'\n",
    "dataset = 'lima'; save_grad_embeddings = True\n",
    "\n",
    "use_dist = False\n",
    "shuffle = True\n",
    "lora_rank = 256\n",
    "lora_alpha = lora_rank\n",
    "# lora_alpha = int(np.sqrt(lora_rank))\n",
    "lora_alpha = int(np.sqrt(lora_rank)**3)\n",
    "grad_randproj_components = 2048\n",
    "encode_fn_type = 'input'  # input, output, sft\n",
    "encode_fn_type = 'output'\n",
    "max_seq_len = 2048\n",
    "text_pooling_type = 'meanpool'\n",
    "add_rsum = False\n",
    "# add_rsum = True\n",
    "\n",
    "#####\n",
    "# model_name = 'llama-7b'; model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name = 'pythia-1b-deduped'; model_name_or_path = 'EleutherAI/pythia-1b-deduped'\n",
    "model_name = 'pythia-160m-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-160m-deduped'\n",
    "# model_name = 'pythia-410m-deduped'; model_name_or_path = '../results/baselines/EleutherAI/pythia-410m-deduped'\n",
    "# model_name = 'mistral-7b'; model_name_or_path = '../results/baselines/mistralai/Mistral-7B-v0.1'\n",
    "compute_grad_embeddings = True; use_lora = True; compute_grad=True; compute_loss = True\n",
    "# compute_grad_embeddings = True; use_lora = False; compute_grad=True; compute_loss = True\n",
    "#####\n",
    "\n",
    "# #####\n",
    "# model_name = 'all-mpnet-base-v2'; model_name_or_path = '../results/baselines/sentence-transformers/all-mpnet-base-v2'\n",
    "# compute_grad_embeddings = False; use_lora = False; compute_grad = False; compute_loss = False\n",
    "# #####\n",
    "\n",
    "\n",
    "save_dir = f\"model_outputs/{encode_fn_type}/{model_name}\"\n",
    "if not test_run:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa6afe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank/local_rank/world_size: 0/0/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dataset in ['tulu_v1_human_mix', \n",
    "               'tulu_v1_mix',\n",
    "               'tulu_v2_human_mix',\n",
    "               'tulu_v2_mix']:\n",
    "    combine_lm_outputs_for_mixes(dataset, save_dir)\n",
    "\n",
    "if use_dist:\n",
    "    dist.init_process_group(\"gloo\", timeout=datetime.timedelta(hours=6))\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank() # global rank\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "print(f'rank/local_rank/world_size: {rank}/{local_rank}/{world_size}\\n')\n",
    "\n",
    "device = f'cuda:{str(local_rank)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33324799",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sentence-transformers' in model_name_or_path:\n",
    "    from transformers import AutoModel\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map=device,\n",
    "#         torch_dtype=torch.float16,\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16010ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing lora(r=256,a=4096)\n",
      "trainable params: 7077888 || all params: 171760128 || trainable%: 4.12\n"
     ]
    }
   ],
   "source": [
    "if use_lora:\n",
    "    if not compute_grad:\n",
    "        raise ValueError('compute_grad must be True if use LoRA!')\n",
    "    \n",
    "    print(f'Initializing lora(r={lora_rank},a={lora_alpha})')\n",
    "    # ensure the same initialization\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    \n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # # the following also applies lora to MLP layers.\n",
    "        # target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "    elif 'pythia' in model_name_or_path:\n",
    "        target_modules = ['query_key_value']\n",
    "    else:\n",
    "        raise ValueError(f'Define new `target_modules` for LoraConfig for {model_name_or_path}')\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        bias='none',\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha, \n",
    "        lora_dropout=0.,\n",
    "        target_modules=target_modules,\n",
    "        init_lora_weights='gaussian',\n",
    "    )\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    ## don't need to compute gradient to `lora_A`, saves computation (i think) but not space.\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora_A' in param_name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2bc2a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if compute_grad:\n",
    "    if any(x in model_name_or_path.lower() for x in ['llama', 'mistral']):\n",
    "        # Computing full gradient for llama is computationally prohibitive.\n",
    "        # Use gradient checkpointing to prevent oom issues.\n",
    "        # Note gradient checkpointing is only applied when in training mode\n",
    "        #     https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L908\n",
    "        # So need to set `model.train()`. This is harmless because\n",
    "        # llama's eval/train computation is exactly the same, since there's no dropout layer.\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.train()\n",
    "else:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd9434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, use_fast=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbcbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_dir = '../data/processed'\n",
    "if 'flan2022' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'flan2022', f'{dataset}_data.jsonl')\n",
    "elif 'ultrachat' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'ultrachat', f'{dataset}_data.jsonl')\n",
    "elif 'open_orca' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'open_orca', f'{dataset}_data.jsonl')\n",
    "elif 'sharegpt' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'sharegpt', f'{dataset}_data.jsonl')\n",
    "elif 'starcoder' in dataset:\n",
    "    train_file = os.path.join(processed_dir, 'starcoder', f'{dataset}.jsonl')\n",
    "else:\n",
    "    train_file = os.path.join(processed_dir, dataset, f'{dataset}_data.jsonl')\n",
    "assert(os.path.isfile(train_file))\n",
    "\n",
    "if encode_fn_type in ['input', 'output']:\n",
    "    encode_function = partial(\n",
    "        encode_just_one_role,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        encode_fn_type=encode_fn_type,\n",
    "        # add eos token to causal models, e.g., llama, since its not added by default.\n",
    "        add_eos_token=False if any(y in model_name_or_path for y in ['mpnet', 'bge']) else True,\n",
    "    )\n",
    "elif encode_fn_type == 'sft':    \n",
    "    encode_function = partial(\n",
    "        encode_with_messages_format,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_len,\n",
    "        add_eos_token=False,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f'encode_fn_type={encode_fn_type} not implemented.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e051e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b660d905654412b723f235a12e6df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /gpfs/u/scratch/PTFM/PTFMqngp/huggingface_cache/datasets/json/default-1ca1bac0eed76345/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8cf7c07d1f39f309_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "if rank == 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    # print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "if use_dist:\n",
    "    dist.barrier()\n",
    "if rank!= 0:\n",
    "    raw_datasets = load_dataset(\"json\", data_files={'train': train_file})\n",
    "    if test_run:\n",
    "        raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n",
    "    # print(f\"{dataset} dataset length = {len(raw_datasets['train'])}\")\n",
    "    lm_datasets = raw_datasets.map(\n",
    "        encode_function, batched=False, num_proc=16,\n",
    "        desc=\"Tokenizing and reformatting instruction data\")\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    output_all_columns=False,\n",
    "    columns=['input_ids', 'labels', 'attention_mask'])\n",
    "if shuffle:\n",
    "    random.seed(0)\n",
    "    shuffle_inds = list(range(len(train_dataset)))\n",
    "    random.shuffle(shuffle_inds)\n",
    "    reverse_shuffle_inds = [(i, ind) for i, ind in enumerate(shuffle_inds)]\n",
    "    reverse_shuffle_inds = sorted(reverse_shuffle_inds, key=lambda x: x[1])\n",
    "    reverse_shuffle_inds = [x[0] for x in reverse_shuffle_inds]\n",
    "    train_dataset = train_dataset.select(shuffle_inds)\n",
    "train_dataset_chunk_sizes = [datasets_shard_chunk_size(len(train_dataset), num_shards=world_size, index=i) \n",
    "            for i in range(world_size)]\n",
    "train_dataset = train_dataset.shard(\n",
    "    num_shards=world_size, \n",
    "    index=rank,\n",
    "    contiguous=True)\n",
    "loader = DataLoader(train_dataset, shuffle=False, batch_size=1, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd9bebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loraB': 'lora_B\\\\.[a-zA-Z_]+\\\\.weight'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_statistic_patterns = get_grad_statistic_pattern(model_name_or_path, use_lora)\n",
    "grad_statistic_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2f99ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random projection for loraB (7077888 -> 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:45<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random projection in 44.201s with random matrix size 43.618 MB\n",
      "Log statistics of projection matrix for loraB to ensure same initialization cross procs:\n",
      "0.0003761366975528307, 1.1397535800933838, 5.313996098266216e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from note_llama_embeddings import mean_pooling\n",
    "\n",
    "if compute_grad_embeddings:\n",
    "    rps = {}\n",
    "    for k in grad_statistic_patterns.keys():\n",
    "        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "\n",
    "output = defaultdict(list)\n",
    "for i, batch in tqdm(enumerate(loader), disable=rank!=0, total=len(loader)):\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    \n",
    "    if compute_grad:\n",
    "        outputs = model(**batch, output_hidden_states=True, use_cache=False)\n",
    "        model.zero_grad()\n",
    "        outputs['loss'].backward()\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    # (bsz, seq_len, hidden_size) -> (bsz, hidden_size)\n",
    "    last_hidden_state = outputs['hidden_states'][-1]\n",
    "    if text_pooling_type == 'meanpool':\n",
    "        text_embedding = mean_pooling(last_hidden_state, batch['attention_mask'])\n",
    "    elif text_pooling_type == 'cls':\n",
    "        text_embedding = last_hidden_state[:, 0]\n",
    "    else:\n",
    "        raise ValueError(f'text_pooling_type={text_pooling_type} not supported.')\n",
    "    output['text_embedding'].append(text_embedding.to(torch.float32).detach().cpu())\n",
    "\n",
    "    if compute_loss:\n",
    "        # average of output token log probs\n",
    "        if 'loss' in outputs:\n",
    "            output['log_prob'].append(-outputs['loss'].detach().cpu())\n",
    "\n",
    "        # el2n scores\n",
    "        losses = compute_losses(outputs['logits'], batch['labels'])\n",
    "        for k in ['el2n_agg=mean', 'el2n_agg=l2n', 'logit_margin']:\n",
    "            output[k].append(losses[k].detach().cpu())\n",
    "            \n",
    "    ## gradient statistic\n",
    "    if compute_grad:\n",
    "        grad_statistics = compute_grad_statistic(model, grad_statistic_patterns)\n",
    "        for k, v in grad_statistics.items():\n",
    "            output[f'grad_{k}'].append(v.detach().cpu())\n",
    "            \n",
    "        if compute_grad_embeddings:\n",
    "            grad_embeddings = gather_grad_embeddings(\n",
    "                model,\n",
    "                {k: v for k, v in grad_statistic_patterns.items() if k in ['qkv', 'loraB']},\n",
    "                stacked=True,\n",
    "                add_rsum=add_rsum,\n",
    "            )\n",
    "            if save_grad_embeddings:\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    output[f'grad_{k}'].append(v)\n",
    "            if i==0:\n",
    "                rps = {}\n",
    "                for k, v in grad_embeddings.items():\n",
    "                    t0 = time.time()\n",
    "                    if 'rsum' not in k:\n",
    "                        rps[k] = SparseRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "                    else:\n",
    "                        rps[k] = GaussianRandomProjection(n_components=grad_randproj_components, random_state=0)\n",
    "                    print(f\"Fitting random projection for {k} ({v.size} -> {grad_randproj_components})\")\n",
    "                    rps[k] = rps[k].fit(v[np.newaxis,...])\n",
    "                    print(f\"Fitting random projection in {time.time() - t0:0.3f}s \"\n",
    "                        f\"with random matrix size {sklearn_rp_mat_size(rps[k]) / 1e6:0.3f} MB\")\n",
    "                    print(f'Log statistics of projection matrix for {k} to ensure same initialization cross procs:\\n'\n",
    "                        f\"{np.mean(rps[k].components_ != 0)}, {np.max(rps[k].components_)}, {np.mean(rps[k].components_[0])}\")\n",
    "            for k in grad_embeddings.keys():\n",
    "                rp = rps[k]\n",
    "                g = grad_embeddings[k]\n",
    "                output[f'grad_rp_{k}'].append(rp.transform(g[np.newaxis,...]).squeeze())\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b14225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local_rank/global=0/0] output=[('text_embedding', (1, 768), dtype('float32')), ('log_prob', (1, 1), dtype('float32')), ('el2n_agg=mean', (1, 1), dtype('float32')), ('el2n_agg=l2n', (1, 1), dtype('float32')), ('logit_margin', (1, 1), dtype('float32')), ('grad_loraB_l2n', (1, 1), dtype('float32')), ('grad_loraB', (1, 7077888), dtype('float32')), ('grad_rp_loraB', (1, 2048), dtype('float32'))]\n"
     ]
    }
   ],
   "source": [
    "for k, v in output.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        output[k] = torch.vstack(v).to(torch.float32).numpy()\n",
    "    else:\n",
    "        output[k] = np.vstack(v)\n",
    "\n",
    "\n",
    "print(f'[local_rank/global={local_rank}/{rank}] '\n",
    "      f'output={[(k, v.shape, v.dtype) for k, v in output.items()]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
