set -e
set -x
CUDA_VISIBLE_DEVICES=0 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10" --max_new_tokens 2048 --save_dir "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt" --eval_batch_size 10 --annotators_config alpaca_eval_gpt4_turbo_fn --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16

CUDA_VISIBLE_DEVICES=0 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3" --max_new_tokens 2048 --save_dir "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=rbf:gamma=1e-3:kmd=llama7br512p4096:kemb=text+embedding_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt" --eval_batch_size 10 --annotators_config alpaca_eval_gpt4_turbo_fn --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16

CUDA_VISIBLE_DEVICES=0 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10" --max_new_tokens 2048 --save_dir "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt" --eval_batch_size 10 --annotators_config alpaca_eval_gpt4_turbo_fn --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16

CUDA_VISIBLE_DEVICES=0 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3" --max_new_tokens 2048 --save_dir "results/oi5_dolly:llama-7b/llama-7b_dolly_score=dppmap:k=vmf:gamma=1:kmd=llama7br512p4096:kemb=grad+rp+loraB_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt" --eval_batch_size 10 --annotators_config alpaca_eval_gpt4_turbo_fn --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16

CUDA_VISIBLE_DEVICES=0 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path "results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10" --max_new_tokens 2048 --save_dir "results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt" --eval_batch_size 10 --annotators_config alpaca_eval_gpt4_turbo_fn --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16

CUDA_VISIBLE_DEVICES=0 python -m eval.alpaca_farm.run_eval --reference_path alpaca_eval_data --model_name_or_path "results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3" --max_new_tokens 2048 --save_dir "results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=30000:ep=3/eval/alpacafarm_ann=alpaca:eval:gpt4:turbo:fn_chatfmt" --eval_batch_size 10 --annotators_config alpaca_eval_gpt4_turbo_fn --use_vllm --use_chat_format --chat_formatting_function eval.templates.create_prompt_with_tulu_chat_format --use_slow_tokenizer --torch_dtype bfloat16