set -e
set -x
CUDA_VISIBLE_DEVICES=0 python -m fastchat.llm_judge.gen_model_answer --model-path results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10 --model-id tulu --bench-name mt_bench --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --max-new-token 2048 --answer-file results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt/model_answer.jsonl --dtype bfloat16 && python -m fastchat.llm_judge.gen_judgment --bench-name mt_bench --judge-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl --judge-model gpt-3.5-turbo-1106 --mode single --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --answer-dir results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt --ref-answer-dir /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer --output-file results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt/gpt-3.5-turbo-1106_single.jsonl --first-n 1 && python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file results/oi5_stanford_alpaca:llama-7b/llama-7b_stanford_alpaca_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt/gpt-3.5-turbo-1106_single.jsonl --mode single --save-to-json