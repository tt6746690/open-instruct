set -e
set -x
CUDA_VISIBLE_DEVICES=0 python -m fastchat.llm_judge.gen_model_answer --model-path results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10 --model-id tulu --bench-name mt_bench --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --max-new-token 2048 --answer-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/model_answer.jsonl --dtype bfloat16 && python -m fastchat.llm_judge.gen_judgment --bench-name mt_bench --judge-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl --judge-model gpt-4-1106-preview --mode single --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --answer-dir results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt --ref-answer-dir /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer --output-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl && python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file results/oi5_dolly:llama-7b/llama-7b_dolly_score=random:s=0_pace=prune:size=10000:ep=10/eval/mtbench_ann=gpt:4:1106:preview_chatfmt/gpt-4-1106-preview_single.jsonl --mode single --save-to-json