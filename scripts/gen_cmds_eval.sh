set -e
set -x
CUDA_VISIBLE_DEVICES=0 python -m fastchat.llm_judge.gen_model_answer --model-path results/baselines/HuggingFaceH4/zephyr-7b-beta --model-id zephyr-7b-beta --bench-name mt_bench --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --max-new-token 2048 --answer-file results/baselines/HuggingFaceH4/zephyr-7b-beta/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt/model_answer.jsonl --dtype bfloat16 && python -m fastchat.llm_judge.gen_judgment --bench-name mt_bench --judge-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/judge_prompts.jsonl --judge-model gpt-3.5-turbo-1106 --mode single --question-file /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl --answer-dir results/baselines/HuggingFaceH4/zephyr-7b-beta/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt --ref-answer-dir /dccstor/data-pruning/wpq/github/mitibm2023/external/FastChat/fastchat/llm_judge/data/mt_bench/reference_answer --output-file results/baselines/HuggingFaceH4/zephyr-7b-beta/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt/gpt-3.5-turbo-1106_single.jsonl && python -m fastchat.llm_judge.show_result --bench-name mt_bench --input-file results/baselines/HuggingFaceH4/zephyr-7b-beta/eval/mtbench_ann=gpt:3.5:turbo:1106_chatfmt/gpt-3.5-turbo-1106_single.jsonl --mode single --save-to-json