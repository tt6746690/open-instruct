Running on cccxc515
accelerate launch     --mixed_precision bf16     --num_machines 1     --num_processes 1     open_instruct/finetune.py     --model_name_or_path gpt2-Large     --tokenizer_name gpt2-Large     --train_file data/processed/flanv2_cot_oasst1_dolly.jsonl     --max_seq_length 1024     --preprocessing_num_workers 16     --per_device_train_batch_size 2     --gradient_accumulation_steps 64     --learning_rate 2e-5     --lr_scheduler_type linear     --warmup_ratio 0.03     --weight_decay 0.     --num_train_epochs 2     --output_dir results/gpt2-Large_human_mix     --with_tracking     --report_to tensorboard     --logging_steps 1
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /dccstor/mit_fm/miniconda/envs/open-instruct did not contain libcudart.so as expected! Searching further paths...
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/ibm/lsfsuite/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/opt/ibm/lsfsuite/ext/ppm/10.2/linux2.6-glibc2.3-x86_64/lib did not contain libcudart.so as expected! Searching further paths...
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/u/wpq/.oh-my-zsh/functions'), PosixPath('/u/wpq/.oh-my-zsh/completions'), PosixPath('/usr/local/share/zsh/site-functions')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-ipc-7b9aede2-40e7-4a50-8c08-387cd7bd00e3.sock')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/wpq/_/default')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/1628998.tmpdir/.1688686882.1628998.acct')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/run/user/701058/vscode-git-e28ec7ca58.sock')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/dccstor/mit_fm/miniconda/envs/open-instruct/etc/xml/catalog file')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\n}')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\'`\' ";\n fi;\n done;\n if [ -n "${_mlre'), PosixPath('-}" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n}'), PosixPath('-0}" = \'1\' ]; then\n case "$-" in \n *v*x*)\n set +vx;\n _mlshdbg=\'vx\'\n ;;\n *v*)\n set +v;\n _mlshdbg=\'v\'\n ;;\n *x*)\n set +x;\n _mlshdbg=\'x\'\n ;;\n *)\n _mlshdbg=\'\'\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n "${IFS+x}" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=\' \';\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('() {  unset _mlshdbg;\n if [ "${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-};\n do\n if [ "${_mlv}" = "${_mlv##*[!A-Za-z0-9_]}" -a "${_mlv}" = "${_mlv#[0-9]}" ]; then\n if [ -n "`eval \'echo ${\'$_mlv\'+x}\'`" ]; then\n _mlre="${_mlre'), PosixPath("-}${_mlv}='`eval 'echo ${'$_mlrv'"), PosixPath('-}${_mlv}_modquar=\'`eval \'echo ${\'$_mlv\'}\'`\' ";\n fi;\n _mlrv="MODULES_RUNENV_${_mlv}";\n _mlre="${_mlre'), PosixPath('-}" ]; then\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \'"$@"\'`;\n else\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash "$@"`;\n fi;\n _mlstatus=$?;\n if [ -n "${_mlIFS+x}" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n "${_mlshdbg')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-0}" = \'1\' ]; then\n typeset swname=\'main\';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname=\'compatibility\';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo "Switching to Modules $swname version";\n source /usr/share/Modules/init/bash;\n else\n echo "Cannot switch to Modules $swname version, command not found";\n return 1;\n fi\n}'), PosixPath('() {  typeset swfound=1;\n if [ "${MODULES_USE_COMPAT_VERSION')}
  warn(msg)
/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ "$1" = "load" -o "$1" = "unload" ]; then\n eval "module $@";\n else\n /usr/bin/scl "$@";\n fi\n}')}
  warn(msg)
07/06/2023 19:43:12 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16


===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 120
CUDA SETUP: Loading binary /dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...
Downloading and preparing dataset json/default to /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1265.63it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 11752 examples [00:00, 54913.75 examples/s]Generating train split: 17273 examples [00:00, 51355.22 examples/s]Generating train split: 37691 examples [00:00, 75618.77 examples/s]Generating train split: 50542 examples [00:00, 71122.44 examples/s]Generating train split: 64333 examples [00:01, 61775.38 examples/s]Generating train split: 71724 examples [00:01, 55091.17 examples/s]Generating train split: 87632 examples [00:01, 71001.49 examples/s]Generating train split: 102046 examples [00:01, 80112.63 examples/s]Generating train split: 140467 examples [00:01, 132892.81 examples/s]Generating train split: 158051 examples [00:01, 138195.49 examples/s]Generating train split: 173992 examples [00:01, 118860.45 examples/s]Generating train split: 190051 examples [00:02, 112072.17 examples/s]Generating train split: 208668 examples [00:02, 109776.73 examples/s]Generating train split: 221523 examples [00:02, 109605.26 examples/s]Generating train split: 235655 examples [00:02, 114258.02 examples/s]Generating train split: 248530 examples [00:02, 111941.97 examples/s]Generating train split: 266457 examples [00:02, 125996.09 examples/s]                                                                     Dataset json downloaded and prepared to /dccstor/mit_fm/wpq/hf_cache/datasets/json/default-247ebf1b4910b0d3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 22.38it/s]
loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-Large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-Large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/vocab.json
loading file merges.txt from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/merges.txt
loading file tokenizer.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-Large",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1280,
  "n_head": 20,
  "n_inner": null,
  "n_layer": 36,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

loading weights file model.safetensors from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/model.safetensors
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "transformers_version": "4.30.2"
}

All model checkpoint weights were used when initializing GPT2LMHeadModel.

All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-Large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
loading configuration file generation_config.json from cache at /dccstor/mit_fm/wpq/hf_cache/hub/models--gpt2-Large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595/generation_config.json
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "transformers_version": "4.30.2"
}

Assigning <pad> to the pad_token key of the tokenizer
Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 0/270679 [00:00<?, ? examples/s]Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 1/270679 [00:00<21:56:07,  3.43 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   0%|          | 766/270679 [00:00<01:49, 2468.82 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   1%|          | 1923/270679 [00:00<00:50, 5305.55 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   1%|          | 3097/270679 [00:00<00:36, 7249.57 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   2%|▏         | 4312/270679 [00:00<00:30, 8718.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   2%|▏         | 5443/270679 [00:00<00:28, 9466.56 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   2%|▏         | 6682/270679 [00:00<00:25, 10204.99 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   3%|▎         | 7789/270679 [00:01<00:27, 9715.28 examples/s] Tokenizing and reformatting instruction data (num_proc=16):   3%|▎         | 8910/270679 [00:01<00:25, 10107.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   4%|▎         | 10031/270679 [00:01<00:25, 10381.40 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   4%|▍         | 11145/270679 [00:01<00:24, 10565.29 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   5%|▍         | 12230/270679 [00:01<00:25, 10337.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   5%|▍         | 13469/270679 [00:01<00:23, 10878.05 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   5%|▌         | 14644/270679 [00:01<00:24, 10567.79 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   6%|▌         | 15714/270679 [00:01<00:25, 10038.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   6%|▌         | 16843/270679 [00:01<00:24, 10353.93 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   7%|▋         | 17894/270679 [00:01<00:24, 10357.45 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   7%|▋         | 18994/270679 [00:02<00:24, 10103.15 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   7%|▋         | 20087/270679 [00:02<00:24, 10127.77 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   8%|▊         | 21129/270679 [00:02<00:24, 10171.52 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   8%|▊         | 22162/270679 [00:02<00:24, 10187.79 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   9%|▊         | 23194/270679 [00:02<00:26, 9368.28 examples/s] Tokenizing and reformatting instruction data (num_proc=16):   9%|▉         | 24273/270679 [00:02<00:25, 9665.63 examples/s]Tokenizing and reformatting instruction data (num_proc=16):   9%|▉         | 25275/270679 [00:02<00:25, 9605.25 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  10%|▉         | 26454/270679 [00:02<00:23, 10188.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  10%|█         | 27596/270679 [00:02<00:23, 10517.46 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  11%|█         | 28683/270679 [00:03<00:30, 7811.67 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  11%|█▏        | 30853/270679 [00:03<00:22, 10840.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  12%|█▏        | 32091/270679 [00:03<00:22, 10432.72 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  12%|█▏        | 33241/270679 [00:03<00:22, 10533.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  13%|█▎        | 34370/270679 [00:03<00:23, 10150.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  13%|█▎        | 35440/270679 [00:03<00:23, 10007.59 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  13%|█▎        | 36487/270679 [00:03<00:23, 10105.89 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  14%|█▍        | 37555/270679 [00:03<00:23, 9907.51 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  14%|█▍        | 38565/270679 [00:04<00:23, 9680.03 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  15%|█▍        | 39557/270679 [00:04<00:24, 9612.40 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  15%|█▌        | 40698/270679 [00:04<00:23, 9865.15 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  15%|█▌        | 41777/270679 [00:04<00:22, 10041.82 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  16%|█▌        | 42938/270679 [00:04<00:22, 10159.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  16%|█▋        | 44018/270679 [00:04<00:22, 10094.29 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  17%|█▋        | 45063/270679 [00:04<00:24, 9050.28 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  17%|█▋        | 46677/270679 [00:04<00:20, 10706.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  18%|█▊        | 47795/270679 [00:04<00:21, 10417.59 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  18%|█▊        | 48865/270679 [00:05<00:22, 10004.91 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  18%|█▊        | 50073/270679 [00:05<00:21, 10425.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  19%|█▉        | 51146/270679 [00:05<00:21, 10144.93 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  19%|█▉        | 52171/270679 [00:05<00:22, 9688.47 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  20%|█▉        | 53309/270679 [00:05<00:22, 9866.94 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  20%|██        | 54303/270679 [00:05<00:31, 6793.23 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  21%|██        | 56581/270679 [00:05<00:21, 10071.52 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  21%|██▏       | 57809/270679 [00:06<00:21, 9704.97 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  22%|██▏       | 58998/270679 [00:06<00:22, 9236.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  22%|██▏       | 60433/270679 [00:06<00:20, 10412.08 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  23%|██▎       | 61652/270679 [00:06<00:20, 9979.96 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  23%|██▎       | 62727/270679 [00:06<00:22, 9126.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  24%|██▎       | 63773/270679 [00:06<00:32, 6435.09 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  25%|██▍       | 66924/270679 [00:06<00:18, 11131.61 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  25%|██▌       | 68402/270679 [00:07<00:19, 10586.00 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  26%|██▌       | 69720/270679 [00:07<00:18, 10677.42 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  26%|██▌       | 71001/270679 [00:07<00:19, 10326.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  27%|██▋       | 72172/270679 [00:07<00:19, 10102.56 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  27%|██▋       | 73280/270679 [00:07<00:19, 10144.69 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  27%|██▋       | 74384/270679 [00:07<00:19, 10070.63 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  28%|██▊       | 75456/270679 [00:07<00:19, 10127.61 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  28%|██▊       | 76546/270679 [00:07<00:18, 10303.36 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  29%|██▊       | 77610/270679 [00:08<00:19, 9940.93 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  29%|██▉       | 78734/270679 [00:08<00:19, 10034.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  29%|██▉       | 79756/270679 [00:08<00:19, 9798.64 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  30%|██▉       | 80823/270679 [00:08<00:19, 9877.69 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  30%|███       | 81869/270679 [00:08<00:18, 10004.19 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  31%|███       | 82929/270679 [00:08<00:18, 10157.60 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  31%|███       | 84092/270679 [00:08<00:17, 10544.17 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  31%|███▏      | 85237/270679 [00:08<00:17, 10789.80 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  32%|███▏      | 86400/270679 [00:08<00:16, 10970.99 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  32%|███▏      | 87520/270679 [00:09<00:17, 10656.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  33%|███▎      | 88611/270679 [00:09<00:17, 10355.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  33%|███▎      | 89651/270679 [00:09<00:17, 10297.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  34%|███▎      | 90712/270679 [00:09<00:17, 10001.39 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  34%|███▍      | 91723/270679 [00:09<00:18, 9767.85 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  34%|███▍      | 92717/270679 [00:09<00:18, 9598.18 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  35%|███▍      | 93783/270679 [00:09<00:17, 9865.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  35%|███▌      | 94798/270679 [00:09<00:17, 9908.42 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  35%|███▌      | 95848/270679 [00:09<00:17, 9755.85 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  36%|███▌      | 96826/270679 [00:09<00:17, 9686.19 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  36%|███▌      | 97798/270679 [00:10<00:18, 9329.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  37%|███▋      | 98889/270679 [00:10<00:17, 9772.42 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  37%|███▋      | 99987/270679 [00:10<00:17, 9963.92 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  37%|███▋      | 101186/270679 [00:10<00:16, 10403.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  38%|███▊      | 102257/270679 [00:10<00:16, 10171.49 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  38%|███▊      | 103301/270679 [00:10<00:16, 10049.92 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  39%|███▊      | 104398/270679 [00:10<00:16, 10047.22 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  39%|███▉      | 105416/270679 [00:10<00:16, 10051.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  39%|███▉      | 106447/270679 [00:10<00:16, 10030.24 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  40%|███▉      | 107521/270679 [00:11<00:16, 9866.58 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  40%|████      | 108543/270679 [00:11<00:17, 9162.55 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  40%|████      | 109475/270679 [00:11<00:18, 8581.99 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  41%|████      | 110750/270679 [00:11<00:16, 9613.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  41%|████▏     | 111847/270679 [00:11<00:15, 9952.23 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  42%|████▏     | 112949/270679 [00:11<00:15, 10139.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  42%|████▏     | 114010/270679 [00:11<00:15, 10021.13 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  43%|████▎     | 115042/270679 [00:11<00:15, 10068.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  43%|████▎     | 116194/270679 [00:11<00:15, 10229.20 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  43%|████▎     | 117242/270679 [00:12<00:15, 9962.20 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  44%|████▎     | 118282/270679 [00:12<00:15, 9861.82 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  44%|████▍     | 119273/270679 [00:12<00:16, 9354.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  44%|████▍     | 120390/270679 [00:12<00:15, 9843.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  45%|████▍     | 121409/270679 [00:12<00:16, 8948.24 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  45%|████▌     | 122426/270679 [00:12<00:15, 9267.94 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  46%|████▌     | 123432/270679 [00:12<00:15, 9478.34 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  46%|████▌     | 124399/270679 [00:12<00:15, 9370.13 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  46%|████▋     | 125373/270679 [00:12<00:15, 9323.44 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  47%|████▋     | 126329/270679 [00:13<00:16, 8830.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  47%|████▋     | 127251/270679 [00:13<00:16, 8891.49 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  47%|████▋     | 128162/270679 [00:13<00:17, 7959.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  48%|████▊     | 129000/270679 [00:13<00:17, 8040.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  48%|████▊     | 130022/270679 [00:13<00:16, 8486.32 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  48%|████▊     | 131100/270679 [00:13<00:15, 8940.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  49%|████▉     | 132116/270679 [00:13<00:15, 9013.76 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  49%|████▉     | 133224/270679 [00:13<00:14, 9446.69 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  50%|████▉     | 134197/270679 [00:13<00:14, 9371.33 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  50%|████▉     | 135146/270679 [00:14<00:15, 8916.68 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  50%|█████     | 136054/270679 [00:14<00:16, 8127.25 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  51%|█████     | 136895/270679 [00:14<00:17, 7443.47 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  51%|█████     | 137674/270679 [00:14<00:18, 7273.24 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  51%|█████     | 138430/270679 [00:14<00:18, 7285.82 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  51%|█████▏    | 139173/270679 [00:14<00:18, 7009.42 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  52%|█████▏    | 139926/270679 [00:14<00:18, 7092.17 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  52%|█████▏    | 140710/270679 [00:14<00:17, 7257.47 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  52%|█████▏    | 141482/270679 [00:14<00:17, 7383.38 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  53%|█████▎    | 142236/270679 [00:15<00:17, 7340.18 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  53%|█████▎    | 143034/270679 [00:15<00:17, 7500.37 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  53%|█████▎    | 143824/270679 [00:15<00:16, 7602.37 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  53%|█████▎    | 144654/270679 [00:15<00:16, 7749.39 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  54%|█████▎    | 145448/270679 [00:15<00:17, 7279.89 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  54%|█████▍    | 146183/270679 [00:15<00:17, 7148.49 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  54%|█████▍    | 146968/270679 [00:15<00:17, 7241.98 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  55%|█████▍    | 147707/270679 [00:15<00:16, 7236.82 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  55%|█████▍    | 148475/270679 [00:15<00:17, 7098.77 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  55%|█████▌    | 149188/270679 [00:16<00:18, 6729.76 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  55%|█████▌    | 149879/270679 [00:16<00:18, 6618.88 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  56%|█████▌    | 150548/270679 [00:16<00:19, 6300.20 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  56%|█████▌    | 151221/270679 [00:16<00:18, 6417.72 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  56%|█████▌    | 151871/270679 [00:16<00:19, 6110.33 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  56%|█████▋    | 152537/270679 [00:16<00:18, 6227.85 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  57%|█████▋    | 153229/270679 [00:16<00:18, 6389.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  57%|█████▋    | 153889/270679 [00:16<00:18, 6393.11 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  57%|█████▋    | 154538/270679 [00:16<00:18, 6240.74 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  57%|█████▋    | 155185/270679 [00:17<00:19, 5955.24 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  58%|█████▊    | 155850/270679 [00:17<00:18, 6141.48 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  58%|█████▊    | 156558/270679 [00:17<00:17, 6374.45 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  58%|█████▊    | 157251/270679 [00:17<00:17, 6378.60 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  58%|█████▊    | 157904/270679 [00:17<00:18, 6207.61 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  59%|█████▊    | 158599/270679 [00:17<00:17, 6248.52 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  59%|█████▉    | 159238/270679 [00:17<00:19, 5830.42 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  59%|█████▉    | 159854/270679 [00:17<00:18, 5833.59 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  59%|█████▉    | 160450/270679 [00:17<00:19, 5786.29 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  59%|█████▉    | 161044/270679 [00:18<00:19, 5750.17 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  60%|█████▉    | 161651/270679 [00:18<00:20, 5260.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  60%|█████▉    | 162192/270679 [00:18<00:20, 5265.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  60%|██████    | 162810/270679 [00:18<00:19, 5501.16 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  60%|██████    | 163498/270679 [00:18<00:18, 5865.96 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  61%|██████    | 164135/270679 [00:18<00:17, 5967.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  61%|██████    | 164758/270679 [00:18<00:17, 5985.60 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  61%|██████    | 165397/270679 [00:18<00:17, 6098.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  61%|██████▏   | 166016/270679 [00:18<00:17, 6097.33 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  62%|██████▏   | 166630/270679 [00:18<00:17, 6020.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  62%|██████▏   | 167253/270679 [00:19<00:17, 5950.90 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  62%|██████▏   | 167873/270679 [00:19<00:17, 5957.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  62%|██████▏   | 168530/270679 [00:19<00:16, 6124.15 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  63%|██████▎   | 169207/270679 [00:19<00:16, 6302.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  63%|██████▎   | 169949/270679 [00:19<00:15, 6611.38 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  63%|██████▎   | 170658/270679 [00:19<00:14, 6668.99 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  63%|██████▎   | 171423/270679 [00:19<00:14, 6937.31 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  64%|██████▎   | 172192/270679 [00:19<00:13, 7155.72 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  64%|██████▍   | 172928/270679 [00:19<00:14, 6630.03 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  64%|██████▍   | 173732/270679 [00:20<00:13, 7006.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  64%|██████▍   | 174484/270679 [00:20<00:13, 7121.63 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  65%|██████▍   | 175224/270679 [00:20<00:13, 7138.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  65%|██████▌   | 175943/270679 [00:20<00:13, 7017.00 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  65%|██████▌   | 176678/270679 [00:20<00:13, 7098.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  66%|██████▌   | 177402/270679 [00:20<00:14, 6599.48 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  66%|██████▌   | 178218/270679 [00:20<00:13, 6870.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  66%|██████▌   | 178917/270679 [00:20<00:13, 6876.59 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  66%|██████▋   | 179622/270679 [00:20<00:13, 6677.77 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  67%|██████▋   | 180371/270679 [00:20<00:13, 6869.62 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  67%|██████▋   | 181138/270679 [00:21<00:12, 7075.19 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  67%|██████▋   | 181916/270679 [00:21<00:12, 7192.66 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  67%|██████▋   | 182651/270679 [00:21<00:12, 7021.32 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  68%|██████▊   | 183362/270679 [00:21<00:12, 7026.55 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  68%|██████▊   | 184074/270679 [00:21<00:12, 6745.82 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  68%|██████▊   | 184786/270679 [00:21<00:12, 6726.87 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  69%|██████▊   | 185484/270679 [00:21<00:13, 6525.53 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  69%|██████▉   | 186170/270679 [00:21<00:13, 6304.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  69%|██████▉   | 186822/270679 [00:21<00:13, 6321.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  69%|██████▉   | 187460/270679 [00:22<00:13, 6306.89 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  70%|██████▉   | 188153/270679 [00:22<00:12, 6429.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  70%|██████▉   | 188831/270679 [00:22<00:12, 6481.33 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  70%|███████   | 189485/270679 [00:22<00:12, 6412.33 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  70%|███████   | 190157/270679 [00:22<00:12, 6236.29 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  70%|███████   | 190784/270679 [00:22<00:12, 6154.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  71%|███████   | 191418/270679 [00:22<00:13, 6085.03 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  71%|███████   | 192068/270679 [00:22<00:12, 6177.38 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  71%|███████   | 192714/270679 [00:22<00:12, 6099.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  71%|███████▏  | 193369/270679 [00:22<00:12, 6185.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  72%|███████▏  | 194008/270679 [00:23<00:12, 6241.44 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  72%|███████▏  | 194634/270679 [00:23<00:13, 5808.91 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  72%|███████▏  | 195268/270679 [00:23<00:12, 5921.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  72%|███████▏  | 195877/270679 [00:23<00:12, 5836.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  73%|███████▎  | 196494/270679 [00:23<00:13, 5569.53 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  73%|███████▎  | 197073/270679 [00:23<00:13, 5427.70 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  73%|███████▎  | 197620/270679 [00:23<00:14, 5084.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  73%|███████▎  | 198139/270679 [00:23<00:14, 5042.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  73%|███████▎  | 198655/270679 [00:24<00:14, 5067.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  74%|███████▎  | 199184/270679 [00:24<00:13, 5117.97 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  74%|███████▍  | 199703/270679 [00:24<00:13, 5084.43 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  74%|███████▍  | 200229/270679 [00:24<00:13, 5077.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  74%|███████▍  | 200762/270679 [00:24<00:13, 5102.39 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  74%|███████▍  | 201282/270679 [00:24<00:14, 4814.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  75%|███████▍  | 201768/270679 [00:24<00:14, 4656.32 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  75%|███████▍  | 202238/270679 [00:24<00:15, 4461.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  75%|███████▍  | 202717/270679 [00:24<00:15, 4477.98 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  75%|███████▌  | 203213/270679 [00:24<00:14, 4608.46 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  75%|███████▌  | 203688/270679 [00:25<00:14, 4568.90 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  75%|███████▌  | 204176/270679 [00:25<00:14, 4612.31 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  76%|███████▌  | 204664/270679 [00:25<00:14, 4659.05 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  76%|███████▌  | 205158/270679 [00:25<00:14, 4532.24 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  76%|███████▌  | 205618/270679 [00:25<00:14, 4542.18 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  76%|███████▌  | 206090/270679 [00:25<00:15, 4277.31 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  76%|███████▋  | 206528/270679 [00:25<00:14, 4278.43 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  76%|███████▋  | 206995/270679 [00:25<00:14, 4377.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  77%|███████▋  | 207436/270679 [00:25<00:14, 4360.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  77%|███████▋  | 207915/270679 [00:26<00:14, 4456.98 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  77%|███████▋  | 208384/270679 [00:26<00:13, 4524.19 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  77%|███████▋  | 208853/270679 [00:26<00:13, 4521.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  77%|███████▋  | 209330/270679 [00:26<00:13, 4567.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  78%|███████▊  | 209805/270679 [00:26<00:13, 4560.72 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  78%|███████▊  | 210271/270679 [00:26<00:13, 4545.08 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  78%|███████▊  | 210743/270679 [00:26<00:13, 4570.56 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  78%|███████▊  | 211211/270679 [00:26<00:12, 4594.09 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  78%|███████▊  | 211675/270679 [00:26<00:13, 4519.63 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  78%|███████▊  | 212145/270679 [00:26<00:12, 4535.93 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  79%|███████▊  | 212610/270679 [00:27<00:12, 4523.75 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  79%|███████▊  | 213075/270679 [00:27<00:12, 4525.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  79%|███████▉  | 213593/270679 [00:27<00:12, 4707.16 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  79%|███████▉  | 214102/270679 [00:27<00:11, 4798.12 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  79%|███████▉  | 214613/270679 [00:27<00:11, 4883.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  79%|███████▉  | 215167/270679 [00:27<00:10, 5069.52 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  80%|███████▉  | 215726/270679 [00:27<00:10, 5090.61 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  80%|███████▉  | 216320/270679 [00:27<00:10, 5290.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  80%|████████  | 216866/270679 [00:27<00:10, 5158.15 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  80%|████████  | 217402/270679 [00:28<00:10, 5126.48 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  81%|████████  | 217948/270679 [00:28<00:10, 5195.77 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  81%|████████  | 218502/270679 [00:28<00:10, 5097.36 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  81%|████████  | 219175/270679 [00:28<00:09, 5426.17 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  81%|████████  | 219758/270679 [00:28<00:09, 5519.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  81%|████████▏ | 220340/270679 [00:28<00:09, 5562.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  82%|████████▏ | 220897/270679 [00:28<00:09, 5482.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  82%|████████▏ | 221452/270679 [00:28<00:09, 5450.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  82%|████████▏ | 222027/270679 [00:28<00:08, 5529.96 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  82%|████████▏ | 222583/270679 [00:28<00:09, 5094.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  82%|████████▏ | 223113/270679 [00:29<00:10, 4732.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  83%|████████▎ | 223604/270679 [00:29<00:10, 4517.85 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  83%|████████▎ | 224093/270679 [00:29<00:10, 4488.79 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  83%|████████▎ | 224549/270679 [00:29<00:10, 4358.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  83%|████████▎ | 224990/270679 [00:29<00:11, 4133.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  83%|████████▎ | 225419/270679 [00:29<00:11, 3946.55 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  83%|████████▎ | 225843/270679 [00:29<00:11, 3958.34 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  84%|████████▎ | 226257/270679 [00:29<00:11, 3920.25 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  84%|████████▍ | 226698/270679 [00:30<00:10, 4034.43 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  84%|████████▍ | 227120/270679 [00:30<00:10, 4062.86 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  84%|████████▍ | 227565/270679 [00:30<00:10, 4047.94 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  84%|████████▍ | 227976/270679 [00:30<00:10, 4056.08 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  84%|████████▍ | 228431/270679 [00:30<00:10, 4190.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  85%|████████▍ | 228891/270679 [00:30<00:09, 4298.50 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  85%|████████▍ | 229361/270679 [00:30<00:09, 4393.39 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  85%|████████▍ | 229821/270679 [00:30<00:09, 4253.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  85%|████████▌ | 230272/270679 [00:30<00:09, 4201.68 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  85%|████████▌ | 230696/270679 [00:30<00:09, 4176.44 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  85%|████████▌ | 231117/270679 [00:31<00:09, 3977.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  86%|████████▌ | 231635/270679 [00:31<00:09, 4196.61 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  86%|████████▌ | 232119/270679 [00:31<00:08, 4313.56 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  86%|████████▌ | 232677/270679 [00:31<00:08, 4660.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  86%|████████▌ | 233187/270679 [00:31<00:08, 4646.39 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  86%|████████▋ | 233654/270679 [00:31<00:07, 4640.59 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  87%|████████▋ | 234191/270679 [00:31<00:07, 4847.63 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  87%|████████▋ | 234681/270679 [00:31<00:07, 4833.50 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  87%|████████▋ | 235174/270679 [00:31<00:07, 4831.34 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  87%|████████▋ | 235665/270679 [00:32<00:07, 4755.12 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  87%|████████▋ | 236150/270679 [00:32<00:07, 4721.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  87%|████████▋ | 236636/270679 [00:32<00:07, 4677.32 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  88%|████████▊ | 237114/270679 [00:32<00:07, 4642.66 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  88%|████████▊ | 237615/270679 [00:32<00:06, 4745.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  88%|████████▊ | 238105/270679 [00:32<00:06, 4694.93 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  88%|████████▊ | 238589/270679 [00:32<00:07, 4438.09 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  88%|████████▊ | 239131/270679 [00:32<00:06, 4662.49 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  89%|████████▊ | 239631/270679 [00:32<00:06, 4734.13 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  89%|████████▊ | 240142/270679 [00:32<00:06, 4807.98 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  89%|████████▉ | 240675/270679 [00:33<00:06, 4769.11 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  89%|████████▉ | 241155/270679 [00:33<00:06, 4628.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  89%|████████▉ | 241631/270679 [00:33<00:06, 4639.25 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  89%|████████▉ | 242105/270679 [00:33<00:06, 4657.90 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  90%|████████▉ | 242623/270679 [00:33<00:05, 4780.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  90%|████████▉ | 243104/270679 [00:33<00:05, 4778.92 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  90%|████████▉ | 243607/270679 [00:33<00:05, 4832.04 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  90%|█████████ | 244093/270679 [00:33<00:05, 4661.80 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  90%|█████████ | 244655/270679 [00:33<00:05, 4782.48 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  91%|█████████ | 245152/270679 [00:34<00:05, 4704.76 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  91%|█████████ | 245672/270679 [00:34<00:05, 4674.41 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  91%|█████████ | 246159/270679 [00:34<00:05, 4615.27 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  91%|█████████ | 246643/270679 [00:34<00:05, 4652.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  91%|█████████▏| 247121/270679 [00:34<00:05, 4648.70 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  91%|█████████▏| 247630/270679 [00:34<00:04, 4732.87 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  92%|█████████▏| 248148/270679 [00:34<00:04, 4812.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  92%|█████████▏| 248647/270679 [00:34<00:04, 4711.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  92%|█████████▏| 249122/270679 [00:34<00:04, 4521.46 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  92%|█████████▏| 249597/270679 [00:34<00:04, 4546.33 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  92%|█████████▏| 250206/270679 [00:35<00:04, 4971.34 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  93%|█████████▎| 250783/270679 [00:35<00:03, 5198.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  93%|█████████▎| 251355/270679 [00:35<00:03, 5332.69 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  93%|█████████▎| 251969/270679 [00:35<00:03, 5539.42 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  93%|█████████▎| 252547/270679 [00:35<00:03, 5483.11 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  94%|█████████▎| 253129/270679 [00:35<00:03, 5561.50 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  94%|█████████▎| 253722/270679 [00:35<00:03, 5651.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  94%|█████████▍| 254290/270679 [00:35<00:02, 5654.98 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  94%|█████████▍| 254863/270679 [00:35<00:02, 5371.25 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  94%|█████████▍| 255418/270679 [00:36<00:03, 5059.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████▍| 255949/270679 [00:36<00:03, 4436.02 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████▍| 256428/270679 [00:36<00:03, 4296.72 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████▍| 256871/270679 [00:36<00:03, 4056.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████▌| 257311/270679 [00:36<00:03, 3990.39 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████▌| 257729/270679 [00:36<00:03, 4030.16 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████▌| 258137/270679 [00:36<00:03, 4012.62 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▌| 258566/270679 [00:36<00:03, 3829.95 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▌| 258970/270679 [00:37<00:03, 3835.44 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▌| 259365/270679 [00:37<00:02, 3836.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▌| 259759/270679 [00:37<00:02, 3659.50 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▌| 260144/270679 [00:37<00:03, 3235.25 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▌| 260490/270679 [00:37<00:03, 3070.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▋| 260808/270679 [00:37<00:03, 2939.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  96%|█████████▋| 261127/270679 [00:37<00:03, 2769.77 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 261415/270679 [00:37<00:03, 2535.84 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 261698/270679 [00:38<00:03, 2367.81 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 261946/270679 [00:38<00:03, 2376.79 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 262200/270679 [00:38<00:03, 2270.62 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 262433/270679 [00:38<00:03, 2114.68 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 262655/270679 [00:38<00:03, 2073.43 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 262895/270679 [00:38<00:03, 2128.22 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 263111/270679 [00:38<00:03, 2117.90 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 263343/270679 [00:38<00:03, 2171.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 263563/270679 [00:38<00:03, 2116.80 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  97%|█████████▋| 263776/270679 [00:39<00:03, 1939.75 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 264034/270679 [00:39<00:03, 2097.12 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 264255/270679 [00:39<00:03, 2124.96 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 264497/270679 [00:39<00:02, 2065.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 264724/270679 [00:39<00:02, 2046.29 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 264943/270679 [00:39<00:02, 2030.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 265162/270679 [00:39<00:02, 2038.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 265369/270679 [00:39<00:02, 2002.08 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 265586/270679 [00:39<00:02, 2042.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 265792/270679 [00:40<00:02, 2032.48 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 266011/270679 [00:40<00:02, 2047.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 266250/270679 [00:40<00:02, 2138.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  98%|█████████▊| 266500/270679 [00:40<00:01, 2241.51 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▊| 266738/270679 [00:40<00:01, 2162.77 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▊| 266974/270679 [00:40<00:01, 1918.56 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▊| 267185/270679 [00:40<00:01, 1864.34 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 267381/270679 [00:40<00:02, 1638.03 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 267559/270679 [00:41<00:02, 1460.95 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 267731/270679 [00:41<00:02, 1386.22 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 267875/270679 [00:41<00:02, 1350.24 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268014/270679 [00:41<00:02, 1047.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268131/270679 [00:42<00:04, 569.69 examples/s] Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268226/270679 [00:42<00:05, 451.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268306/270679 [00:42<00:06, 388.52 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268363/270679 [00:43<00:06, 342.96 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268421/270679 [00:43<00:06, 324.12 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268466/270679 [00:43<00:07, 299.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268505/270679 [00:43<00:07, 286.21 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268540/270679 [00:43<00:07, 269.71 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268569/270679 [00:43<00:07, 267.74 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268607/270679 [00:44<00:08, 257.62 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268645/270679 [00:44<00:08, 253.30 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268680/270679 [00:44<00:08, 242.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268713/270679 [00:44<00:07, 259.58 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268743/270679 [00:44<00:07, 264.84 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268775/270679 [00:44<00:08, 233.90 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268801/270679 [00:44<00:07, 238.97 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268829/270679 [00:44<00:07, 245.19 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268857/270679 [00:45<00:07, 247.47 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268885/270679 [00:45<00:07, 252.76 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268916/270679 [00:45<00:07, 231.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268941/270679 [00:45<00:07, 233.51 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 268973/270679 [00:45<00:07, 220.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269000/270679 [00:45<00:07, 228.60 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269025/270679 [00:45<00:07, 232.14 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269050/270679 [00:45<00:06, 234.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269083/270679 [00:46<00:07, 220.89 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269113/270679 [00:46<00:06, 237.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269151/270679 [00:46<00:06, 236.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269185/270679 [00:46<00:06, 229.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269218/270679 [00:46<00:06, 221.97 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269242/270679 [00:46<00:06, 224.15 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269268/270679 [00:46<00:06, 227.86 examples/s]Tokenizing and reformatting instruction data (num_proc=16):  99%|█████████▉| 269302/270679 [00:47<00:06, 219.97 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269334/270679 [00:47<00:06, 214.85 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269361/270679 [00:47<00:05, 224.16 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269384/270679 [00:47<00:05, 218.57 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269408/270679 [00:47<00:05, 219.10 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269432/270679 [00:47<00:05, 220.67 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269462/270679 [00:47<00:05, 208.90 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269484/270679 [00:47<00:05, 208.12 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269506/270679 [00:47<00:05, 208.19 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269536/270679 [00:48<00:05, 201.95 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269558/270679 [00:48<00:05, 202.60 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269580/270679 [00:48<00:05, 200.13 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269605/270679 [00:48<00:05, 208.07 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269634/270679 [00:48<00:05, 200.51 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269655/270679 [00:48<00:05, 201.41 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269680/270679 [00:48<00:04, 213.57 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269706/270679 [00:48<00:04, 222.54 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269729/270679 [00:49<00:04, 218.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269761/270679 [00:49<00:04, 192.02 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269787/270679 [00:49<00:04, 205.84 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269812/270679 [00:49<00:04, 212.65 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269840/270679 [00:49<00:03, 228.66 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269876/270679 [00:49<00:03, 262.68 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269904/270679 [00:49<00:02, 266.21 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269937/270679 [00:49<00:02, 275.66 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 269969/270679 [00:49<00:02, 286.69 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270012/270679 [00:50<00:02, 325.01 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270056/270679 [00:50<00:01, 355.47 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270104/270679 [00:50<00:01, 388.06 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270149/270679 [00:50<00:01, 404.35 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270192/270679 [00:50<00:01, 409.44 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270240/270679 [00:50<00:01, 429.52 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270290/270679 [00:50<00:00, 444.64 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270348/270679 [00:50<00:00, 415.86 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270411/270679 [00:51<00:00, 412.49 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270467/270679 [00:51<00:00, 397.83 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270508/270679 [00:51<00:00, 399.38 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270553/270679 [00:51<00:00, 409.73 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270611/270679 [00:51<00:00, 400.13 examples/s]Tokenizing and reformatting instruction data (num_proc=16): 100%|█████████▉| 270667/270679 [00:51<00:00, 388.91 examples/s]                                                                                                                           Filter:   0%|          | 0/270679 [00:00<?, ? examples/s]Filter:   0%|          | 1000/270679 [00:00<00:28, 9412.73 examples/s]Filter:   2%|▏         | 5000/270679 [00:00<00:13, 19248.79 examples/s]Filter:   3%|▎         | 9000/270679 [00:00<00:12, 21633.02 examples/s]Filter:   4%|▍         | 12000/270679 [00:00<00:12, 20881.38 examples/s]Filter:   6%|▌         | 15000/270679 [00:00<00:12, 20387.17 examples/s]Filter:   7%|▋         | 18000/270679 [00:00<00:12, 20262.77 examples/s]Filter:   8%|▊         | 22000/270679 [00:01<00:12, 20700.72 examples/s]Filter:  10%|▉         | 26000/270679 [00:01<00:11, 21757.90 examples/s]Filter:  11%|█         | 30000/270679 [00:01<00:10, 22499.63 examples/s]Filter:  13%|█▎        | 34000/270679 [00:01<00:10, 21824.15 examples/s]Filter:  14%|█▍        | 38000/270679 [00:01<00:10, 21433.76 examples/s]Filter:  16%|█▌        | 42000/270679 [00:01<00:10, 21371.90 examples/s]Filter:  17%|█▋        | 46000/270679 [00:02<00:10, 21929.30 examples/s]Filter:  18%|█▊        | 49000/270679 [00:02<00:10, 21665.56 examples/s]Filter:  19%|█▉        | 52000/270679 [00:02<00:10, 20454.32 examples/s]Filter:  20%|██        | 55000/270679 [00:02<00:11, 19580.33 examples/s]Filter:  21%|██▏       | 58000/270679 [00:02<00:10, 19702.43 examples/s]Filter:  23%|██▎       | 61000/270679 [00:03<00:13, 16051.60 examples/s]Filter:  24%|██▍       | 65000/270679 [00:03<00:11, 17744.62 examples/s]Filter:  25%|██▍       | 67000/270679 [00:03<00:11, 17811.38 examples/s]Filter:  25%|██▌       | 69000/270679 [00:03<00:11, 18156.16 examples/s]Filter:  26%|██▌       | 71000/270679 [00:03<00:10, 18350.36 examples/s]Filter:  27%|██▋       | 74000/270679 [00:03<00:10, 19072.26 examples/s]Filter:  29%|██▉       | 78000/270679 [00:03<00:09, 20085.07 examples/s]Filter:  30%|███       | 82000/270679 [00:04<00:09, 20712.32 examples/s]Filter:  31%|███▏      | 85000/270679 [00:04<00:09, 20496.55 examples/s]Filter:  33%|███▎      | 89000/270679 [00:04<00:08, 20348.32 examples/s]Filter:  34%|███▍      | 93000/270679 [00:04<00:08, 20451.33 examples/s]Filter:  35%|███▌      | 96000/270679 [00:04<00:08, 20688.55 examples/s]Filter:  37%|███▋      | 99000/270679 [00:04<00:08, 20668.44 examples/s]Filter:  38%|███▊      | 102000/270679 [00:05<00:08, 20790.14 examples/s]Filter:  39%|███▉      | 106000/270679 [00:05<00:07, 21756.70 examples/s]Filter:  41%|████      | 110000/270679 [00:05<00:07, 22341.13 examples/s]Filter:  42%|████▏     | 114000/270679 [00:05<00:06, 22826.87 examples/s]Filter:  44%|████▎     | 118000/270679 [00:05<00:06, 23087.80 examples/s]Filter:  45%|████▌     | 122000/270679 [00:05<00:06, 23321.96 examples/s]Filter:  47%|████▋     | 126000/270679 [00:06<00:06, 23529.03 examples/s]Filter:  48%|████▊     | 130000/270679 [00:06<00:05, 23671.49 examples/s]Filter:  50%|████▉     | 134000/270679 [00:06<00:05, 23739.38 examples/s]Filter:  51%|█████     | 138000/270679 [00:06<00:05, 23734.61 examples/s]Filter:  52%|█████▏    | 142000/270679 [00:06<00:05, 23720.05 examples/s]Filter:  54%|█████▍    | 146000/270679 [00:06<00:05, 23797.28 examples/s]Filter:  55%|█████▌    | 150000/270679 [00:07<00:05, 23826.16 examples/s]Filter:  57%|█████▋    | 153000/270679 [00:07<00:05, 22669.02 examples/s]Filter:  58%|█████▊    | 156000/270679 [00:07<00:05, 21795.58 examples/s]Filter:  59%|█████▊    | 159000/270679 [00:07<00:05, 21154.59 examples/s]Filter:  60%|█████▉    | 162000/270679 [00:07<00:05, 20671.14 examples/s]Filter:  61%|██████    | 165000/270679 [00:07<00:05, 20369.13 examples/s]Filter:  62%|██████▏   | 168000/270679 [00:08<00:06, 16483.76 examples/s]Filter:  63%|██████▎   | 170000/270679 [00:08<00:05, 17090.88 examples/s]Filter:  64%|██████▎   | 172000/270679 [00:08<00:05, 17615.35 examples/s]Filter:  64%|██████▍   | 174000/270679 [00:08<00:05, 18097.76 examples/s]Filter:  65%|██████▌   | 176000/270679 [00:08<00:05, 18455.91 examples/s]Filter:  66%|██████▌   | 178000/270679 [00:08<00:04, 18748.04 examples/s]Filter:  66%|██████▋   | 180000/270679 [00:08<00:04, 19006.55 examples/s]Filter:  67%|██████▋   | 182000/270679 [00:08<00:04, 19155.28 examples/s]Filter:  68%|██████▊   | 184000/270679 [00:08<00:04, 19219.27 examples/s]Filter:  69%|██████▊   | 186000/270679 [00:09<00:04, 19380.93 examples/s]Filter:  69%|██████▉   | 188000/270679 [00:09<00:04, 19419.25 examples/s]Filter:  70%|███████   | 190000/270679 [00:09<00:04, 19474.67 examples/s]Filter:  71%|███████   | 192000/270679 [00:09<00:04, 19457.87 examples/s]Filter:  72%|███████▏  | 194000/270679 [00:09<00:03, 19444.20 examples/s]Filter:  72%|███████▏  | 196000/270679 [00:09<00:03, 19445.23 examples/s]Filter:  73%|███████▎  | 198000/270679 [00:09<00:03, 19415.95 examples/s]Filter:  74%|███████▍  | 200000/270679 [00:09<00:03, 19439.29 examples/s]Filter:  75%|███████▍  | 202000/270679 [00:09<00:03, 19600.07 examples/s]Filter:  76%|███████▌  | 206000/270679 [00:10<00:03, 19930.08 examples/s]Filter:  78%|███████▊  | 210000/270679 [00:10<00:03, 20084.46 examples/s]Filter:  78%|███████▊  | 212000/270679 [00:10<00:02, 19954.05 examples/s]Filter:  79%|███████▉  | 214000/270679 [00:10<00:02, 19820.87 examples/s]Filter:  80%|███████▉  | 216000/270679 [00:10<00:02, 19603.35 examples/s]Filter:  81%|████████▏ | 220000/270679 [00:10<00:02, 20142.09 examples/s]Filter:  83%|████████▎ | 224000/270679 [00:10<00:02, 20257.30 examples/s]Filter:  84%|████████▍ | 227000/270679 [00:11<00:02, 20182.82 examples/s]Filter:  85%|████████▍ | 230000/270679 [00:11<00:02, 20126.73 examples/s]Filter:  86%|████████▋ | 234000/270679 [00:11<00:01, 20029.75 examples/s]Filter:  87%|████████▋ | 236000/270679 [00:11<00:01, 20015.41 examples/s]Filter:  88%|████████▊ | 239000/270679 [00:11<00:01, 19972.76 examples/s]Filter:  89%|████████▉ | 241000/270679 [00:11<00:01, 19820.21 examples/s]Filter:  90%|████████▉ | 243000/270679 [00:11<00:01, 19615.93 examples/s]Filter:  91%|█████████ | 246000/270679 [00:12<00:01, 19784.56 examples/s]Filter:  92%|█████████▏| 250000/270679 [00:12<00:01, 20038.92 examples/s]Filter:  94%|█████████▍| 254000/270679 [00:12<00:00, 20093.73 examples/s]Filter:  95%|█████████▌| 258000/270679 [00:12<00:00, 20591.17 examples/s]Filter:  97%|█████████▋| 262000/270679 [00:12<00:00, 21290.63 examples/s]Filter:  98%|█████████▊| 266000/270679 [00:12<00:00, 21703.03 examples/s]Filter: 100%|█████████▉| 270000/270679 [00:13<00:00, 22060.82 examples/s]                                                                         07/06/2023 19:45:05 - INFO - __main__ - Sample 111944 of the training set: {'input_ids': tensor([   27,    91,  7220,    91,    29,   198,    48,    25, 30532,    25,
          366,  7594,   314,  1101,   402,  6415, 44927,   921,     1,   318,
          257,  3496,  6264,   416,  1605, 14015,   290,  3496, 16002,  2185,
         6064, 16835,   273,   329,   607,   717,  8034,  5062, 11851,   357,
         4626,   737,   383,  3496,  3033,  1605, 14015,  1757,  9883,    13,
          632,   373,  3194,   416, 16835,   273,    11, 10799, 29594,    11,
          290, 34118,  6213,  4176,    11,   290,  4635,   416,  5180, 29635,
           65, 15339,   290, 16835,   273,    13, 28728,   416, 16781, 13407,
          319,  2795,  2242,    11,  1853,    11,   355, 16835,   273,   338,
         5544,   290,  2457,  2060,   422,   262,  5062,    11,   340,   318,
          257,  5848,  3496,    13,   406,  2417,  1146,    11,   340,   318,
          281, 16915,   290, 20886,  1842,  3496,   351,   257,  7505,   286,
        10800,  1042,   290, 24748,   893,   262,  3275,   284,   407,  1011,
          640,   351,  6151,  3392,   329,  7520,    13,   198,   198, 24361,
           25,   508,  2630,   588,  1312,  1101,  1016,   284,  4425,   345,
           30,   198,  5756,   338,  1577,  4269,   286, 10510,  2174,   198,
           27,    91,   562, 10167,    91,    29,   198,   464,  5981,  6827,
          287,   262, 10066,   318,    25,   632,   373,  3194,   416, 16835,
          273,    11, 10799, 29594,    11,   290, 34118,  6213,  4176,    11,
          290,  4635,   416,  5180, 29635,    65, 15339,   290, 16835,   273,
           13,  1406,   262,  3280,   318, 16835,   273,    13, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,   464,  5981,  6827,
          287,   262, 10066,   318,    25,   632,   373,  3194,   416, 16835,
          273,    11, 10799, 29594,    11,   290, 34118,  6213,  4176,    11,
          290,  4635,   416,  5180, 29635,    65, 15339,   290, 16835,   273,
           13,  1406,   262,  3280,   318, 16835,   273,    13, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
07/06/2023 19:45:05 - INFO - __main__ - Sample 72907 of the training set: {'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  1639,   481,   307,  1813,
          257,  6770,   286,   257,  4876,   717,    11,   788,   617,  5128,
          286,   262,  4876,    13,   198,   818,   428,  4876,    11,   345,
          821,  1813,   257,  1808,    11,   257,  4732, 10066,    11,   290,
         1440,  3689,   543,   389,  2846,   422,   262, 10066,    13,  2293,
         3555,   257, 10066,    11,   345,   481,   651,   257,  4506,  4547,
          286,   262,  2846,    13,  3406,  1693,   318,   284,  5004,   416,
        10342,   290,  3555,  2252,  1321,   286,   543,  3381,   345,   460,
         3280,   262,  1808,    13,  1423,  5344,   534,  3572,   355,   705,
           64,  3256,   705,    65,  3256,   705,    66,  3256,   393,   705,
           67,  4458,  1002,   345,   892,   517,   621,   530,  3038,   318,
        19756,    11,  3853,   262,   517, 17939,  3038,   284,  1037,   345,
         3280,   262,  1808,    13,   198,   198, 24361,    25,  1649,   373,
          262,  1524,   810,   347,  1173,   372,  9713,   287,  6182,  9393,
           30, 46169,    25,    33,  1173,   372,   373,  4642,   287, 48823,
           11,   968, 13910,    13,   679,   373, 15657,   287,   281, 21531,
          379,   968, 10711,   634,    11, 10140,    13,   679,  2540,   465,
         3451,   355,   257, 19834,   287,  6182,    11, 10140,    13,  1649,
          407,  1762,    11,   339,  9713,   379,   262, 47657,  5136,    13,
          679,   635,  9713,   351,  9966,   347,   959, 38863,    11,  3977,
        14433, 12937,    11,   290,  1854,    13,   679, 28681, 30902,  5032,
          287,  1642, 10747,  3640,   422,  3450,    11,   290,   706,  1248,
         3365, 13378,  2241,   284,   262,  1242,   355,   257,  7573,    13,
          679,  4721,   257,  8034,   287,  6182,    11,   290,  1138,   351,
          617,  1943,   612,    13,   554,  1248,  3104,   339,  3888,   284,
          968,  1971,  2254,    11,   290,   379,   262,  2351,  8581,   286,
         8495,   326,   614,   339, 25212,   564,   250, 22603,    12, 12124,
          379,   968, 10711,   634,    13,   447,   251, 15894, 20875,   339,
         2540,   284,   779,  1660,  4033,   669,   287, 12741,   284, 20629,
           11,   290,   287,  1248,  4790,   373,  7147,   257,  2888,   286,
          262,  1605,  5638,  8043,  7023,    13,   554,   262, 37667,    82,
           11,   339,  7525,   750, 30017, 31892, 21641,    11,   351,  3241,
          284,  1660,  8043, 21641,   286, 10747,    11, 16050,    11,   290,
         7051,  3083, 31068,    13,   679,  1690,  3377, 43285,   287,  5675,
         1869,   272,    11,   810,   339,  4635,   884, 12411,  2499,   355,
        14410,   379,  5675,  1869,   272,   357,  1507,  3695,   737,   554,
         1248,  3720,    11,   347,  1173,   372,   373,  7018,   656,   262,
         2351,  8581,   286,  8495,   355,   281, 22669,  2888,    13,   198,
        21691,    25,   257,    13, 11474, 12036,   275,    13,  1605,  5638,
         8043,  7023,   269,    13, 14410,   379,  5675,  1869,   272,   288,
           13, 47657,  5136,   198, 26410,    25,   198,    27,    91,   562,
        10167,    91,    29,   198,    67, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,    67, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1])}.
07/06/2023 19:45:05 - INFO - __main__ - Sample 22259 of the training set: {'input_ids': tensor([   27,    91,  7220,    91,    29,   198,  8291, 17660,   366,   464,
         7035,   447,   247,  3241,   318,  5670,   319,   262,   749,  8780,
         2428,   326,  5004,   471,    13,    50,    13,  3298,  2292,   287,
          262,   890,    12,  5143,  6650,    11,   355,   880,   355,   287,
          262, 16280,   286,   262,  3648,    12, 10531,  3298, 21971,  3176,
          290,  3034,  4902,    13,   383,  2426,   286,   262,  2267,   318,
          262,  3716,   987,    67, 15091,  1022,  2260,   290,  3298,  2478,
          526,   284,  3394,    30,   198,    27,    91,   562, 10167,    91,
           29,   198,  1212,   318,   262,   717,  9207,   286,   262,  2168,
        11946,   564,   250, 16347,   290,   262, 10766, 19229,  3167,  1670,
         3263,   447,   251,    11,   543,   318,   284,   307,  4884,   287,
          262,  9355,   286,  6466,  1628,  9177,   416,   262,  5136,   286,
         2159, 18493,   290,  4037, 13883,   357,  3955,  3620,    46,     8,
          290,   262, 19229, 25238, 18362,    11,  3457,    13,   357, 11251,
           40,   737, 50256]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  1212,   318,   262,   717,  9207,   286,   262,  2168,
        11946,   564,   250, 16347,   290,   262, 10766, 19229,  3167,  1670,
         3263,   447,   251,    11,   543,   318,   284,   307,  4884,   287,
          262,  9355,   286,  6466,  1628,  9177,   416,   262,  5136,   286,
         2159, 18493,   290,  4037, 13883,   357,  3955,  3620,    46,     8,
          290,   262, 19229, 25238, 18362,    11,  3457,    13,   357, 11251,
           40,   737, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
