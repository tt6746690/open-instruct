{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06afe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mit_fm/wpq/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ce895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from eval.mmlu.categories import subcategories, categories\n",
    "from eval.utils import get_next_word_predictions, load_hf_lm_and_tokenizer, query_openai_chat_model\n",
    "import torch\n",
    "\n",
    "from eval.mmlu.run_eval import format_subject, format_example, gen_prompt, eval_hf_model\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048ba357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--ntrain\", type=int, default=5)\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/mmlu\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/mmlu/llama-7B/\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--subjects\", nargs=\"*\", help=\"which subjects to evaluate. If not specified, all the 57 subjects will be evaluated.\")\n",
    "parser.add_argument(\"--n_instances\", type=int, help=\"if specified, a maximum of n_instances per subject will be used for the evaluation.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b/'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --ntrain 0 \\\n",
    "    --data_dir ../data/eval/mmlu \\\n",
    "    --save_dir {model_name_or_path}/eval/mmlu/ \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 2 \\\n",
    "    --n_instances 10\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff11cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f283484931994929a3ad826b86a67ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if args.model_name_or_path:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        model_name_or_path=args.model_name_or_path, \n",
    "        tokenizer_name_or_path=args.tokenizer_name_or_path,\n",
    "        load_in_8bit=args.load_in_8bit, \n",
    "        gptq_model=args.gptq\n",
    "    )\n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a8df8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6551497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_algebra',\n",
       " 'anatomy',\n",
       " 'astronomy',\n",
       " 'business_ethics',\n",
       " 'clinical_knowledge',\n",
       " 'college_biology',\n",
       " 'college_chemistry',\n",
       " 'college_computer_science',\n",
       " 'college_mathematics',\n",
       " 'college_medicine',\n",
       " 'college_physics',\n",
       " 'computer_security',\n",
       " 'conceptual_physics',\n",
       " 'econometrics',\n",
       " 'electrical_engineering',\n",
       " 'elementary_mathematics',\n",
       " 'formal_logic',\n",
       " 'global_facts',\n",
       " 'high_school_biology',\n",
       " 'high_school_chemistry',\n",
       " 'high_school_computer_science',\n",
       " 'high_school_european_history',\n",
       " 'high_school_geography',\n",
       " 'high_school_government_and_politics',\n",
       " 'high_school_macroeconomics',\n",
       " 'high_school_mathematics',\n",
       " 'high_school_microeconomics',\n",
       " 'high_school_physics',\n",
       " 'high_school_psychology',\n",
       " 'high_school_statistics',\n",
       " 'high_school_us_history',\n",
       " 'high_school_world_history',\n",
       " 'human_aging',\n",
       " 'human_sexuality',\n",
       " 'international_law',\n",
       " 'jurisprudence',\n",
       " 'logical_fallacies',\n",
       " 'machine_learning',\n",
       " 'management',\n",
       " 'marketing',\n",
       " 'medical_genetics',\n",
       " 'miscellaneous',\n",
       " 'moral_disputes',\n",
       " 'moral_scenarios',\n",
       " 'nutrition',\n",
       " 'philosophy',\n",
       " 'prehistory',\n",
       " 'professional_accounting',\n",
       " 'professional_law',\n",
       " 'professional_medicine',\n",
       " 'professional_psychology',\n",
       " 'public_relations',\n",
       " 'security_studies',\n",
       " 'sociology',\n",
       " 'us_foreign_policy',\n",
       " 'virology',\n",
       " 'world_religions']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "subjects = sorted(\n",
    "    [\n",
    "        f.split(\"_test.csv\")[0]\n",
    "        for f in os.listdir(os.path.join(args.data_dir, \"test\"))\n",
    "        if \"_test.csv\" in f\n",
    "    ]\n",
    ")\n",
    "\n",
    "if args.subjects:\n",
    "    assert all(subj in subjects for subj in args.subjects), f\"Some of the subjects you specified are not valid: {args.subjects}\"\n",
    "    subjects = args.subjects\n",
    "\n",
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "if not os.path.exists(os.path.join(args.save_dir)):\n",
    "    os.makedirs(os.path.join(args.save_dir))\n",
    "if not os.path.exists(os.path.join(args.save_dir, \"csvs\")):\n",
    "    os.makedirs(os.path.join(args.save_dir, \"csvs\"))\n",
    "\n",
    "all_cors = []\n",
    "subcat_cors = {\n",
    "    subcat: [] for subcat_lists in subcategories.values() for subcat in subcat_lists\n",
    "}\n",
    "cat_cors = {cat: [] for cat in categories}\n",
    "\n",
    "subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40b8e4",
   "metadata": {},
   "source": [
    "# function `eval_hf_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94670ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract_algebra\n"
     ]
    }
   ],
   "source": [
    "subject = subjects[0]\n",
    "print(subject)\n",
    "\n",
    "dev_df = pd.read_csv(\n",
    "    os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None\n",
    ")[: args.ntrain]\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    ")\n",
    "if args.n_instances and args.n_instances < test_df.shape[0]:\n",
    "    test_df = test_df.sample(args.n_instances, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5082d9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following are multiple choice questions (with answers) about  abstract algebra.\\n\\nStatement 1 | If a group has an element of order 10, then it has elements of orders 1, 2, and 5. Statement 2 | If a group has an element of order 2 and an element of order 3, then it has an element of order 6.\\nA. True, True\\nB. False, False\\nC. True, False\\nD. False, True\\nAnswer:'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# cors, acc, probs = eval_hf_model(\n",
    "#     args, subject, model, tokenizer, dev_df, test_df, args.eval_batch_size)\n",
    "batch_size = args.eval_batch_size\n",
    "\n",
    "prompts = []\n",
    "for i in range(0, test_df.shape[0]):\n",
    "    k = args.ntrain\n",
    "    prompt_end = format_example(test_df, i, include_answer=False)\n",
    "    train_prompt = gen_prompt(dev_df, subject, k)\n",
    "    prompt = train_prompt + prompt_end\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    # make sure every prompt is less than 2048 tokens\n",
    "    while tokenized_prompt.shape[-1] > 2048:\n",
    "        k -= 1\n",
    "        train_prompt = gen_prompt(dev_df, subject, k)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "\n",
    "    if args.use_chat_format:\n",
    "        prompt = \"<|user|>\\n\" + prompt.strip() + \"\\n<|assistant|>\\nThe answer is:\"\n",
    "\n",
    "    prompts.append(prompt)\n",
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8a667d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy 0.000 - abstract_algebra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the answer for all examples\n",
    "# note: here we cannot directly use convert_tokens_to_ids because the some tokenizers will automatically add space prefix.\n",
    "answer_choice_ids = [tokenizer.encode(answer_choice, add_special_tokens=False)[0] for answer_choice in choices]\n",
    "pred_indices, all_probs = get_next_word_predictions(\n",
    "    model, tokenizer, prompts, candidate_token_ids=answer_choice_ids, return_token_predictions=False, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# get the metrics\n",
    "cors = []\n",
    "groud_truths = test_df.iloc[:, -1].values\n",
    "for i in range(len(pred_indices)):\n",
    "    prediction = choices[pred_indices[i]]\n",
    "    ground_truth = groud_truths[i]\n",
    "    cors.append(prediction == ground_truth)\n",
    "\n",
    "acc = np.mean(cors)\n",
    "cors = np.array(cors)\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "print(\"Average accuracy {:.3f} - {}\".format(acc, subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf61b8",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "182d06f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abstract_algebra', 100),\n",
       " ('anatomy', 135),\n",
       " ('astronomy', 152),\n",
       " ('business_ethics', 100),\n",
       " ('clinical_knowledge', 265),\n",
       " ('college_biology', 144),\n",
       " ('college_chemistry', 100),\n",
       " ('college_computer_science', 100),\n",
       " ('college_mathematics', 100),\n",
       " ('college_medicine', 173),\n",
       " ('college_physics', 102),\n",
       " ('computer_security', 100),\n",
       " ('conceptual_physics', 235),\n",
       " ('econometrics', 114),\n",
       " ('electrical_engineering', 145),\n",
       " ('elementary_mathematics', 378),\n",
       " ('formal_logic', 126),\n",
       " ('global_facts', 100),\n",
       " ('high_school_biology', 310),\n",
       " ('high_school_chemistry', 203),\n",
       " ('high_school_computer_science', 100),\n",
       " ('high_school_european_history', 165),\n",
       " ('high_school_geography', 198),\n",
       " ('high_school_government_and_politics', 193),\n",
       " ('high_school_macroeconomics', 390),\n",
       " ('high_school_mathematics', 270),\n",
       " ('high_school_microeconomics', 238),\n",
       " ('high_school_physics', 151),\n",
       " ('high_school_psychology', 545),\n",
       " ('high_school_statistics', 216),\n",
       " ('high_school_us_history', 204),\n",
       " ('high_school_world_history', 237),\n",
       " ('human_aging', 223),\n",
       " ('human_sexuality', 131),\n",
       " ('international_law', 121),\n",
       " ('jurisprudence', 108),\n",
       " ('logical_fallacies', 163),\n",
       " ('machine_learning', 112),\n",
       " ('management', 103),\n",
       " ('marketing', 234),\n",
       " ('medical_genetics', 100),\n",
       " ('miscellaneous', 783),\n",
       " ('moral_disputes', 346),\n",
       " ('moral_scenarios', 895),\n",
       " ('nutrition', 306),\n",
       " ('philosophy', 311),\n",
       " ('prehistory', 324),\n",
       " ('professional_accounting', 282),\n",
       " ('professional_law', 1534),\n",
       " ('professional_medicine', 272),\n",
       " ('professional_psychology', 612),\n",
       " ('public_relations', 110),\n",
       " ('security_studies', 245),\n",
       " ('sociology', 201),\n",
       " ('us_foreign_policy', 100),\n",
       " ('virology', 166),\n",
       " ('world_religions', 171),\n",
       " ('avg', 14042)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for subject in subjects:\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    "    )\n",
    "    l.append((subject, len(test_df)))\n",
    "    \n",
    "l.append(('avg', sum([x[1] for x in l])))\n",
    "l # 14k problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0974bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating subjects:  51%|█████     | 29/57 [00:00<00:00, 159.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "135\n",
      "152\n",
      "100\n",
      "265\n",
      "144\n",
      "100\n",
      "100\n",
      "100\n",
      "173\n",
      "102\n",
      "100\n",
      "235\n",
      "114\n",
      "145\n",
      "378\n",
      "126\n",
      "100\n",
      "310\n",
      "203\n",
      "100\n",
      "165\n",
      "198\n",
      "193\n",
      "390\n",
      "270\n",
      "238\n",
      "151\n",
      "545\n",
      "216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating subjects:  79%|███████▉  | 45/57 [00:00<00:00, 154.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "237\n",
      "223\n",
      "131\n",
      "121\n",
      "108\n",
      "163\n",
      "112\n",
      "103\n",
      "234\n",
      "100\n",
      "783\n",
      "346\n",
      "895\n",
      "306\n",
      "311\n",
      "324\n",
      "282\n",
      "1534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating subjects: 100%|██████████| 57/57 [00:00<00:00, 120.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n",
      "612\n",
      "110\n",
      "245\n",
      "201\n",
      "100\n",
      "166\n",
      "171\n",
      "Average accuracy 0.220 - math\n",
      "Average accuracy 0.275 - health\n",
      "Average accuracy 0.300 - physics\n",
      "Average accuracy 0.367 - business\n",
      "Average accuracy 0.200 - biology\n",
      "Average accuracy 0.300 - chemistry\n",
      "Average accuracy 0.350 - computer science\n",
      "Average accuracy 0.200 - economics\n",
      "Average accuracy 0.400 - engineering\n",
      "Average accuracy 0.467 - philosophy\n",
      "Average accuracy 0.367 - other\n",
      "Average accuracy 0.250 - history\n",
      "Average accuracy 0.400 - geography\n",
      "Average accuracy 0.250 - politics\n",
      "Average accuracy 0.350 - psychology\n",
      "Average accuracy 0.450 - culture\n",
      "Average accuracy 0.233 - law\n",
      "Average accuracy 0.283 - STEM\n",
      "Average accuracy 0.346 - humanities\n",
      "Average accuracy 0.300 - social sciences\n",
      "Average accuracy 0.314 - other (business, health, misc.)\n",
      "Average accuracy: 0.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for subject in tqdm(subjects, desc=f\"Evaluating subjects: \"):\n",
    "\n",
    "    dev_df = pd.read_csv(\n",
    "        os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None\n",
    "    )[: args.ntrain]\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    "    )\n",
    "    if args.n_instances and args.n_instances < test_df.shape[0]:\n",
    "        test_df = test_df.sample(args.n_instances, random_state=42)\n",
    "\n",
    "    if args.model_name_or_path:\n",
    "        cors, acc, probs = eval_hf_model(args, subject, model, tokenizer, dev_df, test_df, args.eval_batch_size)\n",
    "    else:\n",
    "        cors, acc, probs = eval_openai_chat_engine(args, subject, args.openai_engine, dev_df, test_df, args.eval_batch_size)\n",
    "\n",
    "    subcats = subcategories[subject]\n",
    "    for subcat in subcats:\n",
    "        subcat_cors[subcat].append(cors)\n",
    "        for key in categories.keys():\n",
    "            if subcat in categories[key]:\n",
    "                cat_cors[key].append(cors)\n",
    "    all_cors.append(cors)\n",
    "\n",
    "    test_df[\"correct\"] = cors\n",
    "    for j in range(probs.shape[1]):\n",
    "        choice = choices[j]\n",
    "        test_df[\"choice{}_probs\".format(choice)] = probs[:, j]\n",
    "    test_df.to_csv(\n",
    "        os.path.join(\n",
    "            args.save_dir, 'csvs', \"{}.csv\".format(subject)\n",
    "        ),\n",
    "        index=None,\n",
    "    )\n",
    "\n",
    "for subcat in subcat_cors:\n",
    "    subcat_acc = np.mean(np.concatenate(subcat_cors[subcat]))\n",
    "    print(\"Average accuracy {:.3f} - {}\".format(subcat_acc, subcat))\n",
    "\n",
    "for cat in cat_cors:\n",
    "    cat_acc = np.mean(np.concatenate(cat_cors[cat]))\n",
    "    print(\"Average accuracy {:.3f} - {}\".format(cat_acc, cat))\n",
    "weighted_acc = np.mean(np.concatenate(all_cors))\n",
    "print(\"Average accuracy: {:.3f}\".format(weighted_acc))\n",
    "\n",
    "# save results\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"average_acc\": weighted_acc,\n",
    "            \"subcat_acc\": {\n",
    "                subcat: np.mean(np.concatenate(subcat_cors[subcat]))\n",
    "                for subcat in subcat_cors\n",
    "            },\n",
    "            \"cat_acc\": {\n",
    "                cat: np.mean(np.concatenate(cat_cors[cat]))\n",
    "                for cat in cat_cors\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
