{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06afe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/'\n",
    "                if platform.uname().processor == 'x86_64' \n",
    "                else '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from eval.mmlu.categories import subcategories, categories\n",
    "from eval.utils import get_next_word_predictions, load_hf_lm_and_tokenizer, query_openai_chat_model\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "from eval.mmlu.run_eval import format_subject, format_example, gen_prompt, eval_hf_model\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "048ba357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(ntrain=0, data_dir='../data/eval/mmlu', save_dir='../results/baselines/gpt2-medium//eval/mmlu/', model_name_or_path='../results/baselines/gpt2-medium/', tokenizer_name_or_path='../results/baselines/gpt2-medium/', openai_engine=None, subjects=None, n_instances=None, eval_batch_size=40, load_in_8bit=False, gptq=False, use_chat_format=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--ntrain\", type=int, default=5)\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/mmlu\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/mmlu/llama-7B/\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--subjects\", nargs=\"*\", help=\"which subjects to evaluate. If not specified, all the 57 subjects will be evaluated.\")\n",
    "parser.add_argument(\"--n_instances\", type=int, help=\"if specified, a maximum of n_instances per subject will be used for the evaluation.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b/'\n",
    "model_name_or_path = '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-1400'\n",
    "model_name_or_path = '../results/baselines/gpt2-medium/'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --ntrain 0 \\\n",
    "    --data_dir ../data/eval/mmlu \\\n",
    "    --save_dir {model_name_or_path}/eval/mmlu/ \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --tokenizer_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 40 \\\n",
    "    --use_chat_format\n",
    "\"\"\"\n",
    "# --eval_batch_size 2 \n",
    "#     --n_instances 10\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff11cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at ../results/baselines/gpt2-medium/ and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if args.model_name_or_path:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        model_name_or_path=args.model_name_or_path, \n",
    "        tokenizer_name_or_path=args.tokenizer_name_or_path,\n",
    "        load_in_8bit=args.load_in_8bit, \n",
    "        gptq_model=args.gptq\n",
    "    )\n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6551497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_algebra',\n",
       " 'anatomy',\n",
       " 'astronomy',\n",
       " 'business_ethics',\n",
       " 'clinical_knowledge',\n",
       " 'college_biology',\n",
       " 'college_chemistry',\n",
       " 'college_computer_science',\n",
       " 'college_mathematics',\n",
       " 'college_medicine',\n",
       " 'college_physics',\n",
       " 'computer_security',\n",
       " 'conceptual_physics',\n",
       " 'econometrics',\n",
       " 'electrical_engineering',\n",
       " 'elementary_mathematics',\n",
       " 'formal_logic',\n",
       " 'global_facts',\n",
       " 'high_school_biology',\n",
       " 'high_school_chemistry',\n",
       " 'high_school_computer_science',\n",
       " 'high_school_european_history',\n",
       " 'high_school_geography',\n",
       " 'high_school_government_and_politics',\n",
       " 'high_school_macroeconomics',\n",
       " 'high_school_mathematics',\n",
       " 'high_school_microeconomics',\n",
       " 'high_school_physics',\n",
       " 'high_school_psychology',\n",
       " 'high_school_statistics',\n",
       " 'high_school_us_history',\n",
       " 'high_school_world_history',\n",
       " 'human_aging',\n",
       " 'human_sexuality',\n",
       " 'international_law',\n",
       " 'jurisprudence',\n",
       " 'logical_fallacies',\n",
       " 'machine_learning',\n",
       " 'management',\n",
       " 'marketing',\n",
       " 'medical_genetics',\n",
       " 'miscellaneous',\n",
       " 'moral_disputes',\n",
       " 'moral_scenarios',\n",
       " 'nutrition',\n",
       " 'philosophy',\n",
       " 'prehistory',\n",
       " 'professional_accounting',\n",
       " 'professional_law',\n",
       " 'professional_medicine',\n",
       " 'professional_psychology',\n",
       " 'public_relations',\n",
       " 'security_studies',\n",
       " 'sociology',\n",
       " 'us_foreign_policy',\n",
       " 'virology',\n",
       " 'world_religions']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "subjects = sorted(\n",
    "    [\n",
    "        f.split(\"_test.csv\")[0]\n",
    "        for f in os.listdir(os.path.join(args.data_dir, \"test\"))\n",
    "        if \"_test.csv\" in f\n",
    "    ]\n",
    ")\n",
    "\n",
    "if args.subjects:\n",
    "    assert all(subj in subjects for subj in args.subjects), f\"Some of the subjects you specified are not valid: {args.subjects}\"\n",
    "    subjects = args.subjects\n",
    "\n",
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "if not os.path.exists(os.path.join(args.save_dir)):\n",
    "    os.makedirs(os.path.join(args.save_dir))\n",
    "if not os.path.exists(os.path.join(args.save_dir, \"csvs\")):\n",
    "    os.makedirs(os.path.join(args.save_dir, \"csvs\"))\n",
    "    \n",
    "\n",
    "# wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    max_input_seq_len = model.config.max_position_embeddings-1\n",
    "else:\n",
    "    max_input_seq_len = 2048-1\n",
    "\n",
    "all_cors = []\n",
    "subcat_cors = {\n",
    "    subcat: [] for subcat_lists in subcategories.values() for subcat in subcat_lists\n",
    "}\n",
    "cat_cors = {cat: [] for cat in categories}\n",
    "\n",
    "subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40b8e4",
   "metadata": {},
   "source": [
    "# function `eval_hf_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94670ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "college_medicine\n"
     ]
    }
   ],
   "source": [
    "subject = subjects[9]\n",
    "print(subject)\n",
    "\n",
    "dev_df = pd.read_csv(\n",
    "    os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None\n",
    ")[: args.ntrain]\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    ")\n",
    "if args.n_instances and args.n_instances < test_df.shape[0]:\n",
    "    test_df = test_df.sample(args.n_instances, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5082d9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncate question #66: seq_len = 1029 -> 1023\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tokenized_prompt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_input_seq_len:\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# cors, acc, probs = eval_hf_model(\n",
    "#     args, subject, model, tokenizer, dev_df, test_df, args.eval_batch_size)\n",
    "batch_size = args.eval_batch_size\n",
    "\n",
    "prompts = []\n",
    "for i in range(0, test_df.shape[0]):\n",
    "    prompt_end = format_example(test_df, i, include_answer=False)\n",
    "    for k in list(range(-1, args.ntrain+1)[::-1]):\n",
    "        ## wpq: in case zero-shot ICL exceeds `max_input_seq_len`\n",
    "        # truncate the question on the left.\n",
    "        if k == -1:\n",
    "            tokenized_prompt_end = tokenizer(prompt_end, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "            if args.use_chat_format:\n",
    "                prompt_other_than_prompt_end = \"<|user|>\\n\"+train_prompt+\"\\n<|assistant|>\\nThe answer is:\"\n",
    "            else:\n",
    "                prompt_other_than_prompt_end = train_prompt\n",
    "            tokenized_prompt_other_than_prompt_end = tokenizer(prompt_other_than_prompt_end, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "            train_prompt_max_len = max_input_seq_len-tokenized_prompt_other_than_prompt_end.shape[-1]\n",
    "            prompt_end = tokenizer.decode(tokenized_prompt_end.squeeze()[-train_prompt_max_len:], skip_special_tokens=False)\n",
    "            print(f'Truncate question #{i}: seq_len = {tokenized_prompt.shape[-1]} -> {max_input_seq_len}')\n",
    "\n",
    "        train_prompt = gen_prompt(dev_df, subject, k)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        if args.use_chat_format:\n",
    "            prompt = \"<|user|>\\n\" + prompt.strip() + \"\\n<|assistant|>\\nThe answer is:\"\n",
    "\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        if tokenized_prompt.shape[-1] <= max_input_seq_len:\n",
    "            break\n",
    "    prompts.append(prompt)\n",
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8724ecce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following are multiple choice questions (with answers) about  college medicine.\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa46f4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_start_ctx_and_end = tokenizer(\n",
    "    \"<|user|>\\n\"+train_prompt+\"\\n<|assistant|>\\nThe answer is:\", \n",
    "    return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "prompt_start_ctx_and_end.shape\n",
    "\n",
    "prompt_start_ctx_and_end.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3dde64c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' referred to as \"sauna bathing,\" is characterized by short-term passive exposure to extreme heat. This exposure elicits mild hyperthermia – an increase in the body\\'s core temperature – that induces a thermoregulatory response involving neuroendocrine, cardiovascular, and cytoprotective mechanisms that work together to restore homeostasis and condition the body for future heat stressors… In recent decades, sauna bathing has emerged as a means to increase lifespan and improve overall health, based on compelling data from observational, interventional, and mechanistic studies. Of particular interest are the findings from studies of participants in the Kuopio Ischemic Heart Disease Risk Factor (KIHD) Study, an ongoing prospective population-based cohort study of health outcomes in more than 2,300 middle-aged men from eastern Finland, which identified strong links between sauna use and reduced death and disease… The KIHD findings showed that men who used the sauna two to three times per week were 27 percent less likely to die from cardiovascular-related causes than men who didn\\'t use the sauna.[2] Furthermore, the benefits they experienced were found to be dose-dependent: Men who used the sauna roughly twice as often, about four to seven times per week, experienced roughly twice the benefits – and were 50 percent less likely to die from cardiovascular-related causes.[2] In addition, frequent sauna users were found to be 40 percent less likely to die from all causes of premature death. These findings held true even when considering age, activity levels, and lifestyle factors that might have influenced the men\\'s health.[2]... The KIHD also revealed that frequent sauna use reduced the risk of developing dementia and Alzheimer\\'s disease in a dose-dependent manner. Men who used the sauna two to three times per week had a 66 percent lower risk of developing dementia and a 65 percent lower risk of developing Alzheimer\\'s disease, compared to men who used the sauna only one time per week… The health benefits associated with sauna use extended to other aspects of mental health, as well. Men participating in the KIHD study who used the sauna four to seven times per week were 77 percent less likely to develop psychotic disorders, regardless of the men\\'s dietary habits, socioeconomic status, physical activity, and inflammatory status (as measured by C-reactive protein)…Exposure to high temperature stresses the body, eliciting a rapid, robust response. The skin and core body temperatures increase markedly, and sweating ensues. The skin heats first, rising to 40°C (104°F), and then changes in core body temperature occur, rising slowly from 37°C (98.6°F, or normal) to 38°C (100.4°F) and then rapidly increasing to 39°C (102.2°F)…  Cardiac output, a measure of the amount of work the heart performs in response to the body\\'s need for oxygen, increases by 60 to 70 percent, while the heart rate (the number of beats per minute) increases and the stroke volume (the amount of blood pumped) remains unchanged.[5] During this time, approximately 50 to 70 percent of the body\\'s blood flow is redistributed from the core to the skin to facilitate sweating. The average person loses approximately 0.5 kg of sweat while sauna bathing.[11] Acute heat exposure also induces a transient increase in overall plasma volume to mitigate the decrease in core blood volume. This increase in plasma volume not only provides a reserve source of fluid for sweating, but it also acts like the water in a car\\'s radiator, cooling the body to prevent rapid increases in core body temperature and promoting heat tolerance… Repeated sauna use acclimates the body to heat and optimizes the body\\'s response to future exposures, likely due to a biological phenomenon known as hormesis, a compensatory defense response following exposure to a mild stressor that is disproportionate to the magnitude of the stressor. Hormesis triggers a vast array of protective mechanisms that not only repair cell damage but also provide protection from subsequent exposures to more devastating stressors… The physiological responses to sauna use are remarkably similar to those experienced during moderate- to vigorous-intensity exercise. In fact, sauna use has been proposed as an alternative to exercise for people who are unable to engage in physical activity due to chronic disease or physical limitations.[13]\\n\\nThe review article sources a lot of data from Finland population studies, where the incidence of sauna use is substantially higher than most countries. Using the data, which of the following is something that is more plausible in Finland than elsewhere?\\nA. More gold medals in adolescent skiing.\\nB. An 86-year old male mayor who is revered in the community.\\nC. Increased rate of pets in the household.\\nD. Improved marriage satisfaction rates.\\nAnswer:']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "prompt_end = format_example(test_df, 66, include_answer=False)\n",
    "\n",
    "train_prompt = gen_prompt(dev_df, subject, k)\n",
    "prompt = train_prompt + prompt_end\n",
    "if args.use_chat_format:\n",
    "    prompt = \"<|user|>\\n\" + prompt.strip() + \"\\n<|assistant|>\\nThe answer is:\"\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt_end, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "# tokenizer.decode(tokenized_prompt.squeeze()[], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "\n",
    "# train_prompt_max_len = max_input_seq_len-prompt_start_ctx_and_end.shape[-1]\n",
    "\n",
    "# tokenized_prompt[-prompt_start_ctx_and_end:].shape, tokenized_prompt.shape, train_prompt_max_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa6d3ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The descending loop of Henle of the nephron of the kidney is permeable to which of the following substances?\\nA. Na+\\nB. H2O\\nC. K+\\nD. Cl-\\nAnswer:'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "275094f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([173, 1029])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(prompts, padding=True, return_tensors='pt')\n",
    "x.input_ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2402339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-47.2500, -47.6250, -51.3125,  ..., -54.5000, -55.4062, -46.1250],\n",
       "        [-42.4062, -43.5938, -46.7812,  ..., -49.7188, -51.4688, -41.0312],\n",
       "        [-42.4688, -43.5000, -46.6562,  ..., -49.6250, -51.5000, -41.2812]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(prompts, padding=True, return_tensors='pt')\n",
    "input_ids = x.input_ids\n",
    "input_ids = input_ids[:3].cuda()\n",
    "attention_mask = x.attention_mask\n",
    "attention_mask = attention_mask[:3].cuda()\n",
    "\n",
    "batch_logits = model(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask).logits[:, -1, :]\n",
    "batch_logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e4ef53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1011])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a667d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get the answer for all examples\n",
    "# note: here we cannot directly use convert_tokens_to_ids because the some tokenizers will automatically add space prefix.\n",
    "answer_choice_ids = [tokenizer.encode(answer_choice, add_special_tokens=False)[0] for answer_choice in choices]\n",
    "pred_indices, all_probs = get_next_word_predictions(\n",
    "    model, tokenizer, prompts, candidate_token_ids=answer_choice_ids, return_token_predictions=False, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# get the metrics\n",
    "cors = []\n",
    "groud_truths = test_df.iloc[:, -1].values\n",
    "for i in range(len(pred_indices)):\n",
    "    prediction = choices[pred_indices[i]]\n",
    "    ground_truth = groud_truths[i]\n",
    "    cors.append(prediction == ground_truth)\n",
    "\n",
    "acc = np.mean(cors)\n",
    "cors = np.array(cors)\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "print(\"Average accuracy {:.3f} - {}\".format(acc, subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf61b8",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d06f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for subject in subjects:\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    "    )\n",
    "    l.append((subject, len(test_df)))\n",
    "    \n",
    "l.append(('avg', sum([x[1] for x in l])))\n",
    "l # 14k problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for subject in tqdm(subjects, desc=f\"Evaluating subjects: \"):\n",
    "\n",
    "    dev_df = pd.read_csv(\n",
    "        os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None\n",
    "    )[: args.ntrain]\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None\n",
    "    )\n",
    "    if args.n_instances and args.n_instances < test_df.shape[0]:\n",
    "        test_df = test_df.sample(args.n_instances, random_state=42)\n",
    "\n",
    "    if args.model_name_or_path:\n",
    "        cors, acc, probs = eval_hf_model(args, subject, model, tokenizer, dev_df, test_df, args.eval_batch_size)\n",
    "    else:\n",
    "        cors, acc, probs = eval_openai_chat_engine(args, subject, args.openai_engine, dev_df, test_df, args.eval_batch_size)\n",
    "\n",
    "    subcats = subcategories[subject]\n",
    "    for subcat in subcats:\n",
    "        subcat_cors[subcat].append(cors)\n",
    "        for key in categories.keys():\n",
    "            if subcat in categories[key]:\n",
    "                cat_cors[key].append(cors)\n",
    "    all_cors.append(cors)\n",
    "\n",
    "    test_df[\"correct\"] = cors\n",
    "    for j in range(probs.shape[1]):\n",
    "        choice = choices[j]\n",
    "        test_df[\"choice{}_probs\".format(choice)] = probs[:, j]\n",
    "    test_df.to_csv(\n",
    "        os.path.join(\n",
    "            args.save_dir, 'csvs', \"{}.csv\".format(subject)\n",
    "        ),\n",
    "        index=None,\n",
    "    )\n",
    "\n",
    "for subcat in subcat_cors:\n",
    "    subcat_acc = np.mean(np.concatenate(subcat_cors[subcat]))\n",
    "    print(\"Average accuracy {:.3f} - {}\".format(subcat_acc, subcat))\n",
    "\n",
    "for cat in cat_cors:\n",
    "    cat_acc = np.mean(np.concatenate(cat_cors[cat]))\n",
    "    print(\"Average accuracy {:.3f} - {}\".format(cat_acc, cat))\n",
    "weighted_acc = np.mean(np.concatenate(all_cors))\n",
    "print(\"Average accuracy: {:.3f}\".format(weighted_acc))\n",
    "\n",
    "# save results\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"average_acc\": weighted_acc,\n",
    "            \"subcat_acc\": {\n",
    "                subcat: np.mean(np.concatenate(subcat_cors[subcat]))\n",
    "                for subcat in subcat_cors\n",
    "            },\n",
    "            \"cat_acc\": {\n",
    "                cat: np.mean(np.concatenate(cat_cors[cat]))\n",
    "                for cat in cat_cors\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
