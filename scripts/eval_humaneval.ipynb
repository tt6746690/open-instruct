{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28feda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/'\n",
    "                if platform.uname().processor == 'x86_64' \n",
    "                else '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da3634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-23 19:20:37,837] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from eval.utils import generate_completions, load_hf_lm_and_tokenizer, query_openai_chat_model\n",
    "from eval.codex_humaneval.data import write_jsonl, read_problems\n",
    "from eval.codex_humaneval.evaluation import evaluate_functional_correctness\n",
    "from transformers import GPT2LMHeadModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd67fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_file='data/eval/codex_humaneval/HumanEval.jsonl.gz', max_num_examples=None, model_name_or_path='../results/baselines/huggyllama/llama-7b', tokenizer_name_or_path=None, use_slow_tokenizer=False, openai_engine=None, save_dir='../results/baselines/huggyllama/llama-7b/eval/humaneval/', eval_batch_size=50, eval_pass_at_ks=[1, 5], unbiased_sampling_size_n=5, temperature=0.1, load_in_8bit=False, gptq=False, use_chat_format=True, chat_formatting_function='eval.templates.create_prompt_with_tulu_chat_format', max_new_tokens=256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_file\", type=str, default=\"data/codex_eval/HumanEval.jsonl.gz\", help=\"Path to the HumanEval data file.\")\n",
    "parser.add_argument(\"--max_num_examples\",  type=int, default=None, help=\"Maximum number of examples to evaluate.\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"If specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"If specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--use_slow_tokenizer\", action=\"store_true\", help=\"If given, we will use the slow tokenizer.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"If specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/codex_eval\", help=\"Directory to save the results.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"Batch size for evaluation.\")\n",
    "parser.add_argument(\"--eval_pass_at_ks\", nargs=\"+\", type=int, default=[1], help=\"Multiple k's that we will report pass@k.\")\n",
    "parser.add_argument(\"--unbiased_sampling_size_n\", type=int, default=20,help=\"Codex HumanEval requires `n` sampled generations per prompt, to estimate the unbiased pass@k. \")\n",
    "parser.add_argument(\"--temperature\", type=float, default=0.1,help=\"Temperature for sampling. This is should be low for evaluating smaller pass@k, and high for larger pass@k.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"Load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--max_new_tokens\", type=int, default=256)\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b'\n",
    "# model_name_or_path = '../results/baselines/gpt2-medium'\n",
    "\n",
    "#     --eval_batch_size 10\n",
    "cmd = f\"\"\"\n",
    "    --data_file data/eval/codex_humaneval/HumanEval.jsonl.gz \\\n",
    "    --eval_pass_at_ks 1 5 \\\n",
    "    --unbiased_sampling_size_n 5 \\\n",
    "    --temperature 0.1 \\\n",
    "    --save_dir {model_name_or_path}/eval/humaneval/ \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 50 \\\n",
    "    --use_chat_format\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (args.openai_engine is None), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "assert args.unbiased_sampling_size_n >= max(args.eval_pass_at_ks), \"n should be larger than the largest k in eval_pass_at_ks.\"\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "444e9e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea6a141b582481d8dcbb2f97951652d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=args.model_name_or_path, \n",
    "    tokenizer_name_or_path=args.tokenizer_name_or_path, \n",
    "    load_in_8bit=args.load_in_8bit, \n",
    "    # device map is determined by the number of gpus available.\n",
    "    device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "    gptq_model=args.gptq,\n",
    "    use_fast_tokenizer=True,\n",
    ")\n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b33db748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 164\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    \n",
    "# wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    max_input_seq_len = model.config.max_position_embeddings - args.max_new_tokens\n",
    "else:\n",
    "    max_input_seq_len = 2048 - args.max_new_tokens\n",
    "\n",
    "\n",
    "test_data = list(read_problems(args.data_file).values())\n",
    "if args.max_num_examples is not None and len(test_data) > args.max_num_examples:\n",
    "    test_data = random.sample(test_data, args.max_num_examples)\n",
    "print(\"Number of examples:\", len(test_data))\n",
    "\n",
    "\n",
    "# def format_prompt(x):\n",
    "#     if args.use_chat_format:\n",
    "#         return \"<|user|>\\n\" + \"Complete the following python function.\\n\\n\\n\" + \\\n",
    "#                 x + \\\n",
    "#                 \"\\n<|assistant|>\\n\" + \"Here is the completed function:\\n\\n\\n\"\n",
    "#     else:\n",
    "#         return x\n",
    "\n",
    "\n",
    "# prompts = []\n",
    "\n",
    "# for example in test_data:\n",
    "#     prompt = format_prompt(example['prompt'])\n",
    "#     tokenized_prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "#     if tokenized_prompt_len >= max_input_seq_len:\n",
    "#         print('here', tokenized_prompt_len)\n",
    "#     prompts.append(prompt)\n",
    "    \n",
    "# print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb2584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([164, 464])\n"
     ]
    }
   ],
   "source": [
    "if args.use_chat_format:\n",
    "    prompts = [\n",
    "        \"<|user|>\\n\" + \"Complete the following python function.\\n\\n\\n\" \n",
    "        + example[\"prompt\"] + \"\\n<|assistant|>\\n\" + \"Here is the completed function:\\n\\n\\n\"\n",
    "        for example in test_data\n",
    "    ]\n",
    "else:\n",
    "    prompts = [example[\"prompt\"] for example in test_data]\n",
    "\n",
    "print(tokenizer(prompts, return_tensors='pt', padding=True).input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96ac1133",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([164, 895])\n"
     ]
    }
   ],
   "source": [
    "# print(prompts[0])\n",
    "prompts = [\n",
    "            \"<|user|>\\n\" + \"Complete the following python function.\\n\\n\\n\" \n",
    "            + example[\"prompt\"] + \"\\n<|assistant|>\\n\" + \"Here is the completed function:\\n\\n\\n\" + example[\"prompt\"]\n",
    "            for example in test_data\n",
    "        ]\n",
    "\n",
    "print(tokenizer(prompts, return_tensors='pt', padding=True).input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5342d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling iter: 0 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating Completions:   0%|          | 0/164 [00:00<?, ?it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts_subset = prompts[:5]  # wpq: do very small batches.\n",
    "prompts_subset = prompts\n",
    "\n",
    "# these stop sequences are those mentioned in the codex paper.\n",
    "stop_sequences = [\"\\nclass\", \"\\ndef\", \"\\n#\", \"\\nif\", \"\\nprint\"]\n",
    "# Because many tokenizers will treat the word after space differently from the original word alone, \n",
    "# to be consistent, we add a space before tokenization and remove it after tokenization.\n",
    "stop_sequences = [tokenizer.encode(\" \" + x, add_special_tokens=False)[1:] for x in stop_sequences]\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    generation_kwargs = {'max_length': model.config.max_position_embeddings} # 1024\n",
    "else:\n",
    "    generation_kwargs = {'max_new_tokens': 512}\n",
    "\n",
    "\n",
    "outputs_per_sampling_iter = []\n",
    "for sampling_iter in range(args.unbiased_sampling_size_n):\n",
    "    print(f\"Sampling iter: {sampling_iter} / {args.unbiased_sampling_size_n}\")\n",
    "    samping_outputs = generate_completions(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompts=prompts_subset,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        stop_id_sequences=stop_sequences,\n",
    "        num_return_sequences=1,  # we don't use the hf num_return_sequences, because otherwise the real batch size will be multiplied by it and often cause oom.\n",
    "        do_sample=True,  # if only pass@1 is evaluated, we do greedy decoding.\n",
    "        top_p=0.95,\n",
    "        temperature=args.temperature,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    outputs_per_sampling_iter.append(samping_outputs)\n",
    "# regroup the outputs to match the number of test data.\n",
    "outputs = []\n",
    "for i in range(len(prompts_subset)):\n",
    "    for j in range(args.unbiased_sampling_size_n):\n",
    "        outputs.append(outputs_per_sampling_iter[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b41f275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([164, 628])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prompts, return_tensors='pt', padding=True).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbedc8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = []\n",
    "for i in range(len(prompts_subset)):\n",
    "    for j in range(args.unbiased_sampling_size_n):\n",
    "        outputs.append(outputs_per_sampling_iter[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55e21f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "0\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "\n",
      "1\n",
      "    paren_groups = []\n",
      "    for char in paren_string:\n",
      "        if char == '(':\n",
      "            paren_groups.append(paren_groups[-1])\n",
      "        elif char == ')':\n",
      "            paren_groups[-1] += paren_groups[-1]\n",
      "            paren_groups.pop()\n",
      "    return paren_groups\n",
      "\n",
      "\n",
      "2\n",
      "    return number - int(number)\n",
      "\n",
      "\n",
      "3\n",
      "    for operation in operations:\n",
      "        if operation < 0:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "\n",
      "4\n",
      "    return sum(abs(x - mean(numbers)) for x in numbers) / len(numbers)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])\n",
    "for i in range(len(outputs_per_sampling_iter[0])):\n",
    "    print(i)\n",
    "    print(outputs_per_sampling_iter[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbcefadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa457ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:00, 6089.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 1/25 [00:00<00:03,  6.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 5/25 [00:00<00:01, 16.82it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:00<00:00, 21.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:00<00:00, 25.43it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [00:00<00:00, 24.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 19/25 [00:00<00:00, 23.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [00:01<00:00, 24.11it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to ../results/baselines/huggyllama/llama-7b/eval/humaneval/codex_eval_predictions.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [00:00<00:00, 22755.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass@1': 0.4, 'pass@5': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# duplicates test data to match the number of outputs.\n",
    "duplicate_test_data = [\n",
    "    example for example in test_data[:5] for _ in range(args.unbiased_sampling_size_n)\n",
    "]\n",
    "assert len(duplicate_test_data) == len(outputs)\n",
    "predictions = [{\"task_id\": example[\"task_id\"], \"prompt\": example[\"prompt\"], \"completion\": output} \n",
    "               for example, output in zip(duplicate_test_data, outputs)]\n",
    "prediction_save_path = os.path.join(args.save_dir, \"codex_eval_predictions.jsonl\")\n",
    "write_jsonl(prediction_save_path, predictions)\n",
    "\n",
    "pass_at_k_results = evaluate_functional_correctness(\n",
    "    sample_file=prediction_save_path,\n",
    "    k=args.eval_pass_at_ks,\n",
    "    problems={example[\"task_id\"]: example for example in test_data[:5]},\n",
    ")\n",
    "\n",
    "print(pass_at_k_results)\n",
    "\n",
    "# with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as fout:\n",
    "#     json.dump(pass_at_k_results, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e96e311b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/baselines/huggyllama/llama-7b/eval/humaneval/codex_eval_predictions.jsonl'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
