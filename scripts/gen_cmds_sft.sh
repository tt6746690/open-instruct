set -e
set -x
CUDA_VISIBLE_DEVICES= cd .. && TORCHELASTIC_ERROR_FILE=/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/scripts/error_file TORCH_CPP_LOG_LEVEL=INFO NCCL_DEBUG=INFO LOGLEVEL=INFO CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=10002 open_instruct/finetune_trainer.py --model_name_or_path=results/baselines/huggyllama/llama-7b --tokenizer_name=results/baselines/huggyllama/llama-7b --use_fast_tokenizer=True --train_file=data/processed/dolly/dolly_data.jsonl --max_seq_length=2048 --do_train --preprocessing_num_workers=32 --per_device_train_batch_size=2 --gradient_accumulation_steps=32 --learning_rate=2e-5 --lr_scheduler_type=linear --warmup_ratio=0.03 --weight_decay=0. --optim=adamw_hf --evaluation_strategy=no --eval_steps=100 --report_to tensorboard --run_name /dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/oi2/jpt_llama-7b_dolly_ep=2 --logging_strategy=steps --logging_first_step --logging_steps=1 --save_strategy=steps --save_steps=100 --save_total_limit=1 --num_train_epochs=2 --ddp_timeout=7200 --fsdp="full_shard auto_wrap" --fsdp_transformer_layer_cls_to_wrap="LlamaDecoderLayer" --gradient_checkpointing --torch_dtype=bfloat16 --dataloader_num_workers=8 --bf16=True --overwrite_output_dir --use_flash_attn True --low_cpu_mem_usage --output_dir="/dccstor/data-pruning/wpq/github/mitibm2023/external/open-instruct/results/oi2/jpt_llama-7b_dolly_ep=2"