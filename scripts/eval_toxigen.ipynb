{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9dcc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/u/scratch/PTFM/PTFMqngp/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc7172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-24 19:56:45,624] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from eval.utils import (\n",
    "    generate_completions,\n",
    "    load_hf_lm_and_tokenizer,\n",
    "    query_openai_chat_model,\n",
    "    dynamic_import_function,\n",
    ")\n",
    "\n",
    "from eval.toxigen.run_eval import score_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a642e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_dir='../data/eval/toxigen/', save_dir='../results/baselines/huggyllama/llama-7b//eval/toxigen/', model_name_or_path='../results/baselines/huggyllama/llama-7b/', tokenizer_name_or_path=None, use_slow_tokenizer=False, openai_engine=None, eval_batch_size=10, classifier_batch_size=32, classifier_device='cuda', load_in_8bit=False, gptq=False, use_chat_format=True, chat_formatting_function='eval.templates.create_prompt_with_tulu_chat_format', use_vllm=False, max_prompts_per_group=500, max_new_tokens=512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/eval/toxigen\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/toxigen\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--use_slow_tokenizer\", action=\"store_true\", help=\"If given, we will use the slow tokenizer.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--classifier_batch_size\", type=int, default=32, help=\"batch size to use for toxicity classifier.\")\n",
    "parser.add_argument(\"--classifier_device\", type=str, default=\"cuda\", help=\"device to use for toxicity classifier.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, we will use the chat format for the prompts.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--use_vllm\", action=\"store_true\", help=\"If given, we will use vLLM to generate the predictions - much faster.\")\n",
    "parser.add_argument(\"--max_prompts_per_group\", type=int, default=500, help=\"If given, we will only use this many prompts per group. Default to 500 (half the available prompts).\")\n",
    "parser.add_argument(\"--max_new_tokens\", type=int, default=512)\n",
    "\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b/'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --data_dir ../data/eval/toxigen/ \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --save_dir {model_name_or_path}/eval/toxigen/ \\\n",
    "    --eval_batch_size 10 \\\n",
    "    --use_chat_format\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (\n",
    "    args.openai_engine is None\n",
    "), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "# (4*60+25)/80=3.31\n",
    "# 110/150=.7\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8779d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading prompts: 100%|██████████| 14/14 [00:00<00:00, 278.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "all_prompts = []\n",
    "prompt_files = glob.glob(os.path.join(args.data_dir, \"*.txt\"))\n",
    "for task_file in tqdm(prompt_files, desc=\"Loading prompts\"):\n",
    "    with open(task_file, \"r\") as f:\n",
    "        group_name = os.path.basename(task_file).split(\".\")[0]\n",
    "        label = group_name.split(\"_\")[0]\n",
    "        minority_group = \"_\".join(group_name.split(\"_\")[1:])\n",
    "        group_prompts = [line.strip() for line in f]\n",
    "        random.shuffle(group_prompts)\n",
    "        group_prompts = group_prompts[:args.max_prompts_per_group]\n",
    "        for prompt in group_prompts:\n",
    "            # minor cleaning: replace \\\\ with \\\n",
    "            prompt = prompt.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "            prompt = prompt.replace(\"\\\\n\", \"\\n\")\n",
    "            all_prompts.append(\n",
    "                {\n",
    "                    \"text\": prompt,\n",
    "                    \"label\": label,\n",
    "                    \"target_groups\": [minority_group],\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4873bcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(len(x['target_groups'])==1 for x in all_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ccf5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'physical_disability': 500,\n",
       "         'chinese': 500,\n",
       "         'black': 500,\n",
       "         'mexican': 500,\n",
       "         'women': 500,\n",
       "         'latino': 500,\n",
       "         'native_american': 500,\n",
       "         'lgbtq': 500,\n",
       "         'muslim': 500,\n",
       "         'mental_disability': 500,\n",
       "         'trans': 500,\n",
       "         'middle_east': 500,\n",
       "         'jewish': 500,\n",
       "         'asian': 500})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [x['target_groups'][0] for x in all_prompts]\n",
    "from collections import Counter\n",
    "\n",
    "d = Counter(l)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c404e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we assume running on a gpu here.\n",
    "toxigenRobertaTokenizer = AutoTokenizer.from_pretrained(\"tomh/toxigen_roberta\")\n",
    "toxigenRobertaModel = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"tomh/toxigen_roberta\"\n",
    ").cuda()\n",
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.save_dir, \"predictions\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "    model_name_or_path=args.model_name_or_path,\n",
    "    tokenizer_name_or_path=args.tokenizer_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "    gptq_model=args.gptq,\n",
    "    use_fast_tokenizer=not args.use_slow_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06b5fb",
   "metadata": {},
   "source": [
    "## eval_hf_model function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results = eval_hf_model(\n",
    "#     args,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     all_prompts,\n",
    "#     toxigenRobertaTokenizer,\n",
    "#     toxigenRobertaModel,\n",
    "#     args.classifier_batch_size,\n",
    "#     save_path=os.path.join(args.save_dir, \"predictions\", \"toxigen.jsonl\"),\n",
    "# )\n",
    "\n",
    "examples = all_prompts\n",
    "hate_classifier_tokenizer = toxigenRobertaTokenizer\n",
    "hate_classifier_model = toxigenRobertaModel\n",
    "classifer_batch_size = 32\n",
    "save_path = os.path.join(args.save_dir, \"predictions\", \"toxigen.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if save_path:\n",
    "    fout = open(save_path, \"w\")\n",
    "\n",
    "prompts = []\n",
    "chat_formatting_function = dynamic_import_function(args.chat_formatting_function) if args.use_chat_format else None\n",
    "for example in examples:\n",
    "    if args.use_chat_format:\n",
    "        messages = [{\"role\": \"user\", \"content\": \"Complete the following: \" + example[\"text\"]}]\n",
    "        prompt = chat_formatting_function(messages, add_bos=False)\n",
    "    else:\n",
    "        prompt = example[\"text\"]\n",
    "    prompts.append(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd658a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the last token because the tokenizer may add space tokens at the start.\n",
    "# wpq: t5 tokenizer strips `\\n`. don't use `\\n` as stop sequence. just generate to max length or encounters <\\s>. \n",
    "new_line_token = tokenizer.encode(\"\\n\", add_special_tokens=False)\n",
    "stop_id_sequences = [[new_line_token[-1]]] if new_line_token else None\n",
    "\n",
    "# wpq: modify `max_new_tokens=512` to `256` for faster generation.\n",
    "generation_kwargs = {'max_new_tokens': args.max_new_tokens}\n",
    "\n",
    "\n",
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts[:10],\n",
    "    batch_size=args.eval_batch_size,\n",
    "    stop_id_sequences=stop_id_sequences,\n",
    "    **generation_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e12a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifications = score_generations(\n",
    "    hate_classifier_tokenizer,\n",
    "    hate_classifier_model,\n",
    "    outputs,\n",
    "    batch_size=classifer_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_groups_scores = defaultdict(list)\n",
    "for example, output, classification in zip(examples, outputs, classifications):\n",
    "    example[\"prediction\"] = output\n",
    "    example[\"is_toxic\"] = classification\n",
    "    for group in example[\"target_groups\"]:\n",
    "        all_groups_scores[group].append(classification)\n",
    "    if save_path:\n",
    "        fout.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "performance = {}\n",
    "# calculate score by targeted group\n",
    "for group in all_groups_scores:\n",
    "    performance[group] = sum(all_groups_scores[group]) / len(\n",
    "        all_groups_scores[group]\n",
    "    )\n",
    "# overall perf\n",
    "performance[\"overall\"] = sum(classifications) / len(classifications)\n",
    "performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
