{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf816f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/'\n",
    "                if platform.uname().processor == 'x86_64' \n",
    "                else '/gpfs/u/scratch/PTFM/PTFMqngp/github/mitibm2023/external/open-instruct/')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import torch\n",
    "from eval.utils import (\n",
    "    load_hf_lm_and_tokenizer,\n",
    "    generate_completions,\n",
    "    query_openai_chat_model,\n",
    "    dynamic_import_function,\n",
    ")\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366cc32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/bbh\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/bbh\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--use_slow_tokenizer\", action=\"store_true\", help=\"If given, we will use the slow tokenizer.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--no_cot\", action=\"store_true\", help=\"if specified, chain of thoughts will be removed from the prompts.\")\n",
    "parser.add_argument(\"--max_num_examples_per_task\", type=int, default=None, help=\"maximum number of examples to evaluate per task.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "parser.add_argument(\"--chat_formatting_function\", type=str, default=\"eval.templates.create_prompt_with_tulu_chat_format\", help=\"The function to use to create the chat format. This function will be dynamically imported. Please see examples in `eval/templates.py`.\")\n",
    "parser.add_argument(\"--max_new_tokens\", type=int, default=256)\n",
    "parser.add_argument(\"--n_shot\", type=int, default=3)\n",
    "\n",
    "\n",
    "model_name_or_path = '../results/baselines/gpt2-large'\n",
    "# model_name_or_path = '../results/baselines/gpt2-medium'\n",
    "# model_name_or_path = '../results/gpt2-Large_human_mix'\n",
    "# model_name_or_path = '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-200'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b/'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --data_dir ../data/eval/bbh/ \\\n",
    "    --save_dir {model_name_or_path}/eval/bbh/ \\\n",
    "    --max_num_examples_per_task 40 \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 10\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (args.openai_engine is None), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3077f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.exact_match = evaluate.load(\"exact_match\", experiment_id=args.model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "data_size = 0\n",
    "all_tasks = {}\n",
    "task_files = glob.glob(os.path.join(args.data_dir, \"bbh\", \"*.json\"))\n",
    "for task_file in tqdm.tqdm(task_files, desc=\"Loading tasks\"):\n",
    "    with open(task_file, \"r\") as f:\n",
    "        task_name = os.path.basename(task_file).split(\".\")[0]\n",
    "        all_tasks[task_name] = json.load(f)[\"examples\"]\n",
    "        print(f'{task_name}\\t{len(all_tasks[task_name])}')\n",
    "        data_size += len(all_tasks[task_name])\n",
    "        if args.max_num_examples_per_task:\n",
    "            all_tasks[task_name] = random.sample(all_tasks[task_name], args.max_num_examples_per_task)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_import_function(args.chat_formatting_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1622650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_prompts = {}\n",
    "cot_prompt_files = glob.glob(os.path.join(args.data_dir, \"cot-prompts\", \"*.txt\"))\n",
    "for cot_prompt_file in tqdm.tqdm(cot_prompt_files, desc=\"Loading prompts\"):\n",
    "    with open(cot_prompt_file, \"r\") as f:\n",
    "        task_name = os.path.basename(cot_prompt_file).split(\".\")[0]\n",
    "        task_prompt = \"\".join(f.readlines()[2:])\n",
    "\n",
    "        prompt_fields = task_prompt.split(\"\\n\\n\")\n",
    "        # `new_prompt_fields[0]`: sub-task instruction/description\n",
    "        # `new_prompt_fields[1:]`: demonstrations\n",
    "        new_prompt_fields = []\n",
    "        for prompt_field in prompt_fields:\n",
    "            if prompt_field.startswith(\"Q:\"):\n",
    "                assert \"So the answer is\" in prompt_field, f\"`So the answer is` not found in prompt field of {task_name}.txt.\"\n",
    "                assert \"\\nA:\" in prompt_field, \"`\\nA:` not found in prompt field.\"\n",
    "                question, answer = prompt_field.split(\"\\nA:\")\n",
    "                question, answer = question.strip(), answer.strip()\n",
    "                if args.no_cot:\n",
    "                    answer = answer.split(\"So the answer is\")[-1].strip()\n",
    "                new_prompt_fields.append(question+'\\nA: '+answer)\n",
    "            else:\n",
    "                new_prompt_fields.append(prompt_field)\n",
    "        # prompt instruction span two lines! concat them.\n",
    "        if task_name == 'snarks':\n",
    "            new_prompt_fields = ['\\n\\n'.join(new_prompt_fields[:2])]+new_prompt_fields[2:]\n",
    "        assert(len(new_prompt_fields) == 4)\n",
    "        all_prompts[task_name] = new_prompt_fields\n",
    "\n",
    "# ## previous impl. modified to enable using <=3 in-context examples when prompt is too long.\n",
    "# \n",
    "#     with open(cot_prompt_file, \"r\") as f:\n",
    "#         task_name = os.path.basename(cot_prompt_file).split(\".\")[0]\n",
    "#         task_prompt = \"\".join(f.readlines()[2:])\n",
    "#         if args.no_cot:\n",
    "#             prompt_fields = task_prompt.split(\"\\n\\n\")\n",
    "#             new_prompt_fields = []\n",
    "#             for prompt_field in prompt_fields:\n",
    "#                 if prompt_field.startswith(\"Q:\"):\n",
    "#                     assert \"So the answer is\" in prompt_field, f\"`So the answer is` not found in prompt field of {task_name}.txt.\"\n",
    "#                     assert \"\\nA:\" in prompt_field, \"`\\nA:` not found in prompt field.\"\n",
    "#                     answer = prompt_field.split(\"So the answer is\")[-1].strip()\n",
    "#                     question = prompt_field.split(\"\\nA:\")[0].strip()\n",
    "#                     new_prompt_fields.append(question + \"\\nA: \" + answer)\n",
    "#                 else:\n",
    "#                     new_prompt_fields.append(prompt_field)\n",
    "#             task_prompt = \"\\n\\n\".join(new_prompt_fields)\n",
    "#         all_prompts[task_name] = task_prompt\n",
    "\n",
    "assert set(all_tasks.keys()) == set(all_prompts.keys()), \"task names in task data and task prompts are not the same.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419ac2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# task_prompt = all_prompts[task_name]\n",
    "# examples = all_tasks[task_name]\n",
    "\n",
    "# def get_task_prompt(n_shot):\n",
    "#     assert(0 <= n_shot <= 3)\n",
    "#     return \"\\n\\n\".join(task_prompt[:n_shot+1]).strip()\n",
    "\n",
    "# prompts = []\n",
    "# for example in examples:\n",
    "#     for n_shot in list(range(args.n_shot+1)[::-1]):\n",
    "#         task_prompt_concat = get_task_prompt(n_shot)\n",
    "#         if args.use_chat_format:\n",
    "#             prompt = \"<|user|>\\n\" + task_prompt_concat + \"\\n\\nQ: \" + example[\"input\"] + \"\\n<|assistant|>\\nA:\"\n",
    "#         else:\n",
    "#             prompt = task_prompt_concat + \"\\n\\nQ: \" + example[\"input\"] + \"\\nA:\"\n",
    "#         tokenized_prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "#         if tokenized_prompt_len < max_input_seq_len:\n",
    "#             break\n",
    "#     if n_shot != args.n_shot:\n",
    "#         print(f'n_shot: {args.n_shot} -> {n_shot}')\n",
    "#     prompts.append(prompt)\n",
    "\n",
    "# max_input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6a6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # print(prompt_field)\n",
    "\n",
    "# question, answer = prompt_field.split(\"\\nA:\")\n",
    "# question, answer = question.strip(), answer.strip()\n",
    "# if args.no_cot:\n",
    "#     answer = answer.split(\"So the answer is\")[-1].strip()\n",
    "# answer =  \"A: \" + answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97174d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.save_dir, \"predictions\"), exist_ok=True)\n",
    "\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        model_name_or_path=args.model_name_or_path, \n",
    "        tokenizer_name_or_path=args.tokenizer_name_or_path, \n",
    "        load_in_8bit=args.load_in_8bit, \n",
    "        device_map=\"balanced_low_0\" if torch.cuda.device_count() > 1 else \"auto\",\n",
    "        gptq_model=args.gptq,\n",
    "        use_fast_tokenizer=not args.use_slow_tokenizer,\n",
    "    )\n",
    "    \n",
    "# wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    max_input_seq_len = model.config.max_position_embeddings - args.max_new_tokens\n",
    "else:\n",
    "    max_input_seq_len = 2048 - args.max_new_tokens\n",
    "\n",
    "    \n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6d670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_name = next(iter(all_tasks.keys()))\n",
    "task_examples = all_tasks[task_name]\n",
    "prompt = all_prompts[task_name]\n",
    "print(prompt)\n",
    "print(len(prompt))\n",
    "print()\n",
    "print(task_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88598731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_perf = eval_hf_model(\n",
    "#     args, \n",
    "#     model, \n",
    "#     tokenizer, \n",
    "#     task_examples, \n",
    "#     prompt, \n",
    "#     save_path=os.path.join(args.save_dir, \"predictions\", f\"{task_name}.jsonl\")\n",
    "# )\n",
    "\n",
    "# performance[task_name] = task_perf\n",
    "# print(f\"Task {task_name} - EM: {task_perf}\")\n",
    "\n",
    "examples = task_examples\n",
    "task_prompt = prompt\n",
    "save_path=os.path.join(args.save_dir, \"predictions\", f\"{task_name}.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [example[\"target\"] for example in examples]\n",
    "if save_path:\n",
    "    fout = open(save_path, \"w\")\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71851bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## wpq: for k=3 shots, reduce number of demonstrations if the prompt is too long.\n",
    "prompts = []\n",
    "chat_formatting_function = dynamic_import_function(args.chat_formatting_function) if args.use_chat_format else None\n",
    "for example in examples:\n",
    "    for n_shot in list(range(args.n_shot+1)[::-1]):\n",
    "        task_prompt_concat = get_task_prompt(n_shot)\n",
    "        prompt = task_prompt_concat + \"\\n\\nQ: \" + example[\"input\"]\n",
    "        if args.use_chat_format:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            prompt = chat_formatting_function(messages, add_bos=False)\n",
    "            if prompt[-1] in [\"\\n\", \" \"]:\n",
    "                prompt += \"A:\"\n",
    "            else:\n",
    "                prompt += \" A:\"\n",
    "        else:\n",
    "            prompt += \"\\nA:\"\n",
    "        tokenized_prompt_len = len(tokenizer(prompt, add_special_tokens=False)['input_ids'])\n",
    "        if tokenized_prompt_len <= max_input_seq_len:\n",
    "            break\n",
    "    if n_shot != args.n_shot:\n",
    "        print(f'n_shot: {args.n_shot} -> {n_shot}')\n",
    "    prompts.append(prompt)\n",
    "\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [' ', '\\n', '\\n\\n', 'A: (B)\\n\\n']:\n",
    "    print(s, tokenizer.encode(s, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for s in [[13], [13,13], [29871,13]]:\n",
    "    l.append(tokenizer.decode(s, add_special_tokens=False))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.no_cot:\n",
    "    # get the last token because the tokenizer may add space tokens at the start.\n",
    "    stop_id_sequences = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    stop_id_sequences = [stop_id_sequences[-2:]] if stop_id_sequences else None\n",
    "else:\n",
    "    # let's not use the stop sequence for cot now since it's too inefficient when the generation is long. \n",
    "    # instead, we'll do some post-processing to extract the answer.\n",
    "    stop_sequnce = None\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    # wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "    generation_kwargs = {'max_length': model.config.max_position_embeddings} # 1024\n",
    "else:\n",
    "    # wpq: modify `max_new_tokens=512` to `128` for faster generation.\n",
    "    # for non-cot multiple choice answers, e.g., ' (G).' requires just 5 tokens\n",
    "    generation_kwargs = {'max_new_tokens': 10 if args.no_cot else 256}\n",
    "\n",
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts,\n",
    "    batch_size=args.eval_batch_size if args.eval_batch_size else 1,\n",
    "    stop_id_sequences=stop_id_sequences,\n",
    "    **generation_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2049b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions = []\n",
    "for example, output in zip(examples, outputs):\n",
    "    example[\"raw_output\"] = output\n",
    "\n",
    "    # only keep the first part of the output - this is mainly for vanilla language models.\n",
    "    output = output.strip().split(\"\\n\\n\")[0].strip()\n",
    "\n",
    "    # extract the first answer after `So the answer is` and before the next period.\n",
    "    # if there is no such answer, we will just use the raw output.\n",
    "    results = re.search(r\"So the answer is (.*?)\\.\", output)\n",
    "    if results:\n",
    "        prediction = results.group(1).strip()\n",
    "    else:\n",
    "        prediction = output.strip()\n",
    "\n",
    "    example[\"prediction\"] = prediction\n",
    "    predictions.append(prediction)\n",
    "    if save_path:\n",
    "        fout.write(json.dumps(example) + \"\\n\")        \n",
    "\n",
    "assert len(predictions) == len(targets), \"number of predictions and targets are not the same.\"\n",
    "task_perf = args.exact_match.compute(predictions=predictions, references=targets, ignore_case=True, ignore_punctuation=True)[\"exact_match\"]\n",
    "task_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "performance[task_name] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75208e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as fout:\n",
    "    performance[\"average_exact_match\"] = sum(performance.values()) / len(performance)\n",
    "    print(f\"Average EM: {performance['average_exact_match']}\")\n",
    "    json.dump(performance, fout, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
