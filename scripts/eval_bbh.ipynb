{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf816f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mit_fm/wpq/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0864c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "from eval.utils import load_hf_lm_and_tokenizer, generate_completions, query_openai_chat_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5366cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/bbh\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/bbh\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--no_cot\", action=\"store_true\", help=\"if specified, chain of thoughts will be removed from the prompts.\")\n",
    "parser.add_argument(\"--max_num_examples_per_task\", type=int, default=None, help=\"maximum number of examples to evaluate per task.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "\n",
    "\n",
    "model_name_or_path = '../results/baselines/gpt2-large'\n",
    "model_name_or_path = '../results/baselines/gpt2'\n",
    "# model_name_or_path = '../results/gpt2-Large_human_mix'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --data_dir ../data/eval/bbh/ \\\n",
    "    --save_dir {model_name_or_path}/eval/bbh/ \\\n",
    "    --max_num_examples_per_task 40 \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 10 \\\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (args.openai_engine is None), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9627c7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks: 100%|██████████| 27/27 [00:00<00:00, 1339.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logical_deduction_seven_objects\t250\n",
      "ruin_names\t250\n",
      "dyck_languages\t250\n",
      "causal_judgement\t187\n",
      "sports_understanding\t250\n",
      "word_sorting\t250\n",
      "temporal_sequences\t250\n",
      "multistep_arithmetic_two\t250\n",
      "tracking_shuffled_objects_seven_objects\t250\n",
      "web_of_lies\t250\n",
      "formal_fallacies\t250\n",
      "salient_translation_error_detection\t250\n",
      "reasoning_about_colored_objects\t250\n",
      "disambiguation_qa\t250\n",
      "movie_recommendation\t250\n",
      "logical_deduction_five_objects\t250\n",
      "hyperbaton\t250\n",
      "geometric_shapes\t250\n",
      "navigate\t250\n",
      "object_counting\t250\n",
      "tracking_shuffled_objects_three_objects\t250\n",
      "snarks\t178\n",
      "penguins_in_a_table\t146\n",
      "date_understanding\t250\n",
      "boolean_expressions\t250\n",
      "logical_deduction_three_objects\t250\n",
      "tracking_shuffled_objects_five_objects\t250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6511"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "data_size = 0\n",
    "all_tasks = {}\n",
    "task_files = glob.glob(os.path.join(args.data_dir, \"bbh\", \"*.json\"))\n",
    "for task_file in tqdm.tqdm(task_files, desc=\"Loading tasks\"):\n",
    "    with open(task_file, \"r\") as f:\n",
    "        task_name = os.path.basename(task_file).split(\".\")[0]\n",
    "        all_tasks[task_name] = json.load(f)[\"examples\"]\n",
    "        print(f'{task_name}\\t{len(all_tasks[task_name])}')\n",
    "        data_size += len(all_tasks[task_name])\n",
    "        if args.max_num_examples_per_task:\n",
    "            all_tasks[task_name] = random.sample(all_tasks[task_name], args.max_num_examples_per_task)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1622650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading prompts: 100%|██████████| 27/27 [00:00<00:00, 1392.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_prompts = {}\n",
    "cot_prompt_files = glob.glob(os.path.join(args.data_dir, \"cot-prompts\", \"*.txt\"))\n",
    "for cot_prompt_file in tqdm.tqdm(cot_prompt_files, desc=\"Loading prompts\"):\n",
    "    with open(cot_prompt_file, \"r\") as f:\n",
    "        task_name = os.path.basename(cot_prompt_file).split(\".\")[0]\n",
    "        task_prompt = \"\".join(f.readlines()[2:])\n",
    "        if args.no_cot:\n",
    "            prompt_fields = task_prompt.split(\"\\n\\n\")\n",
    "            new_prompt_fields = []\n",
    "            for prompt_field in prompt_fields:\n",
    "                if prompt_field.startswith(\"Q:\"):\n",
    "                    assert \"So the answer is\" in prompt_field, f\"`So the answer is` not found in prompt field of {task_name}.txt.\"\n",
    "                    assert \"\\nA:\" in prompt_field, \"`\\nA:` not found in prompt field.\"\n",
    "                    answer = prompt_field.split(\"So the answer is\")[-1].strip()\n",
    "                    question = prompt_field.split(\"\\nA:\")[0].strip()\n",
    "                    new_prompt_fields.append(question + \"\\nA: \" + answer)\n",
    "                else:\n",
    "                    new_prompt_fields.append(prompt_field)\n",
    "            task_prompt = \"\\n\\n\".join(new_prompt_fields)\n",
    "        all_prompts[task_name] = task_prompt\n",
    "\n",
    "assert set(all_tasks.keys()) == set(all_prompts.keys()), \"task names in task data and task prompts are not the same.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09e9f3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A logical deduction task which requires deducing the order of a sequence of objects.\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
      "Options:\n",
      "(A) Amy finished last\n",
      "(B) Eli finished last\n",
      "(C) Eve finished last\n",
      "A: Let's think step by step.\n",
      "(1) Eve finished above Amy: \"(above) ? Eve ? Amy ? (below)\".\n",
      "(2) Eli finished below Amy: \"(above) ? Amy ? Eli ? (below)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(above) Eve Amy Eli (below)\".\n",
      "According to this ordering, the person who finished last (the one at the bottom of this list) is Eli.\n",
      "Eli finished last. So the answer is (B).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
      "Options:\n",
      "(A) The white book is the leftmost\n",
      "(B) The green book is the leftmost\n",
      "(C) The orange book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The green book is to the right of the white book: \"(left) ? white ? green ? (right)\".\n",
      "(2) The orange book is the rightmost: \"(left) ? white ? green orange (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white green orange (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (A).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
      "Options:\n",
      "(A) The red book is the leftmost\n",
      "(B) The gray book is the leftmost\n",
      "(C) The white book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The white book is to the left of the gray book: \"(left) ? white ? gray ? (right)\".\n",
      "(2) The red book is the second from the left: \"(left) ? white red gray ? (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white red gray (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (C).\n"
     ]
    }
   ],
   "source": [
    "task_name = 'logical_deduction_seven_objects'\n",
    "print(all_prompts[task_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d97174d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), torch.bfloat16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.save_dir, \"predictions\"), exist_ok=True)\n",
    "\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        model_name_or_path=args.model_name_or_path, \n",
    "        tokenizer_name_or_path=args.tokenizer_name_or_path, \n",
    "        load_in_8bit=args.load_in_8bit, \n",
    "        gptq_model=args.gptq,\n",
    "        device_map='cpu',\n",
    "    )\n",
    "    \n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c615bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[438, 677, 580, 881, 458, 763, 900, 204, 505, 726, 1004, 726, 804, 756, 677, 914, 1432, 726, 1866, 905, 918, 697, 1224, 815, 709, 719, 677]\n"
     ]
    }
   ],
   "source": [
    "# get an idea of prompt length for the tasks.\n",
    "\n",
    "task_name = 'logical_deduction_seven_objects'\n",
    "print([tokenizer(x, return_tensors='pt').input_ids.shape[1] for x in all_prompts.values()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0e6d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = next(iter(all_tasks.keys()))\n",
    "task_examples = all_tasks[task_name]\n",
    "prompt = all_prompts[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a75208e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--gptq\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m\"\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf given, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre evaluating a 4-bit quantized GPTQ model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--use_chat_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m\"\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf given, the prompt will be encoded as a chat format with the roles in prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/baselines/gpt2-large\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model_name_or_path = '../results/gpt2-Large_human_mix'\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/argparse.py:1836\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[1;32m   1835\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1836\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/argparse.py:2594\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   2593\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2594\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/mit_fm/miniconda/envs/open-instruct/lib/python3.10/argparse.py:2581\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m   2580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2581\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "performance = {}\n",
    "for task_name in tqdm.tqdm(all_tasks.keys(), desc=\"Evaluating\"):\n",
    "    task_examples = all_tasks[task_name]\n",
    "    prompt = all_prompts[task_name]\n",
    "    if args.model_name_or_path:\n",
    "        task_perf = eval_hf_model(\n",
    "            args, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            task_examples, \n",
    "            prompt, \n",
    "            save_path=os.path.join(args.save_dir, \"predictions\", f\"{task_name}.jsonl\")\n",
    "        )\n",
    "    else:\n",
    "        task_perf = eval_openai_chat_engine(\n",
    "            args,\n",
    "            task_examples,\n",
    "            prompt,\n",
    "            save_path=os.path.join(args.save_dir, \"predictions\", f\"{task_name}.jsonl\")\n",
    "        )\n",
    "    performance[task_name] = task_perf\n",
    "    print(f\"Task {task_name} - EM: {task_perf}\")\n",
    "\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as fout:\n",
    "    performance[\"average_exact_match\"] = sum(performance.values()) / len(performance)\n",
    "    print(f\"Average EM: {performance['average_exact_match']}\")\n",
    "    json.dump(performance, fout, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
