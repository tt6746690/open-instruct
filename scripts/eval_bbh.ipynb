{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf816f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mit_fm/wpq/github/rosemary/src/rosemary/__init__.py:25: UserWarning: Install `torch` for functionalities dependent on torch\n",
      "  warn(f'Install `torch` for functionalities dependent on torch')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rosemary import jpt_parse_args, jpt_setup; jpt_setup()\n",
    "\n",
    "import sys\n",
    "sys.path.append('/dccstor/mit_fm/wpq/github/mitibm2023/external/open-instruct/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0864c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "from eval.utils import load_hf_lm_and_tokenizer, generate_completions, query_openai_chat_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5366cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/bbh\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"results/bbh\")\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=None, help=\"if specified, we will load the model to generate the predictions.\")\n",
    "parser.add_argument(\"--tokenizer_name_or_path\", type=str, default=None, help=\"if specified, we will load the tokenizer from here.\")\n",
    "parser.add_argument(\"--openai_engine\", type=str, default=None, help=\"if specified, we will use the OpenAI API to generate the predictions.\")\n",
    "parser.add_argument(\"--no_cot\", action=\"store_true\", help=\"if specified, chain of thoughts will be removed from the prompts.\")\n",
    "parser.add_argument(\"--max_num_examples_per_task\", type=int, default=None, help=\"maximum number of examples to evaluate per task.\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"batch size for evaluation.\")\n",
    "parser.add_argument(\"--load_in_8bit\", action=\"store_true\", help=\"load model in 8bit mode, which will reduce memory and speed up inference.\")\n",
    "parser.add_argument(\"--gptq\", action=\"store_true\", help=\"If given, we're evaluating a 4-bit quantized GPTQ model.\")\n",
    "parser.add_argument(\"--use_chat_format\", action=\"store_true\", help=\"If given, the prompt will be encoded as a chat format with the roles in prompt.\")\n",
    "\n",
    "\n",
    "model_name_or_path = '../results/baselines/gpt2-large'\n",
    "model_name_or_path = '../results/baselines/gpt2'\n",
    "# model_name_or_path = '../results/gpt2-Large_human_mix'\n",
    "# model_name_or_path = '../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-200'\n",
    "model_name_or_path = '../results/baselines/huggyllama/llama-7b/'\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --data_dir ../data/eval/bbh/ \\\n",
    "    --save_dir {model_name_or_path}/eval/bbh/ \\\n",
    "    --max_num_examples_per_task 40 \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --eval_batch_size 10\n",
    "\"\"\"\n",
    "\n",
    "args = jpt_parse_args(parser, cmd)\n",
    "\n",
    "# model_name_or_path and openai_engine cannot be both None or both not None.\n",
    "assert (args.model_name_or_path is None) != (args.openai_engine is None), \"Either model_name_or_path or openai_engine should be specified.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9627c7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks:  33%|███▎      | 9/27 [00:00<00:00, 85.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logical_deduction_seven_objects\t250\n",
      "ruin_names\t250\n",
      "dyck_languages\t250\n",
      "causal_judgement\t187\n",
      "sports_understanding\t250\n",
      "word_sorting\t250\n",
      "temporal_sequences\t250\n",
      "multistep_arithmetic_two\t250\n",
      "tracking_shuffled_objects_seven_objects\t250\n",
      "web_of_lies\t250\n",
      "formal_fallacies\t250\n",
      "salient_translation_error_detection\t250\n",
      "reasoning_about_colored_objects\t250\n",
      "disambiguation_qa\t250\n",
      "movie_recommendation\t250\n",
      "logical_deduction_five_objects\t250\n",
      "hyperbaton\t250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tasks: 100%|██████████| 27/27 [00:00<00:00, 84.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geometric_shapes\t250\n",
      "navigate\t250\n",
      "object_counting\t250\n",
      "tracking_shuffled_objects_three_objects\t250\n",
      "snarks\t178\n",
      "penguins_in_a_table\t146\n",
      "date_understanding\t250\n",
      "boolean_expressions\t250\n",
      "logical_deduction_three_objects\t250\n",
      "tracking_shuffled_objects_five_objects\t250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6511"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "data_size = 0\n",
    "all_tasks = {}\n",
    "task_files = glob.glob(os.path.join(args.data_dir, \"bbh\", \"*.json\"))\n",
    "for task_file in tqdm.tqdm(task_files, desc=\"Loading tasks\"):\n",
    "    with open(task_file, \"r\") as f:\n",
    "        task_name = os.path.basename(task_file).split(\".\")[0]\n",
    "        all_tasks[task_name] = json.load(f)[\"examples\"]\n",
    "        print(f'{task_name}\\t{len(all_tasks[task_name])}')\n",
    "        data_size += len(all_tasks[task_name])\n",
    "        if args.max_num_examples_per_task:\n",
    "            all_tasks[task_name] = random.sample(all_tasks[task_name], args.max_num_examples_per_task)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1622650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading prompts: 100%|██████████| 27/27 [00:00<00:00, 402.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_prompts = {}\n",
    "cot_prompt_files = glob.glob(os.path.join(args.data_dir, \"cot-prompts\", \"*.txt\"))\n",
    "for cot_prompt_file in tqdm.tqdm(cot_prompt_files, desc=\"Loading prompts\"):\n",
    "    with open(cot_prompt_file, \"r\") as f:\n",
    "        task_name = os.path.basename(cot_prompt_file).split(\".\")[0]\n",
    "        task_prompt = \"\".join(f.readlines()[2:])\n",
    "        if args.no_cot:\n",
    "            prompt_fields = task_prompt.split(\"\\n\\n\")\n",
    "            new_prompt_fields = []\n",
    "            for prompt_field in prompt_fields:\n",
    "                if prompt_field.startswith(\"Q:\"):\n",
    "                    assert \"So the answer is\" in prompt_field, f\"`So the answer is` not found in prompt field of {task_name}.txt.\"\n",
    "                    assert \"\\nA:\" in prompt_field, \"`\\nA:` not found in prompt field.\"\n",
    "                    answer = prompt_field.split(\"So the answer is\")[-1].strip()\n",
    "                    question = prompt_field.split(\"\\nA:\")[0].strip()\n",
    "                    new_prompt_fields.append(question + \"\\nA: \" + answer)\n",
    "                else:\n",
    "                    new_prompt_fields.append(prompt_field)\n",
    "            task_prompt = \"\\n\\n\".join(new_prompt_fields)\n",
    "        all_prompts[task_name] = task_prompt\n",
    "\n",
    "assert set(all_tasks.keys()) == set(all_prompts.keys()), \"task names in task data and task prompts are not the same.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e9f3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A logical deduction task which requires deducing the order of a sequence of objects.\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
      "Options:\n",
      "(A) Amy finished last\n",
      "(B) Eli finished last\n",
      "(C) Eve finished last\n",
      "A: Let's think step by step.\n",
      "(1) Eve finished above Amy: \"(above) ? Eve ? Amy ? (below)\".\n",
      "(2) Eli finished below Amy: \"(above) ? Amy ? Eli ? (below)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(above) Eve Amy Eli (below)\".\n",
      "According to this ordering, the person who finished last (the one at the bottom of this list) is Eli.\n",
      "Eli finished last. So the answer is (B).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
      "Options:\n",
      "(A) The white book is the leftmost\n",
      "(B) The green book is the leftmost\n",
      "(C) The orange book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The green book is to the right of the white book: \"(left) ? white ? green ? (right)\".\n",
      "(2) The orange book is the rightmost: \"(left) ? white ? green orange (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white green orange (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (A).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
      "Options:\n",
      "(A) The red book is the leftmost\n",
      "(B) The gray book is the leftmost\n",
      "(C) The white book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The white book is to the left of the gray book: \"(left) ? white ? gray ? (right)\".\n",
      "(2) The red book is the second from the left: \"(left) ? white red gray ? (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white red gray (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (C).\n"
     ]
    }
   ],
   "source": [
    "task_name = 'logical_deduction_seven_objects'\n",
    "print(all_prompts[task_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d97174d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc7e2f636054d5f9a612f27a8cb3422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.save_dir, \"predictions\"), exist_ok=True)\n",
    "\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        model_name_or_path=args.model_name_or_path, \n",
    "        tokenizer_name_or_path=args.tokenizer_name_or_path, \n",
    "        load_in_8bit=args.load_in_8bit, \n",
    "        gptq_model=args.gptq,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    \n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c615bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt lengths:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('date_understanding', 630),\n",
       " ('logical_deduction_five_objects', 703),\n",
       " ('movie_recommendation', 716),\n",
       " ('multistep_arithmetic_two', 986),\n",
       " ('object_counting', 535),\n",
       " ('word_sorting', 813),\n",
       " ('hyperbaton', 990),\n",
       " ('sports_understanding', 227),\n",
       " ('boolean_expressions', 519),\n",
       " ('tracking_shuffled_objects_seven_objects', 803),\n",
       " ('ruin_names', 1076),\n",
       " ('tracking_shuffled_objects_three_objects', 803),\n",
       " ('causal_judgement', 867),\n",
       " ('reasoning_about_colored_objects', 796),\n",
       " ('logical_deduction_seven_objects', 703),\n",
       " ('temporal_sequences', 1082),\n",
       " ('salient_translation_error_detection', 1488),\n",
       " ('tracking_shuffled_objects_five_objects', 803),\n",
       " ('geometric_shapes', 2329),\n",
       " ('disambiguation_qa', 969),\n",
       " ('dyck_languages', 946),\n",
       " ('navigate', 750),\n",
       " ('formal_fallacies', 1342),\n",
       " ('web_of_lies', 851),\n",
       " ('snarks', 808),\n",
       " ('penguins_in_a_table', 846),\n",
       " ('logical_deduction_three_objects', 703)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get an idea of prompt length for the tasks.\n",
    "# - no   cot: prompts are pretty short!\n",
    "# - with cot: some prompt >2k sequence length, potentially a problem.\n",
    "\n",
    "print('prompt lengths:')\n",
    "[(k, tokenizer(x, return_tensors='pt').input_ids.shape[1]) \n",
    "       for k, x in all_prompts.items()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0e6d670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A logical deduction task which requires deducing the order of a sequence of objects.\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
      "Options:\n",
      "(A) Amy finished last\n",
      "(B) Eli finished last\n",
      "(C) Eve finished last\n",
      "A: Let's think step by step.\n",
      "(1) Eve finished above Amy: \"(above) ? Eve ? Amy ? (below)\".\n",
      "(2) Eli finished below Amy: \"(above) ? Amy ? Eli ? (below)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(above) Eve Amy Eli (below)\".\n",
      "According to this ordering, the person who finished last (the one at the bottom of this list) is Eli.\n",
      "Eli finished last. So the answer is (B).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
      "Options:\n",
      "(A) The white book is the leftmost\n",
      "(B) The green book is the leftmost\n",
      "(C) The orange book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The green book is to the right of the white book: \"(left) ? white ? green ? (right)\".\n",
      "(2) The orange book is the rightmost: \"(left) ? white ? green orange (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white green orange (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (A).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
      "Options:\n",
      "(A) The red book is the leftmost\n",
      "(B) The gray book is the leftmost\n",
      "(C) The white book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The white book is to the left of the gray book: \"(left) ? white ? gray ? (right)\".\n",
      "(2) The red book is the second from the left: \"(left) ? white red gray ? (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white red gray (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (C).\n",
      "\n",
      "{'input': 'The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are seven birds: a hawk, a raven, a hummingbird, a falcon, an owl, a quail, and a cardinal. The quail is to the left of the hummingbird. The raven is to the right of the hummingbird. The cardinal is the second from the left. The owl is to the left of the cardinal. The raven is the third from the right. The falcon is the rightmost.\\nOptions:\\n(A) The hawk is the leftmost\\n(B) The raven is the leftmost\\n(C) The hummingbird is the leftmost\\n(D) The falcon is the leftmost\\n(E) The owl is the leftmost\\n(F) The quail is the leftmost\\n(G) The cardinal is the leftmost', 'target': '(E)'}\n"
     ]
    }
   ],
   "source": [
    "task_name = next(iter(all_tasks.keys()))\n",
    "task_examples = all_tasks[task_name]\n",
    "prompt = all_prompts[task_name]\n",
    "print(prompt)\n",
    "print()\n",
    "print(task_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88598731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_perf = eval_hf_model(\n",
    "#     args, \n",
    "#     model, \n",
    "#     tokenizer, \n",
    "#     task_examples, \n",
    "#     prompt, \n",
    "#     save_path=os.path.join(args.save_dir, \"predictions\", f\"{task_name}.jsonl\")\n",
    "# )\n",
    "\n",
    "# performance[task_name] = task_perf\n",
    "# print(f\"Task {task_name} - EM: {task_perf}\")\n",
    "\n",
    "examples = task_examples\n",
    "task_prompt = prompt\n",
    "save_path=os.path.join(args.save_dir, \"predictions\", f\"{task_name}.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b199f3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/huggyllama:llama-7b_human_mix-trainer_savebystep/checkpoint-200/eval/bbh/predictions/logical_deduction_seven_objects.jsonl'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = [example[\"target\"] for example in examples]\n",
    "if save_path:\n",
    "    fout = open(save_path, \"w\")\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71851bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A logical deduction task which requires deducing the order of a sequence of objects.\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
      "Options:\n",
      "(A) Amy finished last\n",
      "(B) Eli finished last\n",
      "(C) Eve finished last\n",
      "A: Let's think step by step.\n",
      "(1) Eve finished above Amy: \"(above) ? Eve ? Amy ? (below)\".\n",
      "(2) Eli finished below Amy: \"(above) ? Amy ? Eli ? (below)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(above) Eve Amy Eli (below)\".\n",
      "According to this ordering, the person who finished last (the one at the bottom of this list) is Eli.\n",
      "Eli finished last. So the answer is (B).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
      "Options:\n",
      "(A) The white book is the leftmost\n",
      "(B) The green book is the leftmost\n",
      "(C) The orange book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The green book is to the right of the white book: \"(left) ? white ? green ? (right)\".\n",
      "(2) The orange book is the rightmost: \"(left) ? white ? green orange (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white green orange (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (A).\n",
      "\n",
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
      "Options:\n",
      "(A) The red book is the leftmost\n",
      "(B) The gray book is the leftmost\n",
      "(C) The white book is the leftmost\n",
      "A: Let's think step by step.\n",
      "(1) The white book is to the left of the gray book: \"(left) ? white ? gray ? (right)\".\n",
      "(2) The red book is the second from the left: \"(left) ? white red gray ? (right)\".\n",
      "(3) Combining (1) and (2) we get the following ordering: \"(left) white red gray (right)\".\n",
      "According to this ordering, the leftmost book is the white book.\n",
      "The white book is the leftmost. So the answer is (C).\n",
      "\n",
      "Q: The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are seven birds: a hawk, a raven, a hummingbird, a falcon, an owl, a quail, and a cardinal. The quail is to the left of the hummingbird. The raven is to the right of the hummingbird. The cardinal is the second from the left. The owl is to the left of the cardinal. The raven is the third from the right. The falcon is the rightmost.\n",
      "Options:\n",
      "(A) The hawk is the leftmost\n",
      "(B) The raven is the leftmost\n",
      "(C) The hummingbird is the leftmost\n",
      "(D) The falcon is the leftmost\n",
      "(E) The owl is the leftmost\n",
      "(F) The quail is the leftmost\n",
      "(G) The cardinal is the leftmost\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = []\n",
    "for example in examples:\n",
    "    if args.use_chat_format:\n",
    "        prompt = \"<|user|>\\n\" + task_prompt.strip() + \"\\n\\nQ: \" + example[\"input\"] + \"\\n<|assistant|>\\nA:\"\n",
    "    else:\n",
    "        prompt = task_prompt.strip() + \"\\n\\nQ: \" + example[\"input\"] + \"\\nA:\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c39e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [259]\n",
      "\n",
      " [29871, 13]\n",
      "\n",
      "\n",
      " [29871, 13, 13]\n",
      "A: (B)\n",
      "\n",
      " [319, 29901, 313, 29933, 29897, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "for s in [' ', '\\n', '\\n\\n', 'A: (B)\\n\\n']:\n",
    "    print(s, tokenizer.encode(s, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e589ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '\\n\\n', '\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for s in [[13], [13,13], [29871,13]]:\n",
    "    l.append(tokenizer.decode(s, add_special_tokens=False))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9647a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Completions: 100%|██████████| 40/40 [03:45<00:00,  5.64s/it]\n"
     ]
    }
   ],
   "source": [
    "if args.no_cot:\n",
    "    # get the last token because the tokenizer may add space tokens at the start.\n",
    "    stop_id_sequences = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    stop_id_sequences = [stop_id_sequences[-2:]] if stop_id_sequences else None\n",
    "else:\n",
    "    # let's not use the stop sequence for cot now since it's too inefficient when the generation is long. \n",
    "    # instead, we'll do some post-processing to extract the answer.\n",
    "    stop_sequnce = None\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "if isinstance(model, GPT2LMHeadModel):\n",
    "    # wpq: for gpt-2 model, need to enforce `max_length` constraints to avoid `position_id` index errors.\n",
    "    generation_kwargs = {'max_length': model.config.max_position_embeddings} # 1024\n",
    "else:\n",
    "    # wpq: modify `max_new_tokens=512` to `128` for faster generation.\n",
    "    # for non-cot multiple choice answers, e.g., ' (G).' requires just 5 tokens\n",
    "    generation_kwargs = {'max_new_tokens': 10 if args.no_cot else 256}\n",
    "\n",
    "outputs = generate_completions(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=prompts,\n",
    "    batch_size=args.eval_batch_size if args.eval_batch_size else 1,\n",
    "    stop_id_sequences=stop_id_sequences,\n",
    "    **generation_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb2049b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29871, 313, 29954, 467]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f00d1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[0;32m---> 21\u001b[0m         \u001b[43mfout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(targets), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of predictions and targets are not the same.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m task_perf \u001b[38;5;241m=\u001b[39m exact_match\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mtargets, ignore_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_punctuation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact_match\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = []\n",
    "for example, output in zip(examples, outputs):\n",
    "    example[\"raw_output\"] = output\n",
    "\n",
    "    # only keep the first part of the output - this is mainly for vanilla language models.\n",
    "    output = output.strip().split(\"\\n\\n\")[0].strip()\n",
    "\n",
    "    # extract the first answer after `So the answer is` and before the next period.\n",
    "    # if there is no such answer, we will just use the raw output.\n",
    "    results = re.search(r\"So the answer is (.*?)\\.\", output)\n",
    "    if results:\n",
    "        prediction = results.group(1).strip()\n",
    "    else:\n",
    "        prediction = output.strip()\n",
    "\n",
    "    example[\"prediction\"] = prediction\n",
    "    predictions.append(prediction)\n",
    "    if save_path:\n",
    "        fout.write(json.dumps(example) + \"\\n\")        \n",
    "\n",
    "assert len(predictions) == len(targets), \"number of predictions and targets are not the same.\"\n",
    "task_perf = exact_match.compute(predictions=predictions, references=targets, ignore_case=True, ignore_punctuation=True)[\"exact_match\"]\n",
    "task_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8fb5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "performance[task_name] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a75208e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average EM: 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(os.path.join(args.save_dir, \"metrics.json\"), \"w\") as fout:\n",
    "    performance[\"average_exact_match\"] = sum(performance.values()) / len(performance)\n",
    "    print(f\"Average EM: {performance['average_exact_match']}\")\n",
    "    json.dump(performance, fout, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-instruct]",
   "language": "python",
   "name": "conda-env-open-instruct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
